,index,code,prompt,bad_prompt,bad_code,prompt_names_dict,numerical_prompt,numerical_code,prompt_numerical_dict
0,0,"def cellname(x, y):
    ""Translate a cell coordinate to a fancy cell name (e.g. (1, 1)->'A1').""
    assert x > 0 # Column 0 has an empty name, so can't use that
    return colnum2name(x) + str(y)","def str(val):
    """"""Convert float to string, taking the locale into account.""""""
    return _format(""%.12g"", val)

def colnum2name(n):
    ""Translate a column number to name (e.g. 1->'A', etc.).""
    assert n > 0
    s = """"
    while n:
        n, m = divmod(n-1, 26)
        s = chr(m+ord('A')) + s
    return s","def locale_aware_float_to_string(val):
    """"""Convert float to locale_aware_float_to_stringing, taking the locale into account.""""""
    return _format(""%.12g"", val)

def convert_column_number_to_name(n):
    ""Translate a column number to name (e.g. 1->'A', etc.).""
    assert n > 0
    s = """"
    while n:
        n, m = divmod(n-1, 26)
        s = chr(m+ord('A')) + s
    return s","def generate_cell_name(x, y):
    ""Translate a cell coordinate to a fancy cell name (e.g. (1, 1)->'A1').""
    assert x > 0 # Column 0 has an empty name, so can't use that
    return convert_column_number_to_name(x) + locale_aware_float_to_string(y)","{""str"": ""locale_aware_float_to_string"", ""colnum2name"": ""convert_column_number_to_name""}","def 0(val):
    """"""Convert float to string, taking the locale into account.""""""
    return _format(""%.12g"", val)

def 1(n):
    ""Translate a column number to name (e.g. 1->'A', etc.).""
    assert n > 0
    s = """"
    while n:
        n, m = divmod(n-1, 26)
        s = chr(m+ord('A')) + s
    return s","def 2(x, y):
    ""Translate a cell coordinate to a fancy cell name (e.g. (1, 1)->'A1').""
    assert x > 0 # Column 0 has an empty name, so can't use that
    return 1(x) + 0(y)","{""str"": ""0"", ""colnum2name"": ""1""}"
1,1,"def test_basic():
    ""Basic non-gui self-test.""
    a = Sheet()
    for x in range(1, 11):
        for y in range(1, 11):
            if x == 1:
                cell = NumericCell(y)
            elif y == 1:
                cell = NumericCell(x)
            else:
                c1 = cellname(x, 1)
                c2 = cellname(1, y)
                formula = ""%s*%s"" % (c1, c2)
                cell = FormulaCell(formula)
            a.setcell(x, y, cell)
##    if os.path.isfile(""sheet1.xml""):
##        print ""Loading from sheet1.xml""
##        a.load(""sheet1.xml"")
    a.display()
    a.save(""sheet1.xml"")","def cellname(x, y):
    ""Translate a cell coordinate to a fancy cell name (e.g. (1, 1)->'A1').""
    assert x > 0 # Column 0 has an empty name, so can't use that
    return colnum2name(x) + str(y)","def generate_cell_name(x, y):
    ""Translate a cell coordinate to a fancy cell name (e.g. (1, 1)->'A1').""
    assert x > 0 # Column 0 has an empty name, so can't use that
    return colnum2name(x) + str(y)","def perform_basic_non_gui_self_test():
    ""Basic non-gui self-test.""
    a = Sheet()
    for x in range(1, 11):
        for y in range(1, 11):
            if x == 1:
                cell = NumericCell(y)
            elif y == 1:
                cell = NumericCell(x)
            else:
                c1 = generate_cell_name(x, 1)
                c2 = generate_cell_name(1, y)
                formula = ""%s*%s"" % (c1, c2)
                cell = FormulaCell(formula)
            a.setcell(x, y, cell)
##    if os.path.isfile(""sheet1.xml""):
##        print ""Loading from sheet1.xml""
##        a.load(""sheet1.xml"")
    a.display()
    a.save(""sheet1.xml"")","{""cellname"": ""generate_cell_name""}","def 0(x, y):
    ""Translate a cell coordinate to a fancy cell name (e.g. (1, 1)->'A1').""
    assert x > 0 # Column 0 has an empty name, so can't use that
    return colnum2name(x) + str(y)","def 1():
    ""Basic non-gui self-test.""
    a = Sheet()
    for x in range(1, 11):
        for y in range(1, 11):
            if x == 1:
                cell = NumericCell(y)
            elif y == 1:
                cell = NumericCell(x)
            else:
                c1 = 0(x, 1)
                c2 = 0(1, y)
                formula = ""%s*%s"" % (c1, c2)
                cell = FormulaCell(formula)
            a.setcell(x, y, cell)
##    if os.path.isfile(""sheet1.xml""):
##        print ""Loading from sheet1.xml""
##        a.load(""sheet1.xml"")
    a.display()
    a.save(""sheet1.xml"")","{""cellname"": ""0""}"
2,2,"def display_menu(stdscr, menu_y):
    ""Display the menu of possible keystroke commands""
    erase_menu(stdscr, menu_y)

    # If color, then light the menu up :-)
    if curses.has_colors():
        stdscr.attrset(curses.color_pair(1))
    stdscr.addstr(menu_y, 4,
        'Use the cursor keys to move, and space or Enter to toggle a cell.')
    stdscr.addstr(menu_y + 1, 4,
        'E)rase the board, R)andom fill, S)tep once or C)ontinuously, Q)uit')
    stdscr.attrset(0)","def erase_menu(stdscr, menu_y):
    ""Clear the space where the menu resides""
    stdscr.move(menu_y, 0)
    stdscr.clrtoeol()
    stdscr.move(menu_y + 1, 0)
    stdscr.clrtoeol()","def erase_menu_clears_menu_space(stdscr, menu_y):
    ""Clear the space where the menu resides""
    stdscr.move(menu_y, 0)
    stdscr.clrtoeol()
    stdscr.move(menu_y + 1, 0)
    stdscr.clrtoeol()","def display_menu_commands_and_instructions(stdscr, menu_y):
    ""Display the menu of possible keystroke commands""
    erase_menu_clears_menu_space(stdscr, menu_y)

    # If color, then light the menu up :-)
    if curses.has_colors():
        stdscr.attrset(curses.color_pair(1))
    stdscr.addstr(menu_y, 4,
        'Use the cursor keys to move, and space or Enter to toggle a cell.')
    stdscr.addstr(menu_y + 1, 4,
        'E)rase the board, R)andom fill, S)tep once or C)ontinuously, Q)uit')
    stdscr.attrset(0)","{""erase_menu"": ""erase_menu_clears_menu_space""}","def 0(stdscr, menu_y):
    ""Clear the space where the menu resides""
    stdscr.move(menu_y, 0)
    stdscr.clrtoeol()
    stdscr.move(menu_y + 1, 0)
    stdscr.clrtoeol()","def 1(stdscr, menu_y):
    ""Display the menu of possible keystroke commands""
    0(stdscr, menu_y)

    # If color, then light the menu up :-)
    if curses.has_colors():
        stdscr.attrset(curses.color_pair(1))
    stdscr.addstr(menu_y, 4,
        'Use the cursor keys to move, and space or Enter to toggle a cell.')
    stdscr.addstr(menu_y + 1, 4,
        'E)rase the board, R)andom fill, S)tep once or C)ontinuously, Q)uit')
    stdscr.attrset(0)","{""erase_menu"": ""0""}"
3,3,"def merge_profile():
    """"""Reads sys.getdxp() and merges it into this module's cached copy.

    We need this because sys.getdxp() 0s itself every time it's called.""""""

    with _profile_lock:
        new_profile = sys.getdxp()
        if has_pairs(new_profile):
            for first_inst in range(len(_cumulative_profile)):
                for second_inst in range(len(_cumulative_profile[first_inst])):
                    _cumulative_profile[first_inst][second_inst] += (
                        new_profile[first_inst][second_inst])
        else:
            for inst in range(len(_cumulative_profile)):
                _cumulative_profile[inst] += new_profile[inst]","def has_pairs(profile):
    """"""Returns True if the Python that produced the argument profile
    was built with -DDXPAIRS.""""""

    return len(profile) > 0 and isinstance(profile[0], list)","def check_for_pairs(profile):
    """"""Returns True if the Python that produced the argument profile
    was built with -DDXPAIRS.""""""

    return len(profile) > 0 and isinstance(profile[0], list)","def merge_profile_into_module_cached_copy():
    """"""Reads sys.getdxp() and merges it into this module's cached copy.

    We need this because sys.getdxp() 0s itself every time it's called.""""""

    with _profile_lock:
        new_profile = sys.getdxp()
        if check_for_pairs(new_profile):
            for first_inst in range(len(_cumulative_profile)):
                for second_inst in range(len(_cumulative_profile[first_inst])):
                    _cumulative_profile[first_inst][second_inst] += (
                        new_profile[first_inst][second_inst])
        else:
            for inst in range(len(_cumulative_profile)):
                _cumulative_profile[inst] += new_profile[inst]","{""has_pairs"": ""check_for_pairs""}","def 0(profile):
    """"""Returns True if the Python that produced the argument profile
    was built with -DDXPAIRS.""""""

    return len(profile) > 0 and isinstance(profile[0], list)","def 1():
    """"""Reads sys.getdxp() and merges it into this module's cached copy.

    We need this because sys.getdxp() 0s itself every time it's called.""""""

    with _profile_lock:
        new_profile = sys.getdxp()
        if 0(new_profile):
            for first_inst in range(len(_cumulative_profile)):
                for second_inst in range(len(_cumulative_profile[first_inst])):
                    _cumulative_profile[first_inst][second_inst] += (
                        new_profile[first_inst][second_inst])
        else:
            for inst in range(len(_cumulative_profile)):
                _cumulative_profile[inst] += new_profile[inst]","{""has_pairs"": ""0""}"
4,4,"def snapshot_profile():
    """"""Returns the cumulative execution profile until this call.""""""
    with _profile_lock:
        merge_profile()
        return copy.deepcopy(_cumulative_profile)","def merge_profile():
    """"""Reads sys.getdxp() and merges it into this module's cached copy.

    We need this because sys.getdxp() 0s itself every time it's called.""""""

    with _profile_lock:
        new_profile = sys.getdxp()
        if has_pairs(new_profile):
            for first_inst in range(len(_cumulative_profile)):
                for second_inst in range(len(_cumulative_profile[first_inst])):
                    _cumulative_profile[first_inst][second_inst] += (
                        new_profile[first_inst][second_inst])
        else:
            for inst in range(len(_cumulative_profile)):
                _cumulative_profile[inst] += new_profile[inst]","def merge_profile_into_module_cached_copy():
    """"""Reads sys.getdxp() and merges it into this module's cached copy.

    We need this because sys.getdxp() 0s itself every time it's called.""""""

    with _profile_lock:
        new_profile = sys.getdxp()
        if has_pairs(new_profile):
            for first_inst in range(len(_cumulative_profile)):
                for second_inst in range(len(_cumulative_profile[first_inst])):
                    _cumulative_profile[first_inst][second_inst] += (
                        new_profile[first_inst][second_inst])
        else:
            for inst in range(len(_cumulative_profile)):
                _cumulative_profile[inst] += new_profile[inst]","def get_cumulative_execution_profile():
    """"""Returns the cumulative execution profile until this call.""""""
    with _profile_lock:
        merge_profile_into_module_cached_copy()
        return copy.deepcopy(_cumulative_profile)","{""merge_profile"": ""merge_profile_into_module_cached_copy""}","def 0():
    """"""Reads sys.getdxp() and merges it into this module's cached copy.

    We need this because sys.getdxp() 0s itself every time it's called.""""""

    with _profile_lock:
        new_profile = sys.getdxp()
        if has_pairs(new_profile):
            for first_inst in range(len(_cumulative_profile)):
                for second_inst in range(len(_cumulative_profile[first_inst])):
                    _cumulative_profile[first_inst][second_inst] += (
                        new_profile[first_inst][second_inst])
        else:
            for inst in range(len(_cumulative_profile)):
                _cumulative_profile[inst] += new_profile[inst]","def 1():
    """"""Returns the cumulative execution profile until this call.""""""
    with _profile_lock:
        0()
        return copy.deepcopy(_cumulative_profile)","{""merge_profile"": ""0""}"
5,5,"def common_instructions(profile):
    """"""Returns the most common opcodes in order of descending frequency.

    The result is a list of tuples of the form
      (opcode, opname, # of occurrences)

    """"""
    if has_pairs(profile) and profile:
        inst_list = profile[-1]
    else:
        inst_list = profile
    result = [(op, opcode.opname[op], count)
              for op, count in enumerate(inst_list)
              if count > 0]
    result.sort(key=operator.itemgetter(2), reverse=True)
    return result","def has_pairs(profile):
    """"""Returns True if the Python that produced the argument profile
    was built with -DDXPAIRS.""""""

    return len(profile) > 0 and isinstance(profile[0], list)

def enumerate():
    """"""Return a list of all Thread objects currently alive.

    The list includes daemonic threads, dummy thread objects created by
    current_thread(), and the main thread. It excludes terminated threads and
    threads that have not yet been started.

    """"""
    with _active_limbo_lock:
        return list(_active.values()) + list(_limbo.values())","def check_for_pairs(profile):
    """"""Returns True if the Python that produced the argument profile
    was built with -DDXPAIRS.""""""

    return len(profile) > 0 and isinstance(profile[0], list)

def get_alive_threads():
    """"""Return a list of all Thread objects currently alive.

    The list includes daemonic threads, dummy thread objects created by
    current_thread(), and the main thread. It excludes terminated threads and
    threads that have not yet been started.

    """"""
    with _active_limbo_lock:
        return list(_active.values()) + list(_limbo.values())","def get_most_common_opcodes(profile):
    """"""Returns the most common opcodes in order of descending frequency.

    The result is a list of tuples of the form
      (opcode, opname, # of occurrences)

    """"""
    if check_for_pairs(profile) and profile:
        inst_list = profile[-1]
    else:
        inst_list = profile
    result = [(op, opcode.opname[op], count)
              for op, count in get_alive_threads(inst_list)
              if count > 0]
    result.sort(key=operator.itemgetter(2), reverse=True)
    return result","{""has_pairs"": ""check_for_pairs"", ""enumerate"": ""get_alive_threads""}","def 0(profile):
    """"""Returns True if the Python that produced the argument profile
    was built with -DDXPAIRS.""""""

    return len(profile) > 0 and isinstance(profile[0], list)

def 1():
    """"""Return a list of all Thread objects currently alive.

    The list includes daemonic threads, dummy thread objects created by
    current_thread(), and the main thread. It excludes terminated threads and
    threads that have not yet been started.

    """"""
    with _active_limbo_lock:
        return list(_active.values()) + list(_limbo.values())","def 2(profile):
    """"""Returns the most common opcodes in order of descending frequency.

    The result is a list of tuples of the form
      (opcode, opname, # of occurrences)

    """"""
    if 0(profile) and profile:
        inst_list = profile[-1]
    else:
        inst_list = profile
    result = [(op, opcode.opname[op], count)
              for op, count in 1(inst_list)
              if count > 0]
    result.sort(key=operator.itemgetter(2), reverse=True)
    return result","{""has_pairs"": ""0"", ""enumerate"": ""1""}"
6,6,"def common_pairs(profile):
    """"""Returns the most common opcode pairs in order of descending frequency.

    The result is a list of tuples of the form
      ((1st opcode, 2nd opcode),
       (1st opname, 2nd opname),
       # of occurrences of the pair)

    """"""
    if not has_pairs(profile):
        return []
    result = [((op1, op2), (opcode.opname[op1], opcode.opname[op2]), count)
              # Drop the row of single-op profiles with [:-1]
              for op1, op1profile in enumerate(profile[:-1])
              for op2, count in enumerate(op1profile)
              if count > 0]
    result.sort(key=operator.itemgetter(2), reverse=True)
    return result","def has_pairs(profile):
    """"""Returns True if the Python that produced the argument profile
    was built with -DDXPAIRS.""""""

    return len(profile) > 0 and isinstance(profile[0], list)

def enumerate():
    """"""Return a list of all Thread objects currently alive.

    The list includes daemonic threads, dummy thread objects created by
    current_thread(), and the main thread. It excludes terminated threads and
    threads that have not yet been started.

    """"""
    with _active_limbo_lock:
        return list(_active.values()) + list(_limbo.values())","def check_for_pairs(profile):
    """"""Returns True if the Python that produced the argument profile
    was built with -DDXPAIRS.""""""

    return len(profile) > 0 and isinstance(profile[0], list)

def get_alive_threads():
    """"""Return a list of all Thread objects currently alive.

    The list includes daemonic threads, dummy thread objects created by
    current_thread(), and the main thread. It excludes terminated threads and
    threads that have not yet been started.

    """"""
    with _active_limbo_lock:
        return list(_active.values()) + list(_limbo.values())","def find_most_common_opcode_pairs(profile):
    """"""Returns the most common opcode pairs in order of descending frequency.

    The result is a list of tuples of the form
      ((1st opcode, 2nd opcode),
       (1st opname, 2nd opname),
       # of occurrences of the pair)

    """"""
    if not check_for_pairs(profile):
        return []
    result = [((op1, op2), (opcode.opname[op1], opcode.opname[op2]), count)
              # Drop the row of single-op profiles with [:-1]
              for op1, op1profile in get_alive_threads(profile[:-1])
              for op2, count in get_alive_threads(op1profile)
              if count > 0]
    result.sort(key=operator.itemgetter(2), reverse=True)
    return result","{""has_pairs"": ""check_for_pairs"", ""enumerate"": ""get_alive_threads""}","def 0(profile):
    """"""Returns True if the Python that produced the argument profile
    was built with -DDXPAIRS.""""""

    return len(profile) > 0 and isinstance(profile[0], list)

def 1():
    """"""Return a list of all Thread objects currently alive.

    The list includes daemonic threads, dummy thread objects created by
    current_thread(), and the main thread. It excludes terminated threads and
    threads that have not yet been started.

    """"""
    with _active_limbo_lock:
        return list(_active.values()) + list(_limbo.values())","def 2(profile):
    """"""Returns the most common opcode pairs in order of descending frequency.

    The result is a list of tuples of the form
      ((1st opcode, 2nd opcode),
       (1st opname, 2nd opname),
       # of occurrences of the pair)

    """"""
    if not 0(profile):
        return []
    result = [((op1, op2), (opcode.opname[op1], opcode.opname[op2]), count)
              # Drop the row of single-op profiles with [:-1]
              for op1, op1profile in 1(profile[:-1])
              for op2, count in 1(op1profile)
              if count > 0]
    result.sort(key=operator.itemgetter(2), reverse=True)
    return result","{""has_pairs"": ""0"", ""enumerate"": ""1""}"
7,7,"def render_common_pairs(profile=None):
    """"""Renders the most common opcode pairs to a string in order of
    descending frequency.

    The result is a series of lines of the form:
      # of occurrences: ('1st opname', '2nd opname')

    """"""
    if profile is None:
        profile = snapshot_profile()
    def seq():
        for _, ops, count in common_pairs(profile):
            yield ""%s: %s\n"" % (count, ops)
    return ''.join(seq())","def common_pairs(profile):
    """"""Returns the most common opcode pairs in order of descending frequency.

    The result is a list of tuples of the form
      ((1st opcode, 2nd opcode),
       (1st opname, 2nd opname),
       # of occurrences of the pair)

    """"""
    if not has_pairs(profile):
        return []
    result = [((op1, op2), (opcode.opname[op1], opcode.opname[op2]), count)
              # Drop the row of single-op profiles with [:-1]
              for op1, op1profile in enumerate(profile[:-1])
              for op2, count in enumerate(op1profile)
              if count > 0]
    result.sort(key=operator.itemgetter(2), reverse=True)
    return result

def snapshot_profile():
    """"""Returns the cumulative execution profile until this call.""""""
    with _profile_lock:
        merge_profile()
        return copy.deepcopy(_cumulative_profile)","def find_most_common_opcode_pairs(profile):
    """"""Returns the most common opcode pairs in order of descending frequency.

    The result is a list of tuples of the form
      ((1st opcode, 2nd opcode),
       (1st opname, 2nd opname),
       # of occurrences of the pair)

    """"""
    if not has_pairs(profile):
        return []
    result = [((op1, op2), (opcode.opname[op1], opcode.opname[op2]), count)
              # Drop the row of single-op profiles with [:-1]
              for op1, op1profile in enumerate(profile[:-1])
              for op2, count in enumerate(op1profile)
              if count > 0]
    result.sort(key=operator.itemgetter(2), reverse=True)
    return result

def get_cumulative_execution_profile():
    """"""Returns the cumulative execution profile until this call.""""""
    with _profile_lock:
        merge_profile()
        return copy.deepcopy(_cumulative_profile)","def render_find_most_common_opcode_pairs(profile=None):
    """"""Renders the most common opcode pairs to a string in order of
    descending frequency.

    The result is a series of lines of the form:
      # of occurrences: ('1st opname', '2nd opname')

    """"""
    if profile is None:
        profile = get_cumulative_execution_profile()
    def generate_sequence_of_opcode_pairs():
        for _, ops, count in find_most_common_opcode_pairs(profile):
            yield ""%s: %s\n"" % (count, ops)
    return ''.join(generate_sequence_of_opcode_pairs())","{""common_pairs"": ""find_most_common_opcode_pairs"", ""snapshot_profile"": ""get_cumulative_execution_profile""}","def 0(profile):
    """"""Returns the most common opcode pairs in order of descending frequency.

    The result is a list of tuples of the form
      ((1st opcode, 2nd opcode),
       (1st opname, 2nd opname),
       # of occurrences of the pair)

    """"""
    if not has_pairs(profile):
        return []
    result = [((op1, op2), (opcode.opname[op1], opcode.opname[op2]), count)
              # Drop the row of single-op profiles with [:-1]
              for op1, op1profile in enumerate(profile[:-1])
              for op2, count in enumerate(op1profile)
              if count > 0]
    result.sort(key=operator.itemgetter(2), reverse=True)
    return result

def 1():
    """"""Returns the cumulative execution profile until this call.""""""
    with _profile_lock:
        merge_profile()
        return copy.deepcopy(_cumulative_profile)","def render_0(profile=None):
    """"""Renders the most common opcode pairs to a string in order of
    descending frequency.

    The result is a series of lines of the form:
      # of occurrences: ('1st opname', '2nd opname')

    """"""
    if profile is None:
        profile = 1()
    def 3():
        for _, ops, count in 0(profile):
            yield ""%s: %s\n"" % (count, ops)
    return ''.join(3())","{""common_pairs"": ""0"", ""snapshot_profile"": ""1""}"
8,8,"def build_html_page(classified_text, title='python',
                    css=default_css, html=default_html):
    'Create a complete HTML page with colorized source code'
    css_str = '\n'.join(['%s %s' % item for item in css.items()])
    result = html_highlight(classified_text)
    title = html_module.escape(title)
    return html.format(title=title, css=css_str, body=result)","def html_highlight(classified_text,opener='<pre class=""python"">\n', closer='</pre>\n'):
    'Convert classified text to an HTML fragment'
    result = [opener]
    for kind, text in classified_text:
        if kind:
            result.append('<span class=""%s"">' % kind)
        result.append(html_module.escape(text))
        if kind:
            result.append('</span>')
    result.append(closer)
    return ''.join(result)","def convert_classified_text_to_html(classified_text,opener='<pre class=""python"">\n', closer='</pre>\n'):
    'Convert classified text to an HTML fragment'
    result = [opener]
    for kind, text in classified_text:
        if kind:
            result.append('<span class=""%s"">' % kind)
        result.append(html_module.escape(text))
        if kind:
            result.append('</span>')
    result.append(closer)
    return ''.join(result)","def create_html_page_with_colorized_code(classified_text, title='python',
                    css=default_css, html=default_html):
    'Create a complete HTML page with colorized source code'
    css_str = '\n'.join(['%s %s' % item for item in css.items()])
    result = convert_classified_text_to_html(classified_text)
    title = html_module.escape(title)
    return html.format(title=title, css=css_str, body=result)","{""html_highlight"": ""convert_classified_text_to_html""}","def 0(classified_text,opener='<pre class=""python"">\n', closer='</pre>\n'):
    'Convert classified text to an HTML fragment'
    result = [opener]
    for kind, text in classified_text:
        if kind:
            result.append('<span class=""%s"">' % kind)
        result.append(html_module.escape(text))
        if kind:
            result.append('</span>')
    result.append(closer)
    return ''.join(result)","def 1(classified_text, title='python',
                    css=default_css, html=default_html):
    'Create a complete HTML page with colorized source code'
    css_str = '\n'.join(['%s %s' % item for item in css.items()])
    result = 0(classified_text)
    title = html_module.escape(title)
    return html.format(title=title, css=css_str, body=result)","{""html_highlight"": ""0""}"
9,9,"def latex_highlight(classified_text, title = 'python',
                    commands = default_latex_commands,
                    document = default_latex_document):
    'Create a complete LaTeX document with colorized source code'
    macros = '\n'.join(r'\newcommand{\py%s}[1]{%s}' % c for c in commands.items())
    result = []
    for kind, text in classified_text:
        if kind:
            result.append(r'\py%s{' % kind)
        result.append(alltt_escape(text))
        if kind:
            result.append('}')
    return default_latex_document % dict(title=title, macros=macros, body=''.join(result))","def alltt_escape(s):
    'Replace backslash and braces with their escaped equivalents'
    xlat = {'{': r'\{', '}': r'\}', '\\': r'\textbackslash{}'}
    return re.sub(r'[\\{}]', lambda mo: xlat[mo.group()], s)","def replace_backslash_and_braces_with_escaped_equivalents(s):
    'Replace backslash and braces with their escaped equivalents'
    xlat = {'{': r'\{', '}': r'\}', '\\': r'\textbackslash{}'}
    return re.sub(r'[\\{}]', lambda mo: xlat[mo.group()], s)","def create_latex_document_with_colorized_code(classified_text, title = 'python',
                    commands = default_latex_commands,
                    document = default_latex_document):
    'Create a complete LaTeX document with colorized source code'
    macros = '\n'.join(r'\newcommand{\py%s}[1]{%s}' % c for c in commands.items())
    result = []
    for kind, text in classified_text:
        if kind:
            result.append(r'\py%s{' % kind)
        result.append(replace_backslash_and_braces_with_escaped_equivalents(text))
        if kind:
            result.append('}')
    return default_latex_document % dict(title=title, macros=macros, body=''.join(result))","{""alltt_escape"": ""replace_backslash_and_braces_with_escaped_equivalents""}","def 0(s):
    'Replace backslash and braces with their escaped equivalents'
    xlat = {'{': r'\{', '}': r'\}', '\\': r'\textbackslash{}'}
    return re.sub(r'[\\{}]', lambda mo: xlat[mo.group()], s)","def 1(classified_text, title = 'python',
                    commands = default_latex_commands,
                    document = default_latex_document):
    'Create a complete LaTeX document with colorized source code'
    macros = '\n'.join(r'\newcommand{\py%s}[1]{%s}' % c for c in commands.items())
    result = []
    for kind, text in classified_text:
        if kind:
            result.append(r'\py%s{' % kind)
        result.append(0(text))
        if kind:
            result.append('}')
    return default_latex_document % dict(title=title, macros=macros, body=''.join(result))","{""alltt_escape"": ""0""}"
10,10,"def get_json(url):
    """"""Download the json file from the url and returns a decoded object.""""""
    with urlopen(url) as f:
        data = f.read().decode('utf-8')
    return json.loads(data)","def urlopen(url, data=None, timeout=socket._GLOBAL_DEFAULT_TIMEOUT,
            *, cafile=None, capath=None, cadefault=False, context=None):
    '''Open the URL url, which can be either a string or a Request object.

    *data* must be an object specifying additional data to be sent to
    the server, or None if no such data is needed.  See Request for
    details.

    urllib.request module uses HTTP/1.1 and includes a ""Connection:close""
    header in its HTTP requests.

    The optional *timeout* parameter specifies a timeout in seconds for
    blocking operations like the connection attempt (if not specified, the
    global default timeout setting will be used). This only works for HTTP,
    HTTPS and FTP connections.

    If *context* is specified, it must be a ssl.SSLContext instance describing
    the various SSL options. See HTTPSConnection for more details.

    The optional *cafile* and *capath* parameters specify a set of trusted CA
    certificates for HTTPS requests. cafile should point to a single file
    containing a bundle of CA certificates, whereas capath should point to a
    directory of hashed certificate files. More information can be found in
    ssl.SSLContext.load_verify_locations().

    The *cadefault* parameter is ignored.


    This function always returns an object which can work as a
    context manager and has the properties url, headers, and status.
    See urllib.response.addinfourl for more detail on these properties.

    For HTTP and HTTPS URLs, this function returns a http.client.HTTPResponse
    object slightly modified. In addition to the three new methods above, the
    msg attribute contains the same information as the reason attribute ---
    the reason phrase returned by the server --- instead of the response
    headers as it is specified in the documentation for HTTPResponse.

    For FTP, file, and data URLs and requests explicitly handled by legacy
    URLopener and FancyURLopener classes, this function returns a
    urllib.response.addinfourl object.

    Note that None may be returned if no handler handles the request (though
    the default installed global OpenerDirector uses UnknownHandler to ensure
    this never happens).

    In addition, if proxy settings are detected (for example, when a *_proxy
    environment variable like http_proxy is set), ProxyHandler is default
    installed and makes sure the requests are handled through the proxy.

    '''
    global _opener
    if cafile or capath or cadefault:
        import warnings
        warnings.warn(""cafile, capath and cadefault are deprecated, use a ""
                      ""custom context instead."", DeprecationWarning, 2)
        if context is not None:
            raise ValueError(
                ""You can't pass both context and any of cafile, capath, and ""
                ""cadefault""
            )
        if not _have_ssl:
            raise ValueError('SSL support not available')
        context = ssl.create_default_context(ssl.Purpose.SERVER_AUTH,
                                             cafile=cafile,
                                             capath=capath)
        # send ALPN extension to indicate HTTP/1.1 protocol
        context.set_alpn_protocols(['http/1.1'])
        https_handler = HTTPSHandler(context=context)
        opener = build_opener(https_handler)
    elif context:
        https_handler = HTTPSHandler(context=context)
        opener = build_opener(https_handler)
    elif _opener is None:
        _opener = opener = build_opener()
    else:
        opener = _opener
    return opener.open(url, data, timeout)","def urlopen_open_url_with_data_and_timeout(url, data=None, timeout=socket._GLOBAL_DEFAULT_TIMEOUT,
            *, cafile=None, capath=None, cadefault=False, context=None):
    '''Open the URL url, which can be either a string or a Request object.

    *data* must be an object specifying additional data to be sent to
    the server, or None if no such data is needed.  See Request for
    details.

    urllib.request module uses HTTP/1.1 and includes a ""Connection:close""
    header in its HTTP requests.

    The optional *timeout* parameter specifies a timeout in seconds for
    blocking operations like the connection attempt (if not specified, the
    global default timeout setting will be used). This only works for HTTP,
    HTTPS and FTP connections.

    If *context* is specified, it must be a ssl.SSLContext instance describing
    the various SSL options. See HTTPSConnection for more details.

    The optional *cafile* and *capath* parameters specify a set of trusted CA
    certificates for HTTPS requests. cafile should point to a single file
    containing a bundle of CA certificates, whereas capath should point to a
    directory of hashed certificate files. More information can be found in
    ssl.SSLContext.load_verify_locations().

    The *cadefault* parameter is ignored.


    This function always returns an object which can work as a
    context manager and has the properties url, headers, and status.
    See urllib.response.addinfourl for more detail on these properties.

    For HTTP and HTTPS URLs, this function returns a http.client.HTTPResponse
    object slightly modified. In addition to the three new methods above, the
    msg attribute contains the same information as the reason attribute ---
    the reason phrase returned by the server --- instead of the response
    headers as it is specified in the documentation for HTTPResponse.

    For FTP, file, and data URLs and requests explicitly handled by legacy
    URLopener and FancyURLopener classes, this function returns a
    urllib.response.addinfourl object.

    Note that None may be returned if no handler handles the request (though
    the default installed global OpenerDirector uses UnknownHandler to ensure
    this never happens).

    In addition, if proxy settings are detected (for example, when a *_proxy
    environment variable like http_proxy is set), ProxyHandler is default
    installed and makes sure the requests are handled through the proxy.

    '''
    global _opener
    if cafile or capath or cadefault:
        import warnings
        warnings.warn(""cafile, capath and cadefault are deprecated, use a ""
                      ""custom context instead."", DeprecationWarning, 2)
        if context is not None:
            raise ValueError(
                ""You can't pass both context and any of cafile, capath, and ""
                ""cadefault""
            )
        if not _have_ssl:
            raise ValueError('SSL support not available')
        context = ssl.create_default_context(ssl.Purpose.SERVER_AUTH,
                                             cafile=cafile,
                                             capath=capath)
        # send ALPN extension to indicate HTTP/1.1 protocol
        context.set_alpn_protocols(['http/1.1'])
        https_handler = HTTPSHandler(context=context)
        opener = build_opener(https_handler)
    elif context:
        https_handler = HTTPSHandler(context=context)
        opener = build_opener(https_handler)
    elif _opener is None:
        _opener = opener = build_opener()
    else:
        opener = _opener
    return opener.open(url, data, timeout)","def download_and_decode_json_from_url(url):
    """"""Download the json file from the url and returns a decoded object.""""""
    with urlopen_open_url_with_data_and_timeout(url) as f:
        data = f.read().decode('utf-8')
    return json.loads(data)","{""urlopen"": ""urlopen_open_url_with_data_and_timeout""}","def 0(url, data=None, timeout=socket._GLOBAL_DEFAULT_TIMEOUT,
            *, cafile=None, capath=None, cadefault=False, context=None):
    '''Open the URL url, which can be either a string or a Request object.

    *data* must be an object specifying additional data to be sent to
    the server, or None if no such data is needed.  See Request for
    details.

    urllib.request module uses HTTP/1.1 and includes a ""Connection:close""
    header in its HTTP requests.

    The optional *timeout* parameter specifies a timeout in seconds for
    blocking operations like the connection attempt (if not specified, the
    global default timeout setting will be used). This only works for HTTP,
    HTTPS and FTP connections.

    If *context* is specified, it must be a ssl.SSLContext instance describing
    the various SSL options. See HTTPSConnection for more details.

    The optional *cafile* and *capath* parameters specify a set of trusted CA
    certificates for HTTPS requests. cafile should point to a single file
    containing a bundle of CA certificates, whereas capath should point to a
    directory of hashed certificate files. More information can be found in
    ssl.SSLContext.load_verify_locations().

    The *cadefault* parameter is ignored.


    This function always returns an object which can work as a
    context manager and has the properties url, headers, and status.
    See urllib.response.addinfourl for more detail on these properties.

    For HTTP and HTTPS URLs, this function returns a http.client.HTTPResponse
    object slightly modified. In addition to the three new methods above, the
    msg attribute contains the same information as the reason attribute ---
    the reason phrase returned by the server --- instead of the response
    headers as it is specified in the documentation for HTTPResponse.

    For FTP, file, and data URLs and requests explicitly handled by legacy
    URLopener and FancyURLopener classes, this function returns a
    urllib.response.addinfourl object.

    Note that None may be returned if no handler handles the request (though
    the default installed global OpenerDirector uses UnknownHandler to ensure
    this never happens).

    In addition, if proxy settings are detected (for example, when a *_proxy
    environment variable like http_proxy is set), ProxyHandler is default
    installed and makes sure the requests are handled through the proxy.

    '''
    global _opener
    if cafile or capath or cadefault:
        import warnings
        warnings.warn(""cafile, capath and cadefault are deprecated, use a ""
                      ""custom context instead."", DeprecationWarning, 2)
        if context is not None:
            raise ValueError(
                ""You can't pass both context and any of cafile, capath, and ""
                ""cadefault""
            )
        if not _have_ssl:
            raise ValueError('SSL support not available')
        context = ssl.create_default_context(ssl.Purpose.SERVER_AUTH,
                                             cafile=cafile,
                                             capath=capath)
        # send ALPN extension to indicate HTTP/1.1 protocol
        context.set_alpn_protocols(['http/1.1'])
        https_handler = HTTPSHandler(context=context)
        opener = build_opener(https_handler)
    elif context:
        https_handler = HTTPSHandler(context=context)
        opener = build_opener(https_handler)
    elif _opener is None:
        _opener = opener = build_opener()
    else:
        opener = _opener
    return opener.open(url, data, timeout)","def 1(url):
    """"""Download the json file from the url and returns a decoded object.""""""
    with 0(url) as f:
        data = f.read().decode('utf-8')
    return json.loads(data)","{""urlopen"": ""0""}"
11,11,"def _sanitize_params(prefix, suffix, dir):
    """"""Common parameter processing for most APIs in this module.""""""
    output_type = _infer_return_type(prefix, suffix, dir)
    if suffix is None:
        suffix = output_type()
    if prefix is None:
        if output_type is str:
            prefix = template
        else:
            prefix = _os.fsencode(template)
    if dir is None:
        if output_type is str:
            dir = gettempdir()
        else:
            dir = gettempdirb()
    return prefix, suffix, dir, output_type","def gettempdirb():
    """"""Returns tempfile.tempdir as bytes.""""""
    return _os.fsencode(_gettempdir())

def _infer_return_type(*args):
    """"""Look at the type of all args and divine their implied return type.""""""
    return_type = None
    for arg in args:
        if arg is None:
            continue

        if isinstance(arg, _os.PathLike):
            arg = _os.fspath(arg)

        if isinstance(arg, bytes):
            if return_type is str:
                raise TypeError(""Can't mix bytes and non-bytes in ""
                                ""path components."")
            return_type = bytes
        else:
            if return_type is bytes:
                raise TypeError(""Can't mix bytes and non-bytes in ""
                                ""path components."")
            return_type = str
    if return_type is None:
        if tempdir is None or isinstance(tempdir, str):
            return str  # tempfile APIs return a str by default.
        else:
            # we could check for bytes but it'll fail later on anyway
            return bytes
    return return_type

def gettempdir():
    """"""Returns tempfile.tempdir as str.""""""
    return _os.fsdecode(_gettempdir())","def get_tempdir_as_bytes():
    """"""Returns tempfile.tempdir as bytes.""""""
    return _os.fsencode(_get_tempdir_as_string())

def infer_implied_return_type(*args):
    """"""Look at the type of all args and divine their implied return type.""""""
    return_type = None
    for arg in args:
        if arg is None:
            continue

        if isinstance(arg, _os.PathLike):
            arg = _os.fspath(arg)

        if isinstance(arg, bytes):
            if return_type is str:
                raise TypeError(""Can't mix bytes and non-bytes in ""
                                ""path components."")
            return_type = bytes
        else:
            if return_type is bytes:
                raise TypeError(""Can't mix bytes and non-bytes in ""
                                ""path components."")
            return_type = str
    if return_type is None:
        if tempdir is None or isinstance(tempdir, str):
            return str  # tempfile APIs return a str by default.
        else:
            # we could check for bytes but it'll fail later on anyway
            return bytes
    return return_type

def get_tempdir_as_string():
    """"""Returns tempfile.tempdir as str.""""""
    return _os.fsdecode(_get_tempdir_as_string())","def sanitize_and_infer_params_for_api(prefix, suffix, dir):
    """"""Common parameter processing for most APIs in this module.""""""
    output_type = infer_implied_return_type(prefix, suffix, dir)
    if suffix is None:
        suffix = output_type()
    if prefix is None:
        if output_type is str:
            prefix = template
        else:
            prefix = _os.fsencode(template)
    if dir is None:
        if output_type is str:
            dir = get_tempdir_as_string()
        else:
            dir = get_tempdir_as_bytes()
    return prefix, suffix, dir, output_type","{""gettempdirb"": ""get_tempdir_as_bytes"", ""_infer_return_type"": ""infer_implied_return_type"", ""gettempdir"": ""get_tempdir_as_string""}","def 0():
    """"""Returns tempfile.tempdir as bytes.""""""
    return _os.fsencode(_2())

def 1(*args):
    """"""Look at the type of all args and divine their implied return type.""""""
    return_type = None
    for arg in args:
        if arg is None:
            continue

        if isinstance(arg, _os.PathLike):
            arg = _os.fspath(arg)

        if isinstance(arg, bytes):
            if return_type is str:
                raise TypeError(""Can't mix bytes and non-bytes in ""
                                ""path components."")
            return_type = bytes
        else:
            if return_type is bytes:
                raise TypeError(""Can't mix bytes and non-bytes in ""
                                ""path components."")
            return_type = str
    if return_type is None:
        if tempdir is None or isinstance(tempdir, str):
            return str  # tempfile APIs return a str by default.
        else:
            # we could check for bytes but it'll fail later on anyway
            return bytes
    return return_type

def 2():
    """"""Returns tempfile.tempdir as str.""""""
    return _os.fsdecode(_2())","def 3(prefix, suffix, dir):
    """"""Common parameter processing for most APIs in this module.""""""
    output_type = 1(prefix, suffix, dir)
    if suffix is None:
        suffix = output_type()
    if prefix is None:
        if output_type is str:
            prefix = template
        else:
            prefix = _os.fsencode(template)
    if dir is None:
        if output_type is str:
            dir = 2()
        else:
            dir = 0()
    return prefix, suffix, dir, output_type","{""gettempdirb"": ""0"", ""_infer_return_type"": ""1"", ""gettempdir"": ""2""}"
12,12,"def _mkstemp_inner(dir, pre, suf, flags, output_type):
    """"""Code common to mkstemp, TemporaryFile, and NamedTemporaryFile.""""""

    names = _get_candidate_names()
    if output_type is bytes:
        names = map(_os.fsencode, names)

    for seq in range(TMP_MAX):
        name = next(names)
        file = _os.path.join(dir, pre + name + suf)
        _sys.audit(""tempfile.mkstemp"", file)
        try:
            fd = _os.open(file, flags, 0o600)
        except FileExistsError:
            continue    # try again
        except PermissionError:
            # This exception is thrown when a directory with the chosen name
            # already exists on windows.
            if (_os.name == 'nt' and _os.path.isdir(dir) and
                _os.access(dir, _os.W_OK)):
                continue
            else:
                raise
        return (fd, _os.path.abspath(file))

    raise FileExistsError(_errno.EEXIST,
                          ""No usable temporary file name found"")","def _get_candidate_names():
    """"""Common setup sequence for all user-callable interfaces.""""""

    global _name_sequence
    if _name_sequence is None:
        _once_lock.acquire()
        try:
            if _name_sequence is None:
                _name_sequence = _RandomNameSequence()
        finally:
            _once_lock.release()
    return _name_sequence","def get_candidate_names():
    """"""Common setup sequence for all user-callable interfaces.""""""

    global _name_sequence
    if _name_sequence is None:
        _once_lock.acquire()
        try:
            if _name_sequence is None:
                _name_sequence = _RandomNameSequence()
        finally:
            _once_lock.release()
    return _name_sequence","def create_temporary_file_and_return_fd(dir, pre, suf, flags, output_type):
    """"""Code common to mkstemp, TemporaryFile, and NamedTemporaryFile.""""""

    names = get_candidate_names()
    if output_type is bytes:
        names = map(_os.fsencode, names)

    for seq in range(TMP_MAX):
        name = next(names)
        file = _os.path.join(dir, pre + name + suf)
        _sys.audit(""tempfile.mkstemp"", file)
        try:
            fd = _os.open(file, flags, 0o600)
        except FileExistsError:
            continue    # try again
        except PermissionError:
            # This exception is thrown when a directory with the chosen name
            # already exists on windows.
            if (_os.name == 'nt' and _os.path.isdir(dir) and
                _os.access(dir, _os.W_OK)):
                continue
            else:
                raise
        return (fd, _os.path.abspath(file))

    raise FileExistsError(_errno.EEXIST,
                          ""No usable temporary file name found"")","{""_get_candidate_names"": ""get_candidate_names""}","def 0():
    """"""Common setup sequence for all user-callable interfaces.""""""

    global _name_sequence
    if _name_sequence is None:
        _once_lock.acquire()
        try:
            if _name_sequence is None:
                _name_sequence = _RandomNameSequence()
        finally:
            _once_lock.release()
    return _name_sequence","def 1(dir, pre, suf, flags, output_type):
    """"""Code common to mkstemp, TemporaryFile, and NamedTemporaryFile.""""""

    names = 0()
    if output_type is bytes:
        names = map(_os.fsencode, names)

    for seq in range(TMP_MAX):
        name = next(names)
        file = _os.path.join(dir, pre + name + suf)
        _sys.audit(""tempfile.mkstemp"", file)
        try:
            fd = _os.open(file, flags, 0o600)
        except FileExistsError:
            continue    # try again
        except PermissionError:
            # This exception is thrown when a directory with the chosen name
            # already exists on windows.
            if (_os.name == 'nt' and _os.path.isdir(dir) and
                _os.access(dir, _os.W_OK)):
                continue
            else:
                raise
        return (fd, _os.path.abspath(file))

    raise FileExistsError(_errno.EEXIST,
                          ""No usable temporary file name found"")","{""_get_candidate_names"": ""0""}"
13,13,"def _gettempdir():
    """"""Private accessor for tempfile.tempdir.""""""
    global tempdir
    if tempdir is None:
        _once_lock.acquire()
        try:
            if tempdir is None:
                tempdir = _get_default_tempdir()
        finally:
            _once_lock.release()
    return tempdir","def _get_default_tempdir():
    """"""Calculate the default directory to use for temporary files.
    This routine should be called exactly once.

    We determine whether or not a candidate temp dir is usable by
    trying to create and write to a file in that directory.  If this
    is successful, the test file is deleted.  To prevent denial of
    service, the name of the test file must be randomized.""""""

    namer = _RandomNameSequence()
    dirlist = _candidate_tempdir_list()

    for dir in dirlist:
        if dir != _os.curdir:
            dir = _os.path.abspath(dir)
        # Try only a few names per directory.
        for seq in range(100):
            name = next(namer)
            filename = _os.path.join(dir, name)
            try:
                fd = _os.open(filename, _bin_openflags, 0o600)
                try:
                    try:
                        with _io.open(fd, 'wb', closefd=False) as fp:
                            fp.write(b'blat')
                    finally:
                        _os.close(fd)
                finally:
                    _os.unlink(filename)
                return dir
            except FileExistsError:
                pass
            except PermissionError:
                # This exception is thrown when a directory with the chosen name
                # already exists on windows.
                if (_os.name == 'nt' and _os.path.isdir(dir) and
                    _os.access(dir, _os.W_OK)):
                    continue
                break   # no point trying more names in this directory
            except OSError:
                break   # no point trying more names in this directory
    raise FileNotFoundError(_errno.ENOENT,
                            ""No usable temporary directory found in %s"" %
                            dirlist)","def _calculate_default_temporary_directory():
    """"""Calculate the default directory to use for temporary files.
    This routine should be called exactly once.

    We determine whether or not a candidate temp dir is usable by
    trying to create and write to a file in that directory.  If this
    is successful, the test file is deleted.  To prevent denial of
    service, the name of the test file must be randomized.""""""

    namer = _RandomNameSequence()
    dirlist = _candidate_tempdir_list()

    for dir in dirlist:
        if dir != _os.curdir:
            dir = _os.path.abspath(dir)
        # Try only a few names per directory.
        for seq in range(100):
            name = next(namer)
            filename = _os.path.join(dir, name)
            try:
                fd = _os.open(filename, _bin_openflags, 0o600)
                try:
                    try:
                        with _io.open(fd, 'wb', closefd=False) as fp:
                            fp.write(b'blat')
                    finally:
                        _os.close(fd)
                finally:
                    _os.unlink(filename)
                return dir
            except FileExistsError:
                pass
            except PermissionError:
                # This exception is thrown when a directory with the chosen name
                # already exists on windows.
                if (_os.name == 'nt' and _os.path.isdir(dir) and
                    _os.access(dir, _os.W_OK)):
                    continue
                break   # no point trying more names in this directory
            except OSError:
                break   # no point trying more names in this directory
    raise FileNotFoundError(_errno.ENOENT,
                            ""No usable temporary directory found in %s"" %
                            dirlist)","def _private_accessor_for_tempfile_tempdir():
    """"""Private accessor for tempfile.tempdir.""""""
    global tempdir
    if tempdir is None:
        _once_lock.acquire()
        try:
            if tempdir is None:
                tempdir = _calculate_default_temporary_directory()
        finally:
            _once_lock.release()
    return tempdir","{""_get_default_tempdir"": ""_calculate_default_temporary_directory""}","def 0():
    """"""Calculate the default directory to use for temporary files.
    This routine should be called exactly once.

    We determine whether or not a candidate temp dir is usable by
    trying to create and write to a file in that directory.  If this
    is successful, the test file is deleted.  To prevent denial of
    service, the name of the test file must be randomized.""""""

    namer = _RandomNameSequence()
    dirlist = _candidate_tempdir_list()

    for dir in dirlist:
        if dir != _os.curdir:
            dir = _os.path.abspath(dir)
        # Try only a few names per directory.
        for seq in range(100):
            name = next(namer)
            filename = _os.path.join(dir, name)
            try:
                fd = _os.open(filename, _bin_openflags, 0o600)
                try:
                    try:
                        with _io.open(fd, 'wb', closefd=False) as fp:
                            fp.write(b'blat')
                    finally:
                        _os.close(fd)
                finally:
                    _os.unlink(filename)
                return dir
            except FileExistsError:
                pass
            except PermissionError:
                # This exception is thrown when a directory with the chosen name
                # already exists on windows.
                if (_os.name == 'nt' and _os.path.isdir(dir) and
                    _os.access(dir, _os.W_OK)):
                    continue
                break   # no point trying more names in this directory
            except OSError:
                break   # no point trying more names in this directory
    raise FileNotFoundError(_errno.ENOENT,
                            ""No usable temporary directory found in %s"" %
                            dirlist)","def 1():
    """"""Private accessor for tempfile.tempdir.""""""
    global tempdir
    if tempdir is None:
        _once_lock.acquire()
        try:
            if tempdir is None:
                tempdir = 0()
        finally:
            _once_lock.release()
    return tempdir","{""_get_default_tempdir"": ""0""}"
14,14,"def gettempdir():
    """"""Returns tempfile.tempdir as str.""""""
    return _os.fsdecode(_gettempdir())","def _gettempdir():
    """"""Private accessor for tempfile.tempdir.""""""
    global tempdir
    if tempdir is None:
        _once_lock.acquire()
        try:
            if tempdir is None:
                tempdir = _get_default_tempdir()
        finally:
            _once_lock.release()
    return tempdir","def _private_accessor_for_tempfile_tempdir():
    """"""Private accessor for tempfile.tempdir.""""""
    global tempdir
    if tempdir is None:
        _once_lock.acquire()
        try:
            if tempdir is None:
                tempdir = _get_default_tempdir()
        finally:
            _once_lock.release()
    return tempdir","def get_tempdir_as_string():
    """"""Returns tempfile.tempdir as str.""""""
    return _os.fsdecode(_private_accessor_for_tempfile_tempdir())","{""_gettempdir"": ""_private_accessor_for_tempfile_tempdir""}","def 0():
    """"""Private accessor for tempfile.tempdir.""""""
    global tempdir
    if tempdir is None:
        _once_lock.acquire()
        try:
            if tempdir is None:
                tempdir = _get_default_tempdir()
        finally:
            _once_lock.release()
    return tempdir","def 1():
    """"""Returns tempfile.tempdir as str.""""""
    return _os.fsdecode(0())","{""_gettempdir"": ""0""}"
15,15,"def gettempdirb():
    """"""Returns tempfile.tempdir as bytes.""""""
    return _os.fsencode(_gettempdir())","def _gettempdir():
    """"""Private accessor for tempfile.tempdir.""""""
    global tempdir
    if tempdir is None:
        _once_lock.acquire()
        try:
            if tempdir is None:
                tempdir = _get_default_tempdir()
        finally:
            _once_lock.release()
    return tempdir","def _private_accessor_for_tempfile_tempdir():
    """"""Private accessor for tempfile.tempdir.""""""
    global tempdir
    if tempdir is None:
        _once_lock.acquire()
        try:
            if tempdir is None:
                tempdir = _get_default_tempdir()
        finally:
            _once_lock.release()
    return tempdir","def get_tempdir_as_bytes():
    """"""Returns tempfile.tempdir as bytes.""""""
    return _os.fsencode(_private_accessor_for_tempfile_tempdir())","{""_gettempdir"": ""_private_accessor_for_tempfile_tempdir""}","def 0():
    """"""Private accessor for tempfile.tempdir.""""""
    global tempdir
    if tempdir is None:
        _once_lock.acquire()
        try:
            if tempdir is None:
                tempdir = _get_default_tempdir()
        finally:
            _once_lock.release()
    return tempdir","def 1():
    """"""Returns tempfile.tempdir as bytes.""""""
    return _os.fsencode(0())","{""_gettempdir"": ""0""}"
16,16,"def crypt(word, salt=None):
    """"""Return a string representing the one-way hash of a password, with a salt
    prepended.

    If ``salt`` is not specified or is ``None``, the strongest
    available method will be selected and a salt generated.  Otherwise,
    ``salt`` may be one of the ``crypt.METHOD_*`` values, or a string as
    returned by ``crypt.mksalt()``.

    """"""
    if salt is None or isinstance(salt, _Method):
        salt = mksalt(salt)
    return _crypt.crypt(word, salt)","def mksalt(method=None, *, rounds=None):
    """"""Generate a salt for the specified method.

    If not specified, the strongest available method will be used.

    """"""
    if method is None:
        method = methods[0]
    if rounds is not None and not isinstance(rounds, int):
        raise TypeError(f'{rounds.__class__.__name__} object cannot be '
                        f'interpreted as an integer')
    if not method.ident:  # traditional
        s = ''
    else:  # modular
        s = f'${method.ident}$'

    if method.ident and method.ident[0] == '2':  # Blowfish variants
        if rounds is None:
            log_rounds = 12
        else:
            log_rounds = int.bit_length(rounds-1)
            if rounds != 1 << log_rounds:
                raise ValueError('rounds must be a power of 2')
            if not 4 <= log_rounds <= 31:
                raise ValueError('rounds out of the range 2**4 to 2**31')
        s += f'{log_rounds:02d}$'
    elif method.ident in ('5', '6'):  # SHA-2
        if rounds is not None:
            if not 1000 <= rounds <= 999_999_999:
                raise ValueError('rounds out of the range 1000 to 999_999_999')
            s += f'rounds={rounds}$'
    elif rounds is not None:
        raise ValueError(f""{method} doesn't support the rounds argument"")

    s += ''.join(_sr.choice(_saltchars) for char in range(method.salt_chars))
    return s","def generate_salt(method=None, *, rounds=None):
    """"""Generate a salt for the specified method.

    If not specified, the strongest available method will be used.

    """"""
    if method is None:
        method = methods[0]
    if rounds is not None and not isinstance(rounds, int):
        raise TypeError(f'{rounds.__class__.__name__} object cannot be '
                        f'interpreted as an integer')
    if not method.ident:  # traditional
        s = ''
    else:  # modular
        s = f'${method.ident}$'

    if method.ident and method.ident[0] == '2':  # Blowfish variants
        if rounds is None:
            log_rounds = 12
        else:
            log_rounds = int.bit_length(rounds-1)
            if rounds != 1 << log_rounds:
                raise ValueError('rounds must be a power of 2')
            if not 4 <= log_rounds <= 31:
                raise ValueError('rounds out of the range 2**4 to 2**31')
        s += f'{log_rounds:02d}$'
    elif method.ident in ('5', '6'):  # SHA-2
        if rounds is not None:
            if not 1000 <= rounds <= 999_999_999:
                raise ValueError('rounds out of the range 1000 to 999_999_999')
            s += f'rounds={rounds}$'
    elif rounds is not None:
        raise ValueError(f""{method} doesn't support the rounds argument"")

    s += ''.join(_sr.choice(_saltchars) for char in range(method.salt_chars))
    return s","def return_one_way_hash_with_salt(word, salt=None):
    """"""Return a string representing the one-way hash of a password, with a salt
    prepended.

    If ``salt`` is not specified or is ``None``, the strongest
    available method will be selected and a salt generated.  Otherwise,
    ``salt`` may be one of the ``return_one_way_hash_with_salt.METHOD_*`` values, or a string as
    returned by ``return_one_way_hash_with_salt.generate_salt()``.

    """"""
    if salt is None or isinstance(salt, _Method):
        salt = generate_salt(salt)
    return _return_one_way_hash_with_salt.return_one_way_hash_with_salt(word, salt)","{""mksalt"": ""generate_salt""}","def 0(method=None, *, rounds=None):
    """"""Generate a salt for the specified method.

    If not specified, the strongest available method will be used.

    """"""
    if method is None:
        method = methods[0]
    if rounds is not None and not isinstance(rounds, int):
        raise TypeError(f'{rounds.__class__.__name__} object cannot be '
                        f'interpreted as an integer')
    if not method.ident:  # traditional
        s = ''
    else:  # modular
        s = f'${method.ident}$'

    if method.ident and method.ident[0] == '2':  # Blowfish variants
        if rounds is None:
            log_rounds = 12
        else:
            log_rounds = int.bit_length(rounds-1)
            if rounds != 1 << log_rounds:
                raise ValueError('rounds must be a power of 2')
            if not 4 <= log_rounds <= 31:
                raise ValueError('rounds out of the range 2**4 to 2**31')
        s += f'{log_rounds:02d}$'
    elif method.ident in ('5', '6'):  # SHA-2
        if rounds is not None:
            if not 1000 <= rounds <= 999_999_999:
                raise ValueError('rounds out of the range 1000 to 999_999_999')
            s += f'rounds={rounds}$'
    elif rounds is not None:
        raise ValueError(f""{method} doesn't support the rounds argument"")

    s += ''.join(_sr.choice(_saltchars) for char in range(method.salt_chars))
    return s","def 1(word, salt=None):
    """"""Return a string representing the one-way hash of a password, with a salt
    prepended.

    If ``salt`` is not specified or is ``None``, the strongest
    available method will be selected and a salt generated.  Otherwise,
    ``salt`` may be one of the ``crypt.METHOD_*`` values, or a string as
    returned by ``crypt.0()``.

    """"""
    if salt is None or isinstance(salt, _Method):
        salt = 0(salt)
    return _crypt.1(word, salt)","{""mksalt"": ""0""}"
17,17,"def iter_modules(path=None, prefix=''):
    """"""Yields ModuleInfo for all submodules on path,
    or, if path is None, all top-level modules on sys.path.

    'path' should be either None or a list of paths to look for
    modules in.

    'prefix' is a string to output on the front of every module name
    on output.
    """"""
    if path is None:
        importers = iter_importers()
    elif isinstance(path, str):
        raise ValueError(""path must be None or list of paths to look for ""
                        ""modules in"")
    else:
        importers = map(get_importer, path)

    yielded = {}
    for i in importers:
        for name, ispkg in iter_importer_modules(i, prefix):
            if name not in yielded:
                yielded[name] = 1
                yield ModuleInfo(i, name, ispkg)","def iter_importers(fullname=""""):
    """"""Yield finders for the given module name

    If fullname contains a '.', the finders will be for the package
    containing fullname, otherwise they will be all registered top level
    finders (i.e. those on both sys.meta_path and sys.path_hooks).

    If the named module is in a package, that package is imported as a side
    effect of invoking this function.

    If no module name is specified, all top level finders are produced.
    """"""
    if fullname.startswith('.'):
        msg = ""Relative module name {!r} not supported"".format(fullname)
        raise ImportError(msg)
    if '.' in fullname:
        # Get the containing package's __path__
        pkg_name = fullname.rpartition(""."")[0]
        pkg = importlib.import_module(pkg_name)
        path = getattr(pkg, '__path__', None)
        if path is None:
            return
    else:
        yield from sys.meta_path
        path = sys.path
    for item in path:
        yield get_importer(item)","def yield_finders_for_module(fullname=""""):
    """"""Yield finders for the given module name

    If fullname contains a '.', the finders will be for the package
    containing fullname, otherwise they will be all registered top level
    finders (i.e. those on both sys.meta_path and sys.path_hooks).

    If the named module is in a package, that package is imported as a side
    effect of invoking this function.

    If no module name is specified, all top level finders are produced.
    """"""
    if fullname.startswith('.'):
        msg = ""Relative module name {!r} not supported"".format(fullname)
        raise ImportError(msg)
    if '.' in fullname:
        # Get the containing package's __path__
        pkg_name = fullname.rpartition(""."")[0]
        pkg = importlib.import_module(pkg_name)
        path = getattr(pkg, '__path__', None)
        if path is None:
            return
    else:
        yield from sys.meta_path
        path = sys.path
    for item in path:
        yield get_importer(item)","def iterate_over_modules_on_path(path=None, prefix=''):
    """"""Yields ModuleInfo for all submodules on path,
    or, if path is None, all top-level modules on sys.path.

    'path' should be either None or a list of paths to look for
    modules in.

    'prefix' is a string to output on the front of every module name
    on output.
    """"""
    if path is None:
        importers = yield_finders_for_module()
    elif isinstance(path, str):
        raise ValueError(""path must be None or list of paths to look for ""
                        ""modules in"")
    else:
        importers = map(get_importer, path)

    yielded = {}
    for i in importers:
        for name, ispkg in iter_importer_modules(i, prefix):
            if name not in yielded:
                yielded[name] = 1
                yield ModuleInfo(i, name, ispkg)","{""iter_importers"": ""yield_finders_for_module""}","def 0(fullname=""""):
    """"""Yield finders for the given module name

    If fullname contains a '.', the finders will be for the package
    containing fullname, otherwise they will be all registered top level
    finders (i.e. those on both sys.meta_path and sys.path_hooks).

    If the named module is in a package, that package is imported as a side
    effect of invoking this function.

    If no module name is specified, all top level finders are produced.
    """"""
    if fullname.startswith('.'):
        msg = ""Relative module name {!r} not supported"".format(fullname)
        raise ImportError(msg)
    if '.' in fullname:
        # Get the containing package's __path__
        pkg_name = fullname.rpartition(""."")[0]
        pkg = importlib.import_module(pkg_name)
        path = getattr(pkg, '__path__', None)
        if path is None:
            return
    else:
        yield from sys.meta_path
        path = sys.path
    for item in path:
        yield get_importer(item)","def 1(path=None, prefix=''):
    """"""Yields ModuleInfo for all submodules on path,
    or, if path is None, all top-level modules on sys.path.

    'path' should be either None or a list of paths to look for
    modules in.

    'prefix' is a string to output on the front of every module name
    on output.
    """"""
    if path is None:
        importers = 0()
    elif isinstance(path, str):
        raise ValueError(""path must be None or list of paths to look for ""
                        ""modules in"")
    else:
        importers = map(get_importer, path)

    yielded = {}
    for i in importers:
        for name, ispkg in iter_importer_modules(i, prefix):
            if name not in yielded:
                yielded[name] = 1
                yield ModuleInfo(i, name, ispkg)","{""iter_importers"": ""0""}"
18,18,"def iter_importers(fullname=""""):
    """"""Yield finders for the given module name

    If fullname contains a '.', the finders will be for the package
    containing fullname, otherwise they will be all registered top level
    finders (i.e. those on both sys.meta_path and sys.path_hooks).

    If the named module is in a package, that package is imported as a side
    effect of invoking this function.

    If no module name is specified, all top level finders are produced.
    """"""
    if fullname.startswith('.'):
        msg = ""Relative module name {!r} not supported"".format(fullname)
        raise ImportError(msg)
    if '.' in fullname:
        # Get the containing package's __path__
        pkg_name = fullname.rpartition(""."")[0]
        pkg = importlib.import_module(pkg_name)
        path = getattr(pkg, '__path__', None)
        if path is None:
            return
    else:
        yield from sys.meta_path
        path = sys.path
    for item in path:
        yield get_importer(item)","def get_importer(path_item):
    """"""Retrieve a finder for the given path item

    The returned finder is cached in sys.path_importer_cache
    if it was newly created by a path hook.

    The cache (or part of it) can be cleared manually if a
    rescan of sys.path_hooks is necessary.
    """"""
    try:
        importer = sys.path_importer_cache[path_item]
    except KeyError:
        for path_hook in sys.path_hooks:
            try:
                importer = path_hook(path_item)
                sys.path_importer_cache.setdefault(path_item, importer)
                break
            except ImportError:
                pass
        else:
            importer = None
    return importer","def retrieve_finder_for_path_item(path_item):
    """"""Retrieve a finder for the given path item

    The returned finder is cached in sys.path_importer_cache
    if it was newly created by a path hook.

    The cache (or part of it) can be cleared manually if a
    rescan of sys.path_hooks is necessary.
    """"""
    try:
        importer = sys.path_importer_cache[path_item]
    except KeyError:
        for path_hook in sys.path_hooks:
            try:
                importer = path_hook(path_item)
                sys.path_importer_cache.setdefault(path_item, importer)
                break
            except ImportError:
                pass
        else:
            importer = None
    return importer","def yield_finders_for_module(fullname=""""):
    """"""Yield finders for the given module name

    If fullname contains a '.', the finders will be for the package
    containing fullname, otherwise they will be all registered top level
    finders (i.e. those on both sys.meta_path and sys.path_hooks).

    If the named module is in a package, that package is imported as a side
    effect of invoking this function.

    If no module name is specified, all top level finders are produced.
    """"""
    if fullname.startswith('.'):
        msg = ""Relative module name {!r} not supported"".format(fullname)
        raise ImportError(msg)
    if '.' in fullname:
        # Get the containing package's __path__
        pkg_name = fullname.rpartition(""."")[0]
        pkg = importlib.import_module(pkg_name)
        path = getattr(pkg, '__path__', None)
        if path is None:
            return
    else:
        yield from sys.meta_path
        path = sys.path
    for item in path:
        yield retrieve_finder_for_path_item(item)","{""get_importer"": ""retrieve_finder_for_path_item""}","def 0(path_item):
    """"""Retrieve a finder for the given path item

    The returned finder is cached in sys.path_importer_cache
    if it was newly created by a path hook.

    The cache (or part of it) can be cleared manually if a
    rescan of sys.path_hooks is necessary.
    """"""
    try:
        importer = sys.path_importer_cache[path_item]
    except KeyError:
        for path_hook in sys.path_hooks:
            try:
                importer = path_hook(path_item)
                sys.path_importer_cache.setdefault(path_item, importer)
                break
            except ImportError:
                pass
        else:
            importer = None
    return importer","def 1(fullname=""""):
    """"""Yield finders for the given module name

    If fullname contains a '.', the finders will be for the package
    containing fullname, otherwise they will be all registered top level
    finders (i.e. those on both sys.meta_path and sys.path_hooks).

    If the named module is in a package, that package is imported as a side
    effect of invoking this function.

    If no module name is specified, all top level finders are produced.
    """"""
    if fullname.startswith('.'):
        msg = ""Relative module name {!r} not supported"".format(fullname)
        raise ImportError(msg)
    if '.' in fullname:
        # Get the containing package's __path__
        pkg_name = fullname.rpartition(""."")[0]
        pkg = importlib.import_module(pkg_name)
        path = getattr(pkg, '__path__', None)
        if path is None:
            return
    else:
        yield from sys.meta_path
        path = sys.path
    for item in path:
        yield 0(item)","{""get_importer"": ""0""}"
19,19,"def _split_optional_netmask(address):
    """"""Helper to split the netmask and raise AddressValueError if needed""""""
    addr = str(address).split('/')
    if len(addr) > 2:
        raise AddressValueError(f""Only one '/' permitted in {address!r}"")
    return addr","def str(val):
    """"""Convert float to string, taking the locale into account.""""""
    return _format(""%.12g"", val)","def locale_aware_float_to_string(val):
    """"""Convert float to locale_aware_float_to_stringing, taking the locale into account.""""""
    return _format(""%.12g"", val)","def split_netmask_and_raise_error_if_needed(address):
    """"""Helper to split the netmask and raise AddressValueError if needed""""""
    addr = locale_aware_float_to_string(address).split('/')
    if len(addr) > 2:
        raise AddressValueError(f""Only one '/' permitted in {address!r}"")
    return addr","{""str"": ""locale_aware_float_to_string""}","def 0(val):
    """"""Convert float to string, taking the locale into account.""""""
    return _format(""%.12g"", val)","def 1(address):
    """"""Helper to split the netmask and raise AddressValueError if needed""""""
    addr = 0(address).split('/')
    if len(addr) > 2:
        raise AddressValueError(f""Only one '/' permitted in {address!r}"")
    return addr","{""str"": ""0""}"
20,20,"def _find_lines(code, strs):
    """"""Return lineno dict for all code objects reachable from code.""""""
    # get all of the lineno information from the code of this scope level
    linenos = _find_lines_from_code(code, strs)

    # and check the constants for references to other code objects
    for c in code.co_consts:
        if inspect.iscode(c):
            # find another code object, so recurse into it
            linenos.update(_find_lines(c, strs))
    return linenos","def _find_lines(code, strs):
    """"""Return lineno dict for all code objects reachable from code.""""""
    # get all of the lineno information from the code of this scope level
    linenos = _find_lines_from_code(code, strs)

    # and check the constants for references to other code objects
    for c in code.co_consts:
        if inspect.iscode(c):
            # find another code object, so recurse into it
            linenos.update(_find_lines(c, strs))
    return linenos

def _find_lines_from_code(code, strs):
    """"""Return dict where keys are lines in the line number table.""""""
    linenos = {}

    for _, lineno in dis.findlinestarts(code):
        if lineno not in strs:
            linenos[lineno] = 1

    return linenos","def get_line_numbers_for_code_objects(code, strs):
    """"""Return lineno dict for all code objects reachable from code.""""""
    # get all of the lineno information from the code of this scope level
    linenos = get_line_numbers_for_code_objects_from_code(code, strs)

    # and check the constants for references to other code objects
    for c in code.co_consts:
        if inspect.iscode(c):
            # find another code object, so recurse into it
            linenos.update(get_line_numbers_for_code_objects(c, strs))
    return linenos

def get_line_numbers_for_code_objects_from_code(code, strs):
    """"""Return dict where keys are lines in the line number table.""""""
    linenos = {}

    for _, lineno in dis.findlinestarts(code):
        if lineno not in strs:
            linenos[lineno] = 1

    return linenos","def get_line_numbers_for_code_objects(code, strs):
    """"""Return lineno dict for all code objects reachable from code.""""""
    # get all of the lineno information from the code of this scope level
    linenos = get_line_numbers_for_code_objects_from_code(code, strs)

    # and check the constants for references to other code objects
    for c in code.co_consts:
        if inspect.iscode(c):
            # find another code object, so recurse into it
            linenos.update(get_line_numbers_for_code_objects(c, strs))
    return linenos","{""_find_lines"": ""get_line_numbers_for_code_objects"", ""_find_lines_from_code"": ""search_for_lines_from_code""}","def 0(code, strs):
    """"""Return lineno dict for all code objects reachable from code.""""""
    # get all of the lineno information from the code of this scope level
    linenos = 1(code, strs)

    # and check the constants for references to other code objects
    for c in code.co_consts:
        if inspect.iscode(c):
            # find another code object, so recurse into it
            linenos.update(0(c, strs))
    return linenos

def 1(code, strs):
    """"""Return dict where keys are lines in the line number table.""""""
    linenos = {}

    for _, lineno in dis.findlinestarts(code):
        if lineno not in strs:
            linenos[lineno] = 1

    return linenos","def 0(code, strs):
    """"""Return lineno dict for all code objects reachable from code.""""""
    # get all of the lineno information from the code of this scope level
    linenos = 1(code, strs)

    # and check the constants for references to other code objects
    for c in code.co_consts:
        if inspect.iscode(c):
            # find another code object, so recurse into it
            linenos.update(0(c, strs))
    return linenos","{""_find_lines"": ""0"", ""_find_lines_from_code"": ""1""}"
21,21,"def distb(tb=None, *, file=None):
    """"""Disassemble a traceback (default: last traceback).""""""
    if tb is None:
        try:
            tb = sys.last_traceback
        except AttributeError:
            raise RuntimeError(""no last traceback to disassemble"") from None
        while tb.tb_next: tb = tb.tb_next
    disassemble(tb.tb_frame.f_code, tb.tb_lasti, file=file)","def disassemble(co, lasti=-1, *, file=None):
    """"""Disassemble a code object.""""""
    cell_names = co.co_cellvars + co.co_freevars
    linestarts = dict(findlinestarts(co))
    _disassemble_bytes(co.co_code, lasti, co.co_varnames, co.co_names,
                       co.co_consts, cell_names, linestarts, file=file)","def disassemble_code_object(co, lasti=-1, *, file=None):
    """"""Disassemble a code object.""""""
    cell_names = co.co_cellvars + co.co_freevars
    linestarts = dict(findlinestarts(co))
    _disassemble_code_object_bytes(co.co_code, lasti, co.co_varnames, co.co_names,
                       co.co_consts, cell_names, linestarts, file=file)","def disassemble_traceback(tb=None, *, file=None):
    """"""Disassemble a traceback (default: last traceback).""""""
    if tb is None:
        try:
            tb = sys.last_traceback
        except AttributeError:
            raise RuntimeError(""no last traceback to disassemble_code_object"") from None
        while tb.tb_next: tb = tb.tb_next
    disassemble_code_object(tb.tb_frame.f_code, tb.tb_lasti, file=file)","{""disassemble"": ""disassemble_code_object""}","def 0(co, lasti=-1, *, file=None):
    """"""Disassemble a code object.""""""
    cell_names = co.co_cellvars + co.co_freevars
    linestarts = dict(findlinestarts(co))
    _disassemble_bytes(co.co_code, lasti, co.co_varnames, co.co_names,
                       co.co_consts, cell_names, linestarts, file=file)","def 1(tb=None, *, file=None):
    """"""Disassemble a traceback (default: last traceback).""""""
    if tb is None:
        try:
            tb = sys.last_traceback
        except AttributeError:
            raise RuntimeError(""no last traceback to disassemble"") from None
        while tb.tb_next: tb = tb.tb_next
    0(tb.tb_frame.f_code, tb.tb_lasti, file=file)","{""disassemble"": ""0""}"
22,22,"def _get_code_object(x):
    """"""Helper to handle methods, compiled or raw code objects, and strings.""""""
    # Extract functions from methods.
    if hasattr(x, '__func__'):
        x = x.__func__
    # Extract compiled code objects from...
    if hasattr(x, '__code__'):  # ...a function, or
        x = x.__code__
    elif hasattr(x, 'gi_code'):  #...a generator object, or
        x = x.gi_code
    elif hasattr(x, 'ag_code'):  #...an asynchronous generator object, or
        x = x.ag_code
    elif hasattr(x, 'cr_code'):  #...a coroutine.
        x = x.cr_code
    # Handle source code.
    if isinstance(x, str):
        x = _try_compile(x, ""<disassembly>"")
    # By now, if we don't have a code object, we can't disassemble x.
    if hasattr(x, 'co_code'):
        return x
    raise TypeError(""don't know how to disassemble %s objects"" %
                    type(x).__name__)","def _try_compile(source, name):
    """"""Attempts to compile the given source, first as an expression and
       then as a statement if the first approach fails.

       Utility function to accept strings in functions that otherwise
       expect code objects
    """"""
    try:
        c = compile(source, name, 'eval')
    except SyntaxError:
        c = compile(source, name, 'exec')
    return c","def attempt_compile(source, name):
    """"""Attempts to compile the given source, first as an expression and
       then as a statement if the first approach fails.

       Utility function to accept strings in functions that otherwise
       expect code objects
    """"""
    try:
        c = compile(source, name, 'eval')
    except SyntaxError:
        c = compile(source, name, 'exec')
    return c","def obtain_code_object(x):
    """"""Helper to handle methods, compiled or raw code objects, and strings.""""""
    # Extract functions from methods.
    if hasattr(x, '__func__'):
        x = x.__func__
    # Extract compiled code objects from...
    if hasattr(x, '__code__'):  # ...a function, or
        x = x.__code__
    elif hasattr(x, 'gi_code'):  #...a generator object, or
        x = x.gi_code
    elif hasattr(x, 'ag_code'):  #...an asynchronous generator object, or
        x = x.ag_code
    elif hasattr(x, 'cr_code'):  #...a coroutine.
        x = x.cr_code
    # Handle source code.
    if isinstance(x, str):
        x = attempt_compile(x, ""<disassembly>"")
    # By now, if we don't have a code object, we can't disassemble x.
    if hasattr(x, 'co_code'):
        return x
    raise TypeError(""don't know how to disassemble %s objects"" %
                    type(x).__name__)","{""_try_compile"": ""attempt_compile""}","def 0(source, name):
    """"""Attempts to compile the given source, first as an expression and
       then as a statement if the first approach fails.

       Utility function to accept strings in functions that otherwise
       expect code objects
    """"""
    try:
        c = compile(source, name, 'eval')
    except SyntaxError:
        c = compile(source, name, 'exec')
    return c","def 1(x):
    """"""Helper to handle methods, compiled or raw code objects, and strings.""""""
    # Extract functions from methods.
    if hasattr(x, '__func__'):
        x = x.__func__
    # Extract compiled code objects from...
    if hasattr(x, '__code__'):  # ...a function, or
        x = x.__code__
    elif hasattr(x, 'gi_code'):  #...a generator object, or
        x = x.gi_code
    elif hasattr(x, 'ag_code'):  #...an asynchronous generator object, or
        x = x.ag_code
    elif hasattr(x, 'cr_code'):  #...a coroutine.
        x = x.cr_code
    # Handle source code.
    if isinstance(x, str):
        x = 0(x, ""<disassembly>"")
    # By now, if we don't have a code object, we can't disassemble x.
    if hasattr(x, 'co_code'):
        return x
    raise TypeError(""don't know how to disassemble %s objects"" %
                    type(x).__name__)","{""_try_compile"": ""0""}"
23,23,"def code_info(x):
    """"""Formatted details of methods, functions, or code.""""""
    return _format_code_info(_get_code_object(x))","def _format_code_info(co):
    lines = []
    lines.append(""Name:              %s"" % co.co_name)
    lines.append(""Filename:          %s"" % co.co_filename)
    lines.append(""Argument count:    %s"" % co.co_argcount)
    lines.append(""Positional-only arguments: %s"" % co.co_posonlyargcount)
    lines.append(""Kw-only arguments: %s"" % co.co_kwonlyargcount)
    lines.append(""Number of locals:  %s"" % co.co_nlocals)
    lines.append(""Stack size:        %s"" % co.co_stacksize)
    lines.append(""Flags:             %s"" % pretty_flags(co.co_flags))
    if co.co_consts:
        lines.append(""Constants:"")
        for i_c in enumerate(co.co_consts):
            lines.append(""%4d: %r"" % i_c)
    if co.co_names:
        lines.append(""Names:"")
        for i_n in enumerate(co.co_names):
            lines.append(""%4d: %s"" % i_n)
    if co.co_varnames:
        lines.append(""Variable names:"")
        for i_n in enumerate(co.co_varnames):
            lines.append(""%4d: %s"" % i_n)
    if co.co_freevars:
        lines.append(""Free variables:"")
        for i_n in enumerate(co.co_freevars):
            lines.append(""%4d: %s"" % i_n)
    if co.co_cellvars:
        lines.append(""Cell variables:"")
        for i_n in enumerate(co.co_cellvars):
            lines.append(""%4d: %s"" % i_n)
    return ""\n"".join(lines)

def _get_code_object(x):
    """"""Helper to handle methods, compiled or raw code objects, and strings.""""""
    # Extract functions from methods.
    if hasattr(x, '__func__'):
        x = x.__func__
    # Extract compiled code objects from...
    if hasattr(x, '__code__'):  # ...a function, or
        x = x.__code__
    elif hasattr(x, 'gi_code'):  #...a generator object, or
        x = x.gi_code
    elif hasattr(x, 'ag_code'):  #...an asynchronous generator object, or
        x = x.ag_code
    elif hasattr(x, 'cr_code'):  #...a coroutine.
        x = x.cr_code
    # Handle source code.
    if isinstance(x, str):
        x = _try_compile(x, ""<disassembly>"")
    # By now, if we don't have a code object, we can't disassemble x.
    if hasattr(x, 'co_code'):
        return x
    raise TypeError(""don't know how to disassemble %s objects"" %
                    type(x).__name__)","def generate_code_info_format(co):
    lines = []
    lines.append(""Name:              %s"" % co.co_name)
    lines.append(""Filename:          %s"" % co.co_filename)
    lines.append(""Argument count:    %s"" % co.co_argcount)
    lines.append(""Positional-only arguments: %s"" % co.co_posonlyargcount)
    lines.append(""Kw-only arguments: %s"" % co.co_kwonlyargcount)
    lines.append(""Number of locals:  %s"" % co.co_nlocals)
    lines.append(""Stack size:        %s"" % co.co_stacksize)
    lines.append(""Flags:             %s"" % pretty_flags(co.co_flags))
    if co.co_consts:
        lines.append(""Constants:"")
        for i_c in enumerate(co.co_consts):
            lines.append(""%4d: %r"" % i_c)
    if co.co_names:
        lines.append(""Names:"")
        for i_n in enumerate(co.co_names):
            lines.append(""%4d: %s"" % i_n)
    if co.co_varnames:
        lines.append(""Variable names:"")
        for i_n in enumerate(co.co_varnames):
            lines.append(""%4d: %s"" % i_n)
    if co.co_freevars:
        lines.append(""Free variables:"")
        for i_n in enumerate(co.co_freevars):
            lines.append(""%4d: %s"" % i_n)
    if co.co_cellvars:
        lines.append(""Cell variables:"")
        for i_n in enumerate(co.co_cellvars):
            lines.append(""%4d: %s"" % i_n)
    return ""\n"".join(lines)

def obtain_code_object(x):
    """"""Helper to handle methods, compiled or raw code objects, and strings.""""""
    # Extract functions from methods.
    if hasattr(x, '__func__'):
        x = x.__func__
    # Extract compiled code objects from...
    if hasattr(x, '__code__'):  # ...a function, or
        x = x.__code__
    elif hasattr(x, 'gi_code'):  #...a generator object, or
        x = x.gi_code
    elif hasattr(x, 'ag_code'):  #...an asynchronous generator object, or
        x = x.ag_code
    elif hasattr(x, 'cr_code'):  #...a coroutine.
        x = x.cr_code
    # Handle source code.
    if isinstance(x, str):
        x = _try_compile(x, ""<disassembly>"")
    # By now, if we don't have a code object, we can't disassemble x.
    if hasattr(x, 'co_code'):
        return x
    raise TypeError(""don't know how to disassemble %s objects"" %
                    type(x).__name__)","def format_details_of_methods_or_functions(x):
    """"""Formatted details of methods, functions, or code.""""""
    return generate_format_details_of_methods_or_functions_format(obtain_code_object(x))","{""_format_code_info"": ""generate_code_info_format"", ""_get_code_object"": ""obtain_code_object""}","def 0(co):
    lines = []
    lines.append(""Name:              %s"" % co.co_name)
    lines.append(""Filename:          %s"" % co.co_filename)
    lines.append(""Argument count:    %s"" % co.co_argcount)
    lines.append(""Positional-only arguments: %s"" % co.co_posonlyargcount)
    lines.append(""Kw-only arguments: %s"" % co.co_kwonlyargcount)
    lines.append(""Number of locals:  %s"" % co.co_nlocals)
    lines.append(""Stack size:        %s"" % co.co_stacksize)
    lines.append(""Flags:             %s"" % pretty_flags(co.co_flags))
    if co.co_consts:
        lines.append(""Constants:"")
        for i_c in enumerate(co.co_consts):
            lines.append(""%4d: %r"" % i_c)
    if co.co_names:
        lines.append(""Names:"")
        for i_n in enumerate(co.co_names):
            lines.append(""%4d: %s"" % i_n)
    if co.co_varnames:
        lines.append(""Variable names:"")
        for i_n in enumerate(co.co_varnames):
            lines.append(""%4d: %s"" % i_n)
    if co.co_freevars:
        lines.append(""Free variables:"")
        for i_n in enumerate(co.co_freevars):
            lines.append(""%4d: %s"" % i_n)
    if co.co_cellvars:
        lines.append(""Cell variables:"")
        for i_n in enumerate(co.co_cellvars):
            lines.append(""%4d: %s"" % i_n)
    return ""\n"".join(lines)

def 1(x):
    """"""Helper to handle methods, compiled or raw code objects, and strings.""""""
    # Extract functions from methods.
    if hasattr(x, '__func__'):
        x = x.__func__
    # Extract compiled code objects from...
    if hasattr(x, '__code__'):  # ...a function, or
        x = x.__code__
    elif hasattr(x, 'gi_code'):  #...a generator object, or
        x = x.gi_code
    elif hasattr(x, 'ag_code'):  #...an asynchronous generator object, or
        x = x.ag_code
    elif hasattr(x, 'cr_code'):  #...a coroutine.
        x = x.cr_code
    # Handle source code.
    if isinstance(x, str):
        x = _try_compile(x, ""<disassembly>"")
    # By now, if we don't have a code object, we can't disassemble x.
    if hasattr(x, 'co_code'):
        return x
    raise TypeError(""don't know how to disassemble %s objects"" %
                    type(x).__name__)","def 2(x):
    """"""Formatted details of methods, functions, or code.""""""
    return 0(1(x))","{""_format_code_info"": ""0"", ""_get_code_object"": ""1""}"
24,24,"def get_instructions(x, *, first_line=None):
    """"""Iterator for the opcodes in methods, functions or code

    Generates a series of Instruction named tuples giving the details of
    each operations in the supplied code.

    If *first_line* is not None, it indicates the line number that should
    be reported for the first source line in the disassembled code.
    Otherwise, the source line information (if any) is taken directly from
    the disassembled code object.
    """"""
    co = _get_code_object(x)
    cell_names = co.co_cellvars + co.co_freevars
    linestarts = dict(findlinestarts(co))
    if first_line is not None:
        line_offset = first_line - co.co_firstlineno
    else:
        line_offset = 0
    return _get_instructions_bytes(co.co_code, co.co_varnames, co.co_names,
                                   co.co_consts, cell_names, linestarts,
                                   line_offset)","def _get_code_object(x):
    """"""Helper to handle methods, compiled or raw code objects, and strings.""""""
    # Extract functions from methods.
    if hasattr(x, '__func__'):
        x = x.__func__
    # Extract compiled code objects from...
    if hasattr(x, '__code__'):  # ...a function, or
        x = x.__code__
    elif hasattr(x, 'gi_code'):  #...a generator object, or
        x = x.gi_code
    elif hasattr(x, 'ag_code'):  #...an asynchronous generator object, or
        x = x.ag_code
    elif hasattr(x, 'cr_code'):  #...a coroutine.
        x = x.cr_code
    # Handle source code.
    if isinstance(x, str):
        x = _try_compile(x, ""<disassembly>"")
    # By now, if we don't have a code object, we can't disassemble x.
    if hasattr(x, 'co_code'):
        return x
    raise TypeError(""don't know how to disassemble %s objects"" %
                    type(x).__name__)

def findlinestarts(code):
    """"""Find the offsets in a byte code which are start of lines in the source.

    Generate pairs (offset, lineno)
    """"""
    lastline = None
    for start, end, line in code.co_lines():
        if line is not None and line != lastline:
            lastline = line
            yield start, line
    return

def _get_instructions_bytes(code, varnames=None, names=None, constants=None,
                      cells=None, linestarts=None, line_offset=0):
    """"""Iterate over the instructions in a bytecode string.

    Generates a sequence of Instruction namedtuples giving the details of each
    opcode.  Additional information about the code's runtime environment
    (e.g. variable names, constants) can be specified using optional
    arguments.

    """"""
    labels = findlabels(code)
    starts_line = None
    for offset, op, arg in _unpack_opargs(code):
        if linestarts is not None:
            starts_line = linestarts.get(offset, None)
            if starts_line is not None:
                starts_line += line_offset
        is_jump_target = offset in labels
        argval = None
        argrepr = ''
        if arg is not None:
            #  Set argval to the dereferenced value of the argument when
            #  available, and argrepr to the string representation of argval.
            #    _disassemble_bytes needs the string repr of the
            #    raw name index for LOAD_GLOBAL, LOAD_CONST, etc.
            argval = arg
            if op in hasconst:
                argval, argrepr = _get_const_info(arg, constants)
            elif op in hasname:
                argval, argrepr = _get_name_info(arg, names)
            elif op in hasjabs:
                argval = arg*2
                argrepr = ""to "" + repr(argval)
            elif op in hasjrel:
                argval = offset + 2 + arg*2
                argrepr = ""to "" + repr(argval)
            elif op in haslocal:
                argval, argrepr = _get_name_info(arg, varnames)
            elif op in hascompare:
                argval = cmp_op[arg]
                argrepr = argval
            elif op in hasfree:
                argval, argrepr = _get_name_info(arg, cells)
            elif op == FORMAT_VALUE:
                argval, argrepr = FORMAT_VALUE_CONVERTERS[arg & 0x3]
                argval = (argval, bool(arg & 0x4))
                if argval[1]:
                    if argrepr:
                        argrepr += ', '
                    argrepr += 'with format'
            elif op == MAKE_FUNCTION:
                argrepr = ', '.join(s for i, s in enumerate(MAKE_FUNCTION_FLAGS)
                                    if arg & (1<<i))
        yield Instruction(opname[op], op,
                          arg, argval, argrepr,
                          offset, starts_line, is_jump_target)","def obtain_code_object(x):
    """"""Helper to handle methods, compiled or raw code objects, and strings.""""""
    # Extract functions from methods.
    if hasattr(x, '__func__'):
        x = x.__func__
    # Extract compiled code objects from...
    if hasattr(x, '__code__'):  # ...a function, or
        x = x.__code__
    elif hasattr(x, 'gi_code'):  #...a generator object, or
        x = x.gi_code
    elif hasattr(x, 'ag_code'):  #...an asynchronous generator object, or
        x = x.ag_code
    elif hasattr(x, 'cr_code'):  #...a coroutine.
        x = x.cr_code
    # Handle source code.
    if isinstance(x, str):
        x = _try_compile(x, ""<disassembly>"")
    # By now, if we don't have a code object, we can't disassemble x.
    if hasattr(x, 'co_code'):
        return x
    raise TypeError(""don't know how to disassemble %s objects"" %
                    type(x).__name__)

def locate_line_starts(code):
    """"""Find the offsets in a byte code which are start of lines in the source.

    Generate pairs (offset, lineno)
    """"""
    lastline = None
    for start, end, line in code.co_lines():
        if line is not None and line != lastline:
            lastline = line
            yield start, line
    return

def _iterate_over_instructions_in_bytecode_string(code, varnames=None, names=None, constants=None,
                      cells=None, linestarts=None, line_offset=0):
    """"""Iterate over the instructions in a bytecode string.

    Generates a sequence of Instruction namedtuples giving the details of each
    opcode.  Additional information about the code's runtime environment
    (e.g. variable names, constants) can be specified using optional
    arguments.

    """"""
    labels = findlabels(code)
    starts_line = None
    for offset, op, arg in _unpack_opargs(code):
        if linestarts is not None:
            starts_line = linestarts.get(offset, None)
            if starts_line is not None:
                starts_line += line_offset
        is_jump_target = offset in labels
        argval = None
        argrepr = ''
        if arg is not None:
            #  Set argval to the dereferenced value of the argument when
            #  available, and argrepr to the string representation of argval.
            #    _disassemble_bytes needs the string repr of the
            #    raw name index for LOAD_GLOBAL, LOAD_CONST, etc.
            argval = arg
            if op in hasconst:
                argval, argrepr = _get_const_info(arg, constants)
            elif op in hasname:
                argval, argrepr = _get_name_info(arg, names)
            elif op in hasjabs:
                argval = arg*2
                argrepr = ""to "" + repr(argval)
            elif op in hasjrel:
                argval = offset + 2 + arg*2
                argrepr = ""to "" + repr(argval)
            elif op in haslocal:
                argval, argrepr = _get_name_info(arg, varnames)
            elif op in hascompare:
                argval = cmp_op[arg]
                argrepr = argval
            elif op in hasfree:
                argval, argrepr = _get_name_info(arg, cells)
            elif op == FORMAT_VALUE:
                argval, argrepr = FORMAT_VALUE_CONVERTERS[arg & 0x3]
                argval = (argval, bool(arg & 0x4))
                if argval[1]:
                    if argrepr:
                        argrepr += ', '
                    argrepr += 'with format'
            elif op == MAKE_FUNCTION:
                argrepr = ', '.join(s for i, s in enumerate(MAKE_FUNCTION_FLAGS)
                                    if arg & (1<<i))
        yield Instruction(opname[op], op,
                          arg, argval, argrepr,
                          offset, starts_line, is_jump_target)","def iterate_over_opcodes_in_code(x, *, first_line=None):
    """"""Iterator for the opcodes in methods, functions or code

    Generates a series of Instruction named tuples giving the details of
    each operations in the supplied code.

    If *first_line* is not None, it indicates the line number that should
    be reported for the first source line in the disassembled code.
    Otherwise, the source line information (if any) is taken directly from
    the disassembled code object.
    """"""
    co = obtain_code_object(x)
    cell_names = co.co_cellvars + co.co_freevars
    linestarts = dict(locate_line_starts(co))
    if first_line is not None:
        line_offset = first_line - co.co_firstlineno
    else:
        line_offset = 0
    return _iterate_over_instructions_in_bytecode_string(co.co_code, co.co_varnames, co.co_names,
                                   co.co_consts, cell_names, linestarts,
                                   line_offset)","{""_get_code_object"": ""obtain_code_object"", ""findlinestarts"": ""locate_line_starts"", ""_get_instructions_bytes"": ""_iterate_over_instructions_in_bytecode_string""}","def 0(x):
    """"""Helper to handle methods, compiled or raw code objects, and strings.""""""
    # Extract functions from methods.
    if hasattr(x, '__func__'):
        x = x.__func__
    # Extract compiled code objects from...
    if hasattr(x, '__code__'):  # ...a function, or
        x = x.__code__
    elif hasattr(x, 'gi_code'):  #...a generator object, or
        x = x.gi_code
    elif hasattr(x, 'ag_code'):  #...an asynchronous generator object, or
        x = x.ag_code
    elif hasattr(x, 'cr_code'):  #...a coroutine.
        x = x.cr_code
    # Handle source code.
    if isinstance(x, str):
        x = _try_compile(x, ""<disassembly>"")
    # By now, if we don't have a code object, we can't disassemble x.
    if hasattr(x, 'co_code'):
        return x
    raise TypeError(""don't know how to disassemble %s objects"" %
                    type(x).__name__)

def 1(code):
    """"""Find the offsets in a byte code which are start of lines in the source.

    Generate pairs (offset, lineno)
    """"""
    lastline = None
    for start, end, line in code.co_lines():
        if line is not None and line != lastline:
            lastline = line
            yield start, line
    return

def 2(code, varnames=None, names=None, constants=None,
                      cells=None, linestarts=None, line_offset=0):
    """"""Iterate over the instructions in a bytecode string.

    Generates a sequence of Instruction namedtuples giving the details of each
    opcode.  Additional information about the code's runtime environment
    (e.g. variable names, constants) can be specified using optional
    arguments.

    """"""
    labels = findlabels(code)
    starts_line = None
    for offset, op, arg in _unpack_opargs(code):
        if linestarts is not None:
            starts_line = linestarts.get(offset, None)
            if starts_line is not None:
                starts_line += line_offset
        is_jump_target = offset in labels
        argval = None
        argrepr = ''
        if arg is not None:
            #  Set argval to the dereferenced value of the argument when
            #  available, and argrepr to the string representation of argval.
            #    _disassemble_bytes needs the string repr of the
            #    raw name index for LOAD_GLOBAL, LOAD_CONST, etc.
            argval = arg
            if op in hasconst:
                argval, argrepr = _get_const_info(arg, constants)
            elif op in hasname:
                argval, argrepr = _get_name_info(arg, names)
            elif op in hasjabs:
                argval = arg*2
                argrepr = ""to "" + repr(argval)
            elif op in hasjrel:
                argval = offset + 2 + arg*2
                argrepr = ""to "" + repr(argval)
            elif op in haslocal:
                argval, argrepr = _get_name_info(arg, varnames)
            elif op in hascompare:
                argval = cmp_op[arg]
                argrepr = argval
            elif op in hasfree:
                argval, argrepr = _get_name_info(arg, cells)
            elif op == FORMAT_VALUE:
                argval, argrepr = FORMAT_VALUE_CONVERTERS[arg & 0x3]
                argval = (argval, bool(arg & 0x4))
                if argval[1]:
                    if argrepr:
                        argrepr += ', '
                    argrepr += 'with format'
            elif op == MAKE_FUNCTION:
                argrepr = ', '.join(s for i, s in enumerate(MAKE_FUNCTION_FLAGS)
                                    if arg & (1<<i))
        yield Instruction(opname[op], op,
                          arg, argval, argrepr,
                          offset, starts_line, is_jump_target)","def 3(x, *, first_line=None):
    """"""Iterator for the opcodes in methods, functions or code

    Generates a series of Instruction named tuples giving the details of
    each operations in the supplied code.

    If *first_line* is not None, it indicates the line number that should
    be reported for the first source line in the disassembled code.
    Otherwise, the source line information (if any) is taken directly from
    the disassembled code object.
    """"""
    co = 0(x)
    cell_names = co.co_cellvars + co.co_freevars
    linestarts = dict(1(co))
    if first_line is not None:
        line_offset = first_line - co.co_firstlineno
    else:
        line_offset = 0
    return 2(co.co_code, co.co_varnames, co.co_names,
                                   co.co_consts, cell_names, linestarts,
                                   line_offset)","{""_get_code_object"": ""0"", ""findlinestarts"": ""1"", ""_get_instructions_bytes"": ""2""}"
25,25,"def disassemble(co, lasti=-1, *, file=None):
    """"""Disassemble a code object.""""""
    cell_names = co.co_cellvars + co.co_freevars
    linestarts = dict(findlinestarts(co))
    _disassemble_bytes(co.co_code, lasti, co.co_varnames, co.co_names,
                       co.co_consts, cell_names, linestarts, file=file)","def findlinestarts(code):
    """"""Find the offsets in a byte code which are start of lines in the source.

    Generate pairs (offset, lineno)
    """"""
    lastline = None
    for start, end, line in code.co_lines():
        if line is not None and line != lastline:
            lastline = line
            yield start, line
    return

def _disassemble_bytes(code, lasti=-1, varnames=None, names=None,
                       constants=None, cells=None, linestarts=None,
                       *, file=None, line_offset=0):
    # Omit the line number column entirely if we have no line number info
    show_lineno = bool(linestarts)
    if show_lineno:
        maxlineno = max(linestarts.values()) + line_offset
        if maxlineno >= 1000:
            lineno_width = len(str(maxlineno))
        else:
            lineno_width = 3
    else:
        lineno_width = 0
    maxoffset = len(code) - 2
    if maxoffset >= 10000:
        offset_width = len(str(maxoffset))
    else:
        offset_width = 4
    for instr in _get_instructions_bytes(code, varnames, names,
                                         constants, cells, linestarts,
                                         line_offset=line_offset):
        new_source_line = (show_lineno and
                           instr.starts_line is not None and
                           instr.offset > 0)
        if new_source_line:
            print(file=file)
        is_current_instr = instr.offset == lasti
        print(instr._disassemble(lineno_width, is_current_instr, offset_width),
              file=file)","def locate_line_starts(code):
    """"""Find the offsets in a byte code which are start of lines in the source.

    Generate pairs (offset, lineno)
    """"""
    lastline = None
    for start, end, line in code.co_lines():
        if line is not None and line != lastline:
            lastline = line
            yield start, line
    return

def _disassemble_code_bytes(code, lasti=-1, varnames=None, names=None,
                       constants=None, cells=None, linestarts=None,
                       *, file=None, line_offset=0):
    # Omit the line number column entirely if we have no line number info
    show_lineno = bool(linestarts)
    if show_lineno:
        maxlineno = max(linestarts.values()) + line_offset
        if maxlineno >= 1000:
            lineno_width = len(str(maxlineno))
        else:
            lineno_width = 3
    else:
        lineno_width = 0
    maxoffset = len(code) - 2
    if maxoffset >= 10000:
        offset_width = len(str(maxoffset))
    else:
        offset_width = 4
    for instr in _get_instructions_bytes(code, varnames, names,
                                         constants, cells, linestarts,
                                         line_offset=line_offset):
        new_source_line = (show_lineno and
                           instr.starts_line is not None and
                           instr.offset > 0)
        if new_source_line:
            print(file=file)
        is_current_instr = instr.offset == lasti
        print(instr._disassemble(lineno_width, is_current_instr, offset_width),
              file=file)","def disassemble_code_object(co, lasti=-1, *, file=None):
    """"""Disassemble a code object.""""""
    cell_names = co.co_cellvars + co.co_freevars
    linestarts = dict(locate_line_starts(co))
    _disassemble_code_object_code_bytes(co.co_code, lasti, co.co_varnames, co.co_names,
                       co.co_consts, cell_names, linestarts, file=file)","{""findlinestarts"": ""locate_line_starts"", ""_disassemble_bytes"": ""_disassemble_code_bytes""}","def 0(code):
    """"""Find the offsets in a byte code which are start of lines in the source.

    Generate pairs (offset, lineno)
    """"""
    lastline = None
    for start, end, line in code.co_lines():
        if line is not None and line != lastline:
            lastline = line
            yield start, line
    return

def 1(code, lasti=-1, varnames=None, names=None,
                       constants=None, cells=None, linestarts=None,
                       *, file=None, line_offset=0):
    # Omit the line number column entirely if we have no line number info
    show_lineno = bool(linestarts)
    if show_lineno:
        maxlineno = max(linestarts.values()) + line_offset
        if maxlineno >= 1000:
            lineno_width = len(str(maxlineno))
        else:
            lineno_width = 3
    else:
        lineno_width = 0
    maxoffset = len(code) - 2
    if maxoffset >= 10000:
        offset_width = len(str(maxoffset))
    else:
        offset_width = 4
    for instr in _get_instructions_bytes(code, varnames, names,
                                         constants, cells, linestarts,
                                         line_offset=line_offset):
        new_source_line = (show_lineno and
                           instr.starts_line is not None and
                           instr.offset > 0)
        if new_source_line:
            print(file=file)
        is_current_instr = instr.offset == lasti
        print(instr._disassemble(lineno_width, is_current_instr, offset_width),
              file=file)","def 2(co, lasti=-1, *, file=None):
    """"""Disassemble a code object.""""""
    cell_names = co.co_cellvars + co.co_freevars
    linestarts = dict(0(co))
    1(co.co_code, lasti, co.co_varnames, co.co_names,
                       co.co_consts, cell_names, linestarts, file=file)","{""findlinestarts"": ""0"", ""_disassemble_bytes"": ""1""}"
26,26,"def _disassemble_str(source, **kwargs):
    """"""Compile the source string, then disassemble the code object.""""""
    _disassemble_recursive(_try_compile(source, '<dis>'), **kwargs)","def _try_compile(source, name):
    """"""Attempts to compile the given source, first as an expression and
       then as a statement if the first approach fails.

       Utility function to accept strings in functions that otherwise
       expect code objects
    """"""
    try:
        c = compile(source, name, 'eval')
    except SyntaxError:
        c = compile(source, name, 'exec')
    return c

def _disassemble_recursive(co, *, file=None, depth=None):
    disassemble(co, file=file)
    if depth is None or depth > 0:
        if depth is not None:
            depth = depth - 1
        for x in co.co_consts:
            if hasattr(x, 'co_code'):
                print(file=file)
                print(""Disassembly of %r:"" % (x,), file=file)
                _disassemble_recursive(x, file=file, depth=depth)","def attempt_compile(source, name):
    """"""Attempts to compile the given source, first as an expression and
       then as a statement if the first approach fails.

       Utility function to accept strings in functions that otherwise
       expect code objects
    """"""
    try:
        c = compile(source, name, 'eval')
    except SyntaxError:
        c = compile(source, name, 'exec')
    return c

def _recursive_disassembly_of_code_object(co, *, file=None, depth=None):
    disassemble(co, file=file)
    if depth is None or depth > 0:
        if depth is not None:
            depth = depth - 1
        for x in co.co_consts:
            if hasattr(x, 'co_code'):
                print(file=file)
                print(""Disassembly of %r:"" % (x,), file=file)
                _recursive_disassembly_of_code_object(x, file=file, depth=depth)","def compile_and_disassemble_source_string(source, **kwargs):
    """"""Compile the source string, then disassemble the code object.""""""
    _recursive_disassembly_of_code_object(attempt_compile(source, '<dis>'), **kwargs)","{""_try_compile"": ""attempt_compile"", ""_disassemble_recursive"": ""_recursive_disassembly_of_code_object""}","def 0(source, name):
    """"""Attempts to compile the given source, first as an expression and
       then as a statement if the first approach fails.

       Utility function to accept strings in functions that otherwise
       expect code objects
    """"""
    try:
        c = compile(source, name, 'eval')
    except SyntaxError:
        c = compile(source, name, 'exec')
    return c

def 1(co, *, file=None, depth=None):
    disassemble(co, file=file)
    if depth is None or depth > 0:
        if depth is not None:
            depth = depth - 1
        for x in co.co_consts:
            if hasattr(x, 'co_code'):
                print(file=file)
                print(""Disassembly of %r:"" % (x,), file=file)
                1(x, file=file, depth=depth)","def 2(source, **kwargs):
    """"""Compile the source string, then disassemble the code object.""""""
    1(0(source, '<dis>'), **kwargs)","{""_try_compile"": ""0"", ""_disassemble_recursive"": ""1""}"
27,27,"def findlabels(code):
    """"""Detect all offsets in a byte code which are jump targets.

    Return the list of offsets.

    """"""
    labels = []
    for offset, op, arg in _unpack_opargs(code):
        if arg is not None:
            if op in hasjrel:
                label = offset + 2 + arg*2
            elif op in hasjabs:
                label = arg*2
            else:
                continue
            if label not in labels:
                labels.append(label)
    return labels","def _unpack_opargs(code):
    extended_arg = 0
    for i in range(0, len(code), 2):
        op = code[i]
        if op >= HAVE_ARGUMENT:
            arg = code[i+1] | extended_arg
            extended_arg = (arg << 8) if op == EXTENDED_ARG else 0
        else:
            arg = None
            extended_arg = 0
        yield (i, op, arg)","def _unpack_opcode_arguments(code):
    extended_arg = 0
    for i in range(0, len(code), 2):
        op = code[i]
        if op >= HAVE_ARGUMENT:
            arg = code[i+1] | extended_arg
            extended_arg = (arg << 8) if op == EXTENDED_ARG else 0
        else:
            arg = None
            extended_arg = 0
        yield (i, op, arg)","def detect_offsets_of_jump_targets(code):
    """"""Detect all offsets in a byte code which are jump targets.

    Return the list of offsets.

    """"""
    labels = []
    for offset, op, arg in _unpack_opcode_arguments(code):
        if arg is not None:
            if op in hasjrel:
                label = offset + 2 + arg*2
            elif op in hasjabs:
                label = arg*2
            else:
                continue
            if label not in labels:
                labels.append(label)
    return labels","{""_unpack_opargs"": ""_unpack_opcode_arguments""}","def 0(code):
    extended_arg = 0
    for i in range(0, len(code), 2):
        op = code[i]
        if op >= HAVE_ARGUMENT:
            arg = code[i+1] | extended_arg
            extended_arg = (arg << 8) if op == EXTENDED_ARG else 0
        else:
            arg = None
            extended_arg = 0
        yield (i, op, arg)","def 1(code):
    """"""Detect all offsets in a byte code which are jump targets.

    Return the list of offsets.

    """"""
    labels = []
    for offset, op, arg in 0(code):
        if arg is not None:
            if op in hasjrel:
                label = offset + 2 + arg*2
            elif op in hasjabs:
                label = arg*2
            else:
                continue
            if label not in labels:
                labels.append(label)
    return labels","{""_unpack_opargs"": ""0""}"
28,28,"def _copy_archive(archive, new_archive, interpreter=None):
    """"""Copy an application archive, modifying the shebang line.""""""
    with _maybe_open(archive, 'rb') as src:
        # Skip the shebang line from the source.
        # Read 2 bytes of the source and check if they are #!.
        first_2 = src.read(2)
        if first_2 == b'#!':
            # Discard the initial 2 bytes and the rest of the shebang line.
            first_2 = b''
            src.readline()

        with _maybe_open(new_archive, 'wb') as dst:
            _write_file_prefix(dst, interpreter)
            # If there was no shebang, ""first_2"" contains the first 2 bytes
            # of the source file, so write them before copying the rest
            # of the file.
            dst.write(first_2)
            shutil.copyfileobj(src, dst)

    if interpreter and isinstance(new_archive, str):
        os.chmod(new_archive, os.stat(new_archive).st_mode | stat.S_IEXEC)","def _write_file_prefix(f, interpreter):
    """"""Write a shebang line.""""""
    if interpreter:
        shebang = b'#!' + interpreter.encode(shebang_encoding) + b'\n'
        f.write(shebang)","def _write_shebang_line(f, interpreter):
    """"""Write a shebang line.""""""
    if interpreter:
        shebang = b'#!' + interpreter.encode(shebang_encoding) + b'\n'
        f.write(shebang)","def copy_application_archive_with_shebang_modification(archive, new_archive, interpreter=None):
    """"""Copy an application archive, modifying the shebang line.""""""
    with _maybe_open(archive, 'rb') as src:
        # Skip the shebang line from the source.
        # Read 2 bytes of the source and check if they are #!.
        first_2 = src.read(2)
        if first_2 == b'#!':
            # Discard the initial 2 bytes and the rest of the shebang line.
            first_2 = b''
            src.readline()

        with _maybe_open(new_archive, 'wb') as dst:
            _write_shebang_line(dst, interpreter)
            # If there was no shebang, ""first_2"" contains the first 2 bytes
            # of the source file, so write them before copying the rest
            # of the file.
            dst.write(first_2)
            shutil.copyfileobj(src, dst)

    if interpreter and isinstance(new_archive, str):
        os.chmod(new_archive, os.stat(new_archive).st_mode | stat.S_IEXEC)","{""_write_file_prefix"": ""_write_shebang_line""}","def 0(f, interpreter):
    """"""Write a shebang line.""""""
    if interpreter:
        shebang = b'#!' + interpreter.encode(shebang_encoding) + b'\n'
        f.write(shebang)","def 1(archive, new_archive, interpreter=None):
    """"""Copy an application archive, modifying the shebang line.""""""
    with _maybe_open(archive, 'rb') as src:
        # Skip the shebang line from the source.
        # Read 2 bytes of the source and check if they are #!.
        first_2 = src.read(2)
        if first_2 == b'#!':
            # Discard the initial 2 bytes and the rest of the shebang line.
            first_2 = b''
            src.readline()

        with _maybe_open(new_archive, 'wb') as dst:
            0(dst, interpreter)
            # If there was no shebang, ""first_2"" contains the first 2 bytes
            # of the source file, so write them before copying the rest
            # of the file.
            dst.write(first_2)
            shutil.copyfileobj(src, dst)

    if interpreter and isinstance(new_archive, str):
        os.chmod(new_archive, os.stat(new_archive).st_mode | stat.S_IEXEC)","{""_write_file_prefix"": ""0""}"
29,29,"def standard_b64encode(s):
    """"""Encode bytes-like object s using the standard Base64 alphabet.

    The result is returned as a bytes object.
    """"""
    return b64encode(s)","def b64encode(s, altchars=None):
    """"""Encode the bytes-like object s using Base64 and return a bytes object.

    Optional altchars should be a byte string of length 2 which specifies an
    alternative alphabet for the '+' and '/' characters.  This allows an
    application to e.g. generate url or filesystem safe Base64 strings.
    """"""
    encoded = binascii.b2a_base64(s, newline=False)
    if altchars is not None:
        assert len(altchars) == 2, repr(altchars)
        return encoded.translate(bytes.maketrans(b'+/', altchars))
    return encoded","def encode_base64(s, altchars=None):
    """"""Encode the bytes-like object s using Base64 and return a bytes object.

    Optional altchars should be a byte string of length 2 which specifies an
    alternative alphabet for the '+' and '/' characters.  This allows an
    application to e.g. generate url or filesystem safe Base64 strings.
    """"""
    encoded = binascii.b2a_base64(s, newline=False)
    if altchars is not None:
        assert len(altchars) == 2, repr(altchars)
        return encoded.translate(bytes.maketrans(b'+/', altchars))
    return encoded","def standard_encode_base64(s):
    """"""Encode bytes-like object s using the standard Base64 alphabet.

    The result is returned as a bytes object.
    """"""
    return encode_base64(s)","{""b64encode"": ""encode_base64""}","def 0(s, altchars=None):
    """"""Encode the bytes-like object s using Base64 and return a bytes object.

    Optional altchars should be a byte string of length 2 which specifies an
    alternative alphabet for the '+' and '/' characters.  This allows an
    application to e.g. generate url or filesystem safe Base64 strings.
    """"""
    encoded = binascii.b2a_base64(s, newline=False)
    if altchars is not None:
        assert len(altchars) == 2, repr(altchars)
        return encoded.translate(bytes.maketrans(b'+/', altchars))
    return encoded","def standard_0(s):
    """"""Encode bytes-like object s using the standard Base64 alphabet.

    The result is returned as a bytes object.
    """"""
    return 0(s)","{""b64encode"": ""0""}"
30,30,"def standard_b64decode(s):
    """"""Decode bytes encoded with the standard Base64 alphabet.

    Argument s is a bytes-like object or ASCII string to decode.  The result
    is returned as a bytes object.  A binascii.Error is raised if the input
    is incorrectly padded.  Characters that are not in the standard alphabet
    are discarded prior to the padding check.
    """"""
    return b64decode(s)","def b64decode(s, altchars=None, validate=False):
    """"""Decode the Base64 encoded bytes-like object or ASCII string s.

    Optional altchars must be a bytes-like object or ASCII string of length 2
    which specifies the alternative alphabet used instead of the '+' and '/'
    characters.

    The result is returned as a bytes object.  A binascii.Error is raised if
    s is incorrectly padded.

    If validate is False (the default), characters that are neither in the
    normal base-64 alphabet nor the alternative alphabet are discarded prior
    to the padding check.  If validate is True, these non-alphabet characters
    in the input result in a binascii.Error.
    """"""
    s = _bytes_from_decode_data(s)
    if altchars is not None:
        altchars = _bytes_from_decode_data(altchars)
        assert len(altchars) == 2, repr(altchars)
        s = s.translate(bytes.maketrans(altchars, b'+/'))
    if validate and not re.fullmatch(b'[A-Za-z0-9+/]*={0,2}', s):
        raise binascii.Error('Non-base64 digit found')
    return binascii.a2b_base64(s)","def decode_base64(s, altchars=None, validate=False):
    """"""Decode the Base64 encoded bytes-like object or ASCII string s.

    Optional altchars must be a bytes-like object or ASCII string of length 2
    which specifies the alternative alphabet used instead of the '+' and '/'
    characters.

    The result is returned as a bytes object.  A binascii.Error is raised if
    s is incorrectly padded.

    If validate is False (the default), characters that are neither in the
    normal base-64 alphabet nor the alternative alphabet are discarded prior
    to the padding check.  If validate is True, these non-alphabet characters
    in the input result in a binascii.Error.
    """"""
    s = _bytes_from_decode_data(s)
    if altchars is not None:
        altchars = _bytes_from_decode_data(altchars)
        assert len(altchars) == 2, repr(altchars)
        s = s.translate(bytes.maketrans(altchars, b'+/'))
    if validate and not re.fullmatch(b'[A-Za-z0-9+/]*={0,2}', s):
        raise binascii.Error('Non-base64 digit found')
    return binascii.a2b_base64(s)","def standard_decode_base64(s):
    """"""Decode bytes encoded with the standard Base64 alphabet.

    Argument s is a bytes-like object or ASCII string to decode.  The result
    is returned as a bytes object.  A binascii.Error is raised if the input
    is incorrectly padded.  Characters that are not in the standard alphabet
    are discarded prior to the padding check.
    """"""
    return decode_base64(s)","{""b64decode"": ""decode_base64""}","def 0(s, altchars=None, validate=False):
    """"""Decode the Base64 encoded bytes-like object or ASCII string s.

    Optional altchars must be a bytes-like object or ASCII string of length 2
    which specifies the alternative alphabet used instead of the '+' and '/'
    characters.

    The result is returned as a bytes object.  A binascii.Error is raised if
    s is incorrectly padded.

    If validate is False (the default), characters that are neither in the
    normal base-64 alphabet nor the alternative alphabet are discarded prior
    to the padding check.  If validate is True, these non-alphabet characters
    in the input result in a binascii.Error.
    """"""
    s = _bytes_from_decode_data(s)
    if altchars is not None:
        altchars = _bytes_from_decode_data(altchars)
        assert len(altchars) == 2, repr(altchars)
        s = s.translate(bytes.maketrans(altchars, b'+/'))
    if validate and not re.fullmatch(b'[A-Za-z0-9+/]*={0,2}', s):
        raise binascii.Error('Non-base64 digit found')
    return binascii.a2b_base64(s)","def standard_0(s):
    """"""Decode bytes encoded with the standard Base64 alphabet.

    Argument s is a bytes-like object or ASCII string to decode.  The result
    is returned as a bytes object.  A binascii.Error is raised if the input
    is incorrectly padded.  Characters that are not in the standard alphabet
    are discarded prior to the padding check.
    """"""
    return 0(s)","{""b64decode"": ""0""}"
31,31,"def urlsafe_b64encode(data):
    """"""urlsafe_b64encode without padding""""""
    return base64.urlsafe_b64encode(data).rstrip(b'=')","def b64encode(s, altchars=None):
    """"""Encode the bytes-like object s using Base64 and return a bytes object.

    Optional altchars should be a byte string of length 2 which specifies an
    alternative alphabet for the '+' and '/' characters.  This allows an
    application to e.g. generate url or filesystem safe Base64 strings.
    """"""
    encoded = binascii.b2a_base64(s, newline=False)
    if altchars is not None:
        assert len(altchars) == 2, repr(altchars)
        return encoded.translate(bytes.maketrans(b'+/', altchars))
    return encoded","def encode_base64(s, altchars=None):
    """"""Encode the bytes-like object s using Base64 and return a bytes object.

    Optional altchars should be a byte string of length 2 which specifies an
    alternative alphabet for the '+' and '/' characters.  This allows an
    application to e.g. generate url or filesystem safe Base64 strings.
    """"""
    encoded = binascii.b2a_base64(s, newline=False)
    if altchars is not None:
        assert len(altchars) == 2, repr(altchars)
        return encoded.translate(bytes.maketrans(b'+/', altchars))
    return encoded","def urlsafe_encode_base64(data):
    """"""urlsafe_encode_base64 without padding""""""
    return base64.urlsafe_encode_base64(data).rstrip(b'=')","{""b64encode"": ""encode_base64""}","def 0(s, altchars=None):
    """"""Encode the bytes-like object s using Base64 and return a bytes object.

    Optional altchars should be a byte string of length 2 which specifies an
    alternative alphabet for the '+' and '/' characters.  This allows an
    application to e.g. generate url or filesystem safe Base64 strings.
    """"""
    encoded = binascii.b2a_base64(s, newline=False)
    if altchars is not None:
        assert len(altchars) == 2, repr(altchars)
        return encoded.translate(bytes.maketrans(b'+/', altchars))
    return encoded","def urlsafe_0(data):
    """"""urlsafe_b64encode without padding""""""
    return base64.urlsafe_0(data).rstrip(b'=')","{""b64encode"": ""0""}"
32,32,"def urlsafe_b64decode(data):
    """"""urlsafe_b64decode without padding""""""
    pad = b'=' * (4 - (len(data) & 3))
    return base64.urlsafe_b64decode(data + pad)","def b64decode(s, altchars=None, validate=False):
    """"""Decode the Base64 encoded bytes-like object or ASCII string s.

    Optional altchars must be a bytes-like object or ASCII string of length 2
    which specifies the alternative alphabet used instead of the '+' and '/'
    characters.

    The result is returned as a bytes object.  A binascii.Error is raised if
    s is incorrectly padded.

    If validate is False (the default), characters that are neither in the
    normal base-64 alphabet nor the alternative alphabet are discarded prior
    to the padding check.  If validate is True, these non-alphabet characters
    in the input result in a binascii.Error.
    """"""
    s = _bytes_from_decode_data(s)
    if altchars is not None:
        altchars = _bytes_from_decode_data(altchars)
        assert len(altchars) == 2, repr(altchars)
        s = s.translate(bytes.maketrans(altchars, b'+/'))
    if validate and not re.fullmatch(b'[A-Za-z0-9+/]*={0,2}', s):
        raise binascii.Error('Non-base64 digit found')
    return binascii.a2b_base64(s)","def decode_base64(s, altchars=None, validate=False):
    """"""Decode the Base64 encoded bytes-like object or ASCII string s.

    Optional altchars must be a bytes-like object or ASCII string of length 2
    which specifies the alternative alphabet used instead of the '+' and '/'
    characters.

    The result is returned as a bytes object.  A binascii.Error is raised if
    s is incorrectly padded.

    If validate is False (the default), characters that are neither in the
    normal base-64 alphabet nor the alternative alphabet are discarded prior
    to the padding check.  If validate is True, these non-alphabet characters
    in the input result in a binascii.Error.
    """"""
    s = _bytes_from_decode_data(s)
    if altchars is not None:
        altchars = _bytes_from_decode_data(altchars)
        assert len(altchars) == 2, repr(altchars)
        s = s.translate(bytes.maketrans(altchars, b'+/'))
    if validate and not re.fullmatch(b'[A-Za-z0-9+/]*={0,2}', s):
        raise binascii.Error('Non-base64 digit found')
    return binascii.a2b_base64(s)","def urlsafe_decode_base64(data):
    """"""urlsafe_decode_base64 without padding""""""
    pad = b'=' * (4 - (len(data) & 3))
    return base64.urlsafe_decode_base64(data + pad)","{""b64decode"": ""decode_base64""}","def 0(s, altchars=None, validate=False):
    """"""Decode the Base64 encoded bytes-like object or ASCII string s.

    Optional altchars must be a bytes-like object or ASCII string of length 2
    which specifies the alternative alphabet used instead of the '+' and '/'
    characters.

    The result is returned as a bytes object.  A binascii.Error is raised if
    s is incorrectly padded.

    If validate is False (the default), characters that are neither in the
    normal base-64 alphabet nor the alternative alphabet are discarded prior
    to the padding check.  If validate is True, these non-alphabet characters
    in the input result in a binascii.Error.
    """"""
    s = _bytes_from_decode_data(s)
    if altchars is not None:
        altchars = _bytes_from_decode_data(altchars)
        assert len(altchars) == 2, repr(altchars)
        s = s.translate(bytes.maketrans(altchars, b'+/'))
    if validate and not re.fullmatch(b'[A-Za-z0-9+/]*={0,2}', s):
        raise binascii.Error('Non-base64 digit found')
    return binascii.a2b_base64(s)","def urlsafe_0(data):
    """"""urlsafe_b64decode without padding""""""
    pad = b'=' * (4 - (len(data) & 3))
    return base64.urlsafe_0(data + pad)","{""b64decode"": ""0""}"
33,33,"def b16decode(s, casefold=False):
    """"""Decode the Base16 encoded bytes-like object or ASCII string s.

    Optional casefold is a flag specifying whether a lowercase alphabet is
    acceptable as input.  For security purposes, the default is False.

    The result is returned as a bytes object.  A binascii.Error is raised if
    s is incorrectly padded or if there are non-alphabet characters present
    in the input.
    """"""
    s = _bytes_from_decode_data(s)
    if casefold:
        s = s.upper()
    if re.search(b'[^0-9A-F]', s):
        raise binascii.Error('Non-base16 digit found')
    return binascii.unhexlify(s)","def _bytes_from_decode_data(s):
    if isinstance(s, str):
        try:
            return s.encode('ascii')
        except UnicodeEncodeError:
            raise ValueError('string argument should contain only ASCII characters')
    if isinstance(s, bytes_types):
        return s
    try:
        return memoryview(s).tobytes()
    except TypeError:
        raise TypeError(""argument should be a bytes-like object or ASCII ""
                        ""string, not %r"" % s.__class__.__name__) from None","def extract_bytes_from_decode_data(s):
    if isinstance(s, str):
        try:
            return s.encode('ascii')
        except UnicodeEncodeError:
            raise ValueError('string argument should contain only ASCII characters')
    if isinstance(s, bytes_types):
        return s
    try:
        return memoryview(s).tobytes()
    except TypeError:
        raise TypeError(""argument should be a bytes-like object or ASCII ""
                        ""string, not %r"" % s.__class__.__name__) from None","def decode_bytes_with_base16(s, casefold=False):
    """"""Decode the Base16 encoded bytes-like object or ASCII string s.

    Optional casefold is a flag specifying whether a lowercase alphabet is
    acceptable as input.  For security purposes, the default is False.

    The result is returned as a bytes object.  A binascii.Error is raised if
    s is incorrectly padded or if there are non-alphabet characters present
    in the input.
    """"""
    s = extract_bytes_from_decode_data(s)
    if casefold:
        s = s.upper()
    if re.search(b'[^0-9A-F]', s):
        raise binascii.Error('Non-base16 digit found')
    return binascii.unhexlify(s)","{""_bytes_from_decode_data"": ""extract_bytes_from_decode_data""}","def 0(s):
    if isinstance(s, str):
        try:
            return s.encode('ascii')
        except UnicodeEncodeError:
            raise ValueError('string argument should contain only ASCII characters')
    if isinstance(s, bytes_types):
        return s
    try:
        return memoryview(s).tobytes()
    except TypeError:
        raise TypeError(""argument should be a bytes-like object or ASCII ""
                        ""string, not %r"" % s.__class__.__name__) from None","def 1(s, casefold=False):
    """"""Decode the Base16 encoded bytes-like object or ASCII string s.

    Optional casefold is a flag specifying whether a lowercase alphabet is
    acceptable as input.  For security purposes, the default is False.

    The result is returned as a bytes object.  A binascii.Error is raised if
    s is incorrectly padded or if there are non-alphabet characters present
    in the input.
    """"""
    s = 0(s)
    if casefold:
        s = s.upper()
    if re.search(b'[^0-9A-F]', s):
        raise binascii.Error('Non-base16 digit found')
    return binascii.unhexlify(s)","{""_bytes_from_decode_data"": ""0""}"
34,34,"def b85encode(b, pad=False):
    """"""Encode bytes-like object b in base85 format and return a bytes object.

    If pad is true, the input is padded with b'\\0' so its length is a multiple of
    4 bytes before encoding.
    """"""
    global _b85chars, _b85chars2
    # Delay the initialization of tables to not waste memory
    # if the function is never called
    if _b85chars2 is None:
        _b85chars = [bytes((i,)) for i in _b85alphabet]
        _b85chars2 = [(a + b) for a in _b85chars for b in _b85chars]
    return _85encode(b, _b85chars, _b85chars2, pad)","def _85encode(b, chars, chars2, pad=False, foldnuls=False, foldspaces=False):
    # Helper function for a85encode and b85encode
    if not isinstance(b, bytes_types):
        b = memoryview(b).tobytes()

    padding = (-len(b)) % 4
    if padding:
        b = b + b'\0' * padding
    words = struct.Struct('!%dI' % (len(b) // 4)).unpack(b)

    chunks = [b'z' if foldnuls and not word else
              b'y' if foldspaces and word == 0x20202020 else
              (chars2[word // 614125] +
               chars2[word // 85 % 7225] +
               chars[word % 85])
              for word in words]

    if padding and not pad:
        if chunks[-1] == b'z':
            chunks[-1] = chars[0] * 5
        chunks[-1] = chunks[-1][:-padding]

    return b''.join(chunks)","def encode_85(b, chars, chars2, pad=False, foldnuls=False, foldspaces=False):
    # Helper function for a85encode and b85encode
    if not isinstance(b, bytes_types):
        b = memoryview(b).tobytes()

    padding = (-len(b)) % 4
    if padding:
        b = b + b'\0' * padding
    words = struct.Struct('!%dI' % (len(b) // 4)).unpack(b)

    chunks = [b'z' if foldnuls and not word else
              b'y' if foldspaces and word == 0x20202020 else
              (chars2[word // 614125] +
               chars2[word // 85 % 7225] +
               chars[word % 85])
              for word in words]

    if padding and not pad:
        if chunks[-1] == b'z':
            chunks[-1] = chars[0] * 5
        chunks[-1] = chunks[-1][:-padding]

    return b''.join(chunks)","def encode_bytes_in_base85_format(b, pad=False):
    """"""Encode bytes-like object b in base85 format and return a bytes object.

    If pad is true, the input is padded with b'\\0' so its length is a multiple of
    4 bytes before encoding.
    """"""
    global _b85chars, _b85chars2
    # Delay the initialization of tables to not waste memory
    # if the function is never called
    if _b85chars2 is None:
        _b85chars = [bytes((i,)) for i in _b85alphabet]
        _b85chars2 = [(a + b) for a in _b85chars for b in _b85chars]
    return encode_85(b, _b85chars, _b85chars2, pad)","{""_85encode"": ""encode_85""}","def 0(b, chars, chars2, pad=False, foldnuls=False, foldspaces=False):
    # Helper function for a85encode and b85encode
    if not isinstance(b, bytes_types):
        b = memoryview(b).tobytes()

    padding = (-len(b)) % 4
    if padding:
        b = b + b'\0' * padding
    words = struct.Struct('!%dI' % (len(b) // 4)).unpack(b)

    chunks = [b'z' if foldnuls and not word else
              b'y' if foldspaces and word == 0x20202020 else
              (chars2[word // 614125] +
               chars2[word // 85 % 7225] +
               chars[word % 85])
              for word in words]

    if padding and not pad:
        if chunks[-1] == b'z':
            chunks[-1] = chars[0] * 5
        chunks[-1] = chunks[-1][:-padding]

    return b''.join(chunks)","def 1(b, pad=False):
    """"""Encode bytes-like object b in base85 format and return a bytes object.

    If pad is true, the input is padded with b'\\0' so its length is a multiple of
    4 bytes before encoding.
    """"""
    global _b85chars, _b85chars2
    # Delay the initialization of tables to not waste memory
    # if the function is never called
    if _b85chars2 is None:
        _b85chars = [bytes((i,)) for i in _b85alphabet]
        _b85chars2 = [(a + b) for a in _b85chars for b in _b85chars]
    return 0(b, _b85chars, _b85chars2, pad)","{""_85encode"": ""0""}"
35,35,"def encodebytes(s):
    """"""Encode a bytestring into a bytes object containing multiple lines
    of base-64 data.""""""
    _input_type_check(s)
    pieces = []
    for i in range(0, len(s), MAXBINSIZE):
        chunk = s[i : i + MAXBINSIZE]
        pieces.append(binascii.b2a_base64(chunk))
    return b"""".join(pieces)","def _input_type_check(s):
    try:
        m = memoryview(s)
    except TypeError as err:
        msg = ""expected bytes-like object, not %s"" % s.__class__.__name__
        raise TypeError(msg) from err
    if m.format not in ('c', 'b', 'B'):
        msg = (""expected single byte elements, not %r from %s"" %
                                          (m.format, s.__class__.__name__))
        raise TypeError(msg)
    if m.ndim != 1:
        msg = (""expected 1-D data, not %d-D data from %s"" %
                                          (m.ndim, s.__class__.__name__))
        raise TypeError(msg)","def _check_input_type(s):
    try:
        m = memoryview(s)
    except TypeError as err:
        msg = ""expected bytes-like object, not %s"" % s.__class__.__name__
        raise TypeError(msg) from err
    if m.format not in ('c', 'b', 'B'):
        msg = (""expected single byte elements, not %r from %s"" %
                                          (m.format, s.__class__.__name__))
        raise TypeError(msg)
    if m.ndim != 1:
        msg = (""expected 1-D data, not %d-D data from %s"" %
                                          (m.ndim, s.__class__.__name__))
        raise TypeError(msg)","def encode_bytestring_into_base64_data(s):
    """"""Encode a bytestring into a bytes object containing multiple lines
    of base-64 data.""""""
    _check_input_type(s)
    pieces = []
    for i in range(0, len(s), MAXBINSIZE):
        chunk = s[i : i + MAXBINSIZE]
        pieces.append(binascii.b2a_base64(chunk))
    return b"""".join(pieces)","{""_input_type_check"": ""_check_input_type""}","def 0(s):
    try:
        m = memoryview(s)
    except TypeError as err:
        msg = ""expected bytes-like object, not %s"" % s.__class__.__name__
        raise TypeError(msg) from err
    if m.format not in ('c', 'b', 'B'):
        msg = (""expected single byte elements, not %r from %s"" %
                                          (m.format, s.__class__.__name__))
        raise TypeError(msg)
    if m.ndim != 1:
        msg = (""expected 1-D data, not %d-D data from %s"" %
                                          (m.ndim, s.__class__.__name__))
        raise TypeError(msg)","def 1(s):
    """"""Encode a bytestring into a bytes object containing multiple lines
    of base-64 data.""""""
    0(s)
    pieces = []
    for i in range(0, len(s), MAXBINSIZE):
        chunk = s[i : i + MAXBINSIZE]
        pieces.append(binascii.b2a_base64(chunk))
    return b"""".join(pieces)","{""_input_type_check"": ""0""}"
36,36,"def decodebytes(s):
    """"""Decode a bytestring of base-64 data into a bytes object.""""""
    _input_type_check(s)
    return binascii.a2b_base64(s)","def _input_type_check(s):
    try:
        m = memoryview(s)
    except TypeError as err:
        msg = ""expected bytes-like object, not %s"" % s.__class__.__name__
        raise TypeError(msg) from err
    if m.format not in ('c', 'b', 'B'):
        msg = (""expected single byte elements, not %r from %s"" %
                                          (m.format, s.__class__.__name__))
        raise TypeError(msg)
    if m.ndim != 1:
        msg = (""expected 1-D data, not %d-D data from %s"" %
                                          (m.ndim, s.__class__.__name__))
        raise TypeError(msg)","def _check_input_type(s):
    try:
        m = memoryview(s)
    except TypeError as err:
        msg = ""expected bytes-like object, not %s"" % s.__class__.__name__
        raise TypeError(msg) from err
    if m.format not in ('c', 'b', 'B'):
        msg = (""expected single byte elements, not %r from %s"" %
                                          (m.format, s.__class__.__name__))
        raise TypeError(msg)
    if m.ndim != 1:
        msg = (""expected 1-D data, not %d-D data from %s"" %
                                          (m.ndim, s.__class__.__name__))
        raise TypeError(msg)","def decode_base64_data_into_bytestring(s):
    """"""Decode a bytestring of base-64 data into a bytes object.""""""
    _check_input_type(s)
    return binascii.a2b_base64(s)","{""_input_type_check"": ""_check_input_type""}","def 0(s):
    try:
        m = memoryview(s)
    except TypeError as err:
        msg = ""expected bytes-like object, not %s"" % s.__class__.__name__
        raise TypeError(msg) from err
    if m.format not in ('c', 'b', 'B'):
        msg = (""expected single byte elements, not %r from %s"" %
                                          (m.format, s.__class__.__name__))
        raise TypeError(msg)
    if m.ndim != 1:
        msg = (""expected 1-D data, not %d-D data from %s"" %
                                          (m.ndim, s.__class__.__name__))
        raise TypeError(msg)","def 1(s):
    """"""Decode a bytestring of base-64 data into a bytes object.""""""
    0(s)
    return binascii.a2b_base64(s)","{""_input_type_check"": ""0""}"
37,37,"def _aix_bosmp64():
    # type: () -> Tuple[str, int]
    """"""
    Return a Tuple[str, int] e.g., ['7.1.4.34', 1806]
    The fileset bos.mp64 is the AIX kernel. It's VRMF and builddate
    reflect the current ABI levels of the runtime environment.
    """"""
    # We expect all AIX systems to have lslpp installed in this location
    out = subprocess.check_output([""/usr/bin/lslpp"", ""-Lqc"", ""bos.mp64""])
    out = out.decode(""utf-8"")
    out = out.strip().split("":"")  # type: ignore
    # Use str() and int() to help mypy see types
    return (str(out[2]), int(out[-1]))","def str(val):
    """"""Convert float to string, taking the locale into account.""""""
    return _format(""%.12g"", val)","def locale_aware_float_to_string(val):
    """"""Convert float to locale_aware_float_to_stringing, taking the locale into account.""""""
    return _format(""%.12g"", val)","def get_version_and_build_date_of_aix_bosmp64():
    # type: () -> Tuple[locale_aware_float_to_string, int]
    """"""
    Return a Tuple[locale_aware_float_to_string, int] e.g., ['7.1.4.34', 1806]
    The fileset bos.mp64 is the AIX kernel. It's VRMF and builddate
    reflect the current ABI levels of the runtime environment.
    """"""
    # We expect all AIX systems to have lslpp installed in this location
    out = subprocess.check_output([""/usr/bin/lslpp"", ""-Lqc"", ""bos.mp64""])
    out = out.decode(""utf-8"")
    out = out.locale_aware_float_to_stringip().split("":"")  # type: ignore
    # Use locale_aware_float_to_string() and int() to help mypy see types
    return (locale_aware_float_to_string(out[2]), int(out[-1]))","{""str"": ""locale_aware_float_to_string""}","def 0(val):
    """"""Convert float to string, taking the locale into account.""""""
    return _format(""%.12g"", val)","def 1():
    # type: () -> Tuple[str, int]
    """"""
    Return a Tuple[str, int] e.g., ['7.1.4.34', 1806]
    The fileset bos.mp64 is the AIX kernel. It's VRMF and builddate
    reflect the current ABI levels of the runtime environment.
    """"""
    # We expect all AIX systems to have lslpp installed in this location
    out = subprocess.check_output([""/usr/bin/lslpp"", ""-Lqc"", ""bos.mp64""])
    out = out.decode(""utf-8"")
    out = out.strip().split("":"")  # type: ignore
    # Use 0() and int() to help mypy see types
    return (0(out[2]), int(out[-1]))","{""str"": ""0""}"
38,38,"def aix_buildtag():
    # type: () -> str
    """"""
    Return the platform_tag of the system Python was built on.
    """"""
    # AIX_BUILDDATE is defined by configure with:
    # lslpp -Lcq bos.mp64 | awk -F:  '{ print $NF }'
    build_date = sysconfig.get_config_var(""AIX_BUILDDATE"")
    try:
        build_date = int(build_date)
    except (ValueError, TypeError):
        raise ValueError(f""AIX_BUILDDATE is not defined or invalid: ""
                         f""{build_date!r}"")
    return _aix_tag(_aix_bgt(), build_date)","def _aix_tag(vrtl, bd):
    # type: (List[int], int) -> str
    # Infer the ABI bitwidth from maxsize (assuming 64 bit as the default)
    _sz = 32 if sys.maxsize == (2**31-1) else 64
    # vrtl[version, release, technology_level]
    return ""aix-{:1x}{:1d}{:02d}-{:04d}-{}"".format(vrtl[0], vrtl[1], vrtl[2], bd, _sz)

def _aix_bgt():
    # type: () -> List[int]
    gnu_type = sysconfig.get_config_var(""BUILD_GNU_TYPE"")
    if not gnu_type:
        raise ValueError(""BUILD_GNU_TYPE is not defined"")
    return _aix_vrtl(vrmf=gnu_type)","def _infer_abi_bitwidth_for_aix(vrtl, bd):
    # type: (List[int], int) -> str
    # Infer the ABI bitwidth from maxsize (assuming 64 bit as the default)
    _sz = 32 if sys.maxsize == (2**31-1) else 64
    # vrtl[version, release, technology_level]
    return ""aix-{:1x}{:1d}{:02d}-{:04d}-{}"".format(vrtl[0], vrtl[1], vrtl[2], bd, _sz)

def _get_aix_build_gnu_type():
    # type: () -> List[int]
    gnu_type = sysconfig.get_config_var(""BUILD_GNU_TYPE"")
    if not gnu_type:
        raise ValueError(""BUILD_GNU_TYPE is not defined"")
    return _aix_vrtl(vrmf=gnu_type)","def return_platform_tag_of_system_built_on():
    # type: () -> str
    """"""
    Return the platform_tag of the system Python was built on.
    """"""
    # AIX_BUILDDATE is defined by configure with:
    # lslpp -Lcq bos.mp64 | awk -F:  '{ print $NF }'
    build_date = sysconfig.get_config_var(""AIX_BUILDDATE"")
    try:
        build_date = int(build_date)
    except (ValueError, TypeError):
        raise ValueError(f""AIX_BUILDDATE is not defined or invalid: ""
                         f""{build_date!r}"")
    return _infer_abi_bitwidth_for_aix(_get_aix_build_gnu_type(), build_date)","{""_aix_tag"": ""_infer_abi_bitwidth_for_aix"", ""_aix_bgt"": ""_get_aix_build_gnu_type""}","def 0(vrtl, bd):
    # type: (List[int], int) -> str
    # Infer the ABI bitwidth from maxsize (assuming 64 bit as the default)
    _sz = 32 if sys.maxsize == (2**31-1) else 64
    # vrtl[version, release, technology_level]
    return ""aix-{:1x}{:1d}{:02d}-{:04d}-{}"".format(vrtl[0], vrtl[1], vrtl[2], bd, _sz)

def 1():
    # type: () -> List[int]
    gnu_type = sysconfig.get_config_var(""BUILD_GNU_TYPE"")
    if not gnu_type:
        raise ValueError(""BUILD_GNU_TYPE is not defined"")
    return _aix_vrtl(vrmf=gnu_type)","def 2():
    # type: () -> str
    """"""
    Return the platform_tag of the system Python was built on.
    """"""
    # AIX_BUILDDATE is defined by configure with:
    # lslpp -Lcq bos.mp64 | awk -F:  '{ print $NF }'
    build_date = sysconfig.get_config_var(""AIX_BUILDDATE"")
    try:
        build_date = int(build_date)
    except (ValueError, TypeError):
        raise ValueError(f""AIX_BUILDDATE is not defined or invalid: ""
                         f""{build_date!r}"")
    return 0(1(), build_date)","{""_aix_tag"": ""0"", ""_aix_bgt"": ""1""}"
39,39,"def readmodule(module, path=None):
    """"""Return Class objects for the top-level classes in module.

    This is the original interface, before Functions were added.
    """"""

    res = {}
    for key, value in _readmodule(module, path or []).items():
        if isinstance(value, Class):
            res[key] = value
    return res","def _readmodule(module, path, inpackage=None):
    """"""Do the hard work for readmodule[_ex].

    If inpackage is given, it must be the dotted name of the package in
    which we are searching for a submodule, and then PATH must be the
    package search path; otherwise, we are searching for a top-level
    module, and path is combined with sys.path.
    """"""
    # Compute the full module name (prepending inpackage if set).
    if inpackage is not None:
        fullmodule = ""%s.%s"" % (inpackage, module)
    else:
        fullmodule = module

    # Check in the cache.
    if fullmodule in _modules:
        return _modules[fullmodule]

    # Initialize the dict for this module's contents.
    tree = {}

    # Check if it is a built-in module; we don't do much for these.
    if module in sys.builtin_module_names and inpackage is None:
        _modules[module] = tree
        return tree

    # Check for a dotted module name.
    i = module.rfind('.')
    if i >= 0:
        package = module[:i]
        submodule = module[i+1:]
        parent = _readmodule(package, path, inpackage)
        if inpackage is not None:
            package = ""%s.%s"" % (inpackage, package)
        if not '__path__' in parent:
            raise ImportError('No package named {}'.format(package))
        return _readmodule(submodule, parent['__path__'], package)

    # Search the path for the module.
    f = None
    if inpackage is not None:
        search_path = path
    else:
        search_path = path + sys.path
    spec = importlib.util._find_spec_from_path(fullmodule, search_path)
    if spec is None:
        raise ModuleNotFoundError(f""no module named {fullmodule!r}"", name=fullmodule)
    _modules[fullmodule] = tree
    # Is module a package?
    if spec.submodule_search_locations is not None:
        tree['__path__'] = spec.submodule_search_locations
    try:
        source = spec.loader.get_source(fullmodule)
    except (AttributeError, ImportError):
        # If module is not Python source, we cannot do anything.
        return tree
    else:
        if source is None:
            return tree

    fname = spec.loader.get_filename(fullmodule)
    return _create_tree(fullmodule, path, fname, source, tree, inpackage)","def process_module_read(module, path, inpackage=None):
    """"""Do the hard work for readmodule[_ex].

    If inpackage is given, it must be the dotted name of the package in
    which we are searching for a submodule, and then PATH must be the
    package search path; otherwise, we are searching for a top-level
    module, and path is combined with sys.path.
    """"""
    # Compute the full module name (prepending inpackage if set).
    if inpackage is not None:
        fullmodule = ""%s.%s"" % (inpackage, module)
    else:
        fullmodule = module

    # Check in the cache.
    if fullmodule in _modules:
        return _modules[fullmodule]

    # Initialize the dict for this module's contents.
    tree = {}

    # Check if it is a built-in module; we don't do much for these.
    if module in sys.builtin_module_names and inpackage is None:
        _modules[module] = tree
        return tree

    # Check for a dotted module name.
    i = module.rfind('.')
    if i >= 0:
        package = module[:i]
        submodule = module[i+1:]
        parent = process_module_read(package, path, inpackage)
        if inpackage is not None:
            package = ""%s.%s"" % (inpackage, package)
        if not '__path__' in parent:
            raise ImportError('No package named {}'.format(package))
        return process_module_read(submodule, parent['__path__'], package)

    # Search the path for the module.
    f = None
    if inpackage is not None:
        search_path = path
    else:
        search_path = path + sys.path
    spec = importlib.util._find_spec_from_path(fullmodule, search_path)
    if spec is None:
        raise ModuleNotFoundError(f""no module named {fullmodule!r}"", name=fullmodule)
    _modules[fullmodule] = tree
    # Is module a package?
    if spec.submodule_search_locations is not None:
        tree['__path__'] = spec.submodule_search_locations
    try:
        source = spec.loader.get_source(fullmodule)
    except (AttributeError, ImportError):
        # If module is not Python source, we cannot do anything.
        return tree
    else:
        if source is None:
            return tree

    fname = spec.loader.get_filename(fullmodule)
    return _create_tree(fullmodule, path, fname, source, tree, inpackage)","def return_classes_in_module(module, path=None):
    """"""Return Class objects for the top-level classes in module.

    This is the original interface, before Functions were added.
    """"""

    res = {}
    for key, value in process_module_read(module, path or []).items():
        if isinstance(value, Class):
            res[key] = value
    return res","{""_readmodule"": ""process_module_read""}","def 0(module, path, inpackage=None):
    """"""Do the hard work for readmodule[_ex].

    If inpackage is given, it must be the dotted name of the package in
    which we are searching for a submodule, and then PATH must be the
    package search path; otherwise, we are searching for a top-level
    module, and path is combined with sys.path.
    """"""
    # Compute the full module name (prepending inpackage if set).
    if inpackage is not None:
        fullmodule = ""%s.%s"" % (inpackage, module)
    else:
        fullmodule = module

    # Check in the cache.
    if fullmodule in _modules:
        return _modules[fullmodule]

    # Initialize the dict for this module's contents.
    tree = {}

    # Check if it is a built-in module; we don't do much for these.
    if module in sys.builtin_module_names and inpackage is None:
        _modules[module] = tree
        return tree

    # Check for a dotted module name.
    i = module.rfind('.')
    if i >= 0:
        package = module[:i]
        submodule = module[i+1:]
        parent = 0(package, path, inpackage)
        if inpackage is not None:
            package = ""%s.%s"" % (inpackage, package)
        if not '__path__' in parent:
            raise ImportError('No package named {}'.format(package))
        return 0(submodule, parent['__path__'], package)

    # Search the path for the module.
    f = None
    if inpackage is not None:
        search_path = path
    else:
        search_path = path + sys.path
    spec = importlib.util._find_spec_from_path(fullmodule, search_path)
    if spec is None:
        raise ModuleNotFoundError(f""no module named {fullmodule!r}"", name=fullmodule)
    _modules[fullmodule] = tree
    # Is module a package?
    if spec.submodule_search_locations is not None:
        tree['__path__'] = spec.submodule_search_locations
    try:
        source = spec.loader.get_source(fullmodule)
    except (AttributeError, ImportError):
        # If module is not Python source, we cannot do anything.
        return tree
    else:
        if source is None:
            return tree

    fname = spec.loader.get_filename(fullmodule)
    return _create_tree(fullmodule, path, fname, source, tree, inpackage)","def 1(module, path=None):
    """"""Return Class objects for the top-level classes in module.

    This is the original interface, before Functions were added.
    """"""

    res = {}
    for key, value in 0(module, path or []).items():
        if isinstance(value, Class):
            res[key] = value
    return res","{""_readmodule"": ""0""}"
40,40,"def readmodule_ex(module, path=None):
    """"""Return a dictionary with all functions and classes in module.

    Search for module in PATH + sys.path.
    If possible, include imported superclasses.
    Do this by reading source, without importing (and executing) it.
    """"""
    return _readmodule(module, path or [])","def _readmodule(module, path, inpackage=None):
    """"""Do the hard work for readmodule[_ex].

    If inpackage is given, it must be the dotted name of the package in
    which we are searching for a submodule, and then PATH must be the
    package search path; otherwise, we are searching for a top-level
    module, and path is combined with sys.path.
    """"""
    # Compute the full module name (prepending inpackage if set).
    if inpackage is not None:
        fullmodule = ""%s.%s"" % (inpackage, module)
    else:
        fullmodule = module

    # Check in the cache.
    if fullmodule in _modules:
        return _modules[fullmodule]

    # Initialize the dict for this module's contents.
    tree = {}

    # Check if it is a built-in module; we don't do much for these.
    if module in sys.builtin_module_names and inpackage is None:
        _modules[module] = tree
        return tree

    # Check for a dotted module name.
    i = module.rfind('.')
    if i >= 0:
        package = module[:i]
        submodule = module[i+1:]
        parent = _readmodule(package, path, inpackage)
        if inpackage is not None:
            package = ""%s.%s"" % (inpackage, package)
        if not '__path__' in parent:
            raise ImportError('No package named {}'.format(package))
        return _readmodule(submodule, parent['__path__'], package)

    # Search the path for the module.
    f = None
    if inpackage is not None:
        search_path = path
    else:
        search_path = path + sys.path
    spec = importlib.util._find_spec_from_path(fullmodule, search_path)
    if spec is None:
        raise ModuleNotFoundError(f""no module named {fullmodule!r}"", name=fullmodule)
    _modules[fullmodule] = tree
    # Is module a package?
    if spec.submodule_search_locations is not None:
        tree['__path__'] = spec.submodule_search_locations
    try:
        source = spec.loader.get_source(fullmodule)
    except (AttributeError, ImportError):
        # If module is not Python source, we cannot do anything.
        return tree
    else:
        if source is None:
            return tree

    fname = spec.loader.get_filename(fullmodule)
    return _create_tree(fullmodule, path, fname, source, tree, inpackage)","def process_module_read(module, path, inpackage=None):
    """"""Do the hard work for readmodule[_ex].

    If inpackage is given, it must be the dotted name of the package in
    which we are searching for a submodule, and then PATH must be the
    package search path; otherwise, we are searching for a top-level
    module, and path is combined with sys.path.
    """"""
    # Compute the full module name (prepending inpackage if set).
    if inpackage is not None:
        fullmodule = ""%s.%s"" % (inpackage, module)
    else:
        fullmodule = module

    # Check in the cache.
    if fullmodule in _modules:
        return _modules[fullmodule]

    # Initialize the dict for this module's contents.
    tree = {}

    # Check if it is a built-in module; we don't do much for these.
    if module in sys.builtin_module_names and inpackage is None:
        _modules[module] = tree
        return tree

    # Check for a dotted module name.
    i = module.rfind('.')
    if i >= 0:
        package = module[:i]
        submodule = module[i+1:]
        parent = process_module_read(package, path, inpackage)
        if inpackage is not None:
            package = ""%s.%s"" % (inpackage, package)
        if not '__path__' in parent:
            raise ImportError('No package named {}'.format(package))
        return process_module_read(submodule, parent['__path__'], package)

    # Search the path for the module.
    f = None
    if inpackage is not None:
        search_path = path
    else:
        search_path = path + sys.path
    spec = importlib.util._find_spec_from_path(fullmodule, search_path)
    if spec is None:
        raise ModuleNotFoundError(f""no module named {fullmodule!r}"", name=fullmodule)
    _modules[fullmodule] = tree
    # Is module a package?
    if spec.submodule_search_locations is not None:
        tree['__path__'] = spec.submodule_search_locations
    try:
        source = spec.loader.get_source(fullmodule)
    except (AttributeError, ImportError):
        # If module is not Python source, we cannot do anything.
        return tree
    else:
        if source is None:
            return tree

    fname = spec.loader.get_filename(fullmodule)
    return _create_tree(fullmodule, path, fname, source, tree, inpackage)","def retrieve_module_functions_and_classes_without_importing(module, path=None):
    """"""Return a dictionary with all functions and classes in module.

    Search for module in PATH + sys.path.
    If possible, include imported superclasses.
    Do this by reading source, without importing (and executing) it.
    """"""
    return process_module_read(module, path or [])","{""_readmodule"": ""process_module_read""}","def 0(module, path, inpackage=None):
    """"""Do the hard work for readmodule[_ex].

    If inpackage is given, it must be the dotted name of the package in
    which we are searching for a submodule, and then PATH must be the
    package search path; otherwise, we are searching for a top-level
    module, and path is combined with sys.path.
    """"""
    # Compute the full module name (prepending inpackage if set).
    if inpackage is not None:
        fullmodule = ""%s.%s"" % (inpackage, module)
    else:
        fullmodule = module

    # Check in the cache.
    if fullmodule in _modules:
        return _modules[fullmodule]

    # Initialize the dict for this module's contents.
    tree = {}

    # Check if it is a built-in module; we don't do much for these.
    if module in sys.builtin_module_names and inpackage is None:
        _modules[module] = tree
        return tree

    # Check for a dotted module name.
    i = module.rfind('.')
    if i >= 0:
        package = module[:i]
        submodule = module[i+1:]
        parent = 0(package, path, inpackage)
        if inpackage is not None:
            package = ""%s.%s"" % (inpackage, package)
        if not '__path__' in parent:
            raise ImportError('No package named {}'.format(package))
        return 0(submodule, parent['__path__'], package)

    # Search the path for the module.
    f = None
    if inpackage is not None:
        search_path = path
    else:
        search_path = path + sys.path
    spec = importlib.util._find_spec_from_path(fullmodule, search_path)
    if spec is None:
        raise ModuleNotFoundError(f""no module named {fullmodule!r}"", name=fullmodule)
    _modules[fullmodule] = tree
    # Is module a package?
    if spec.submodule_search_locations is not None:
        tree['__path__'] = spec.submodule_search_locations
    try:
        source = spec.loader.get_source(fullmodule)
    except (AttributeError, ImportError):
        # If module is not Python source, we cannot do anything.
        return tree
    else:
        if source is None:
            return tree

    fname = spec.loader.get_filename(fullmodule)
    return _create_tree(fullmodule, path, fname, source, tree, inpackage)","def 1(module, path=None):
    """"""Return a dictionary with all functions and classes in module.

    Search for module in PATH + sys.path.
    If possible, include imported superclasses.
    Do this by reading source, without importing (and executing) it.
    """"""
    return 0(module, path or [])","{""_readmodule"": ""0""}"
41,41,"def insort_right(a, x, lo=0, hi=None, *, key=None):
    """"""Insert item x in list a, and keep it sorted assuming a is sorted.

    If x is already in a, insert it to the right of the rightmost x.

    Optional args lo (default 0) and hi (default len(a)) bound the
    slice of a to be searched.
    """"""
    if key is None:
        lo = bisect_right(a, x, lo, hi)
    else:
        lo = bisect_right(a, key(x), lo, hi, key=key)
    a.insert(lo, x)","def bisect_right(a, x, lo=0, hi=None, *, key=None):
    """"""Return the index where to insert item x in list a, assuming a is sorted.

    The return value i is such that all e in a[:i] have e <= x, and all e in
    a[i:] have e > x.  So if x already appears in the list, a.insert(i, x) will
    insert just after the rightmost x already there.

    Optional args lo (default 0) and hi (default len(a)) bound the
    slice of a to be searched.
    """"""

    if lo < 0:
        raise ValueError('lo must be non-negative')
    if hi is None:
        hi = len(a)
    # Note, the comparison uses ""<"" to match the
    # __lt__() logic in list.sort() and in heapq.
    if key is None:
        while lo < hi:
            mid = (lo + hi) // 2
            if x < a[mid]:
                hi = mid
            else:
                lo = mid + 1
    else:
        while lo < hi:
            mid = (lo + hi) // 2
            if x < key(a[mid]):
                hi = mid
            else:
                lo = mid + 1
    return lo","def find_index_to_insert_right_sorted(a, x, lo=0, hi=None, *, key=None):
    """"""Return the index where to insert item x in list a, assuming a is sorted.

    The return value i is such that all e in a[:i] have e <= x, and all e in
    a[i:] have e > x.  So if x already appears in the list, a.insert(i, x) will
    insert just after the rightmost x already there.

    Optional args lo (default 0) and hi (default len(a)) bound the
    slice of a to be searched.
    """"""

    if lo < 0:
        raise ValueError('lo must be non-negative')
    if hi is None:
        hi = len(a)
    # Note, the comparison uses ""<"" to match the
    # __lt__() logic in list.sort() and in heapq.
    if key is None:
        while lo < hi:
            mid = (lo + hi) // 2
            if x < a[mid]:
                hi = mid
            else:
                lo = mid + 1
    else:
        while lo < hi:
            mid = (lo + hi) // 2
            if x < key(a[mid]):
                hi = mid
            else:
                lo = mid + 1
    return lo","def insert_item_to_right_in_sorted_list_maintaining_order(a, x, lo=0, hi=None, *, key=None):
    """"""Insert item x in list a, and keep it sorted assuming a is sorted.

    If x is already in a, insert it to the right of the rightmost x.

    Optional args lo (default 0) and hi (default len(a)) bound the
    slice of a to be searched.
    """"""
    if key is None:
        lo = find_index_to_insert_right_sorted(a, x, lo, hi)
    else:
        lo = find_index_to_insert_right_sorted(a, key(x), lo, hi, key=key)
    a.insert(lo, x)","{""bisect_right"": ""find_index_to_insert_right_sorted""}","def 0(a, x, lo=0, hi=None, *, key=None):
    """"""Return the index where to insert item x in list a, assuming a is sorted.

    The return value i is such that all e in a[:i] have e <= x, and all e in
    a[i:] have e > x.  So if x already appears in the list, a.insert(i, x) will
    insert just after the rightmost x already there.

    Optional args lo (default 0) and hi (default len(a)) bound the
    slice of a to be searched.
    """"""

    if lo < 0:
        raise ValueError('lo must be non-negative')
    if hi is None:
        hi = len(a)
    # Note, the comparison uses ""<"" to match the
    # __lt__() logic in list.sort() and in heapq.
    if key is None:
        while lo < hi:
            mid = (lo + hi) // 2
            if x < a[mid]:
                hi = mid
            else:
                lo = mid + 1
    else:
        while lo < hi:
            mid = (lo + hi) // 2
            if x < key(a[mid]):
                hi = mid
            else:
                lo = mid + 1
    return lo","def 1(a, x, lo=0, hi=None, *, key=None):
    """"""Insert item x in list a, and keep it sorted assuming a is sorted.

    If x is already in a, insert it to the right of the rightmost x.

    Optional args lo (default 0) and hi (default len(a)) bound the
    slice of a to be searched.
    """"""
    if key is None:
        lo = 0(a, x, lo, hi)
    else:
        lo = 0(a, key(x), lo, hi, key=key)
    a.insert(lo, x)","{""bisect_right"": ""0""}"
42,42,"def insort_left(a, x, lo=0, hi=None, *, key=None):
    """"""Insert item x in list a, and keep it sorted assuming a is sorted.

    If x is already in a, insert it to the left of the leftmost x.

    Optional args lo (default 0) and hi (default len(a)) bound the
    slice of a to be searched.
    """"""

    if key is None:
        lo = bisect_left(a, x, lo, hi)
    else:
        lo = bisect_left(a, key(x), lo, hi, key=key)
    a.insert(lo, x)","def bisect_left(a, x, lo=0, hi=None, *, key=None):
    """"""Return the index where to insert item x in list a, assuming a is sorted.

    The return value i is such that all e in a[:i] have e < x, and all e in
    a[i:] have e >= x.  So if x already appears in the list, a.insert(i, x) will
    insert just before the leftmost x already there.

    Optional args lo (default 0) and hi (default len(a)) bound the
    slice of a to be searched.
    """"""

    if lo < 0:
        raise ValueError('lo must be non-negative')
    if hi is None:
        hi = len(a)
    # Note, the comparison uses ""<"" to match the
    # __lt__() logic in list.sort() and in heapq.
    if key is None:
        while lo < hi:
            mid = (lo + hi) // 2
            if a[mid] < x:
                lo = mid + 1
            else:
                hi = mid
    else:
        while lo < hi:
            mid = (lo + hi) // 2
            if key(a[mid]) < x:
                lo = mid + 1
            else:
                hi = mid
    return lo","def find_index_to_insert_left_sorted(a, x, lo=0, hi=None, *, key=None):
    """"""Return the index where to insert item x in list a, assuming a is sorted.

    The return value i is such that all e in a[:i] have e < x, and all e in
    a[i:] have e >= x.  So if x already appears in the list, a.insert(i, x) will
    insert just before the leftmost x already there.

    Optional args lo (default 0) and hi (default len(a)) bound the
    slice of a to be searched.
    """"""

    if lo < 0:
        raise ValueError('lo must be non-negative')
    if hi is None:
        hi = len(a)
    # Note, the comparison uses ""<"" to match the
    # __lt__() logic in list.sort() and in heapq.
    if key is None:
        while lo < hi:
            mid = (lo + hi) // 2
            if a[mid] < x:
                lo = mid + 1
            else:
                hi = mid
    else:
        while lo < hi:
            mid = (lo + hi) // 2
            if key(a[mid]) < x:
                lo = mid + 1
            else:
                hi = mid
    return lo","def insert_item_to_left_in_sorted_list_maintaining_order(a, x, lo=0, hi=None, *, key=None):
    """"""Insert item x in list a, and keep it sorted assuming a is sorted.

    If x is already in a, insert it to the left of the leftmost x.

    Optional args lo (default 0) and hi (default len(a)) bound the
    slice of a to be searched.
    """"""

    if key is None:
        lo = find_index_to_insert_left_sorted(a, x, lo, hi)
    else:
        lo = find_index_to_insert_left_sorted(a, key(x), lo, hi, key=key)
    a.insert(lo, x)","{""bisect_left"": ""find_index_to_insert_left_sorted""}","def 0(a, x, lo=0, hi=None, *, key=None):
    """"""Return the index where to insert item x in list a, assuming a is sorted.

    The return value i is such that all e in a[:i] have e < x, and all e in
    a[i:] have e >= x.  So if x already appears in the list, a.insert(i, x) will
    insert just before the leftmost x already there.

    Optional args lo (default 0) and hi (default len(a)) bound the
    slice of a to be searched.
    """"""

    if lo < 0:
        raise ValueError('lo must be non-negative')
    if hi is None:
        hi = len(a)
    # Note, the comparison uses ""<"" to match the
    # __lt__() logic in list.sort() and in heapq.
    if key is None:
        while lo < hi:
            mid = (lo + hi) // 2
            if a[mid] < x:
                lo = mid + 1
            else:
                hi = mid
    else:
        while lo < hi:
            mid = (lo + hi) // 2
            if key(a[mid]) < x:
                lo = mid + 1
            else:
                hi = mid
    return lo","def 1(a, x, lo=0, hi=None, *, key=None):
    """"""Insert item x in list a, and keep it sorted assuming a is sorted.

    If x is already in a, insert it to the left of the leftmost x.

    Optional args lo (default 0) and hi (default len(a)) bound the
    slice of a to be searched.
    """"""

    if key is None:
        lo = 0(a, x, lo, hi)
    else:
        lo = 0(a, key(x), lo, hi, key=key)
    a.insert(lo, x)","{""bisect_left"": ""0""}"
43,43,"def heappush(heap, item):
    """"""Push item onto heap, maintaining the heap invariant.""""""
    heap.append(item)
    _siftdown(heap, 0, len(heap)-1)","def _siftdown(heap, startpos, pos):
    newitem = heap[pos]
    # Follow the path to the root, moving parents down until finding a place
    # newitem fits.
    while pos > startpos:
        parentpos = (pos - 1) >> 1
        parent = heap[parentpos]
        if newitem < parent:
            heap[pos] = parent
            pos = parentpos
            continue
        break
    heap[pos] = newitem","def _sift_down_heap(heap, startpos, pos):
    newitem = heap[pos]
    # Follow the path to the root, moving parents down until finding a place
    # newitem fits.
    while pos > startpos:
        parentpos = (pos - 1) >> 1
        parent = heap[parentpos]
        if newitem < parent:
            heap[pos] = parent
            pos = parentpos
            continue
        break
    heap[pos] = newitem","def add_item_to_heap_maintaining_heap_invariant(heap, item):
    """"""Push item onto heap, maintaining the heap invariant.""""""
    heap.append(item)
    _sift_down_heap(heap, 0, len(heap)-1)","{""_siftdown"": ""_sift_down_heap""}","def 0(heap, startpos, pos):
    newitem = heap[pos]
    # Follow the path to the root, moving parents down until finding a place
    # newitem fits.
    while pos > startpos:
        parentpos = (pos - 1) >> 1
        parent = heap[parentpos]
        if newitem < parent:
            heap[pos] = parent
            pos = parentpos
            continue
        break
    heap[pos] = newitem","def 1(heap, item):
    """"""Push item onto heap, maintaining the heap invariant.""""""
    heap.append(item)
    0(heap, 0, len(heap)-1)","{""_siftdown"": ""0""}"
44,44,"def heappop(heap):
    """"""Pop the smallest item off the heap, maintaining the heap invariant.""""""
    lastelt = heap.pop()    # raises appropriate IndexError if heap is empty
    if heap:
        returnitem = heap[0]
        heap[0] = lastelt
        _siftup(heap, 0)
        return returnitem
    return lastelt","def _siftup(heap, pos):
    endpos = len(heap)
    startpos = pos
    newitem = heap[pos]
    # Bubble up the smaller child until hitting a leaf.
    childpos = 2*pos + 1    # leftmost child position
    while childpos < endpos:
        # Set childpos to index of smaller child.
        rightpos = childpos + 1
        if rightpos < endpos and not heap[childpos] < heap[rightpos]:
            childpos = rightpos
        # Move the smaller child up.
        heap[pos] = heap[childpos]
        pos = childpos
        childpos = 2*pos + 1
    # The leaf at pos is empty now.  Put newitem there, and bubble it up
    # to its final resting place (by sifting its parents down).
    heap[pos] = newitem
    _siftdown(heap, startpos, pos)","def _sift_up_heap(heap, pos):
    endpos = len(heap)
    startpos = pos
    newitem = heap[pos]
    # Bubble up the smaller child until hitting a leaf.
    childpos = 2*pos + 1    # leftmost child position
    while childpos < endpos:
        # Set childpos to index of smaller child.
        rightpos = childpos + 1
        if rightpos < endpos and not heap[childpos] < heap[rightpos]:
            childpos = rightpos
        # Move the smaller child up.
        heap[pos] = heap[childpos]
        pos = childpos
        childpos = 2*pos + 1
    # The leaf at pos is empty now.  Put newitem there, and bubble it up
    # to its final resting place (by sifting its parents down).
    heap[pos] = newitem
    _siftdown(heap, startpos, pos)","def remove_and_return_smallest_item_from_heap_maintaining_invariant(heap):
    """"""Pop the smallest item off the heap, maintaining the heap invariant.""""""
    lastelt = heap.pop()    # raises appropriate IndexError if heap is empty
    if heap:
        returnitem = heap[0]
        heap[0] = lastelt
        _sift_up_heap(heap, 0)
        return returnitem
    return lastelt","{""_siftup"": ""_sift_up_heap""}","def 0(heap, pos):
    endpos = len(heap)
    startpos = pos
    newitem = heap[pos]
    # Bubble up the smaller child until hitting a leaf.
    childpos = 2*pos + 1    # leftmost child position
    while childpos < endpos:
        # Set childpos to index of smaller child.
        rightpos = childpos + 1
        if rightpos < endpos and not heap[childpos] < heap[rightpos]:
            childpos = rightpos
        # Move the smaller child up.
        heap[pos] = heap[childpos]
        pos = childpos
        childpos = 2*pos + 1
    # The leaf at pos is empty now.  Put newitem there, and bubble it up
    # to its final resting place (by sifting its parents down).
    heap[pos] = newitem
    _siftdown(heap, startpos, pos)","def 1(heap):
    """"""Pop the smallest item off the heap, maintaining the heap invariant.""""""
    lastelt = heap.pop()    # raises appropriate IndexError if heap is empty
    if heap:
        returnitem = heap[0]
        heap[0] = lastelt
        0(heap, 0)
        return returnitem
    return lastelt","{""_siftup"": ""0""}"
45,45,"def heapreplace(heap, item):
    """"""Pop and return the current smallest value, and add the new item.

    This is more efficient than heappop() followed by heappush(), and can be
    more appropriate when using a fixed-size heap.  Note that the value
    returned may be larger than item!  That constrains reasonable uses of
    this routine unless written as part of a conditional replacement:

        if item > heap[0]:
            item = heapreplace(heap, item)
    """"""
    returnitem = heap[0]    # raises appropriate IndexError if heap is empty
    heap[0] = item
    _siftup(heap, 0)
    return returnitem","def _siftup(heap, pos):
    endpos = len(heap)
    startpos = pos
    newitem = heap[pos]
    # Bubble up the smaller child until hitting a leaf.
    childpos = 2*pos + 1    # leftmost child position
    while childpos < endpos:
        # Set childpos to index of smaller child.
        rightpos = childpos + 1
        if rightpos < endpos and not heap[childpos] < heap[rightpos]:
            childpos = rightpos
        # Move the smaller child up.
        heap[pos] = heap[childpos]
        pos = childpos
        childpos = 2*pos + 1
    # The leaf at pos is empty now.  Put newitem there, and bubble it up
    # to its final resting place (by sifting its parents down).
    heap[pos] = newitem
    _siftdown(heap, startpos, pos)","def _sift_up_heap(heap, pos):
    endpos = len(heap)
    startpos = pos
    newitem = heap[pos]
    # Bubble up the smaller child until hitting a leaf.
    childpos = 2*pos + 1    # leftmost child position
    while childpos < endpos:
        # Set childpos to index of smaller child.
        rightpos = childpos + 1
        if rightpos < endpos and not heap[childpos] < heap[rightpos]:
            childpos = rightpos
        # Move the smaller child up.
        heap[pos] = heap[childpos]
        pos = childpos
        childpos = 2*pos + 1
    # The leaf at pos is empty now.  Put newitem there, and bubble it up
    # to its final resting place (by sifting its parents down).
    heap[pos] = newitem
    _siftdown(heap, startpos, pos)","def pop_and_return_smallest_then_add_new_item_to_heap(heap, item):
    """"""Pop and return the current smallest value, and add the new item.

    This is more efficient than heappop() followed by heappush(), and can be
    more appropriate when using a fixed-size heap.  Note that the value
    returned may be larger than item!  That constrains reasonable uses of
    this routine unless written as part of a conditional replacement:

        if item > heap[0]:
            item = pop_and_return_smallest_then_add_new_item_to_heap(heap, item)
    """"""
    returnitem = heap[0]    # raises appropriate IndexError if heap is empty
    heap[0] = item
    _sift_up_heap(heap, 0)
    return returnitem","{""_siftup"": ""_sift_up_heap""}","def 0(heap, pos):
    endpos = len(heap)
    startpos = pos
    newitem = heap[pos]
    # Bubble up the smaller child until hitting a leaf.
    childpos = 2*pos + 1    # leftmost child position
    while childpos < endpos:
        # Set childpos to index of smaller child.
        rightpos = childpos + 1
        if rightpos < endpos and not heap[childpos] < heap[rightpos]:
            childpos = rightpos
        # Move the smaller child up.
        heap[pos] = heap[childpos]
        pos = childpos
        childpos = 2*pos + 1
    # The leaf at pos is empty now.  Put newitem there, and bubble it up
    # to its final resting place (by sifting its parents down).
    heap[pos] = newitem
    _siftdown(heap, startpos, pos)","def 1(heap, item):
    """"""Pop and return the current smallest value, and add the new item.

    This is more efficient than heappop() followed by heappush(), and can be
    more appropriate when using a fixed-size heap.  Note that the value
    returned may be larger than item!  That constrains reasonable uses of
    this routine unless written as part of a conditional replacement:

        if item > heap[0]:
            item = 1(heap, item)
    """"""
    returnitem = heap[0]    # raises appropriate IndexError if heap is empty
    heap[0] = item
    0(heap, 0)
    return returnitem","{""_siftup"": ""0""}"
46,46,"def heappushpop(heap, item):
    """"""Fast version of a heappush followed by a heappop.""""""
    if heap and heap[0] < item:
        item, heap[0] = heap[0], item
        _siftup(heap, 0)
    return item","def _siftup(heap, pos):
    endpos = len(heap)
    startpos = pos
    newitem = heap[pos]
    # Bubble up the smaller child until hitting a leaf.
    childpos = 2*pos + 1    # leftmost child position
    while childpos < endpos:
        # Set childpos to index of smaller child.
        rightpos = childpos + 1
        if rightpos < endpos and not heap[childpos] < heap[rightpos]:
            childpos = rightpos
        # Move the smaller child up.
        heap[pos] = heap[childpos]
        pos = childpos
        childpos = 2*pos + 1
    # The leaf at pos is empty now.  Put newitem there, and bubble it up
    # to its final resting place (by sifting its parents down).
    heap[pos] = newitem
    _siftdown(heap, startpos, pos)","def _sift_up_heap(heap, pos):
    endpos = len(heap)
    startpos = pos
    newitem = heap[pos]
    # Bubble up the smaller child until hitting a leaf.
    childpos = 2*pos + 1    # leftmost child position
    while childpos < endpos:
        # Set childpos to index of smaller child.
        rightpos = childpos + 1
        if rightpos < endpos and not heap[childpos] < heap[rightpos]:
            childpos = rightpos
        # Move the smaller child up.
        heap[pos] = heap[childpos]
        pos = childpos
        childpos = 2*pos + 1
    # The leaf at pos is empty now.  Put newitem there, and bubble it up
    # to its final resting place (by sifting its parents down).
    heap[pos] = newitem
    _siftdown(heap, startpos, pos)","def fast_version_of_push_followed_by_pop_from_heap(heap, item):
    """"""Fast version of a heappush followed by a heappop.""""""
    if heap and heap[0] < item:
        item, heap[0] = heap[0], item
        _sift_up_heap(heap, 0)
    return item","{""_siftup"": ""_sift_up_heap""}","def 0(heap, pos):
    endpos = len(heap)
    startpos = pos
    newitem = heap[pos]
    # Bubble up the smaller child until hitting a leaf.
    childpos = 2*pos + 1    # leftmost child position
    while childpos < endpos:
        # Set childpos to index of smaller child.
        rightpos = childpos + 1
        if rightpos < endpos and not heap[childpos] < heap[rightpos]:
            childpos = rightpos
        # Move the smaller child up.
        heap[pos] = heap[childpos]
        pos = childpos
        childpos = 2*pos + 1
    # The leaf at pos is empty now.  Put newitem there, and bubble it up
    # to its final resting place (by sifting its parents down).
    heap[pos] = newitem
    _siftdown(heap, startpos, pos)","def 1(heap, item):
    """"""Fast version of a heappush followed by a heappop.""""""
    if heap and heap[0] < item:
        item, heap[0] = heap[0], item
        0(heap, 0)
    return item","{""_siftup"": ""0""}"
47,47,"def heapify(x):
    """"""Transform list into a heap, in-place, in O(len(x)) time.""""""
    n = len(x)
    # Transform bottom-up.  The largest index there's any point to looking at
    # is the largest with a child index in-range, so must have 2*i + 1 < n,
    # or i < (n-1)/2.  If n is even = 2*j, this is (2*j-1)/2 = j-1/2 so
    # j-1 is the largest, which is n//2 - 1.  If n is odd = 2*j+1, this is
    # (2*j+1-1)/2 = j so j-1 is the largest, and that's again n//2-1.
    for i in reversed(range(n//2)):
        _siftup(x, i)","def _siftup(heap, pos):
    endpos = len(heap)
    startpos = pos
    newitem = heap[pos]
    # Bubble up the smaller child until hitting a leaf.
    childpos = 2*pos + 1    # leftmost child position
    while childpos < endpos:
        # Set childpos to index of smaller child.
        rightpos = childpos + 1
        if rightpos < endpos and not heap[childpos] < heap[rightpos]:
            childpos = rightpos
        # Move the smaller child up.
        heap[pos] = heap[childpos]
        pos = childpos
        childpos = 2*pos + 1
    # The leaf at pos is empty now.  Put newitem there, and bubble it up
    # to its final resting place (by sifting its parents down).
    heap[pos] = newitem
    _siftdown(heap, startpos, pos)","def _sift_up_heap(heap, pos):
    endpos = len(heap)
    startpos = pos
    newitem = heap[pos]
    # Bubble up the smaller child until hitting a leaf.
    childpos = 2*pos + 1    # leftmost child position
    while childpos < endpos:
        # Set childpos to index of smaller child.
        rightpos = childpos + 1
        if rightpos < endpos and not heap[childpos] < heap[rightpos]:
            childpos = rightpos
        # Move the smaller child up.
        heap[pos] = heap[childpos]
        pos = childpos
        childpos = 2*pos + 1
    # The leaf at pos is empty now.  Put newitem there, and bubble it up
    # to its final resting place (by sifting its parents down).
    heap[pos] = newitem
    _siftdown(heap, startpos, pos)","def transform_list_into_heap_in_place(x):
    """"""Transform list into a heap, in-place, in O(len(x)) time.""""""
    n = len(x)
    # Transform bottom-up.  The largest index there's any point to looking at
    # is the largest with a child index in-range, so must have 2*i + 1 < n,
    # or i < (n-1)/2.  If n is even = 2*j, this is (2*j-1)/2 = j-1/2 so
    # j-1 is the largest, which is n//2 - 1.  If n is odd = 2*j+1, this is
    # (2*j+1-1)/2 = j so j-1 is the largest, and that's again n//2-1.
    for i in reversed(range(n//2)):
        _sift_up_heap(x, i)","{""_siftup"": ""_sift_up_heap""}","def 0(heap, pos):
    endpos = len(heap)
    startpos = pos
    newitem = heap[pos]
    # Bubble up the smaller child until hitting a leaf.
    childpos = 2*pos + 1    # leftmost child position
    while childpos < endpos:
        # Set childpos to index of smaller child.
        rightpos = childpos + 1
        if rightpos < endpos and not heap[childpos] < heap[rightpos]:
            childpos = rightpos
        # Move the smaller child up.
        heap[pos] = heap[childpos]
        pos = childpos
        childpos = 2*pos + 1
    # The leaf at pos is empty now.  Put newitem there, and bubble it up
    # to its final resting place (by sifting its parents down).
    heap[pos] = newitem
    _siftdown(heap, startpos, pos)","def 1(x):
    """"""Transform list into a heap, in-place, in O(len(x)) time.""""""
    n = len(x)
    # Transform bottom-up.  The largest index there's any point to looking at
    # is the largest with a child index in-range, so must have 2*i + 1 < n,
    # or i < (n-1)/2.  If n is even = 2*j, this is (2*j-1)/2 = j-1/2 so
    # j-1 is the largest, which is n//2 - 1.  If n is odd = 2*j+1, this is
    # (2*j+1-1)/2 = j so j-1 is the largest, and that's again n//2-1.
    for i in reversed(range(n//2)):
        0(x, i)","{""_siftup"": ""0""}"
48,48,"def _heappop_max(heap):
    """"""Maxheap version of a heappop.""""""
    lastelt = heap.pop()    # raises appropriate IndexError if heap is empty
    if heap:
        returnitem = heap[0]
        heap[0] = lastelt
        _siftup_max(heap, 0)
        return returnitem
    return lastelt","def _siftup_max(heap, pos):
    'Maxheap variant of _siftup'
    endpos = len(heap)
    startpos = pos
    newitem = heap[pos]
    # Bubble up the larger child until hitting a leaf.
    childpos = 2*pos + 1    # leftmost child position
    while childpos < endpos:
        # Set childpos to index of larger child.
        rightpos = childpos + 1
        if rightpos < endpos and not heap[rightpos] < heap[childpos]:
            childpos = rightpos
        # Move the larger child up.
        heap[pos] = heap[childpos]
        pos = childpos
        childpos = 2*pos + 1
    # The leaf at pos is empty now.  Put newitem there, and bubble it up
    # to its final resting place (by sifting its parents down).
    heap[pos] = newitem
    _siftdown_max(heap, startpos, pos)","def _sift_up_max_heap(heap, pos):
    'Maxheap variant of _siftup'
    endpos = len(heap)
    startpos = pos
    newitem = heap[pos]
    # Bubble up the larger child until hitting a leaf.
    childpos = 2*pos + 1    # leftmost child position
    while childpos < endpos:
        # Set childpos to index of larger child.
        rightpos = childpos + 1
        if rightpos < endpos and not heap[rightpos] < heap[childpos]:
            childpos = rightpos
        # Move the larger child up.
        heap[pos] = heap[childpos]
        pos = childpos
        childpos = 2*pos + 1
    # The leaf at pos is empty now.  Put newitem there, and bubble it up
    # to its final resting place (by sifting its parents down).
    heap[pos] = newitem
    _siftdown_max(heap, startpos, pos)","def max_heap_version_of_pop_from_heap(heap):
    """"""Maxheap version of a heappop.""""""
    lastelt = heap.pop()    # raises appropriate IndexError if heap is empty
    if heap:
        returnitem = heap[0]
        heap[0] = lastelt
        _sift_up_max_heap(heap, 0)
        return returnitem
    return lastelt","{""_siftup_max"": ""_sift_up_max_heap""}","def 0(heap, pos):
    'Maxheap variant of _siftup'
    endpos = len(heap)
    startpos = pos
    newitem = heap[pos]
    # Bubble up the larger child until hitting a leaf.
    childpos = 2*pos + 1    # leftmost child position
    while childpos < endpos:
        # Set childpos to index of larger child.
        rightpos = childpos + 1
        if rightpos < endpos and not heap[rightpos] < heap[childpos]:
            childpos = rightpos
        # Move the larger child up.
        heap[pos] = heap[childpos]
        pos = childpos
        childpos = 2*pos + 1
    # The leaf at pos is empty now.  Put newitem there, and bubble it up
    # to its final resting place (by sifting its parents down).
    heap[pos] = newitem
    _siftdown_max(heap, startpos, pos)","def 1(heap):
    """"""Maxheap version of a heappop.""""""
    lastelt = heap.pop()    # raises appropriate IndexError if heap is empty
    if heap:
        returnitem = heap[0]
        heap[0] = lastelt
        0(heap, 0)
        return returnitem
    return lastelt","{""_siftup_max"": ""0""}"
49,49,"def _heapreplace_max(heap, item):
    """"""Maxheap version of a heappop followed by a heappush.""""""
    returnitem = heap[0]    # raises appropriate IndexError if heap is empty
    heap[0] = item
    _siftup_max(heap, 0)
    return returnitem","def _siftup_max(heap, pos):
    'Maxheap variant of _siftup'
    endpos = len(heap)
    startpos = pos
    newitem = heap[pos]
    # Bubble up the larger child until hitting a leaf.
    childpos = 2*pos + 1    # leftmost child position
    while childpos < endpos:
        # Set childpos to index of larger child.
        rightpos = childpos + 1
        if rightpos < endpos and not heap[rightpos] < heap[childpos]:
            childpos = rightpos
        # Move the larger child up.
        heap[pos] = heap[childpos]
        pos = childpos
        childpos = 2*pos + 1
    # The leaf at pos is empty now.  Put newitem there, and bubble it up
    # to its final resting place (by sifting its parents down).
    heap[pos] = newitem
    _siftdown_max(heap, startpos, pos)","def _sift_up_max_heap(heap, pos):
    'Maxheap variant of _siftup'
    endpos = len(heap)
    startpos = pos
    newitem = heap[pos]
    # Bubble up the larger child until hitting a leaf.
    childpos = 2*pos + 1    # leftmost child position
    while childpos < endpos:
        # Set childpos to index of larger child.
        rightpos = childpos + 1
        if rightpos < endpos and not heap[rightpos] < heap[childpos]:
            childpos = rightpos
        # Move the larger child up.
        heap[pos] = heap[childpos]
        pos = childpos
        childpos = 2*pos + 1
    # The leaf at pos is empty now.  Put newitem there, and bubble it up
    # to its final resting place (by sifting its parents down).
    heap[pos] = newitem
    _siftdown_max(heap, startpos, pos)","def max_heap_version_of_pop_followed_by_push_to_heap(heap, item):
    """"""Maxheap version of a heappop followed by a heappush.""""""
    returnitem = heap[0]    # raises appropriate IndexError if heap is empty
    heap[0] = item
    _sift_up_max_heap(heap, 0)
    return returnitem","{""_siftup_max"": ""_sift_up_max_heap""}","def 0(heap, pos):
    'Maxheap variant of _siftup'
    endpos = len(heap)
    startpos = pos
    newitem = heap[pos]
    # Bubble up the larger child until hitting a leaf.
    childpos = 2*pos + 1    # leftmost child position
    while childpos < endpos:
        # Set childpos to index of larger child.
        rightpos = childpos + 1
        if rightpos < endpos and not heap[rightpos] < heap[childpos]:
            childpos = rightpos
        # Move the larger child up.
        heap[pos] = heap[childpos]
        pos = childpos
        childpos = 2*pos + 1
    # The leaf at pos is empty now.  Put newitem there, and bubble it up
    # to its final resting place (by sifting its parents down).
    heap[pos] = newitem
    _siftdown_max(heap, startpos, pos)","def 1(heap, item):
    """"""Maxheap version of a heappop followed by a heappush.""""""
    returnitem = heap[0]    # raises appropriate IndexError if heap is empty
    heap[0] = item
    0(heap, 0)
    return returnitem","{""_siftup_max"": ""0""}"
50,50,"def _heapify_max(x):
    """"""Transform list into a maxheap, in-place, in O(len(x)) time.""""""
    n = len(x)
    for i in reversed(range(n//2)):
        _siftup_max(x, i)","def _siftup_max(heap, pos):
    'Maxheap variant of _siftup'
    endpos = len(heap)
    startpos = pos
    newitem = heap[pos]
    # Bubble up the larger child until hitting a leaf.
    childpos = 2*pos + 1    # leftmost child position
    while childpos < endpos:
        # Set childpos to index of larger child.
        rightpos = childpos + 1
        if rightpos < endpos and not heap[rightpos] < heap[childpos]:
            childpos = rightpos
        # Move the larger child up.
        heap[pos] = heap[childpos]
        pos = childpos
        childpos = 2*pos + 1
    # The leaf at pos is empty now.  Put newitem there, and bubble it up
    # to its final resting place (by sifting its parents down).
    heap[pos] = newitem
    _siftdown_max(heap, startpos, pos)","def _sift_up_max_heap(heap, pos):
    'Maxheap variant of _siftup'
    endpos = len(heap)
    startpos = pos
    newitem = heap[pos]
    # Bubble up the larger child until hitting a leaf.
    childpos = 2*pos + 1    # leftmost child position
    while childpos < endpos:
        # Set childpos to index of larger child.
        rightpos = childpos + 1
        if rightpos < endpos and not heap[rightpos] < heap[childpos]:
            childpos = rightpos
        # Move the larger child up.
        heap[pos] = heap[childpos]
        pos = childpos
        childpos = 2*pos + 1
    # The leaf at pos is empty now.  Put newitem there, and bubble it up
    # to its final resting place (by sifting its parents down).
    heap[pos] = newitem
    _siftdown_max(heap, startpos, pos)","def transform_list_into_max_heap_in_place(x):
    """"""Transform list into a maxheap, in-place, in O(len(x)) time.""""""
    n = len(x)
    for i in reversed(range(n//2)):
        _sift_up_max_heap(x, i)","{""_siftup_max"": ""_sift_up_max_heap""}","def 0(heap, pos):
    'Maxheap variant of _siftup'
    endpos = len(heap)
    startpos = pos
    newitem = heap[pos]
    # Bubble up the larger child until hitting a leaf.
    childpos = 2*pos + 1    # leftmost child position
    while childpos < endpos:
        # Set childpos to index of larger child.
        rightpos = childpos + 1
        if rightpos < endpos and not heap[rightpos] < heap[childpos]:
            childpos = rightpos
        # Move the larger child up.
        heap[pos] = heap[childpos]
        pos = childpos
        childpos = 2*pos + 1
    # The leaf at pos is empty now.  Put newitem there, and bubble it up
    # to its final resting place (by sifting its parents down).
    heap[pos] = newitem
    _siftdown_max(heap, startpos, pos)","def 1(x):
    """"""Transform list into a maxheap, in-place, in O(len(x)) time.""""""
    n = len(x)
    for i in reversed(range(n//2)):
        0(x, i)","{""_siftup_max"": ""0""}"
51,51,"def _siftup_max(heap, pos):
    'Maxheap variant of _siftup'
    endpos = len(heap)
    startpos = pos
    newitem = heap[pos]
    # Bubble up the larger child until hitting a leaf.
    childpos = 2*pos + 1    # leftmost child position
    while childpos < endpos:
        # Set childpos to index of larger child.
        rightpos = childpos + 1
        if rightpos < endpos and not heap[rightpos] < heap[childpos]:
            childpos = rightpos
        # Move the larger child up.
        heap[pos] = heap[childpos]
        pos = childpos
        childpos = 2*pos + 1
    # The leaf at pos is empty now.  Put newitem there, and bubble it up
    # to its final resting place (by sifting its parents down).
    heap[pos] = newitem
    _siftdown_max(heap, startpos, pos)","def _siftdown_max(heap, startpos, pos):
    'Maxheap variant of _siftdown'
    newitem = heap[pos]
    # Follow the path to the root, moving parents down until finding a place
    # newitem fits.
    while pos > startpos:
        parentpos = (pos - 1) >> 1
        parent = heap[parentpos]
        if parent < newitem:
            heap[pos] = parent
            pos = parentpos
            continue
        break
    heap[pos] = newitem","def _sift_down_max_heap(heap, startpos, pos):
    'Maxheap variant of _siftdown'
    newitem = heap[pos]
    # Follow the path to the root, moving parents down until finding a place
    # newitem fits.
    while pos > startpos:
        parentpos = (pos - 1) >> 1
        parent = heap[parentpos]
        if parent < newitem:
            heap[pos] = parent
            pos = parentpos
            continue
        break
    heap[pos] = newitem","def _sift_up_max_heap(heap, pos):
    'Maxheap variant of _siftup'
    endpos = len(heap)
    startpos = pos
    newitem = heap[pos]
    # Bubble up the larger child until hitting a leaf.
    childpos = 2*pos + 1    # leftmost child position
    while childpos < endpos:
        # Set childpos to index of larger child.
        rightpos = childpos + 1
        if rightpos < endpos and not heap[rightpos] < heap[childpos]:
            childpos = rightpos
        # Move the larger child up.
        heap[pos] = heap[childpos]
        pos = childpos
        childpos = 2*pos + 1
    # The leaf at pos is empty now.  Put newitem there, and bubble it up
    # to its final resting place (by sifting its parents down).
    heap[pos] = newitem
    _sift_down_max_heap(heap, startpos, pos)","{""_siftdown_max"": ""_sift_down_max_heap""}","def 0(heap, startpos, pos):
    'Maxheap variant of _siftdown'
    newitem = heap[pos]
    # Follow the path to the root, moving parents down until finding a place
    # newitem fits.
    while pos > startpos:
        parentpos = (pos - 1) >> 1
        parent = heap[parentpos]
        if parent < newitem:
            heap[pos] = parent
            pos = parentpos
            continue
        break
    heap[pos] = newitem","def 1(heap, pos):
    'Maxheap variant of _siftup'
    endpos = len(heap)
    startpos = pos
    newitem = heap[pos]
    # Bubble up the larger child until hitting a leaf.
    childpos = 2*pos + 1    # leftmost child position
    while childpos < endpos:
        # Set childpos to index of larger child.
        rightpos = childpos + 1
        if rightpos < endpos and not heap[rightpos] < heap[childpos]:
            childpos = rightpos
        # Move the larger child up.
        heap[pos] = heap[childpos]
        pos = childpos
        childpos = 2*pos + 1
    # The leaf at pos is empty now.  Put newitem there, and bubble it up
    # to its final resting place (by sifting its parents down).
    heap[pos] = newitem
    0(heap, startpos, pos)","{""_siftdown_max"": ""0""}"
52,52,"def reduce(function, sequence, initial=_initial_missing):
    """"""
    reduce(function, iterable[, initial]) -> value

    Apply a function of two arguments cumulatively to the items of a sequence
    or iterable, from left to right, so as to reduce the iterable to a single
    value.  For example, reduce(lambda x, y: x+y, [1, 2, 3, 4, 5]) calculates
    ((((1+2)+3)+4)+5).  If initial is present, it is placed before the items
    of the iterable in the calculation, and serves as a default when the
    iterable is empty.
    """"""

    it = iter(sequence)

    if initial is _initial_missing:
        try:
            value = next(it)
        except StopIteration:
            raise TypeError(
                ""reduce() of empty iterable with no initial value"") from None
    else:
        value = initial

    for element in it:
        value = function(value, element)

    return value","def function(a, b=Foo): pass","def function_with_default_argument_Foo(a, b=Foo): pass","def cumulative_reduction(function_with_default_argument_Foo, sequence, initial=_initial_missing):
    """"""
    cumulative_reduction(function_with_default_argument_Foo, iterable[, initial]) -> value

    Apply a function_with_default_argument_Foo of two arguments cumulatively to the items of a sequence
    or iterable, from left to right, so as to cumulative_reduction the iterable to a single
    value.  For example, cumulative_reduction(lambda x, y: x+y, [1, 2, 3, 4, 5]) calculates
    ((((1+2)+3)+4)+5).  If initial is present, it is placed before the items
    of the iterable in the calculation, and serves as a default when the
    iterable is empty.
    """"""

    it = iter(sequence)

    if initial is _initial_missing:
        try:
            value = next(it)
        except StopIteration:
            raise TypeError(
                ""cumulative_reduction() of empty iterable with no initial value"") from None
    else:
        value = initial

    for element in it:
        value = function_with_default_argument_Foo(value, element)

    return value","{""function"": ""function_with_default_argument_Foo""}","def 0(a, b=Foo): pass","def 1(function, sequence, initial=_initial_missing):
    """"""
    1(function, iterable[, initial]) -> value

    Apply a function of two arguments cumulatively to the items of a sequence
    or iterable, from left to right, so as to reduce the iterable to a single
    value.  For example, 1(lambda x, y: x+y, [1, 2, 3, 4, 5]) calculates
    ((((1+2)+3)+4)+5).  If initial is present, it is placed before the items
    of the iterable in the calculation, and serves as a default when the
    iterable is empty.
    """"""

    it = iter(sequence)

    if initial is _initial_missing:
        try:
            value = next(it)
        except StopIteration:
            raise TypeError(
                ""1() of empty iterable with no initial value"") from None
    else:
        value = initial

    for element in it:
        value = 0(value, element)

    return value","{""function"": ""0""}"
53,53,"def cache(user_function, /):
    'Simple lightweight unbounded cache.  Sometimes called ""memoize"".'
    return lru_cache(maxsize=None)(user_function)","def lru_cache(maxsize=128, typed=False):
    """"""Least-recently-used cache decorator.

    If *maxsize* is set to None, the LRU features are disabled and the cache
    can grow without bound.

    If *typed* is True, arguments of different types will be cached separately.
    For example, f(3.0) and f(3) will be treated as distinct calls with
    distinct results.

    Arguments to the cached function must be hashable.

    View the cache statistics named tuple (hits, misses, maxsize, currsize)
    with f.cache_info().  Clear the cache and statistics with f.cache_clear().
    Access the underlying function with f.__wrapped__.

    See:  https://en.wikipedia.org/wiki/Cache_replacement_policies#Least_recently_used_(LRU)

    """"""

    # Users should only access the lru_cache through its public API:
    #       cache_info, cache_clear, and f.__wrapped__
    # The internals of the lru_cache are encapsulated for thread safety and
    # to allow the implementation to change (including a possible C version).

    if isinstance(maxsize, int):
        # Negative maxsize is treated as 0
        if maxsize < 0:
            maxsize = 0
    elif callable(maxsize) and isinstance(typed, bool):
        # The user_function was passed in directly via the maxsize argument
        user_function, maxsize = maxsize, 128
        wrapper = _lru_cache_wrapper(user_function, maxsize, typed, _CacheInfo)
        wrapper.cache_parameters = lambda : {'maxsize': maxsize, 'typed': typed}
        return update_wrapper(wrapper, user_function)
    elif maxsize is not None:
        raise TypeError(
            'Expected first argument to be an integer, a callable, or None')

    def decorating_function(user_function):
        wrapper = _lru_cache_wrapper(user_function, maxsize, typed, _CacheInfo)
        wrapper.cache_parameters = lambda : {'maxsize': maxsize, 'typed': typed}
        return update_wrapper(wrapper, user_function)

    return decorating_function","def fancy_cache_decorator(maxsize=128, typed=False):
    """"""Least-recently-used cache decorator.

    If *maxsize* is set to None, the LRU features are disabled and the cache
    can grow without bound.

    If *typed* is True, arguments of different types will be cached separately.
    For example, f(3.0) and f(3) will be treated as distinct calls with
    distinct results.

    Arguments to the cached function must be hashable.

    View the cache statistics named tuple (hits, misses, maxsize, currsize)
    with f.cache_info().  Clear the cache and statistics with f.cache_clear().
    Access the underlying function with f.__wrapped__.

    See:  https://en.wikipedia.org/wiki/Cache_replacement_policies#Least_recently_used_(LRU)

    """"""

    # Users should only access the fancy_cache_decorator through its public API:
    #       cache_info, cache_clear, and f.__wrapped__
    # The internals of the fancy_cache_decorator are encapsulated for thread safety and
    # to allow the implementation to change (including a possible C version).

    if isinstance(maxsize, int):
        # Negative maxsize is treated as 0
        if maxsize < 0:
            maxsize = 0
    elif callable(maxsize) and isinstance(typed, bool):
        # The user_function was passed in directly via the maxsize argument
        user_function, maxsize = maxsize, 128
        wrapper = _fancy_cache_decorator_wrapper(user_function, maxsize, typed, _CacheInfo)
        wrapper.cache_parameters = lambda : {'maxsize': maxsize, 'typed': typed}
        return update_wrapper(wrapper, user_function)
    elif maxsize is not None:
        raise TypeError(
            'Expected first argument to be an integer, a callable, or None')

    def decorating_function(user_function):
        wrapper = _fancy_cache_decorator_wrapper(user_function, maxsize, typed, _CacheInfo)
        wrapper.cache_parameters = lambda : {'maxsize': maxsize, 'typed': typed}
        return update_wrapper(wrapper, user_function)

    return decorating_function","def simple_unbounded_cache(user_function, /):
    'Simple lightweight unbounded simple_unbounded_cache.  Sometimes called ""memoize"".'
    return fancy_simple_unbounded_cache_decorator(maxsize=None)(user_function)","{""lru_cache"": ""fancy_cache_decorator""}","def 0(maxsize=128, typed=False):
    """"""Least-recently-used cache decorator.

    If *maxsize* is set to None, the LRU features are disabled and the cache
    can grow without bound.

    If *typed* is True, arguments of different types will be cached separately.
    For example, f(3.0) and f(3) will be treated as distinct calls with
    distinct results.

    Arguments to the cached function must be hashable.

    View the cache statistics named tuple (hits, misses, maxsize, currsize)
    with f.cache_info().  Clear the cache and statistics with f.cache_clear().
    Access the underlying function with f.__wrapped__.

    See:  https://en.wikipedia.org/wiki/Cache_replacement_policies#Least_recently_used_(LRU)

    """"""

    # Users should only access the lru_cache through its public API:
    #       cache_info, cache_clear, and f.__wrapped__
    # The internals of the lru_cache are encapsulated for thread safety and
    # to allow the implementation to change (including a possible C version).

    if isinstance(maxsize, int):
        # Negative maxsize is treated as 0
        if maxsize < 0:
            maxsize = 0
    elif callable(maxsize) and isinstance(typed, bool):
        # The user_function was passed in directly via the maxsize argument
        user_function, maxsize = maxsize, 128
        wrapper = _lru_cache_wrapper(user_function, maxsize, typed, _CacheInfo)
        wrapper.cache_parameters = lambda : {'maxsize': maxsize, 'typed': typed}
        return update_wrapper(wrapper, user_function)
    elif maxsize is not None:
        raise TypeError(
            'Expected first argument to be an integer, a callable, or None')

    def decorating_function(user_function):
        wrapper = _lru_cache_wrapper(user_function, maxsize, typed, _CacheInfo)
        wrapper.cache_parameters = lambda : {'maxsize': maxsize, 'typed': typed}
        return update_wrapper(wrapper, user_function)

    return decorating_function","def 2(user_function, /):
    'Simple lightweight unbounded cache.  Sometimes called ""memoize"".'
    return 0(maxsize=None)(user_function)","{""lru_cache"": ""0""}"
54,54,"def _find_impl(cls, registry):
    """"""Returns the best matching implementation from *registry* for type *cls*.

    Where there is no registered implementation for a specific type, its method
    resolution order is used to find a more generic implementation.

    Note: if *registry* does not contain an implementation for the base
    *object* type, this function may return None.

    """"""
    mro = _compose_mro(cls, registry.keys())
    match = None
    for t in mro:
        if match is not None:
            # If *match* is an implicit ABC but there is another unrelated,
            # equally matching implicit ABC, refuse the temptation to guess.
            if (t in registry and t not in cls.__mro__
                              and match not in cls.__mro__
                              and not issubclass(match, t)):
                raise RuntimeError(""Ambiguous dispatch: {} or {}"".format(
                    match, t))
            break
        if t in registry:
            match = t
    return registry.get(match)","def _compose_mro(cls, types):
    """"""Calculates the method resolution order for a given class *cls*.

    Includes relevant abstract base classes (with their respective bases) from
    the *types* iterable. Uses a modified C3 linearization algorithm.

    """"""
    bases = set(cls.__mro__)
    # Remove entries which are already present in the __mro__ or unrelated.
    def is_related(typ):
        return (typ not in bases and hasattr(typ, '__mro__')
                                 and not isinstance(typ, GenericAlias)
                                 and issubclass(cls, typ))
    types = [n for n in types if is_related(n)]
    # Remove entries which are strict bases of other entries (they will end up
    # in the MRO anyway.
    def is_strict_base(typ):
        for other in types:
            if typ != other and typ in other.__mro__:
                return True
        return False
    types = [n for n in types if not is_strict_base(n)]
    # Subclasses of the ABCs in *types* which are also implemented by
    # *cls* can be used to stabilize ABC ordering.
    type_set = set(types)
    mro = []
    for typ in types:
        found = []
        for sub in typ.__subclasses__():
            if sub not in bases and issubclass(cls, sub):
                found.append([s for s in sub.__mro__ if s in type_set])
        if not found:
            mro.append(typ)
            continue
        # Favor subclasses with the biggest number of useful bases
        found.sort(key=len, reverse=True)
        for sub in found:
            for subcls in sub:
                if subcls not in mro:
                    mro.append(subcls)
    return _c3_mro(cls, abcs=mro)","def _calculate_method_resolution_order(cls, types):
    """"""Calculates the method resolution order for a given class *cls*.

    Includes relevant abstract base classes (with their respective bases) from
    the *types* iterable. Uses a modified C3 linearization algorithm.

    """"""
    bases = set(cls.__mro__)
    # Remove entries which are already present in the __mro__ or unrelated.
    def is_related(typ):
        return (typ not in bases and hasattr(typ, '__mro__')
                                 and not isinstance(typ, GenericAlias)
                                 and issubclass(cls, typ))
    types = [n for n in types if is_related(n)]
    # Remove entries which are strict bases of other entries (they will end up
    # in the MRO anyway.
    def is_strict_base(typ):
        for other in types:
            if typ != other and typ in other.__mro__:
                return True
        return False
    types = [n for n in types if not is_strict_base(n)]
    # Subclasses of the ABCs in *types* which are also implemented by
    # *cls* can be used to stabilize ABC ordering.
    type_set = set(types)
    mro = []
    for typ in types:
        found = []
        for sub in typ.__subclasses__():
            if sub not in bases and issubclass(cls, sub):
                found.append([s for s in sub.__mro__ if s in type_set])
        if not found:
            mro.append(typ)
            continue
        # Favor subclasses with the biggest number of useful bases
        found.sort(key=len, reverse=True)
        for sub in found:
            for subcls in sub:
                if subcls not in mro:
                    mro.append(subcls)
    return _c3_mro(cls, abcs=mro)","def find_best_matching_implementation_from_registry_for_type(cls, registry):
    """"""Returns the best matching implementation from *registry* for type *cls*.

    Where there is no registered implementation for a specific type, its method
    resolution order is used to find a more generic implementation.

    Note: if *registry* does not contain an implementation for the base
    *object* type, this function may return None.

    """"""
    mro = _calculate_method_resolution_order(cls, registry.keys())
    match = None
    for t in mro:
        if match is not None:
            # If *match* is an implicit ABC but there is another unrelated,
            # equally matching implicit ABC, refuse the temptation to guess.
            if (t in registry and t not in cls.__mro__
                              and match not in cls.__mro__
                              and not issubclass(match, t)):
                raise RuntimeError(""Ambiguous dispatch: {} or {}"".format(
                    match, t))
            break
        if t in registry:
            match = t
    return registry.get(match)","{""_compose_mro"": ""_calculate_method_resolution_order""}","def 0(cls, types):
    """"""Calculates the method resolution order for a given class *cls*.

    Includes relevant abstract base classes (with their respective bases) from
    the *types* iterable. Uses a modified C3 linearization algorithm.

    """"""
    bases = set(cls.__mro__)
    # Remove entries which are already present in the __mro__ or unrelated.
    def is_related(typ):
        return (typ not in bases and hasattr(typ, '__mro__')
                                 and not isinstance(typ, GenericAlias)
                                 and issubclass(cls, typ))
    types = [n for n in types if is_related(n)]
    # Remove entries which are strict bases of other entries (they will end up
    # in the MRO anyway.
    def is_strict_base(typ):
        for other in types:
            if typ != other and typ in other.__mro__:
                return True
        return False
    types = [n for n in types if not is_strict_base(n)]
    # Subclasses of the ABCs in *types* which are also implemented by
    # *cls* can be used to stabilize ABC ordering.
    type_set = set(types)
    mro = []
    for typ in types:
        found = []
        for sub in typ.__subclasses__():
            if sub not in bases and issubclass(cls, sub):
                found.append([s for s in sub.__mro__ if s in type_set])
        if not found:
            mro.append(typ)
            continue
        # Favor subclasses with the biggest number of useful bases
        found.sort(key=len, reverse=True)
        for sub in found:
            for subcls in sub:
                if subcls not in mro:
                    mro.append(subcls)
    return _c3_mro(cls, abcs=mro)","def 3(cls, registry):
    """"""Returns the best matching implementation from *registry* for type *cls*.

    Where there is no registered implementation for a specific type, its method
    resolution order is used to find a more generic implementation.

    Note: if *registry* does not contain an implementation for the base
    *object* type, this function may return None.

    """"""
    mro = 0(cls, registry.keys())
    match = None
    for t in mro:
        if match is not None:
            # If *match* is an implicit ABC but there is another unrelated,
            # equally matching implicit ABC, refuse the temptation to guess.
            if (t in registry and t not in cls.__mro__
                              and match not in cls.__mro__
                              and not issubclass(match, t)):
                raise RuntimeError(""Ambiguous dispatch: {} or {}"".format(
                    match, t))
            break
        if t in registry:
            match = t
    return registry.get(match)","{""_compose_mro"": ""0""}"
55,55,"def __py_new(name, data=b'', **kwargs):
    """"""new(name, data=b'', **kwargs) - Return a new hashing object using the
    named algorithm; optionally initialized with data (which must be
    a bytes-like object).
    """"""
    return __get_builtin_constructor(name)(data, **kwargs)","def __get_builtin_constructor(name):
    cache = __builtin_constructor_cache
    constructor = cache.get(name)
    if constructor is not None:
        return constructor
    try:
        if name in {'SHA1', 'sha1'}:
            import _sha1
            cache['SHA1'] = cache['sha1'] = _sha1.sha1
        elif name in {'MD5', 'md5'}:
            import _md5
            cache['MD5'] = cache['md5'] = _md5.md5
        elif name in {'SHA256', 'sha256', 'SHA224', 'sha224'}:
            import _sha256
            cache['SHA224'] = cache['sha224'] = _sha256.sha224
            cache['SHA256'] = cache['sha256'] = _sha256.sha256
        elif name in {'SHA512', 'sha512', 'SHA384', 'sha384'}:
            import _sha512
            cache['SHA384'] = cache['sha384'] = _sha512.sha384
            cache['SHA512'] = cache['sha512'] = _sha512.sha512
        elif name in {'blake2b', 'blake2s'}:
            import _blake2
            cache['blake2b'] = _blake2.blake2b
            cache['blake2s'] = _blake2.blake2s
        elif name in {'sha3_224', 'sha3_256', 'sha3_384', 'sha3_512'}:
            import _sha3
            cache['sha3_224'] = _sha3.sha3_224
            cache['sha3_256'] = _sha3.sha3_256
            cache['sha3_384'] = _sha3.sha3_384
            cache['sha3_512'] = _sha3.sha3_512
        elif name in {'shake_128', 'shake_256'}:
            import _sha3
            cache['shake_128'] = _sha3.shake_128
            cache['shake_256'] = _sha3.shake_256
    except ImportError:
        pass  # no extension module, this hash is unsupported.

    constructor = cache.get(name)
    if constructor is not None:
        return constructor

    raise ValueError('unsupported hash type ' + name)","def fetch_builtin_constructor(name):
    cache = __builtin_constructor_cache
    constructor = cache.get(name)
    if constructor is not None:
        return constructor
    try:
        if name in {'SHA1', 'sha1'}:
            import _sha1
            cache['SHA1'] = cache['sha1'] = _sha1.sha1
        elif name in {'MD5', 'md5'}:
            import _md5
            cache['MD5'] = cache['md5'] = _md5.md5
        elif name in {'SHA256', 'sha256', 'SHA224', 'sha224'}:
            import _sha256
            cache['SHA224'] = cache['sha224'] = _sha256.sha224
            cache['SHA256'] = cache['sha256'] = _sha256.sha256
        elif name in {'SHA512', 'sha512', 'SHA384', 'sha384'}:
            import _sha512
            cache['SHA384'] = cache['sha384'] = _sha512.sha384
            cache['SHA512'] = cache['sha512'] = _sha512.sha512
        elif name in {'blake2b', 'blake2s'}:
            import _blake2
            cache['blake2b'] = _blake2.blake2b
            cache['blake2s'] = _blake2.blake2s
        elif name in {'sha3_224', 'sha3_256', 'sha3_384', 'sha3_512'}:
            import _sha3
            cache['sha3_224'] = _sha3.sha3_224
            cache['sha3_256'] = _sha3.sha3_256
            cache['sha3_384'] = _sha3.sha3_384
            cache['sha3_512'] = _sha3.sha3_512
        elif name in {'shake_128', 'shake_256'}:
            import _sha3
            cache['shake_128'] = _sha3.shake_128
            cache['shake_256'] = _sha3.shake_256
    except ImportError:
        pass  # no extension module, this hash is unsupported.

    constructor = cache.get(name)
    if constructor is not None:
        return constructor

    raise ValueError('unsupported hash type ' + name)","def create_new_hashing_object_with_named_algorithm(name, data=b'', **kwargs):
    """"""new(name, data=b'', **kwargs) - Return a new hashing object using the
    named algorithm; optionally initialized with data (which must be
    a bytes-like object).
    """"""
    return fetch_builtin_constructor(name)(data, **kwargs)","{""__get_builtin_constructor"": ""fetch_builtin_constructor""}","def 0(name):
    cache = __builtin_constructor_cache
    constructor = cache.get(name)
    if constructor is not None:
        return constructor
    try:
        if name in {'SHA1', 'sha1'}:
            import _sha1
            cache['SHA1'] = cache['sha1'] = _sha1.sha1
        elif name in {'MD5', 'md5'}:
            import _md5
            cache['MD5'] = cache['md5'] = _md5.md5
        elif name in {'SHA256', 'sha256', 'SHA224', 'sha224'}:
            import _sha256
            cache['SHA224'] = cache['sha224'] = _sha256.sha224
            cache['SHA256'] = cache['sha256'] = _sha256.sha256
        elif name in {'SHA512', 'sha512', 'SHA384', 'sha384'}:
            import _sha512
            cache['SHA384'] = cache['sha384'] = _sha512.sha384
            cache['SHA512'] = cache['sha512'] = _sha512.sha512
        elif name in {'blake2b', 'blake2s'}:
            import _blake2
            cache['blake2b'] = _blake2.blake2b
            cache['blake2s'] = _blake2.blake2s
        elif name in {'sha3_224', 'sha3_256', 'sha3_384', 'sha3_512'}:
            import _sha3
            cache['sha3_224'] = _sha3.sha3_224
            cache['sha3_256'] = _sha3.sha3_256
            cache['sha3_384'] = _sha3.sha3_384
            cache['sha3_512'] = _sha3.sha3_512
        elif name in {'shake_128', 'shake_256'}:
            import _sha3
            cache['shake_128'] = _sha3.shake_128
            cache['shake_256'] = _sha3.shake_256
    except ImportError:
        pass  # no extension module, this hash is unsupported.

    constructor = cache.get(name)
    if constructor is not None:
        return constructor

    raise ValueError('unsupported hash type ' + name)","def 1(name, data=b'', **kwargs):
    """"""new(name, data=b'', **kwargs) - Return a new hashing object using the
    named algorithm; optionally initialized with data (which must be
    a bytes-like object).
    """"""
    return 0(name)(data, **kwargs)","{""__get_builtin_constructor"": ""0""}"
56,56,"def __hash_new(name, data=b'', **kwargs):
    """"""new(name, data=b'') - Return a new hashing object using the named algorithm;
    optionally initialized with data (which must be a bytes-like object).
    """"""
    if name in __block_openssl_constructor:
        # Prefer our builtin blake2 implementation.
        return __get_builtin_constructor(name)(data, **kwargs)
    try:
        return _hashlib.new(name, data, **kwargs)
    except ValueError:
        # If the _hashlib module (OpenSSL) doesn't support the named
        # hash, try using our builtin implementations.
        # This allows for SHA224/256 and SHA384/512 support even though
        # the OpenSSL library prior to 0.9.8 doesn't provide them.
        return __get_builtin_constructor(name)(data)","def __get_builtin_constructor(name):
    cache = __builtin_constructor_cache
    constructor = cache.get(name)
    if constructor is not None:
        return constructor
    try:
        if name in {'SHA1', 'sha1'}:
            import _sha1
            cache['SHA1'] = cache['sha1'] = _sha1.sha1
        elif name in {'MD5', 'md5'}:
            import _md5
            cache['MD5'] = cache['md5'] = _md5.md5
        elif name in {'SHA256', 'sha256', 'SHA224', 'sha224'}:
            import _sha256
            cache['SHA224'] = cache['sha224'] = _sha256.sha224
            cache['SHA256'] = cache['sha256'] = _sha256.sha256
        elif name in {'SHA512', 'sha512', 'SHA384', 'sha384'}:
            import _sha512
            cache['SHA384'] = cache['sha384'] = _sha512.sha384
            cache['SHA512'] = cache['sha512'] = _sha512.sha512
        elif name in {'blake2b', 'blake2s'}:
            import _blake2
            cache['blake2b'] = _blake2.blake2b
            cache['blake2s'] = _blake2.blake2s
        elif name in {'sha3_224', 'sha3_256', 'sha3_384', 'sha3_512'}:
            import _sha3
            cache['sha3_224'] = _sha3.sha3_224
            cache['sha3_256'] = _sha3.sha3_256
            cache['sha3_384'] = _sha3.sha3_384
            cache['sha3_512'] = _sha3.sha3_512
        elif name in {'shake_128', 'shake_256'}:
            import _sha3
            cache['shake_128'] = _sha3.shake_128
            cache['shake_256'] = _sha3.shake_256
    except ImportError:
        pass  # no extension module, this hash is unsupported.

    constructor = cache.get(name)
    if constructor is not None:
        return constructor

    raise ValueError('unsupported hash type ' + name)","def fetch_builtin_constructor(name):
    cache = __builtin_constructor_cache
    constructor = cache.get(name)
    if constructor is not None:
        return constructor
    try:
        if name in {'SHA1', 'sha1'}:
            import _sha1
            cache['SHA1'] = cache['sha1'] = _sha1.sha1
        elif name in {'MD5', 'md5'}:
            import _md5
            cache['MD5'] = cache['md5'] = _md5.md5
        elif name in {'SHA256', 'sha256', 'SHA224', 'sha224'}:
            import _sha256
            cache['SHA224'] = cache['sha224'] = _sha256.sha224
            cache['SHA256'] = cache['sha256'] = _sha256.sha256
        elif name in {'SHA512', 'sha512', 'SHA384', 'sha384'}:
            import _sha512
            cache['SHA384'] = cache['sha384'] = _sha512.sha384
            cache['SHA512'] = cache['sha512'] = _sha512.sha512
        elif name in {'blake2b', 'blake2s'}:
            import _blake2
            cache['blake2b'] = _blake2.blake2b
            cache['blake2s'] = _blake2.blake2s
        elif name in {'sha3_224', 'sha3_256', 'sha3_384', 'sha3_512'}:
            import _sha3
            cache['sha3_224'] = _sha3.sha3_224
            cache['sha3_256'] = _sha3.sha3_256
            cache['sha3_384'] = _sha3.sha3_384
            cache['sha3_512'] = _sha3.sha3_512
        elif name in {'shake_128', 'shake_256'}:
            import _sha3
            cache['shake_128'] = _sha3.shake_128
            cache['shake_256'] = _sha3.shake_256
    except ImportError:
        pass  # no extension module, this hash is unsupported.

    constructor = cache.get(name)
    if constructor is not None:
        return constructor

    raise ValueError('unsupported hash type ' + name)","def create_new_hashing_object_with_named_algorithm(name, data=b'', **kwargs):
    """"""new(name, data=b'') - Return a new hashing object using the named algorithm;
    optionally initialized with data (which must be a bytes-like object).
    """"""
    if name in __block_openssl_constructor:
        # Prefer our builtin blake2 implementation.
        return fetch_builtin_constructor(name)(data, **kwargs)
    try:
        return _hashlib.new(name, data, **kwargs)
    except ValueError:
        # If the _hashlib module (OpenSSL) doesn't support the named
        # hash, try using our builtin implementations.
        # This allows for SHA224/256 and SHA384/512 support even though
        # the OpenSSL library prior to 0.9.8 doesn't provide them.
        return fetch_builtin_constructor(name)(data)","{""__get_builtin_constructor"": ""fetch_builtin_constructor""}","def 0(name):
    cache = __builtin_constructor_cache
    constructor = cache.get(name)
    if constructor is not None:
        return constructor
    try:
        if name in {'SHA1', 'sha1'}:
            import _sha1
            cache['SHA1'] = cache['sha1'] = _sha1.sha1
        elif name in {'MD5', 'md5'}:
            import _md5
            cache['MD5'] = cache['md5'] = _md5.md5
        elif name in {'SHA256', 'sha256', 'SHA224', 'sha224'}:
            import _sha256
            cache['SHA224'] = cache['sha224'] = _sha256.sha224
            cache['SHA256'] = cache['sha256'] = _sha256.sha256
        elif name in {'SHA512', 'sha512', 'SHA384', 'sha384'}:
            import _sha512
            cache['SHA384'] = cache['sha384'] = _sha512.sha384
            cache['SHA512'] = cache['sha512'] = _sha512.sha512
        elif name in {'blake2b', 'blake2s'}:
            import _blake2
            cache['blake2b'] = _blake2.blake2b
            cache['blake2s'] = _blake2.blake2s
        elif name in {'sha3_224', 'sha3_256', 'sha3_384', 'sha3_512'}:
            import _sha3
            cache['sha3_224'] = _sha3.sha3_224
            cache['sha3_256'] = _sha3.sha3_256
            cache['sha3_384'] = _sha3.sha3_384
            cache['sha3_512'] = _sha3.sha3_512
        elif name in {'shake_128', 'shake_256'}:
            import _sha3
            cache['shake_128'] = _sha3.shake_128
            cache['shake_256'] = _sha3.shake_256
    except ImportError:
        pass  # no extension module, this hash is unsupported.

    constructor = cache.get(name)
    if constructor is not None:
        return constructor

    raise ValueError('unsupported hash type ' + name)","def 1(name, data=b'', **kwargs):
    """"""new(name, data=b'') - Return a new hashing object using the named algorithm;
    optionally initialized with data (which must be a bytes-like object).
    """"""
    if name in __block_openssl_constructor:
        # Prefer our builtin blake2 implementation.
        return 0(name)(data, **kwargs)
    try:
        return _hashlib.new(name, data, **kwargs)
    except ValueError:
        # If the _hashlib module (OpenSSL) doesn't support the named
        # hash, try using our builtin implementations.
        # This allows for SHA224/256 and SHA384/512 support even though
        # the OpenSSL library prior to 0.9.8 doesn't provide them.
        return 0(name)(data)","{""__get_builtin_constructor"": ""0""}"
57,57,"def compile_command(source, filename=""<input>"", symbol=""single""):
    r""""""Compile a command and determine whether it is incomplete.

    Arguments:

    source -- the source string; may contain \n characters
    filename -- optional filename from which source was read; default
                ""<input>""
    symbol -- optional grammar start symbol; ""single"" (default), ""exec""
              or ""eval""

    Return value / exceptions raised:

    - Return a code object if the command is complete and valid
    - Return None if the command is incomplete
    - Raise SyntaxError, ValueError or OverflowError if the command is a
      syntax error (OverflowError and ValueError can be produced by
      malformed literals).
    """"""
    return _maybe_compile(_compile, source, filename, symbol)","def _maybe_compile(compiler, source, filename, symbol):
    # Check for source consisting of only blank lines and comments.
    for line in source.split(""\n""):
        line = line.strip()
        if line and line[0] != '#':
            break               # Leave it alone.
    else:
        if symbol != ""eval"":
            source = ""pass""     # Replace it with a 'pass' statement

    try:
        return compiler(source, filename, symbol)
    except SyntaxError:  # Let other compile() errors propagate.
        pass

    # Catch syntax warnings after the first compile
    # to emit warnings (SyntaxWarning, DeprecationWarning) at most once.
    with warnings.catch_warnings():
        warnings.simplefilter(""error"")

        try:
            compiler(source + ""\n"", filename, symbol)
        except SyntaxError as e:
            if ""incomplete input"" in str(e):
                return None
            raise","def potentially_compile(compiler, source, filename, symbol):
    # Check for source consisting of only blank lines and comments.
    for line in source.split(""\n""):
        line = line.strip()
        if line and line[0] != '#':
            break               # Leave it alone.
    else:
        if symbol != ""eval"":
            source = ""pass""     # Replace it with a 'pass' statement

    try:
        return compiler(source, filename, symbol)
    except SyntaxError:  # Let other compile() errors propagate.
        pass

    # Catch syntax warnings after the first compile
    # to emit warnings (SyntaxWarning, DeprecationWarning) at most once.
    with warnings.catch_warnings():
        warnings.simplefilter(""error"")

        try:
            compiler(source + ""\n"", filename, symbol)
        except SyntaxError as e:
            if ""incomplete input"" in str(e):
                return None
            raise","def compile_command_and_determine_if_incomplete(source, filename=""<input>"", symbol=""single""):
    r""""""Compile a command and determine whether it is incomplete.

    Arguments:

    source -- the source string; may contain \n characters
    filename -- optional filename from which source was read; default
                ""<input>""
    symbol -- optional grammar start symbol; ""single"" (default), ""exec""
              or ""eval""

    Return value / exceptions raised:

    - Return a code object if the command is complete and valid
    - Return None if the command is incomplete
    - Raise SyntaxError, ValueError or OverflowError if the command is a
      syntax error (OverflowError and ValueError can be produced by
      malformed literals).
    """"""
    return potentially_compile(_compile, source, filename, symbol)","{""_maybe_compile"": ""potentially_compile""}","def 0(compiler, source, filename, symbol):
    # Check for source consisting of only blank lines and comments.
    for line in source.split(""\n""):
        line = line.strip()
        if line and line[0] != '#':
            break               # Leave it alone.
    else:
        if symbol != ""eval"":
            source = ""pass""     # Replace it with a 'pass' statement

    try:
        return compiler(source, filename, symbol)
    except SyntaxError:  # Let other compile() errors propagate.
        pass

    # Catch syntax warnings after the first compile
    # to emit warnings (SyntaxWarning, DeprecationWarning) at most once.
    with warnings.catch_warnings():
        warnings.simplefilter(""error"")

        try:
            compiler(source + ""\n"", filename, symbol)
        except SyntaxError as e:
            if ""incomplete input"" in str(e):
                return None
            raise","def 1(source, filename=""<input>"", symbol=""single""):
    r""""""Compile a command and determine whether it is incomplete.

    Arguments:

    source -- the source string; may contain \n characters
    filename -- optional filename from which source was read; default
                ""<input>""
    symbol -- optional grammar start symbol; ""single"" (default), ""exec""
              or ""eval""

    Return value / exceptions raised:

    - Return a code object if the command is complete and valid
    - Return None if the command is incomplete
    - Raise SyntaxError, ValueError or OverflowError if the command is a
      syntax error (OverflowError and ValueError can be produced by
      malformed literals).
    """"""
    return 0(_compile, source, filename, symbol)","{""_maybe_compile"": ""0""}"
58,58,"def fnmatch(name, pat):
    """"""Test whether FILENAME matches PATTERN.

    Patterns are Unix shell style:

    *       matches everything
    ?       matches any single character
    [seq]   matches any character in seq
    [!seq]  matches any char not in seq

    An initial period in FILENAME is not special.
    Both FILENAME and PATTERN are first case-normalized
    if the operating system requires it.
    If you don't want this, use fnmatchcase(FILENAME, PATTERN).
    """"""
    name = os.path.normcase(name)
    pat = os.path.normcase(pat)
    return fnmatchcase(name, pat)","def fnmatchcase(name, pat):
    """"""Test whether FILENAME matches PATTERN, including case.

    This is a version of fnmatch() which doesn't case-normalize
    its arguments.
    """"""
    match = _compile_pattern(pat)
    return match(name) is not None","def name_match_case(name, pat):
    """"""Test whether FILENAME matches PATTERN, including case.

    This is a version of fnmatch() which doesn't case-normalize
    its arguments.
    """"""
    match = _compile_pattern(pat)
    return match(name) is not None","def test_whether_filename_matches_pattern(name, pat):
    """"""Test whether FILENAME matches PATTERN.

    Patterns are Unix shell style:

    *       matches everything
    ?       matches any single character
    [seq]   matches any character in seq
    [!seq]  matches any char not in seq

    An initial period in FILENAME is not special.
    Both FILENAME and PATTERN are first case-normalized
    if the operating system requires it.
    If you don't want this, use name_match_case(FILENAME, PATTERN).
    """"""
    name = os.path.normcase(name)
    pat = os.path.normcase(pat)
    return name_match_case(name, pat)","{""fnmatchcase"": ""name_match_case""}","def 0(name, pat):
    """"""Test whether FILENAME matches PATTERN, including case.

    This is a version of fnmatch() which doesn't case-normalize
    its arguments.
    """"""
    match = _compile_pattern(pat)
    return match(name) is not None","def 1(name, pat):
    """"""Test whether FILENAME matches PATTERN.

    Patterns are Unix shell style:

    *       matches everything
    ?       matches any single character
    [seq]   matches any character in seq
    [!seq]  matches any char not in seq

    An initial period in FILENAME is not special.
    Both FILENAME and PATTERN are first case-normalized
    if the operating system requires it.
    If you don't want this, use 0(FILENAME, PATTERN).
    """"""
    name = os.path.normcase(name)
    pat = os.path.normcase(pat)
    return 0(name, pat)","{""fnmatchcase"": ""0""}"
59,59,"def filter(names, pat):
    """"""Construct a list from those elements of the iterable NAMES that match PAT.""""""
    result = []
    pat = os.path.normcase(pat)
    match = _compile_pattern(pat)
    if os.path is posixpath:
        # normcase on posix is NOP. Optimize it away from the loop.
        for name in names:
            if match(name):
                result.append(name)
    else:
        for name in names:
            if match(os.path.normcase(name)):
                result.append(name)
    return result","def match(pattern, string, flags=0):
    """"""Try to apply the pattern at the start of the string, returning
    a Match object, or None if no match was found.""""""
    return _compile(pattern, flags).match(string)","def try_match(pattern, string, flags=0):
    """"""Try to apply the pattern at the start of the string, returning
    a Match object, or None if no try_match was found.""""""
    return _compile(pattern, flags).try_match(string)","def filter_names(names, pat):
    """"""Construct a list from those elements of the iterable NAMES that try_match PAT.""""""
    result = []
    pat = os.path.normcase(pat)
    try_match = _compile_pattern(pat)
    if os.path is posixpath:
        # normcase on posix is NOP. Optimize it away from the loop.
        for name in names:
            if try_match(name):
                result.append(name)
    else:
        for name in names:
            if try_match(os.path.normcase(name)):
                result.append(name)
    return result","{""match"": ""try_match""}","def 0(pattern, string, flags=0):
    """"""Try to apply the pattern at the start of the string, returning
    a Match object, or None if no match was found.""""""
    return _compile(pattern, flags).0(string)","def 1(names, pat):
    """"""Construct a list from those elements of the iterable NAMES that match PAT.""""""
    result = []
    pat = os.path.normcase(pat)
    match = _compile_pattern(pat)
    if os.path is posixpath:
        # normcase on posix is NOP. Optimize it away from the loop.
        for name in names:
            if 0(name):
                result.append(name)
    else:
        for name in names:
            if 0(os.path.normcase(name)):
                result.append(name)
    return result","{""match"": ""0""}"
60,60,"def fnmatchcase(name, pat):
    """"""Test whether FILENAME matches PATTERN, including case.

    This is a version of fnmatch() which doesn't case-normalize
    its arguments.
    """"""
    match = _compile_pattern(pat)
    return match(name) is not None","def match(pattern, string, flags=0):
    """"""Try to apply the pattern at the start of the string, returning
    a Match object, or None if no match was found.""""""
    return _compile(pattern, flags).match(string)","def try_match(pattern, string, flags=0):
    """"""Try to apply the pattern at the start of the string, returning
    a Match object, or None if no try_match was found.""""""
    return _compile(pattern, flags).try_match(string)","def fntry_matchcase(name, pat):
    """"""Test whether FILENAME try_matches PATTERN, including case.

    This is a version of fntry_match() which doesn't case-normalize
    its arguments.
    """"""
    try_match = _compile_pattern(pat)
    return try_match(name) is not None","{""match"": ""try_match""}","def 0(pattern, string, flags=0):
    """"""Try to apply the pattern at the start of the string, returning
    a Match object, or None if no match was found.""""""
    return _compile(pattern, flags).0(string)","def 1(name, pat):
    """"""Test whether FILENAME matches PATTERN, including case.

    This is a version of fn0() which doesn't case-normalize
    its arguments.
    """"""
    match = _compile_pattern(pat)
    return 0(name) is not None","{""match"": ""0""}"
61,61,"def print_tb(tb, limit=None, file=None):
    """"""Print up to 'limit' stack trace entries from the traceback 'tb'.

    If 'limit' is omitted or None, all entries are printed.  If 'file'
    is omitted or None, the output goes to sys.stderr; otherwise
    'file' should be an open file or file-like object with a write()
    method.
    """"""
    print_list(extract_tb(tb, limit=limit), file=file)","def print_list(extracted_list, file=None):
    """"""Print the list of tuples as returned by extract_tb() or
    extract_stack() as a formatted stack trace to the given file.""""""
    if file is None:
        file = sys.stderr
    for item in StackSummary.from_list(extracted_list).format():
        print(item, file=file, end="""")

def extract_tb(tb, limit=None):
    """"""
    Return a StackSummary object representing a list of
    pre-processed entries from traceback.

    This is useful for alternate formatting of stack traces.  If
    'limit' is omitted or None, all entries are extracted.  A
    pre-processed stack trace entry is a FrameSummary object
    containing attributes filename, lineno, name, and line
    representing the information that is usually printed for a stack
    trace.  The line is a string with leading and trailing
    whitespace stripped; if the source is not available it is None.
    """"""
    return StackSummary.extract(walk_tb(tb), limit=limit)","def display_extracted_list(extracted_list, file=None):
    """"""Print the list of tuples as returned by harvest_traceback_entries() or
    extract_stack() as a formatted stack trace to the given file.""""""
    if file is None:
        file = sys.stderr
    for item in StackSummary.from_list(extracted_list).format():
        print(item, file=file, end="""")

def harvest_traceback_entries(tb, limit=None):
    """"""
    Return a StackSummary object representing a list of
    pre-processed entries from traceback.

    This is useful for alternate formatting of stack traces.  If
    'limit' is omitted or None, all entries are extracted.  A
    pre-processed stack trace entry is a FrameSummary object
    containing attributes filename, lineno, name, and line
    representing the information that is usually printed for a stack
    trace.  The line is a string with leading and trailing
    whitespace stripped; if the source is not available it is None.
    """"""
    return StackSummary.extract(walk_tb(tb), limit=limit)","def print_stack_trace_entries_up_to_limit(tb, limit=None, file=None):
    """"""Print up to 'limit' stack trace entries from the traceback 'tb'.

    If 'limit' is omitted or None, all entries are printed.  If 'file'
    is omitted or None, the output goes to sys.stderr; otherwise
    'file' should be an open file or file-like object with a write()
    method.
    """"""
    display_extracted_list(harvest_traceback_entries(tb, limit=limit), file=file)","{""print_list"": ""display_extracted_list"", ""extract_tb"": ""harvest_traceback_entries""}","def 0(extracted_list, file=None):
    """"""Print the list of tuples as returned by 1() or
    extract_stack() as a formatted stack trace to the given file.""""""
    if file is None:
        file = sys.stderr
    for item in StackSummary.from_list(extracted_list).format():
        print(item, file=file, end="""")

def 1(tb, limit=None):
    """"""
    Return a StackSummary object representing a list of
    pre-processed entries from traceback.

    This is useful for alternate formatting of stack traces.  If
    'limit' is omitted or None, all entries are extracted.  A
    pre-processed stack trace entry is a FrameSummary object
    containing attributes filename, lineno, name, and line
    representing the information that is usually printed for a stack
    trace.  The line is a string with leading and trailing
    whitespace stripped; if the source is not available it is None.
    """"""
    return StackSummary.extract(walk_tb(tb), limit=limit)","def 2(tb, limit=None, file=None):
    """"""Print up to 'limit' stack trace entries from the traceback 'tb'.

    If 'limit' is omitted or None, all entries are printed.  If 'file'
    is omitted or None, the output goes to sys.stderr; otherwise
    'file' should be an open file or file-like object with a write()
    method.
    """"""
    0(1(tb, limit=limit), file=file)","{""print_list"": ""0"", ""extract_tb"": ""1""}"
62,62,"def format_tb(tb, limit=None):
    """"""A shorthand for 'format_list(extract_tb(tb, limit))'.""""""
    return extract_tb(tb, limit=limit).format()","def extract_tb(tb, limit=None):
    """"""
    Return a StackSummary object representing a list of
    pre-processed entries from traceback.

    This is useful for alternate formatting of stack traces.  If
    'limit' is omitted or None, all entries are extracted.  A
    pre-processed stack trace entry is a FrameSummary object
    containing attributes filename, lineno, name, and line
    representing the information that is usually printed for a stack
    trace.  The line is a string with leading and trailing
    whitespace stripped; if the source is not available it is None.
    """"""
    return StackSummary.extract(walk_tb(tb), limit=limit)","def harvest_traceback_entries(tb, limit=None):
    """"""
    Return a StackSummary object representing a list of
    pre-processed entries from traceback.

    This is useful for alternate formatting of stack traces.  If
    'limit' is omitted or None, all entries are extracted.  A
    pre-processed stack trace entry is a FrameSummary object
    containing attributes filename, lineno, name, and line
    representing the information that is usually printed for a stack
    trace.  The line is a string with leading and trailing
    whitespace stripped; if the source is not available it is None.
    """"""
    return StackSummary.extract(walk_tb(tb), limit=limit)","def format_stack_trace_entries_up_to_limit(tb, limit=None):
    """"""A shorthand for 'format_list(harvest_traceback_entries(tb, limit))'.""""""
    return harvest_traceback_entries(tb, limit=limit).format()","{""extract_tb"": ""harvest_traceback_entries""}","def 0(tb, limit=None):
    """"""
    Return a StackSummary object representing a list of
    pre-processed entries from traceback.

    This is useful for alternate formatting of stack traces.  If
    'limit' is omitted or None, all entries are extracted.  A
    pre-processed stack trace entry is a FrameSummary object
    containing attributes filename, lineno, name, and line
    representing the information that is usually printed for a stack
    trace.  The line is a string with leading and trailing
    whitespace stripped; if the source is not available it is None.
    """"""
    return StackSummary.extract(walk_tb(tb), limit=limit)","def 1(tb, limit=None):
    """"""A shorthand for 'format_list(0(tb, limit))'.""""""
    return 0(tb, limit=limit).format()","{""extract_tb"": ""0""}"
63,63,"def extract_tb(tb, limit=None):
    """"""
    Return a StackSummary object representing a list of
    pre-processed entries from traceback.

    This is useful for alternate formatting of stack traces.  If
    'limit' is omitted or None, all entries are extracted.  A
    pre-processed stack trace entry is a FrameSummary object
    containing attributes filename, lineno, name, and line
    representing the information that is usually printed for a stack
    trace.  The line is a string with leading and trailing
    whitespace stripped; if the source is not available it is None.
    """"""
    return StackSummary.extract(walk_tb(tb), limit=limit)","def walk_tb(tb):
    """"""Walk a traceback yielding the frame and line number for each frame.

    This will follow tb.tb_next (and thus is in the opposite order to
    walk_stack). Usually used with StackSummary.extract.
    """"""
    while tb is not None:
        yield tb.tb_frame, tb.tb_lineno
        tb = tb.tb_next","def walk_traceback_frames_and_line_numbers(tb):
    """"""Walk a traceback yielding the frame and line number for each frame.

    This will follow tb.tb_next (and thus is in the opposite order to
    walk_stack). Usually used with StackSummary.extract.
    """"""
    while tb is not None:
        yield tb.tb_frame, tb.tb_lineno
        tb = tb.tb_next","def harvest_traceback_entries(tb, limit=None):
    """"""
    Return a StackSummary object representing a list of
    pre-processed entries from traceback.

    This is useful for alternate formatting of stack traces.  If
    'limit' is omitted or None, all entries are extracted.  A
    pre-processed stack trace entry is a FrameSummary object
    containing attributes filename, lineno, name, and line
    representing the information that is usually printed for a stack
    trace.  The line is a string with leading and trailing
    whitespace stripped; if the source is not available it is None.
    """"""
    return StackSummary.extract(walk_traceback_frames_and_line_numbers(tb), limit=limit)","{""walk_tb"": ""walk_traceback_frames_and_line_numbers""}","def 0(tb):
    """"""Walk a traceback yielding the frame and line number for each frame.

    This will follow tb.tb_next (and thus is in the opposite order to
    walk_stack). Usually used with StackSummary.extract.
    """"""
    while tb is not None:
        yield tb.tb_frame, tb.tb_lineno
        tb = tb.tb_next","def 1(tb, limit=None):
    """"""
    Return a StackSummary object representing a list of
    pre-processed entries from traceback.

    This is useful for alternate formatting of stack traces.  If
    'limit' is omitted or None, all entries are extracted.  A
    pre-processed stack trace entry is a FrameSummary object
    containing attributes filename, lineno, name, and line
    representing the information that is usually printed for a stack
    trace.  The line is a string with leading and trailing
    whitespace stripped; if the source is not available it is None.
    """"""
    return StackSummary.extract(0(tb), limit=limit)","{""walk_tb"": ""0""}"
64,64,"def format_exception(exc, /, value=_sentinel, tb=_sentinel, limit=None, \
                     chain=True):
    """"""Format a stack trace and the exception information.

    The arguments have the same meaning as the corresponding arguments
    to print_exception().  The return value is a list of strings, each
    ending in a newline and some containing internal newlines.  When
    these lines are concatenated and printed, exactly the same text is
    printed as does print_exception().
    """"""
    value, tb = _parse_value_tb(exc, value, tb)
    te = TracebackException(type(value), value, tb, limit=limit, compact=True)
    return list(te.format(chain=chain))","def _parse_value_tb(exc, value, tb):
    if (value is _sentinel) != (tb is _sentinel):
        raise ValueError(""Both or neither of value and tb must be given"")
    if value is tb is _sentinel:
        if exc is not None:
            return exc, exc.__traceback__
        else:
            return None, None
    return value, tb","def _dissect_error_traceback(exc, value, tb):
    if (value is _sentinel) != (tb is _sentinel):
        raise ValueError(""Both or neither of value and tb must be given"")
    if value is tb is _sentinel:
        if exc is not None:
            return exc, exc.__traceback__
        else:
            return None, None
    return value, tb","def mold_exception_as_text(exc, /, value=_sentinel, tb=_sentinel, limit=None, \
                     chain=True):
    """"""Format a stack trace and the exception information.

    The arguments have the same meaning as the corresponding arguments
    to print_exception().  The return value is a list of strings, each
    ending in a newline and some containing internal newlines.  When
    these lines are concatenated and printed, exactly the same text is
    printed as does print_exception().
    """"""
    value, tb = _dissect_error_traceback(exc, value, tb)
    te = TracebackException(type(value), value, tb, limit=limit, compact=True)
    return list(te.format(chain=chain))","{""_parse_value_tb"": ""_dissect_error_traceback""}","def 0(exc, value, tb):
    if (value is _sentinel) != (tb is _sentinel):
        raise ValueError(""Both or neither of value and tb must be given"")
    if value is tb is _sentinel:
        if exc is not None:
            return exc, exc.__traceback__
        else:
            return None, None
    return value, tb","def 1(exc, /, value=_sentinel, tb=_sentinel, limit=None, \
                     chain=True):
    """"""Format a stack trace and the exception information.

    The arguments have the same meaning as the corresponding arguments
    to print_exception().  The return value is a list of strings, each
    ending in a newline and some containing internal newlines.  When
    these lines are concatenated and printed, exactly the same text is
    printed as does print_exception().
    """"""
    value, tb = 0(exc, value, tb)
    te = TracebackException(type(value), value, tb, limit=limit, compact=True)
    return list(te.format(chain=chain))","{""_parse_value_tb"": ""0""}"
65,65,"def format_exc(limit=None, chain=True):
    """"""Like print_exc() but return a string.""""""
    return """".join(format_exception(*sys.exc_info(), limit=limit, chain=chain))","def format_exception(exc, /, value=_sentinel, tb=_sentinel, limit=None, \
                     chain=True):
    """"""Format a stack trace and the exception information.

    The arguments have the same meaning as the corresponding arguments
    to print_exception().  The return value is a list of strings, each
    ending in a newline and some containing internal newlines.  When
    these lines are concatenated and printed, exactly the same text is
    printed as does print_exception().
    """"""
    value, tb = _parse_value_tb(exc, value, tb)
    te = TracebackException(type(value), value, tb, limit=limit, compact=True)
    return list(te.format(chain=chain))","def mold_exception_as_text(exc, /, value=_sentinel, tb=_sentinel, limit=None, \
                     chain=True):
    """"""Format a stack trace and the exception information.

    The arguments have the same meaning as the corresponding arguments
    to print_exception().  The return value is a list of strings, each
    ending in a newline and some containing internal newlines.  When
    these lines are concatenated and printed, exactly the same text is
    printed as does print_exception().
    """"""
    value, tb = _parse_value_tb(exc, value, tb)
    te = TracebackException(type(value), value, tb, limit=limit, compact=True)
    return list(te.format(chain=chain))","def format_exception_as_string_up_to_limit(limit=None, chain=True):
    """"""Like print_exc() but return a string.""""""
    return """".join(mold_exception_as_text(*sys.exc_info(), limit=limit, chain=chain))","{""format_exception"": ""mold_exception_as_text""}","def 0(exc, /, value=_sentinel, tb=_sentinel, limit=None, \
                     chain=True):
    """"""Format a stack trace and the exception information.

    The arguments have the same meaning as the corresponding arguments
    to print_exception().  The return value is a list of strings, each
    ending in a newline and some containing internal newlines.  When
    these lines are concatenated and printed, exactly the same text is
    printed as does print_exception().
    """"""
    value, tb = _parse_value_tb(exc, value, tb)
    te = TracebackException(type(value), value, tb, limit=limit, compact=True)
    return list(te.format(chain=chain))","def 1(limit=None, chain=True):
    """"""Like print_exc() but return a string.""""""
    return """".join(0(*sys.exc_info(), limit=limit, chain=chain))","{""format_exception"": ""0""}"
66,66,"def extract_stack(f=None, limit=None):
    """"""Replacement for traceback.extract_stack() that only does the
    necessary work for asyncio debug mode.
    """"""
    if f is None:
        f = sys._getframe().f_back
    if limit is None:
        # Limit the amount of work to a reasonable amount, as extract_stack()
        # can be called for each coroutine and future in debug mode.
        limit = constants.DEBUG_STACK_DEPTH
    stack = traceback.StackSummary.extract(traceback.walk_stack(f),
                                           limit=limit,
                                           lookup_lines=False)
    stack.reverse()
    return stack","def walk_stack(f):
    """"""Walk a stack yielding the frame and line number for each frame.

    This will follow f.f_back from the given frame. If no frame is given, the
    current stack is used. Usually used with StackSummary.extract.
    """"""
    if f is None:
        f = sys._getframe().f_back.f_back
    while f is not None:
        yield f, f.f_lineno
        f = f.f_back","def walk_stack_frames_and_line_numbers(f):
    """"""Walk a stack yielding the frame and line number for each frame.

    This will follow f.f_back from the given frame. If no frame is given, the
    current stack is used. Usually used with StackSummary.extract.
    """"""
    if f is None:
        f = sys._getframe().f_back.f_back
    while f is not None:
        yield f, f.f_lineno
        f = f.f_back","def extract_stack_with_limit_replacement_for_asyncio(f=None, limit=None):
    """"""Replacement for traceback.extract_stack_with_limit_replacement_for_asyncio() that only does the
    necessary work for asyncio debug mode.
    """"""
    if f is None:
        f = sys._getframe().f_back
    if limit is None:
        # Limit the amount of work to a reasonable amount, as extract_stack_with_limit_replacement_for_asyncio()
        # can be called for each coroutine and future in debug mode.
        limit = constants.DEBUG_STACK_DEPTH
    stack = traceback.StackSummary.extract(traceback.walk_stack_frames_and_line_numbers(f),
                                           limit=limit,
                                           lookup_lines=False)
    stack.reverse()
    return stack","{""walk_stack"": ""walk_stack_frames_and_line_numbers""}","def 0(f):
    """"""Walk a stack yielding the frame and line number for each frame.

    This will follow f.f_back from the given frame. If no frame is given, the
    current stack is used. Usually used with StackSummary.extract.
    """"""
    if f is None:
        f = sys._getframe().f_back.f_back
    while f is not None:
        yield f, f.f_lineno
        f = f.f_back","def 1(f=None, limit=None):
    """"""Replacement for traceback.1() that only does the
    necessary work for asyncio debug mode.
    """"""
    if f is None:
        f = sys._getframe().f_back
    if limit is None:
        # Limit the amount of work to a reasonable amount, as 1()
        # can be called for each coroutine and future in debug mode.
        limit = constants.DEBUG_STACK_DEPTH
    stack = traceback.StackSummary.extract(traceback.0(f),
                                           limit=limit,
                                           lookup_lines=False)
    stack.reverse()
    return stack","{""walk_stack"": ""0""}"
67,67,"def formatwarning(message, category, filename, lineno, line=None):
    """"""Function to format a warning the standard way.""""""
    msg = WarningMessage(message, category, filename, lineno, None, line)
    return _formatwarnmsg_impl(msg)","def _formatwarnmsg_impl(msg):
    category = msg.category.__name__
    s =  f""{msg.filename}:{msg.lineno}: {category}: {msg.message}\n""

    if msg.line is None:
        try:
            import linecache
            line = linecache.getline(msg.filename, msg.lineno)
        except Exception:
            # When a warning is logged during Python shutdown, linecache
            # and the import machinery don't work anymore
            line = None
            linecache = None
    else:
        line = msg.line
    if line:
        line = line.strip()
        s += ""  %s\n"" % line

    if msg.source is not None:
        try:
            import tracemalloc
        # Logging a warning should not raise a new exception:
        # catch Exception, not only ImportError and RecursionError.
        except Exception:
            # don't suggest to enable tracemalloc if it's not available
            tracing = True
            tb = None
        else:
            tracing = tracemalloc.is_tracing()
            try:
                tb = tracemalloc.get_object_traceback(msg.source)
            except Exception:
                # When a warning is logged during Python shutdown, tracemalloc
                # and the import machinery don't work anymore
                tb = None

        if tb is not None:
            s += 'Object allocated at (most recent call last):\n'
            for frame in tb:
                s += ('  File ""%s"", lineno %s\n'
                      % (frame.filename, frame.lineno))

                try:
                    if linecache is not None:
                        line = linecache.getline(frame.filename, frame.lineno)
                    else:
                        line = None
                except Exception:
                    line = None
                if line:
                    line = line.strip()
                    s += '    %s\n' % line
        elif not tracing:
            s += (f'{category}: Enable tracemalloc to get the object '
                  f'allocation traceback\n')
    return s","def _craft_warning_message(msg):
    category = msg.category.__name__
    s =  f""{msg.filename}:{msg.lineno}: {category}: {msg.message}\n""

    if msg.line is None:
        try:
            import linecache
            line = linecache.getline(msg.filename, msg.lineno)
        except Exception:
            # When a warning is logged during Python shutdown, linecache
            # and the import machinery don't work anymore
            line = None
            linecache = None
    else:
        line = msg.line
    if line:
        line = line.strip()
        s += ""  %s\n"" % line

    if msg.source is not None:
        try:
            import tracemalloc
        # Logging a warning should not raise a new exception:
        # catch Exception, not only ImportError and RecursionError.
        except Exception:
            # don't suggest to enable tracemalloc if it's not available
            tracing = True
            tb = None
        else:
            tracing = tracemalloc.is_tracing()
            try:
                tb = tracemalloc.get_object_traceback(msg.source)
            except Exception:
                # When a warning is logged during Python shutdown, tracemalloc
                # and the import machinery don't work anymore
                tb = None

        if tb is not None:
            s += 'Object allocated at (most recent call last):\n'
            for frame in tb:
                s += ('  File ""%s"", lineno %s\n'
                      % (frame.filename, frame.lineno))

                try:
                    if linecache is not None:
                        line = linecache.getline(frame.filename, frame.lineno)
                    else:
                        line = None
                except Exception:
                    line = None
                if line:
                    line = line.strip()
                    s += '    %s\n' % line
        elif not tracing:
            s += (f'{category}: Enable tracemalloc to get the object '
                  f'allocation traceback\n')
    return s","def format_warning_the_standard_way(message, category, filename, lineno, line=None):
    """"""Function to format a warning the standard way.""""""
    msg = WarningMessage(message, category, filename, lineno, None, line)
    return _craft_warning_message(msg)","{""_formatwarnmsg_impl"": ""_craft_warning_message""}","def 0(msg):
    category = msg.category.__name__
    s =  f""{msg.filename}:{msg.lineno}: {category}: {msg.message}\n""

    if msg.line is None:
        try:
            import linecache
            line = linecache.getline(msg.filename, msg.lineno)
        except Exception:
            # When a warning is logged during Python shutdown, linecache
            # and the import machinery don't work anymore
            line = None
            linecache = None
    else:
        line = msg.line
    if line:
        line = line.strip()
        s += ""  %s\n"" % line

    if msg.source is not None:
        try:
            import tracemalloc
        # Logging a warning should not raise a new exception:
        # catch Exception, not only ImportError and RecursionError.
        except Exception:
            # don't suggest to enable tracemalloc if it's not available
            tracing = True
            tb = None
        else:
            tracing = tracemalloc.is_tracing()
            try:
                tb = tracemalloc.get_object_traceback(msg.source)
            except Exception:
                # When a warning is logged during Python shutdown, tracemalloc
                # and the import machinery don't work anymore
                tb = None

        if tb is not None:
            s += 'Object allocated at (most recent call last):\n'
            for frame in tb:
                s += ('  File ""%s"", lineno %s\n'
                      % (frame.filename, frame.lineno))

                try:
                    if linecache is not None:
                        line = linecache.getline(frame.filename, frame.lineno)
                    else:
                        line = None
                except Exception:
                    line = None
                if line:
                    line = line.strip()
                    s += '    %s\n' % line
        elif not tracing:
            s += (f'{category}: Enable tracemalloc to get the object '
                  f'allocation traceback\n')
    return s","def 1(message, category, filename, lineno, line=None):
    """"""Function to format a warning the standard way.""""""
    msg = WarningMessage(message, category, filename, lineno, None, line)
    return 0(msg)","{""_formatwarnmsg_impl"": ""0""}"
68,68,"def _showwarnmsg(msg):
    """"""Hook to write a warning to a file; replace if you like.""""""
    try:
        sw = showwarning
    except NameError:
        pass
    else:
        if sw is not _showwarning_orig:
            # warnings.showwarning() was replaced
            if not callable(sw):
                raise TypeError(""warnings.showwarning() must be set to a ""
                                ""function or method"")

            sw(msg.message, msg.category, msg.filename, msg.lineno,
               msg.file, msg.line)
            return
    _showwarnmsg_impl(msg)","def _showwarnmsg_impl(msg):
    file = msg.file
    if file is None:
        file = sys.stderr
        if file is None:
            # sys.stderr is None when run with pythonw.exe:
            # warnings get lost
            return
    text = _formatwarnmsg(msg)
    try:
        file.write(text)
    except OSError:
        # the file (probably stderr) is invalid - this warning gets lost.
        pass","def _present_warning_message(msg):
    file = msg.file
    if file is None:
        file = sys.stderr
        if file is None:
            # sys.stderr is None when run with pythonw.exe:
            # warnings get lost
            return
    text = _formatwarnmsg(msg)
    try:
        file.write(text)
    except OSError:
        # the file (probably stderr) is invalid - this warning gets lost.
        pass","def hook_to_write_warning_to_file_replace_if_needed(msg):
    """"""Hook to write a warning to a file; replace if you like.""""""
    try:
        sw = showwarning
    except NameError:
        pass
    else:
        if sw is not _showwarning_orig:
            # warnings.showwarning() was replaced
            if not callable(sw):
                raise TypeError(""warnings.showwarning() must be set to a ""
                                ""function or method"")

            sw(msg.message, msg.category, msg.filename, msg.lineno,
               msg.file, msg.line)
            return
    _present_warning_message(msg)","{""_showwarnmsg_impl"": ""_present_warning_message""}","def 0(msg):
    file = msg.file
    if file is None:
        file = sys.stderr
        if file is None:
            # sys.stderr is None when run with pythonw.exe:
            # warnings get lost
            return
    text = _formatwarnmsg(msg)
    try:
        file.write(text)
    except OSError:
        # the file (probably stderr) is invalid - this warning gets lost.
        pass","def 1(msg):
    """"""Hook to write a warning to a file; replace if you like.""""""
    try:
        sw = showwarning
    except NameError:
        pass
    else:
        if sw is not _showwarning_orig:
            # warnings.showwarning() was replaced
            if not callable(sw):
                raise TypeError(""warnings.showwarning() must be set to a ""
                                ""function or method"")

            sw(msg.message, msg.category, msg.filename, msg.lineno,
               msg.file, msg.line)
            return
    0(msg)","{""_showwarnmsg_impl"": ""0""}"
69,69,"def _formatwarnmsg(msg):
    """"""Function to format a warning the standard way.""""""
    try:
        fw = formatwarning
    except NameError:
        pass
    else:
        if fw is not _formatwarning_orig:
            # warnings.formatwarning() was replaced
            return fw(msg.message, msg.category,
                      msg.filename, msg.lineno, msg.line)
    return _formatwarnmsg_impl(msg)","def _formatwarnmsg_impl(msg):
    category = msg.category.__name__
    s =  f""{msg.filename}:{msg.lineno}: {category}: {msg.message}\n""

    if msg.line is None:
        try:
            import linecache
            line = linecache.getline(msg.filename, msg.lineno)
        except Exception:
            # When a warning is logged during Python shutdown, linecache
            # and the import machinery don't work anymore
            line = None
            linecache = None
    else:
        line = msg.line
    if line:
        line = line.strip()
        s += ""  %s\n"" % line

    if msg.source is not None:
        try:
            import tracemalloc
        # Logging a warning should not raise a new exception:
        # catch Exception, not only ImportError and RecursionError.
        except Exception:
            # don't suggest to enable tracemalloc if it's not available
            tracing = True
            tb = None
        else:
            tracing = tracemalloc.is_tracing()
            try:
                tb = tracemalloc.get_object_traceback(msg.source)
            except Exception:
                # When a warning is logged during Python shutdown, tracemalloc
                # and the import machinery don't work anymore
                tb = None

        if tb is not None:
            s += 'Object allocated at (most recent call last):\n'
            for frame in tb:
                s += ('  File ""%s"", lineno %s\n'
                      % (frame.filename, frame.lineno))

                try:
                    if linecache is not None:
                        line = linecache.getline(frame.filename, frame.lineno)
                    else:
                        line = None
                except Exception:
                    line = None
                if line:
                    line = line.strip()
                    s += '    %s\n' % line
        elif not tracing:
            s += (f'{category}: Enable tracemalloc to get the object '
                  f'allocation traceback\n')
    return s","def _craft_warning_message(msg):
    category = msg.category.__name__
    s =  f""{msg.filename}:{msg.lineno}: {category}: {msg.message}\n""

    if msg.line is None:
        try:
            import linecache
            line = linecache.getline(msg.filename, msg.lineno)
        except Exception:
            # When a warning is logged during Python shutdown, linecache
            # and the import machinery don't work anymore
            line = None
            linecache = None
    else:
        line = msg.line
    if line:
        line = line.strip()
        s += ""  %s\n"" % line

    if msg.source is not None:
        try:
            import tracemalloc
        # Logging a warning should not raise a new exception:
        # catch Exception, not only ImportError and RecursionError.
        except Exception:
            # don't suggest to enable tracemalloc if it's not available
            tracing = True
            tb = None
        else:
            tracing = tracemalloc.is_tracing()
            try:
                tb = tracemalloc.get_object_traceback(msg.source)
            except Exception:
                # When a warning is logged during Python shutdown, tracemalloc
                # and the import machinery don't work anymore
                tb = None

        if tb is not None:
            s += 'Object allocated at (most recent call last):\n'
            for frame in tb:
                s += ('  File ""%s"", lineno %s\n'
                      % (frame.filename, frame.lineno))

                try:
                    if linecache is not None:
                        line = linecache.getline(frame.filename, frame.lineno)
                    else:
                        line = None
                except Exception:
                    line = None
                if line:
                    line = line.strip()
                    s += '    %s\n' % line
        elif not tracing:
            s += (f'{category}: Enable tracemalloc to get the object '
                  f'allocation traceback\n')
    return s","def function_to_format_warning_the_standard_way(msg):
    """"""Function to format a warning the standard way.""""""
    try:
        fw = formatwarning
    except NameError:
        pass
    else:
        if fw is not _formatwarning_orig:
            # warnings.formatwarning() was replaced
            return fw(msg.message, msg.category,
                      msg.filename, msg.lineno, msg.line)
    return _craft_warning_message(msg)","{""_formatwarnmsg_impl"": ""_craft_warning_message""}","def 0(msg):
    category = msg.category.__name__
    s =  f""{msg.filename}:{msg.lineno}: {category}: {msg.message}\n""

    if msg.line is None:
        try:
            import linecache
            line = linecache.getline(msg.filename, msg.lineno)
        except Exception:
            # When a warning is logged during Python shutdown, linecache
            # and the import machinery don't work anymore
            line = None
            linecache = None
    else:
        line = msg.line
    if line:
        line = line.strip()
        s += ""  %s\n"" % line

    if msg.source is not None:
        try:
            import tracemalloc
        # Logging a warning should not raise a new exception:
        # catch Exception, not only ImportError and RecursionError.
        except Exception:
            # don't suggest to enable tracemalloc if it's not available
            tracing = True
            tb = None
        else:
            tracing = tracemalloc.is_tracing()
            try:
                tb = tracemalloc.get_object_traceback(msg.source)
            except Exception:
                # When a warning is logged during Python shutdown, tracemalloc
                # and the import machinery don't work anymore
                tb = None

        if tb is not None:
            s += 'Object allocated at (most recent call last):\n'
            for frame in tb:
                s += ('  File ""%s"", lineno %s\n'
                      % (frame.filename, frame.lineno))

                try:
                    if linecache is not None:
                        line = linecache.getline(frame.filename, frame.lineno)
                    else:
                        line = None
                except Exception:
                    line = None
                if line:
                    line = line.strip()
                    s += '    %s\n' % line
        elif not tracing:
            s += (f'{category}: Enable tracemalloc to get the object '
                  f'allocation traceback\n')
    return s","def 1(msg):
    """"""Function to format a warning the standard way.""""""
    try:
        fw = formatwarning
    except NameError:
        pass
    else:
        if fw is not _formatwarning_orig:
            # warnings.formatwarning() was replaced
            return fw(msg.message, msg.category,
                      msg.filename, msg.lineno, msg.line)
    return 0(msg)","{""_formatwarnmsg_impl"": ""0""}"
70,70,"def _next_external_frame(frame):
    """"""Find the next frame that doesn't involve CPython internals.""""""
    frame = frame.f_back
    while frame is not None and _is_internal_frame(frame):
        frame = frame.f_back
    return frame","def _is_internal_frame(frame):
    """"""Signal whether the frame is an internal CPython implementation detail.""""""
    filename = frame.f_code.co_filename
    return 'importlib' in filename and '_bootstrap' in filename","def _looks_like_internal_frame(frame):
    """"""Signal whether the frame is an internal CPython implementation detail.""""""
    filename = frame.f_code.co_filename
    return 'importlib' in filename and '_bootstrap' in filename","def find_next_frame_not_involving_cpython_internals(frame):
    """"""Find the next frame that doesn't involve CPython internals.""""""
    frame = frame.f_back
    while frame is not None and _looks_like_internal_frame(frame):
        frame = frame.f_back
    return frame","{""_is_internal_frame"": ""_looks_like_internal_frame""}","def 0(frame):
    """"""Signal whether the frame is an internal CPython implementation detail.""""""
    filename = frame.f_code.co_filename
    return 'importlib' in filename and '_bootstrap' in filename","def 1(frame):
    """"""Find the next frame that doesn't involve CPython internals.""""""
    frame = frame.f_back
    while frame is not None and 0(frame):
        frame = frame.f_back
    return frame","{""_is_internal_frame"": ""0""}"
71,71,"def check_call(*popenargs, **kwargs):
    """"""Run command with arguments.  Wait for command to complete.  If
    the exit code was zero then return, otherwise raise
    CalledProcessError.  The CalledProcessError object will have the
    return code in the returncode attribute.

    The arguments are the same as for the call function.  Example:

    check_call([""ls"", ""-l""])
    """"""
    retcode = call(*popenargs, **kwargs)
    if retcode:
        cmd = kwargs.get(""args"")
        if cmd is None:
            cmd = popenargs[0]
        raise CalledProcessError(retcode, cmd)
    return 0","def call(*popenargs, timeout=None, **kwargs):
    """"""Run command with arguments.  Wait for command to complete or
    timeout, then return the returncode attribute.

    The arguments are the same as for the Popen constructor.  Example:

    retcode = call([""ls"", ""-l""])
    """"""
    with Popen(*popenargs, **kwargs) as p:
        try:
            return p.wait(timeout=timeout)
        except:  # Including KeyboardInterrupt, wait handled that.
            p.kill()
            # We don't call p.wait() again as p.__exit__ does that for us.
            raise","def shout_command(*popenargs, timeout=None, **kwargs):
    """"""Run command with arguments.  Wait for command to complete or
    timeout, then return the returncode attribute.

    The arguments are the same as for the Popen constructor.  Example:

    retcode = shout_command([""ls"", ""-l""])
    """"""
    with Popen(*popenargs, **kwargs) as p:
        try:
            return p.wait(timeout=timeout)
        except:  # Including KeyboardInterrupt, wait handled that.
            p.kill()
            # We don't shout_command p.wait() again as p.__exit__ does that for us.
            raise","def check_shout_command(*popenargs, **kwargs):
    """"""Run command with arguments.  Wait for command to complete.  If
    the exit code was zero then return, otherwise raise
    CalledProcessError.  The CalledProcessError object will have the
    return code in the returncode attribute.

    The arguments are the same as for the shout_command function.  Example:

    check_shout_command([""ls"", ""-l""])
    """"""
    retcode = shout_command(*popenargs, **kwargs)
    if retcode:
        cmd = kwargs.get(""args"")
        if cmd is None:
            cmd = popenargs[0]
        raise CalledProcessError(retcode, cmd)
    return 0","{""call"": ""shout_command""}","def 0(*popenargs, timeout=None, **kwargs):
    """"""Run command with arguments.  Wait for command to complete or
    timeout, then return the returncode attribute.

    The arguments are the same as for the Popen constructor.  Example:

    retcode = 0([""ls"", ""-l""])
    """"""
    with Popen(*popenargs, **kwargs) as p:
        try:
            return p.wait(timeout=timeout)
        except:  # Including KeyboardInterrupt, wait handled that.
            p.kill()
            # We don't call p.wait() again as p.__exit__ does that for us.
            raise","def check_0(*popenargs, **kwargs):
    """"""Run command with arguments.  Wait for command to complete.  If
    the exit code was zero then return, otherwise raise
    CalledProcessError.  The CalledProcessError object will have the
    return code in the returncode attribute.

    The arguments are the same as for the call function.  Example:

    check_0([""ls"", ""-l""])
    """"""
    retcode = 0(*popenargs, **kwargs)
    if retcode:
        cmd = kwargs.get(""args"")
        if cmd is None:
            cmd = popenargs[0]
        raise CalledProcessError(retcode, cmd)
    return 0","{""call"": ""0""}"
72,72,"def getoutput(cmd):
    """"""Return output (stdout or stderr) of executing cmd in a shell.

    Like getstatusoutput(), except the exit status is ignored and the return
    value is a string containing the command's output.  Example:

    >>> import subprocess
    >>> subprocess.getoutput('ls /bin/ls')
    '/bin/ls'
    """"""
    return getstatusoutput(cmd)[1]","def getstatusoutput(cmd):
    """"""Return (exitcode, output) of executing cmd in a shell.

    Execute the string 'cmd' in a shell with 'check_output' and
    return a 2-tuple (status, output). The locale encoding is used
    to decode the output and process newlines.

    A trailing newline is stripped from the output.
    The exit status for the command can be interpreted
    according to the rules for the function 'wait'. Example:

    >>> import subprocess
    >>> subprocess.getstatusoutput('ls /bin/ls')
    (0, '/bin/ls')
    >>> subprocess.getstatusoutput('cat /bin/junk')
    (1, 'cat: /bin/junk: No such file or directory')
    >>> subprocess.getstatusoutput('/bin/junk')
    (127, 'sh: /bin/junk: not found')
    >>> subprocess.getstatusoutput('/bin/kill $$')
    (-15, '')
    """"""
    try:
        data = check_output(cmd, shell=True, text=True, stderr=STDOUT)
        exitcode = 0
    except CalledProcessError as ex:
        data = ex.output
        exitcode = ex.returncode
    if data[-1:] == '\n':
        data = data[:-1]
    return exitcode, data","def retrieve_command_status_and_output(cmd):
    """"""Return (exitcode, output) of executing cmd in a shell.

    Execute the string 'cmd' in a shell with 'check_output' and
    return a 2-tuple (status, output). The locale encoding is used
    to decode the output and process newlines.

    A trailing newline is stripped from the output.
    The exit status for the command can be interpreted
    according to the rules for the function 'wait'. Example:

    >>> import subprocess
    >>> subprocess.retrieve_command_status_and_output('ls /bin/ls')
    (0, '/bin/ls')
    >>> subprocess.retrieve_command_status_and_output('cat /bin/junk')
    (1, 'cat: /bin/junk: No such file or directory')
    >>> subprocess.retrieve_command_status_and_output('/bin/junk')
    (127, 'sh: /bin/junk: not found')
    >>> subprocess.retrieve_command_status_and_output('/bin/kill $$')
    (-15, '')
    """"""
    try:
        data = check_output(cmd, shell=True, text=True, stderr=STDOUT)
        exitcode = 0
    except CalledProcessError as ex:
        data = ex.output
        exitcode = ex.returncode
    if data[-1:] == '\n':
        data = data[:-1]
    return exitcode, data","def return_output_of_executing_command_in_shell(cmd):
    """"""Return output (stdout or stderr) of executing cmd in a shell.

    Like retrieve_command_status_and_output(), except the exit status is ignored and the return
    value is a string containing the command's output.  Example:

    >>> import subprocess
    >>> subprocess.return_output_of_executing_command_in_shell('ls /bin/ls')
    '/bin/ls'
    """"""
    return retrieve_command_status_and_output(cmd)[1]","{""getstatusoutput"": ""retrieve_command_status_and_output""}","def 0(cmd):
    """"""Return (exitcode, output) of executing cmd in a shell.

    Execute the string 'cmd' in a shell with 'check_output' and
    return a 2-tuple (status, output). The locale encoding is used
    to decode the output and process newlines.

    A trailing newline is stripped from the output.
    The exit status for the command can be interpreted
    according to the rules for the function 'wait'. Example:

    >>> import subprocess
    >>> subprocess.0('ls /bin/ls')
    (0, '/bin/ls')
    >>> subprocess.0('cat /bin/junk')
    (1, 'cat: /bin/junk: No such file or directory')
    >>> subprocess.0('/bin/junk')
    (127, 'sh: /bin/junk: not found')
    >>> subprocess.0('/bin/kill $$')
    (-15, '')
    """"""
    try:
        data = check_output(cmd, shell=True, text=True, stderr=STDOUT)
        exitcode = 0
    except CalledProcessError as ex:
        data = ex.output
        exitcode = ex.returncode
    if data[-1:] == '\n':
        data = data[:-1]
    return exitcode, data","def 1(cmd):
    """"""Return output (stdout or stderr) of executing cmd in a shell.

    Like 0(), except the exit status is ignored and the return
    value is a string containing the command's output.  Example:

    >>> import subprocess
    >>> subprocess.1('ls /bin/ls')
    '/bin/ls'
    """"""
    return 0(cmd)[1]","{""getstatusoutput"": ""0""}"
73,73,"def iterencode(iterator, encoding, errors='strict', **kwargs):
    """"""
    Encoding iterator.

    Encodes the input strings from the iterator using an IncrementalEncoder.

    errors and kwargs are passed through to the IncrementalEncoder
    constructor.
    """"""
    encoder = getincrementalencoder(encoding)(errors, **kwargs)
    for input in iterator:
        output = encoder.encode(input)
        if output:
            yield output
    output = encoder.encode("""", True)
    if output:
        yield output","def getincrementalencoder(encoding):

    """""" Lookup up the codec for the given encoding and return
        its IncrementalEncoder class or factory function.

        Raises a LookupError in case the encoding cannot be found
        or the codecs doesn't provide an incremental encoder.

    """"""
    encoder = lookup(encoding).incrementalencoder
    if encoder is None:
        raise LookupError(encoding)
    return encoder","def fetch_incremental_encoder(encoding):

    """""" Lookup up the codec for the given encoding and return
        its IncrementalEncoder class or factory function.

        Raises a LookupError in case the encoding cannot be found
        or the codecs doesn't provide an incremental encoder.

    """"""
    encoder = lookup(encoding).incrementalencoder
    if encoder is None:
        raise LookupError(encoding)
    return encoder","def encoding_iterator_using_incremental_encoder(iterator, encoding, errors='strict', **kwargs):
    """"""
    Encoding iterator.

    Encodes the input strings from the iterator using an IncrementalEncoder.

    errors and kwargs are passed through to the IncrementalEncoder
    constructor.
    """"""
    encoder = fetch_incremental_encoder(encoding)(errors, **kwargs)
    for input in iterator:
        output = encoder.encode(input)
        if output:
            yield output
    output = encoder.encode("""", True)
    if output:
        yield output","{""getincrementalencoder"": ""fetch_incremental_encoder""}","def 0(encoding):

    """""" Lookup up the codec for the given encoding and return
        its IncrementalEncoder class or factory function.

        Raises a LookupError in case the encoding cannot be found
        or the codecs doesn't provide an incremental encoder.

    """"""
    encoder = lookup(encoding).incrementalencoder
    if encoder is None:
        raise LookupError(encoding)
    return encoder","def 1(iterator, encoding, errors='strict', **kwargs):
    """"""
    Encoding iterator.

    Encodes the input strings from the iterator using an IncrementalEncoder.

    errors and kwargs are passed through to the IncrementalEncoder
    constructor.
    """"""
    encoder = 0(encoding)(errors, **kwargs)
    for input in iterator:
        output = encoder.encode(input)
        if output:
            yield output
    output = encoder.encode("""", True)
    if output:
        yield output","{""getincrementalencoder"": ""0""}"
74,74,"def iterdecode(iterator, encoding, errors='strict', **kwargs):
    """"""
    Decoding iterator.

    Decodes the input strings from the iterator using an IncrementalDecoder.

    errors and kwargs are passed through to the IncrementalDecoder
    constructor.
    """"""
    decoder = getincrementaldecoder(encoding)(errors, **kwargs)
    for input in iterator:
        output = decoder.decode(input)
        if output:
            yield output
    output = decoder.decode(b"""", True)
    if output:
        yield output","def getincrementaldecoder(encoding):

    """""" Lookup up the codec for the given encoding and return
        its IncrementalDecoder class or factory function.

        Raises a LookupError in case the encoding cannot be found
        or the codecs doesn't provide an incremental decoder.

    """"""
    decoder = lookup(encoding).incrementaldecoder
    if decoder is None:
        raise LookupError(encoding)
    return decoder","def fetch_incremental_decoder(encoding):

    """""" Lookup up the codec for the given encoding and return
        its IncrementalDecoder class or factory function.

        Raises a LookupError in case the encoding cannot be found
        or the codecs doesn't provide an incremental decoder.

    """"""
    decoder = lookup(encoding).incrementaldecoder
    if decoder is None:
        raise LookupError(encoding)
    return decoder","def decoding_iterator_using_incremental_decoder(iterator, encoding, errors='strict', **kwargs):
    """"""
    Decoding iterator.

    Decodes the input strings from the iterator using an IncrementalDecoder.

    errors and kwargs are passed through to the IncrementalDecoder
    constructor.
    """"""
    decoder = fetch_incremental_decoder(encoding)(errors, **kwargs)
    for input in iterator:
        output = decoder.decode(input)
        if output:
            yield output
    output = decoder.decode(b"""", True)
    if output:
        yield output","{""getincrementaldecoder"": ""fetch_incremental_decoder""}","def 0(encoding):

    """""" Lookup up the codec for the given encoding and return
        its IncrementalDecoder class or factory function.

        Raises a LookupError in case the encoding cannot be found
        or the codecs doesn't provide an incremental decoder.

    """"""
    decoder = lookup(encoding).incrementaldecoder
    if decoder is None:
        raise LookupError(encoding)
    return decoder","def 1(iterator, encoding, errors='strict', **kwargs):
    """"""
    Decoding iterator.

    Decodes the input strings from the iterator using an IncrementalDecoder.

    errors and kwargs are passed through to the IncrementalDecoder
    constructor.
    """"""
    decoder = 0(encoding)(errors, **kwargs)
    for input in iterator:
        output = decoder.decode(input)
        if output:
            yield output
    output = decoder.decode(b"""", True)
    if output:
        yield output","{""getincrementaldecoder"": ""0""}"
75,75,"def indexOf(a, b):
    ""Return the first index of b in a.""
    for i, j in enumerate(a):
        if j is b or j == b:
            return i
    else:
        raise ValueError('sequence.index(x): x not in sequence')","def enumerate():
    """"""Return a list of all Thread objects currently alive.

    The list includes daemonic threads, dummy thread objects created by
    current_thread(), and the main thread. It excludes terminated threads and
    threads that have not yet been started.

    """"""
    with _active_limbo_lock:
        return list(_active.values()) + list(_limbo.values())","def get_alive_threads():
    """"""Return a list of all Thread objects currently alive.

    The list includes daemonic threads, dummy thread objects created by
    current_thread(), and the main thread. It excludes terminated threads and
    threads that have not yet been started.

    """"""
    with _active_limbo_lock:
        return list(_active.values()) + list(_limbo.values())","def return_first_index_of_item_in_sequence(a, b):
    ""Return the first index of b in a.""
    for i, j in get_alive_threads(a):
        if j is b or j == b:
            return i
    else:
        raise ValueError('sequence.index(x): x not in sequence')","{""enumerate"": ""get_alive_threads""}","def 0():
    """"""Return a list of all Thread objects currently alive.

    The list includes daemonic threads, dummy thread objects created by
    current_thread(), and the main thread. It excludes terminated threads and
    threads that have not yet been started.

    """"""
    with _active_limbo_lock:
        return list(_active.values()) + list(_limbo.values())","def 1(a, b):
    ""Return the first index of b in a.""
    for i, j in 0(a):
        if j is b or j == b:
            return i
    else:
        raise ValueError('sequence.index(x): x not in sequence')","{""enumerate"": ""0""}"
76,76,"def renames(old: str, new: str) -> None:
    """"""Like os.renames(), but handles renaming across devices.""""""
    # Implementation borrowed from os.renames().
    head, tail = os.path.split(new)
    if head and tail and not os.path.exists(head):
        os.makedirs(head)

    shutil.move(old, new)

    head, tail = os.path.split(old)
    if head and tail:
        try:
            os.removedirs(head)
        except OSError:
            pass","def removedirs(name):
    """"""removedirs(name)

    Super-rmdir; remove a leaf directory and all empty intermediate
    ones.  Works like rmdir except that, if the leaf directory is
    successfully removed, directories corresponding to rightmost path
    segments will be pruned away until either the whole path is
    consumed or an error occurs.  Errors during this latter phase are
    ignored -- they generally mean that a directory was not empty.

    """"""
    rmdir(name)
    head, tail = path.split(name)
    if not tail:
        head, tail = path.split(head)
    while head and tail:
        try:
            rmdir(head)
        except OSError:
            break
        head, tail = path.split(head)

def makedirs(name, mode=0o777, exist_ok=False):
    """"""makedirs(name [, mode=0o777][, exist_ok=False])

    Super-mkdir; create a leaf directory and all intermediate ones.  Works like
    mkdir, except that any intermediate path segment (not just the rightmost)
    will be created if it does not exist. If the target directory already
    exists, raise an OSError if exist_ok is False. Otherwise no exception is
    raised.  This is recursive.

    """"""
    head, tail = path.split(name)
    if not tail:
        head, tail = path.split(head)
    if head and tail and not path.exists(head):
        try:
            makedirs(head, exist_ok=exist_ok)
        except FileExistsError:
            # Defeats race condition when another thread created the path
            pass
        cdir = curdir
        if isinstance(tail, bytes):
            cdir = bytes(curdir, 'ASCII')
        if tail == cdir:           # xxx/newdir/. exists if xxx/newdir exists
            return
    try:
        mkdir(name, mode)
    except OSError:
        # Cannot rely on checking for EEXIST, since the operating system
        # could give priority to other errors like EACCES or EROFS
        if not exist_ok or not path.isdir(name):
            raise","def remove_directory_and_empty_intermediate_dirs(name):
    """"""remove_directory_and_empty_intermediate_dirs(name)

    Super-rmdir; remove a leaf directory and all empty intermediate
    ones.  Works like rmdir except that, if the leaf directory is
    successfully removed, directories corresponding to rightmost path
    segments will be pruned away until either the whole path is
    consumed or an error occurs.  Errors during this latter phase are
    ignored -- they generally mean that a directory was not empty.

    """"""
    rmdir(name)
    head, tail = path.split(name)
    if not tail:
        head, tail = path.split(head)
    while head and tail:
        try:
            rmdir(head)
        except OSError:
            break
        head, tail = path.split(head)

def create_directory_recursive(name, mode=0o777, exist_ok=False):
    """"""create_directory_recursive(name [, mode=0o777][, exist_ok=False])

    Super-mkdir; create a leaf directory and all intermediate ones.  Works like
    mkdir, except that any intermediate path segment (not just the rightmost)
    will be created if it does not exist. If the target directory already
    exists, raise an OSError if exist_ok is False. Otherwise no exception is
    raised.  This is recursive.

    """"""
    head, tail = path.split(name)
    if not tail:
        head, tail = path.split(head)
    if head and tail and not path.exists(head):
        try:
            create_directory_recursive(head, exist_ok=exist_ok)
        except FileExistsError:
            # Defeats race condition when another thread created the path
            pass
        cdir = curdir
        if isinstance(tail, bytes):
            cdir = bytes(curdir, 'ASCII')
        if tail == cdir:           # xxx/newdir/. exists if xxx/newdir exists
            return
    try:
        mkdir(name, mode)
    except OSError:
        # Cannot rely on checking for EEXIST, since the operating system
        # could give priority to other errors like EACCES or EROFS
        if not exist_ok or not path.isdir(name):
            raise","def rename_old_to_new_across_devices_handling_directories(old: str, new: str) -> None:
    """"""Like os.rename_old_to_new_across_devices_handling_directories(), but handles renaming across devices.""""""
    # Implementation borrowed from os.rename_old_to_new_across_devices_handling_directories().
    head, tail = os.path.split(new)
    if head and tail and not os.path.exists(head):
        os.create_directory_recursive(head)

    shutil.move(old, new)

    head, tail = os.path.split(old)
    if head and tail:
        try:
            os.remove_directory_and_empty_intermediate_dirs(head)
        except OSError:
            pass","{""removedirs"": ""remove_directory_and_empty_intermediate_dirs"", ""makedirs"": ""create_directory_recursive""}","def 0(name):
    """"""0(name)

    Super-rmdir; remove a leaf directory and all empty intermediate
    ones.  Works like rmdir except that, if the leaf directory is
    successfully removed, directories corresponding to rightmost path
    segments will be pruned away until either the whole path is
    consumed or an error occurs.  Errors during this latter phase are
    ignored -- they generally mean that a directory was not empty.

    """"""
    rmdir(name)
    head, tail = path.split(name)
    if not tail:
        head, tail = path.split(head)
    while head and tail:
        try:
            rmdir(head)
        except OSError:
            break
        head, tail = path.split(head)

def 1(name, mode=0o777, exist_ok=False):
    """"""1(name [, mode=0o777][, exist_ok=False])

    Super-mkdir; create a leaf directory and all intermediate ones.  Works like
    mkdir, except that any intermediate path segment (not just the rightmost)
    will be created if it does not exist. If the target directory already
    exists, raise an OSError if exist_ok is False. Otherwise no exception is
    raised.  This is recursive.

    """"""
    head, tail = path.split(name)
    if not tail:
        head, tail = path.split(head)
    if head and tail and not path.exists(head):
        try:
            1(head, exist_ok=exist_ok)
        except FileExistsError:
            # Defeats race condition when another thread created the path
            pass
        cdir = curdir
        if isinstance(tail, bytes):
            cdir = bytes(curdir, 'ASCII')
        if tail == cdir:           # xxx/newdir/. exists if xxx/newdir exists
            return
    try:
        mkdir(name, mode)
    except OSError:
        # Cannot rely on checking for EEXIST, since the operating system
        # could give priority to other errors like EACCES or EROFS
        if not exist_ok or not path.isdir(name):
            raise","def 2(old: str, new: str) -> None:
    """"""Like os.2(), but handles renaming across devices.""""""
    # Implementation borrowed from os.2().
    head, tail = os.path.split(new)
    if head and tail and not os.path.exists(head):
        os.1(head)

    shutil.move(old, new)

    head, tail = os.path.split(old)
    if head and tail:
        try:
            os.0(head)
        except OSError:
            pass","{""removedirs"": ""0"", ""makedirs"": ""1""}"
77,77,"def execlp(file, *args):
    """"""execlp(file, *args)

    Execute the executable file (which is searched for along $PATH)
    with argument list args, replacing the current process. """"""
    execvp(file, args)","def execvp(file, args):
    """"""execvp(file, args)

    Execute the executable file (which is searched for along $PATH)
    with argument list args, replacing the current process.
    args may be a list or tuple of strings. """"""
    _execvpe(file, args)","def execute_executable_with_argument_list(file, args):
    """"""execute_executable_with_argument_list(file, args)

    Execute the executable file (which is searched for along $PATH)
    with argument list args, replacing the current process.
    args may be a list or tuple of strings. """"""
    _execute_executable_with_argument_liste(file, args)","def execute_executable_file_along_path_replacing_current_process(file, *args):
    """"""execute_executable_file_along_path_replacing_current_process(file, *args)

    Execute the executable file (which is searched for along $PATH)
    with argument list args, replacing the current process. """"""
    execute_executable_with_argument_list(file, args)","{""execvp"": ""execute_executable_with_argument_list""}","def 0(file, args):
    """"""0(file, args)

    Execute the executable file (which is searched for along $PATH)
    with argument list args, replacing the current process.
    args may be a list or tuple of strings. """"""
    _execvpe(file, args)","def 1(file, *args):
    """"""1(file, *args)

    Execute the executable file (which is searched for along $PATH)
    with argument list args, replacing the current process. """"""
    0(file, args)","{""execvp"": ""0""}"
78,78,"def execlpe(file, *args):
    """"""execlpe(file, *args, env)

    Execute the executable file (which is searched for along $PATH)
    with argument list args and environment env, replacing the current
    process. """"""
    env = args[-1]
    execvpe(file, args[:-1], env)","def execvpe(file, args, env):
    """"""execvpe(file, args, env)

    Execute the executable file (which is searched for along $PATH)
    with argument list args and environment env, replacing the
    current process.
    args may be a list or tuple of strings. """"""
    _execvpe(file, args, env)","def execute_executable_with_argument_list_and_environment(file, args, env):
    """"""execute_executable_with_argument_list_and_environment(file, args, env)

    Execute the executable file (which is searched for along $PATH)
    with argument list args and environment env, replacing the
    current process.
    args may be a list or tuple of strings. """"""
    _execute_executable_with_argument_list_and_environment(file, args, env)","def execute_and_replace_current_process_environment(file, *args):
    """"""execute_and_replace_current_process_environment(file, *args, env)

    Execute the executable file (which is searched for along $PATH)
    with argument list args and environment env, replacing the current
    process. """"""
    env = args[-1]
    execute_executable_with_argument_list_and_environment(file, args[:-1], env)","{""execvpe"": ""execute_executable_with_argument_list_and_environment""}","def 0(file, args, env):
    """"""0(file, args, env)

    Execute the executable file (which is searched for along $PATH)
    with argument list args and environment env, replacing the
    current process.
    args may be a list or tuple of strings. """"""
    _0(file, args, env)","def 1(file, *args):
    """"""1(file, *args, env)

    Execute the executable file (which is searched for along $PATH)
    with argument list args and environment env, replacing the current
    process. """"""
    env = args[-1]
    0(file, args[:-1], env)","{""execvpe"": ""0""}"
79,79,"def execvp(file, args):
    """"""execvp(file, args)

    Execute the executable file (which is searched for along $PATH)
    with argument list args, replacing the current process.
    args may be a list or tuple of strings. """"""
    _execvpe(file, args)","def _execvpe(file, args, env=None):
    if env is not None:
        exec_func = execve
        argrest = (args, env)
    else:
        exec_func = execv
        argrest = (args,)
        env = environ

    if path.dirname(file):
        exec_func(file, *argrest)
        return
    saved_exc = None
    path_list = get_exec_path(env)
    if name != 'nt':
        file = fsencode(file)
        path_list = map(fsencode, path_list)
    for dir in path_list:
        fullname = path.join(dir, file)
        try:
            exec_func(fullname, *argrest)
        except (FileNotFoundError, NotADirectoryError) as e:
            last_exc = e
        except OSError as e:
            last_exc = e
            if saved_exc is None:
                saved_exc = e
    if saved_exc is not None:
        raise saved_exc
    raise last_exc","def _execute_via_path_variable(file, args, env=None):
    if env is not None:
        exec_func = execve
        argrest = (args, env)
    else:
        exec_func = execv
        argrest = (args,)
        env = environ

    if path.dirname(file):
        exec_func(file, *argrest)
        return
    saved_exc = None
    path_list = get_exec_path(env)
    if name != 'nt':
        file = fsencode(file)
        path_list = map(fsencode, path_list)
    for dir in path_list:
        fullname = path.join(dir, file)
        try:
            exec_func(fullname, *argrest)
        except (FileNotFoundError, NotADirectoryError) as e:
            last_exc = e
        except OSError as e:
            last_exc = e
            if saved_exc is None:
                saved_exc = e
    if saved_exc is not None:
        raise saved_exc
    raise last_exc","def execute_executable_with_argument_list(file, args):
    """"""execute_executable_with_argument_list(file, args)

    Execute the executable file (which is searched for along $PATH)
    with argument list args, replacing the current process.
    args may be a list or tuple of strings. """"""
    _execute_via_path_variable(file, args)","{""_execvpe"": ""_execute_via_path_variable""}","def 0(file, args, env=None):
    if env is not None:
        exec_func = execve
        argrest = (args, env)
    else:
        exec_func = execv
        argrest = (args,)
        env = environ

    if path.dirname(file):
        exec_func(file, *argrest)
        return
    saved_exc = None
    path_list = get_exec_path(env)
    if name != 'nt':
        file = fsencode(file)
        path_list = map(fsencode, path_list)
    for dir in path_list:
        fullname = path.join(dir, file)
        try:
            exec_func(fullname, *argrest)
        except (FileNotFoundError, NotADirectoryError) as e:
            last_exc = e
        except OSError as e:
            last_exc = e
            if saved_exc is None:
                saved_exc = e
    if saved_exc is not None:
        raise saved_exc
    raise last_exc","def 1(file, args):
    """"""1(file, args)

    Execute the executable file (which is searched for along $PATH)
    with argument list args, replacing the current process.
    args may be a list or tuple of strings. """"""
    0(file, args)","{""_execvpe"": ""0""}"
80,80,"def execvpe(file, args, env):
    """"""execvpe(file, args, env)

    Execute the executable file (which is searched for along $PATH)
    with argument list args and environment env, replacing the
    current process.
    args may be a list or tuple of strings. """"""
    _execvpe(file, args, env)","def _execvpe(file, args, env=None):
    if env is not None:
        exec_func = execve
        argrest = (args, env)
    else:
        exec_func = execv
        argrest = (args,)
        env = environ

    if path.dirname(file):
        exec_func(file, *argrest)
        return
    saved_exc = None
    path_list = get_exec_path(env)
    if name != 'nt':
        file = fsencode(file)
        path_list = map(fsencode, path_list)
    for dir in path_list:
        fullname = path.join(dir, file)
        try:
            exec_func(fullname, *argrest)
        except (FileNotFoundError, NotADirectoryError) as e:
            last_exc = e
        except OSError as e:
            last_exc = e
            if saved_exc is None:
                saved_exc = e
    if saved_exc is not None:
        raise saved_exc
    raise last_exc","def _execute_via_path_variable(file, args, env=None):
    if env is not None:
        exec_func = execve
        argrest = (args, env)
    else:
        exec_func = execv
        argrest = (args,)
        env = environ

    if path.dirname(file):
        exec_func(file, *argrest)
        return
    saved_exc = None
    path_list = get_exec_path(env)
    if name != 'nt':
        file = fsencode(file)
        path_list = map(fsencode, path_list)
    for dir in path_list:
        fullname = path.join(dir, file)
        try:
            exec_func(fullname, *argrest)
        except (FileNotFoundError, NotADirectoryError) as e:
            last_exc = e
        except OSError as e:
            last_exc = e
            if saved_exc is None:
                saved_exc = e
    if saved_exc is not None:
        raise saved_exc
    raise last_exc","def execute_executable_with_argument_list_and_environment(file, args, env):
    """"""execute_executable_with_argument_list_and_environment(file, args, env)

    Execute the executable file (which is searched for along $PATH)
    with argument list args and environment env, replacing the
    current process.
    args may be a list or tuple of strings. """"""
    _execute_via_path_variable(file, args, env)","{""_execvpe"": ""_execute_via_path_variable""}","def 0(file, args, env=None):
    if env is not None:
        exec_func = execve
        argrest = (args, env)
    else:
        exec_func = execv
        argrest = (args,)
        env = environ

    if path.dirname(file):
        exec_func(file, *argrest)
        return
    saved_exc = None
    path_list = get_exec_path(env)
    if name != 'nt':
        file = fsencode(file)
        path_list = map(fsencode, path_list)
    for dir in path_list:
        fullname = path.join(dir, file)
        try:
            exec_func(fullname, *argrest)
        except (FileNotFoundError, NotADirectoryError) as e:
            last_exc = e
        except OSError as e:
            last_exc = e
            if saved_exc is None:
                saved_exc = e
    if saved_exc is not None:
        raise saved_exc
    raise last_exc","def 1(file, args, env):
    """"""1(file, args, env)

    Execute the executable file (which is searched for along $PATH)
    with argument list args and environment env, replacing the
    current process.
    args may be a list or tuple of strings. """"""
    0(file, args, env)","{""_execvpe"": ""0""}"
81,81,"def isdata(object):
    """"""Check if an object is of a type that probably means it's data.""""""
    return not (inspect.ismodule(object) or _isclass(object) or
                inspect.isroutine(object) or inspect.isframe(object) or
                inspect.istraceback(object) or inspect.iscode(object))","def _isclass(object):
    return inspect.isclass(object) and not isinstance(object, types.GenericAlias)","def _is_non_generic_class(object):
    return inspect.isclass(object) and not isinstance(object, types.GenericAlias)","def check_if_object_represents_data(object):
    """"""Check if an object is of a type that probably means it's data.""""""
    return not (inspect.ismodule(object) or _is_non_generic_class(object) or
                inspect.isroutine(object) or inspect.isframe(object) or
                inspect.istraceback(object) or inspect.iscode(object))","{""_isclass"": ""_is_non_generic_class""}","def 0(object):
    return inspect.isclass(object) and not isinstance(object, types.GenericAlias)","def 1(object):
    """"""Check if an object is of a type that probably means it's data.""""""
    return not (inspect.ismodule(object) or 0(object) or
                inspect.isroutine(object) or inspect.isframe(object) or
                inspect.istraceback(object) or inspect.iscode(object))","{""_isclass"": ""0""}"
82,82,"def sort_attributes(attrs, object):
    'Sort the attrs list in-place by _fields and then alphabetically by name'
    # This allows data descriptors to be ordered according
    # to a _fields attribute if present.
    fields = getattr(object, '_fields', [])
    try:
        field_order = {name : i-len(fields) for (i, name) in enumerate(fields)}
    except TypeError:
        field_order = {}
    keyfunc = lambda attr: (field_order.get(attr[0], 0), attr[0])
    attrs.sort(key=keyfunc)","def enumerate():
    """"""Return a list of all Thread objects currently alive.

    The list includes daemonic threads, dummy thread objects created by
    current_thread(), and the main thread. It excludes terminated threads and
    threads that have not yet been started.

    """"""
    with _active_limbo_lock:
        return list(_active.values()) + list(_limbo.values())","def get_alive_threads():
    """"""Return a list of all Thread objects currently alive.

    The list includes daemonic threads, dummy thread objects created by
    current_thread(), and the main thread. It excludes terminated threads and
    threads that have not yet been started.

    """"""
    with _active_limbo_lock:
        return list(_active.values()) + list(_limbo.values())","def sort_attrs_in_place_by_fields_and_alphabetically(attrs, object):
    'Sort the attrs list in-place by _fields and then alphabetically by name'
    # This allows data descriptors to be ordered according
    # to a _fields attribute if present.
    fields = getattr(object, '_fields', [])
    try:
        field_order = {name : i-len(fields) for (i, name) in get_alive_threads(fields)}
    except TypeError:
        field_order = {}
    keyfunc = lambda attr: (field_order.get(attr[0], 0), attr[0])
    attrs.sort(key=keyfunc)","{""enumerate"": ""get_alive_threads""}","def 0():
    """"""Return a list of all Thread objects currently alive.

    The list includes daemonic threads, dummy thread objects created by
    current_thread(), and the main thread. It excludes terminated threads and
    threads that have not yet been started.

    """"""
    with _active_limbo_lock:
        return list(_active.values()) + list(_limbo.values())","def 1(attrs, object):
    'Sort the attrs list in-place by _fields and then alphabetically by name'
    # This allows data descriptors to be ordered according
    # to a _fields attribute if present.
    fields = getattr(object, '_fields', [])
    try:
        field_order = {name : i-len(fields) for (i, name) in 0(fields)}
    except TypeError:
        field_order = {}
    keyfunc = lambda attr: (field_order.get(attr[0], 0), attr[0])
    attrs.sort(key=keyfunc)","{""enumerate"": ""0""}"
83,83,"def pager(text):
    """"""The first time this is called, determine what kind of pager to use.""""""
    global pager
    pager = getpager()
    pager(text)","def pager(text):
    """"""The first time this is called, determine what kind of pager to use.""""""
    global pager
    pager = getpager()
    pager(text)

def getpager():
    """"""Decide what method to use for paging through text.""""""
    if not hasattr(sys.stdin, ""isatty""):
        return plainpager
    if not hasattr(sys.stdout, ""isatty""):
        return plainpager
    if not sys.stdin.isatty() or not sys.stdout.isatty():
        return plainpager
    use_pager = os.environ.get('MANPAGER') or os.environ.get('PAGER')
    if use_pager:
        if sys.platform == 'win32': # pipes completely broken in Windows
            return lambda text: tempfilepager(plain(text), use_pager)
        elif os.environ.get('TERM') in ('dumb', 'emacs'):
            return lambda text: pipepager(plain(text), use_pager)
        else:
            return lambda text: pipepager(text, use_pager)
    if os.environ.get('TERM') in ('dumb', 'emacs'):
        return plainpager
    if sys.platform == 'win32':
        return lambda text: tempfilepager(plain(text), 'more <')
    if hasattr(os, 'system') and os.system('(less) 2>/dev/null') == 0:
        return lambda text: pipepager(text, 'less')

    import tempfile
    (fd, filename) = tempfile.mkstemp()
    os.close(fd)
    try:
        if hasattr(os, 'system') and os.system('more ""%s""' % filename) == 0:
            return lambda text: pipepager(text, 'more')
        else:
            return ttypager
    finally:
        os.unlink(filename)","def determine_and_use_appropriate_pager(text):
    """"""The first time this is called, determine what kind of determine_and_use_appropriate_pager to use.""""""
    global determine_and_use_appropriate_pager
    determine_and_use_appropriate_pager = getdetermine_and_use_appropriate_pager()
    determine_and_use_appropriate_pager(text)

def getdetermine_and_use_appropriate_pager():
    """"""Decide what method to use for paging through text.""""""
    if not hasattr(sys.stdin, ""isatty""):
        return plaindetermine_and_use_appropriate_pager
    if not hasattr(sys.stdout, ""isatty""):
        return plaindetermine_and_use_appropriate_pager
    if not sys.stdin.isatty() or not sys.stdout.isatty():
        return plaindetermine_and_use_appropriate_pager
    use_determine_and_use_appropriate_pager = os.environ.get('MANPAGER') or os.environ.get('PAGER')
    if use_determine_and_use_appropriate_pager:
        if sys.platform == 'win32': # pipes completely broken in Windows
            return lambda text: tempfiledetermine_and_use_appropriate_pager(plain(text), use_determine_and_use_appropriate_pager)
        elif os.environ.get('TERM') in ('dumb', 'emacs'):
            return lambda text: pipedetermine_and_use_appropriate_pager(plain(text), use_determine_and_use_appropriate_pager)
        else:
            return lambda text: pipedetermine_and_use_appropriate_pager(text, use_determine_and_use_appropriate_pager)
    if os.environ.get('TERM') in ('dumb', 'emacs'):
        return plaindetermine_and_use_appropriate_pager
    if sys.platform == 'win32':
        return lambda text: tempfiledetermine_and_use_appropriate_pager(plain(text), 'more <')
    if hasattr(os, 'system') and os.system('(less) 2>/dev/null') == 0:
        return lambda text: pipedetermine_and_use_appropriate_pager(text, 'less')

    import tempfile
    (fd, filename) = tempfile.mkstemp()
    os.close(fd)
    try:
        if hasattr(os, 'system') and os.system('more ""%s""' % filename) == 0:
            return lambda text: pipedetermine_and_use_appropriate_pager(text, 'more')
        else:
            return ttydetermine_and_use_appropriate_pager
    finally:
        os.unlink(filename)","def determine_and_use_appropriate_determine_and_use_appropriate_pager(text):
    """"""The first time this is called, determine what kind of determine_and_use_appropriate_determine_and_use_appropriate_pager to use.""""""
    global determine_and_use_appropriate_determine_and_use_appropriate_pager
    determine_and_use_appropriate_determine_and_use_appropriate_pager = getdetermine_and_use_appropriate_determine_and_use_appropriate_pager()
    determine_and_use_appropriate_determine_and_use_appropriate_pager(text)","{""pager"": ""determine_and_use_appropriate_pager"", ""getpager"": ""decide_paging_method""}","def 0(text):
    """"""The first time this is called, determine what kind of pager to use.""""""
    global pager
    pager = get0()
    0(text)

def get0():
    """"""Decide what method to use for paging through text.""""""
    if not hasattr(sys.stdin, ""isatty""):
        return plainpager
    if not hasattr(sys.stdout, ""isatty""):
        return plainpager
    if not sys.stdin.isatty() or not sys.stdout.isatty():
        return plainpager
    use_pager = os.environ.get('MANPAGER') or os.environ.get('PAGER')
    if use_pager:
        if sys.platform == 'win32': # pipes completely broken in Windows
            return lambda text: tempfile0(plain(text), use_pager)
        elif os.environ.get('TERM') in ('dumb', 'emacs'):
            return lambda text: pipe0(plain(text), use_pager)
        else:
            return lambda text: pipe0(text, use_pager)
    if os.environ.get('TERM') in ('dumb', 'emacs'):
        return plainpager
    if sys.platform == 'win32':
        return lambda text: tempfile0(plain(text), 'more <')
    if hasattr(os, 'system') and os.system('(less) 2>/dev/null') == 0:
        return lambda text: pipe0(text, 'less')

    import tempfile
    (fd, filename) = tempfile.mkstemp()
    os.close(fd)
    try:
        if hasattr(os, 'system') and os.system('more ""%s""' % filename) == 0:
            return lambda text: pipe0(text, 'more')
        else:
            return ttypager
    finally:
        os.unlink(filename)","def 0(text):
    """"""The first time this is called, determine what kind of pager to use.""""""
    global pager
    pager = get0()
    0(text)","{""pager"": ""0"", ""getpager"": ""1""}"
84,84,"def plainpager(text):
    """"""Simply print unformatted text.  This is the ultimate fallback.""""""
    sys.stdout.write(plain(_escape_stdout(text)))","def _escape_stdout(text):
    # Escape non-encodable characters to avoid encoding errors later
    encoding = getattr(sys.stdout, 'encoding', None) or 'utf-8'
    return text.encode(encoding, 'backslashreplace').decode(encoding)

def plain(text):
    """"""Remove boldface formatting from text.""""""
    return re.sub('.\b', '', text)","def _encode_stdout_with_escape(text):
    # Escape non-encodable characters to avoid encoding errors later
    encoding = getattr(sys.stdout, 'encoding', None) or 'utf-8'
    return text.encode(encoding, 'backslashreplace').decode(encoding)

def remove_bold_formatting(text):
    """"""Remove boldface formatting from text.""""""
    return re.sub('.\b', '', text)","def remove_bold_formattingpager(text):
    """"""Simply print unformatted text.  This is the ultimate fallback.""""""
    sys.stdout.write(remove_bold_formatting(_encode_stdout_with_escape(text)))","{""_escape_stdout"": ""_encode_stdout_with_escape"", ""plain"": ""remove_bold_formatting""}","def 0(text):
    # Escape non-encodable characters to avoid encoding errors later
    encoding = getattr(sys.stdout, 'encoding', None) or 'utf-8'
    return text.encode(encoding, 'backslashreplace').decode(encoding)

def 1(text):
    """"""Remove boldface formatting from text.""""""
    return re.sub('.\b', '', text)","def 2(text):
    """"""Simply print unformatted text.  This is the ultimate fallback.""""""
    sys.stdout.write(1(0(text)))","{""_escape_stdout"": ""0"", ""plain"": ""1""}"
85,85,"def describe(thing):
    """"""Produce a short description of the given thing.""""""
    if inspect.ismodule(thing):
        if thing.__name__ in sys.builtin_module_names:
            return 'built-in module ' + thing.__name__
        if hasattr(thing, '__path__'):
            return 'package ' + thing.__name__
        else:
            return 'module ' + thing.__name__
    if inspect.isbuiltin(thing):
        return 'built-in function ' + thing.__name__
    if inspect.isgetsetdescriptor(thing):
        return 'getset descriptor %s.%s.%s' % (
            thing.__objclass__.__module__, thing.__objclass__.__name__,
            thing.__name__)
    if inspect.ismemberdescriptor(thing):
        return 'member descriptor %s.%s.%s' % (
            thing.__objclass__.__module__, thing.__objclass__.__name__,
            thing.__name__)
    if _isclass(thing):
        return 'class ' + thing.__name__
    if inspect.isfunction(thing):
        return 'function ' + thing.__name__
    if inspect.ismethod(thing):
        return 'method ' + thing.__name__
    return type(thing).__name__","def _isclass(object):
    return inspect.isclass(object) and not isinstance(object, types.GenericAlias)","def _is_non_generic_class(object):
    return inspect.isclass(object) and not isinstance(object, types.GenericAlias)","def produce_short_description_of_given_thing(thing):
    """"""Produce a short description of the given thing.""""""
    if inspect.ismodule(thing):
        if thing.__name__ in sys.builtin_module_names:
            return 'built-in module ' + thing.__name__
        if hasattr(thing, '__path__'):
            return 'package ' + thing.__name__
        else:
            return 'module ' + thing.__name__
    if inspect.isbuiltin(thing):
        return 'built-in function ' + thing.__name__
    if inspect.isgetsetdescriptor(thing):
        return 'getset descriptor %s.%s.%s' % (
            thing.__objclass__.__module__, thing.__objclass__.__name__,
            thing.__name__)
    if inspect.ismemberdescriptor(thing):
        return 'member descriptor %s.%s.%s' % (
            thing.__objclass__.__module__, thing.__objclass__.__name__,
            thing.__name__)
    if _is_non_generic_class(thing):
        return 'class ' + thing.__name__
    if inspect.isfunction(thing):
        return 'function ' + thing.__name__
    if inspect.ismethod(thing):
        return 'method ' + thing.__name__
    return type(thing).__name__","{""_isclass"": ""_is_non_generic_class""}","def 0(object):
    return inspect.isclass(object) and not isinstance(object, types.GenericAlias)","def 1(thing):
    """"""Produce a short description of the given thing.""""""
    if inspect.ismodule(thing):
        if thing.__name__ in sys.builtin_module_names:
            return 'built-in module ' + thing.__name__
        if hasattr(thing, '__path__'):
            return 'package ' + thing.__name__
        else:
            return 'module ' + thing.__name__
    if inspect.isbuiltin(thing):
        return 'built-in function ' + thing.__name__
    if inspect.isgetsetdescriptor(thing):
        return 'getset descriptor %s.%s.%s' % (
            thing.__objclass__.__module__, thing.__objclass__.__name__,
            thing.__name__)
    if inspect.ismemberdescriptor(thing):
        return 'member descriptor %s.%s.%s' % (
            thing.__objclass__.__module__, thing.__objclass__.__name__,
            thing.__name__)
    if 0(thing):
        return 'class ' + thing.__name__
    if inspect.isfunction(thing):
        return 'function ' + thing.__name__
    if inspect.ismethod(thing):
        return 'method ' + thing.__name__
    return type(thing).__name__","{""_isclass"": ""0""}"
86,86,"def writedocs(dir, pkgpath='', done=None):
    """"""Write out HTML documentation for all modules in a directory tree.""""""
    if done is None: done = {}
    for importer, modname, ispkg in pkgutil.walk_packages([dir], pkgpath):
        writedoc(modname)
    return","def writedoc(thing, forceload=0):
    """"""Write HTML documentation to a file in the current directory.""""""
    try:
        object, name = resolve(thing, forceload)
        page = html.page(describe(object), html.document(object, name))
        with open(name + '.html', 'w', encoding='utf-8') as file:
            file.write(page)
        print('wrote', name + '.html')
    except (ImportError, ErrorDuringImport) as value:
        print(value)","def write_html_documentation_to_file(thing, forceload=0):
    """"""Write HTML documentation to a file in the current directory.""""""
    try:
        object, name = resolve(thing, forceload)
        page = html.page(describe(object), html.document(object, name))
        with open(name + '.html', 'w', encoding='utf-8') as file:
            file.write(page)
        print('wrote', name + '.html')
    except (ImportError, ErrorDuringImport) as value:
        print(value)","def write_html_documentation_to_files(dir, pkgpath='', done=None):
    """"""Write out HTML documentation for all modules in a directory tree.""""""
    if done is None: done = {}
    for importer, modname, ispkg in pkgutil.walk_packages([dir], pkgpath):
        write_html_documentation_to_file(modname)
    return","{""writedoc"": ""write_html_documentation_to_file""}","def 0(thing, forceload=0):
    """"""Write HTML documentation to a file in the current directory.""""""
    try:
        object, name = resolve(thing, forceload)
        page = html.page(describe(object), html.document(object, name))
        with open(name + '.html', 'w', encoding='utf-8') as file:
            file.write(page)
        print('wrote', name + '.html')
    except (ImportError, ErrorDuringImport) as value:
        print(value)","def 1(dir, pkgpath='', done=None):
    """"""Write out HTML documentation for all modules in a directory tree.""""""
    if done is None: done = {}
    for importer, modname, ispkg in pkgutil.walk_packages([dir], pkgpath):
        0(modname)
    return","{""writedoc"": ""0""}"
87,87,"def _adjust_cli_sys_path():
    """"""Ensures current directory is on sys.path, and __main__ directory is not.

    Exception: __main__ dir is left alone if it's also pydoc's directory.
    """"""
    revised_path = _get_revised_path(sys.path, sys.argv[0])
    if revised_path is not None:
        sys.path[:] = revised_path","def _get_revised_path(given_path, argv0):
    """"""Ensures current directory is on returned path, and argv0 directory is not

    Exception: argv0 dir is left alone if it's also pydoc's directory.

    Returns a new path entry list, or None if no adjustment is needed.
    """"""
    # Scripts may get the current directory in their path by default if they're
    # run with the -m switch, or directly from the current directory.
    # The interactive prompt also allows imports from the current directory.

    # Accordingly, if the current directory is already present, don't make
    # any changes to the given_path
    if '' in given_path or os.curdir in given_path or os.getcwd() in given_path:
        return None

    # Otherwise, add the current directory to the given path, and remove the
    # script directory (as long as the latter isn't also pydoc's directory.
    stdlib_dir = os.path.dirname(__file__)
    script_dir = os.path.dirname(argv0)
    revised_path = given_path.copy()
    if script_dir in given_path and not os.path.samefile(script_dir, stdlib_dir):
        revised_path.remove(script_dir)
    revised_path.insert(0, os.getcwd())
    return revised_path","def _adjust_path_for_current_directory(given_path, argv0):
    """"""Ensures current directory is on returned path, and argv0 directory is not

    Exception: argv0 dir is left alone if it's also pydoc's directory.

    Returns a new path entry list, or None if no adjustment is needed.
    """"""
    # Scripts may get the current directory in their path by default if they're
    # run with the -m switch, or directly from the current directory.
    # The interactive prompt also allows imports from the current directory.

    # Accordingly, if the current directory is already present, don't make
    # any changes to the given_path
    if '' in given_path or os.curdir in given_path or os.getcwd() in given_path:
        return None

    # Otherwise, add the current directory to the given path, and remove the
    # script directory (as long as the latter isn't also pydoc's directory.
    stdlib_dir = os.path.dirname(__file__)
    script_dir = os.path.dirname(argv0)
    revised_path = given_path.copy()
    if script_dir in given_path and not os.path.samefile(script_dir, stdlib_dir):
        revised_path.remove(script_dir)
    revised_path.insert(0, os.getcwd())
    return revised_path","def ensure_current_dir_on_sys_path_and_exclude_main():
    """"""Ensures current directory is on sys.path, and __main__ directory is not.

    Exception: __main__ dir is left alone if it's also pydoc's directory.
    """"""
    revised_path = _adjust_path_for_current_directory(sys.path, sys.argv[0])
    if revised_path is not None:
        sys.path[:] = revised_path","{""_get_revised_path"": ""_adjust_path_for_current_directory""}","def 0(given_path, argv0):
    """"""Ensures current directory is on returned path, and argv0 directory is not

    Exception: argv0 dir is left alone if it's also pydoc's directory.

    Returns a new path entry list, or None if no adjustment is needed.
    """"""
    # Scripts may get the current directory in their path by default if they're
    # run with the -m switch, or directly from the current directory.
    # The interactive prompt also allows imports from the current directory.

    # Accordingly, if the current directory is already present, don't make
    # any changes to the given_path
    if '' in given_path or os.curdir in given_path or os.getcwd() in given_path:
        return None

    # Otherwise, add the current directory to the given path, and remove the
    # script directory (as long as the latter isn't also pydoc's directory.
    stdlib_dir = os.path.dirname(__file__)
    script_dir = os.path.dirname(argv0)
    revised_path = given_path.copy()
    if script_dir in given_path and not os.path.samefile(script_dir, stdlib_dir):
        revised_path.remove(script_dir)
    revised_path.insert(0, os.getcwd())
    return revised_path","def 1():
    """"""Ensures current directory is on sys.path, and __main__ directory is not.

    Exception: __main__ dir is left alone if it's also pydoc's directory.
    """"""
    revised_path = 0(sys.path, sys.argv[0])
    if revised_path is not None:
        sys.path[:] = revised_path","{""_get_revised_path"": ""0""}"
88,88,"def _keep_alive(x, memo):
    """"""Keeps a reference to the object x in the memo.

    Because we remember objects by their id, we have
    to assure that possibly temporary objects are kept
    alive by referencing them.
    We store a reference at the id of the memo, which should
    normally not be used unless someone tries to deepcopy
    the memo itself...
    """"""
    try:
        memo[id(memo)].append(x)
    except KeyError:
        # aha, this is the first one :-)
        memo[id(memo)]=[x]","def id() -> str:
    """"""
    Return the distro ID of the current distribution, as a
    machine-readable string.

    For a number of OS distributions, the returned distro ID value is
    *reliable*, in the sense that it is documented and that it does not change
    across releases of the distribution.

    This package maintains the following reliable distro ID values:

    ==============  =========================================
    Distro ID       Distribution
    ==============  =========================================
    ""ubuntu""        Ubuntu
    ""debian""        Debian
    ""rhel""          RedHat Enterprise Linux
    ""centos""        CentOS
    ""fedora""        Fedora
    ""sles""          SUSE Linux Enterprise Server
    ""opensuse""      openSUSE
    ""amzn""          Amazon Linux
    ""arch""          Arch Linux
    ""cloudlinux""    CloudLinux OS
    ""exherbo""       Exherbo Linux
    ""gentoo""        GenToo Linux
    ""ibm_powerkvm""  IBM PowerKVM
    ""kvmibm""        KVM for IBM z Systems
    ""linuxmint""     Linux Mint
    ""mageia""        Mageia
    ""mandriva""      Mandriva Linux
    ""parallels""     Parallels
    ""pidora""        Pidora
    ""raspbian""      Raspbian
    ""oracle""        Oracle Linux (and Oracle Enterprise Linux)
    ""scientific""    Scientific Linux
    ""slackware""     Slackware
    ""xenserver""     XenServer
    ""openbsd""       OpenBSD
    ""netbsd""        NetBSD
    ""freebsd""       FreeBSD
    ""midnightbsd""   MidnightBSD
    ""rocky""         Rocky Linux
    ""aix""           AIX
    ==============  =========================================

    If you have a need to get distros for reliable IDs added into this set,
    or if you find that the :func:`distro.id` function returns a different
    distro ID for one of the listed distros, please create an issue in the
    `distro issue tracker`_.

    **Lookup hierarchy and transformations:**

    First, the ID is obtained from the following sources, in the specified
    order. The first available and non-empty value is used:

    * the value of the ""ID"" attribute of the os-release file,

    * the value of the ""Distributor ID"" attribute returned by the lsb_release
      command,

    * the first part of the file name of the distro release file,

    The so determined ID value then passes the following transformations,
    before it is returned by this method:

    * it is translated to lower case,

    * blanks (which should not be there anyway) are translated to underscores,

    * a normalization of the ID is performed, based upon
      `normalization tables`_. The purpose of this normalization is to ensure
      that the ID is as reliable as possible, even across incompatible changes
      in the OS distributions. A common reason for an incompatible change is
      the addition of an os-release file, or the addition of the lsb_release
      command, with ID values that differ from what was previously determined
      from the distro release file name.
    """"""
    return _distro.id()","def id() -> str:
    """"""
    Return the distro ID of the current distribution, as a
    machine-readable string.

    For a number of OS distributions, the returned distro ID value is
    *reliable*, in the sense that it is documented and that it does not change
    across releases of the distribution.

    This package maintains the following reliable distro ID values:

    ==============  =========================================
    Distro ID       Distribution
    ==============  =========================================
    ""ubuntu""        Ubuntu
    ""debian""        Debian
    ""rhel""          RedHat Enterprise Linux
    ""centos""        CentOS
    ""fedora""        Fedora
    ""sles""          SUSE Linux Enterprise Server
    ""opensuse""      openSUSE
    ""amzn""          Amazon Linux
    ""arch""          Arch Linux
    ""cloudlinux""    CloudLinux OS
    ""exherbo""       Exherbo Linux
    ""gentoo""        GenToo Linux
    ""ibm_powerkvm""  IBM PowerKVM
    ""kvmibm""        KVM for IBM z Systems
    ""linuxmint""     Linux Mint
    ""mageia""        Mageia
    ""mandriva""      Mandriva Linux
    ""parallels""     Parallels
    ""pidora""        Pidora
    ""raspbian""      Raspbian
    ""oracle""        Oracle Linux (and Oracle Enterprise Linux)
    ""scientific""    Scientific Linux
    ""slackware""     Slackware
    ""xenserver""     XenServer
    ""openbsd""       OpenBSD
    ""netbsd""        NetBSD
    ""freebsd""       FreeBSD
    ""midnightbsd""   MidnightBSD
    ""rocky""         Rocky Linux
    ""aix""           AIX
    ==============  =========================================

    If you have a need to get distros for reliable IDs added into this set,
    or if you find that the :func:`distro.id` function returns a different
    distro ID for one of the listed distros, please create an issue in the
    `distro issue tracker`_.

    **Lookup hierarchy and transformations:**

    First, the ID is obtained from the following sources, in the specified
    order. The first available and non-empty value is used:

    * the value of the ""ID"" attribute of the os-release file,

    * the value of the ""Distributor ID"" attribute returned by the lsb_release
      command,

    * the first part of the file name of the distro release file,

    The so determined ID value then passes the following transformations,
    before it is returned by this method:

    * it is translated to lower case,

    * blanks (which should not be there anyway) are translated to underscores,

    * a normalization of the ID is performed, based upon
      `normalization tables`_. The purpose of this normalization is to ensure
      that the ID is as reliable as possible, even across incompatible changes
      in the OS distributions. A common reason for an incompatible change is
      the addition of an os-release file, or the addition of the lsb_release
      command, with ID values that differ from what was previously determined
      from the distro release file name.
    """"""
    return _distro.id()","def keep_reference_to_object_in_memo(x, memo):
    """"""Keeps a reference to the object x in the memo.

    Because we remember objects by their id, we have
    to assure that possibly temporary objects are kept
    alive by referencing them.
    We store a reference at the id of the memo, which should
    normally not be used unless someone tries to deepcopy
    the memo itself...
    """"""
    try:
        memo[id(memo)].append(x)
    except KeyError:
        # aha, this is the first one :-)
        memo[id(memo)]=[x]",{},"def id() -> str:
    """"""
    Return the distro ID of the current distribution, as a
    machine-readable string.

    For a number of OS distributions, the returned distro ID value is
    *reliable*, in the sense that it is documented and that it does not change
    across releases of the distribution.

    This package maintains the following reliable distro ID values:

    ==============  =========================================
    Distro ID       Distribution
    ==============  =========================================
    ""ubuntu""        Ubuntu
    ""debian""        Debian
    ""rhel""          RedHat Enterprise Linux
    ""centos""        CentOS
    ""fedora""        Fedora
    ""sles""          SUSE Linux Enterprise Server
    ""opensuse""      openSUSE
    ""amzn""          Amazon Linux
    ""arch""          Arch Linux
    ""cloudlinux""    CloudLinux OS
    ""exherbo""       Exherbo Linux
    ""gentoo""        GenToo Linux
    ""ibm_powerkvm""  IBM PowerKVM
    ""kvmibm""        KVM for IBM z Systems
    ""linuxmint""     Linux Mint
    ""mageia""        Mageia
    ""mandriva""      Mandriva Linux
    ""parallels""     Parallels
    ""pidora""        Pidora
    ""raspbian""      Raspbian
    ""oracle""        Oracle Linux (and Oracle Enterprise Linux)
    ""scientific""    Scientific Linux
    ""slackware""     Slackware
    ""xenserver""     XenServer
    ""openbsd""       OpenBSD
    ""netbsd""        NetBSD
    ""freebsd""       FreeBSD
    ""midnightbsd""   MidnightBSD
    ""rocky""         Rocky Linux
    ""aix""           AIX
    ==============  =========================================

    If you have a need to get distros for reliable IDs added into this set,
    or if you find that the :func:`distro.id` function returns a different
    distro ID for one of the listed distros, please create an issue in the
    `distro issue tracker`_.

    **Lookup hierarchy and transformations:**

    First, the ID is obtained from the following sources, in the specified
    order. The first available and non-empty value is used:

    * the value of the ""ID"" attribute of the os-release file,

    * the value of the ""Distributor ID"" attribute returned by the lsb_release
      command,

    * the first part of the file name of the distro release file,

    The so determined ID value then passes the following transformations,
    before it is returned by this method:

    * it is translated to lower case,

    * blanks (which should not be there anyway) are translated to underscores,

    * a normalization of the ID is performed, based upon
      `normalization tables`_. The purpose of this normalization is to ensure
      that the ID is as reliable as possible, even across incompatible changes
      in the OS distributions. A common reason for an incompatible change is
      the addition of an os-release file, or the addition of the lsb_release
      command, with ID values that differ from what was previously determined
      from the distro release file name.
    """"""
    return _distro.id()","def 1(x, memo):
    """"""Keeps a reference to the object x in the memo.

    Because we remember objects by their id, we have
    to assure that possibly temporary objects are kept
    alive by referencing them.
    We store a reference at the id of the memo, which should
    normally not be used unless someone tries to deepcopy
    the memo itself...
    """"""
    try:
        memo[id(memo)].append(x)
    except KeyError:
        # aha, this is the first one :-)
        memo[id(memo)]=[x]",{}
89,89,"def commonprefix(m):
    ""Given a list of pathnames, returns the longest common leading component""
    if not m: return ''
    # Some people pass in a list of pathname parts to operate in an OS-agnostic
    # fashion; don't try to translate in that case as that's an abuse of the
    # API and they are already doing what they need to be OS-agnostic and so
    # they most likely won't be using an os.PathLike object in the sublists.
    if not isinstance(m[0], (list, tuple)):
        m = tuple(map(os.fspath, m))
    s1 = min(m)
    s2 = max(m)
    for i, c in enumerate(s1):
        if c != s2[i]:
            return s1[:i]
    return s1","def enumerate():
    """"""Return a list of all Thread objects currently alive.

    The list includes daemonic threads, dummy thread objects created by
    current_thread(), and the main thread. It excludes terminated threads and
    threads that have not yet been started.

    """"""
    with _active_limbo_lock:
        return list(_active.values()) + list(_limbo.values())","def get_alive_threads():
    """"""Return a list of all Thread objects currently alive.

    The list includes daemonic threads, dummy thread objects created by
    current_thread(), and the main thread. It excludes terminated threads and
    threads that have not yet been started.

    """"""
    with _active_limbo_lock:
        return list(_active.values()) + list(_limbo.values())","def find_longest_common_leading_component(m):
    ""Given a list of pathnames, returns the longest common leading component""
    if not m: return ''
    # Some people pass in a list of pathname parts to operate in an OS-agnostic
    # fashion; don't try to translate in that case as that's an abuse of the
    # API and they are already doing what they need to be OS-agnostic and so
    # they most likely won't be using an os.PathLike object in the sublists.
    if not isinstance(m[0], (list, tuple)):
        m = tuple(map(os.fspath, m))
    s1 = min(m)
    s2 = max(m)
    for i, c in get_alive_threads(s1):
        if c != s2[i]:
            return s1[:i]
    return s1","{""enumerate"": ""get_alive_threads""}","def 0():
    """"""Return a list of all Thread objects currently alive.

    The list includes daemonic threads, dummy thread objects created by
    current_thread(), and the main thread. It excludes terminated threads and
    threads that have not yet been started.

    """"""
    with _active_limbo_lock:
        return list(_active.values()) + list(_limbo.values())","def 1(m):
    ""Given a list of pathnames, returns the longest common leading component""
    if not m: return ''
    # Some people pass in a list of pathname parts to operate in an OS-agnostic
    # fashion; don't try to translate in that case as that's an abuse of the
    # API and they are already doing what they need to be OS-agnostic and so
    # they most likely won't be using an os.PathLike object in the sublists.
    if not isinstance(m[0], (list, tuple)):
        m = tuple(map(os.fspath, m))
    s1 = min(m)
    s2 = max(m)
    for i, c in 0(s1):
        if c != s2[i]:
            return s1[:i]
    return s1","{""enumerate"": ""0""}"
90,90,"def samefile(f1, f2):
    """"""Test whether two pathnames reference the same actual file or directory

    This is determined by the device number and i-node number and
    raises an exception if an os.stat() call on either pathname fails.
    """"""
    s1 = os.stat(f1)
    s2 = os.stat(f2)
    return samestat(s1, s2)","def samestat(s1, s2):
    """"""Test whether two stat buffers reference the same file""""""
    return (s1.st_ino == s2.st_ino and
            s1.st_dev == s2.st_dev)","def are_stat_buffers_equal(s1, s2):
    """"""Test whether two stat buffers reference the same file""""""
    return (s1.st_ino == s2.st_ino and
            s1.st_dev == s2.st_dev)","def test_whether_pathnames_reference_same_file_or_directory(f1, f2):
    """"""Test whether two pathnames reference the same actual file or directory

    This is determined by the device number and i-node number and
    raises an exception if an os.stat() call on either pathname fails.
    """"""
    s1 = os.stat(f1)
    s2 = os.stat(f2)
    return are_stat_buffers_equal(s1, s2)","{""samestat"": ""are_stat_buffers_equal""}","def 0(s1, s2):
    """"""Test whether two stat buffers reference the same file""""""
    return (s1.st_ino == s2.st_ino and
            s1.st_dev == s2.st_dev)","def 1(f1, f2):
    """"""Test whether two pathnames reference the same actual file or directory

    This is determined by the device number and i-node number and
    raises an exception if an os.stat() call on either pathname fails.
    """"""
    s1 = os.stat(f1)
    s2 = os.stat(f2)
    return 0(s1, s2)","{""samestat"": ""0""}"
91,91,"def sameopenfile(fp1, fp2):
    """"""Test whether two open file objects reference the same file""""""
    s1 = os.fstat(fp1)
    s2 = os.fstat(fp2)
    return samestat(s1, s2)","def samestat(s1, s2):
    """"""Test whether two stat buffers reference the same file""""""
    return (s1.st_ino == s2.st_ino and
            s1.st_dev == s2.st_dev)","def are_stat_buffers_equal(s1, s2):
    """"""Test whether two stat buffers reference the same file""""""
    return (s1.st_ino == s2.st_ino and
            s1.st_dev == s2.st_dev)","def test_whether_open_file_objects_reference_same_file(fp1, fp2):
    """"""Test whether two open file objects reference the same file""""""
    s1 = os.fstat(fp1)
    s2 = os.fstat(fp2)
    return are_stat_buffers_equal(s1, s2)","{""samestat"": ""are_stat_buffers_equal""}","def 0(s1, s2):
    """"""Test whether two stat buffers reference the same file""""""
    return (s1.st_ino == s2.st_ino and
            s1.st_dev == s2.st_dev)","def 1(fp1, fp2):
    """"""Test whether two open file objects reference the same file""""""
    s1 = os.fstat(fp1)
    s2 = os.fstat(fp2)
    return 0(s1, s2)","{""samestat"": ""0""}"
92,92,"def getline(filename, lineno, module_globals=None):
    """"""Get a line for a Python source file from the cache.
    Update the cache if it doesn't contain an entry for this file already.""""""

    lines = getlines(filename, module_globals)
    if 1 <= lineno <= len(lines):
        return lines[lineno - 1]
    return ''","def getlines(filename, module_globals=None):
    """"""Get the lines for a Python source file from the cache.
    Update the cache if it doesn't contain an entry for this file already.""""""

    if filename in cache:
        entry = cache[filename]
        if len(entry) != 1:
            return cache[filename][2]

    try:
        return updatecache(filename, module_globals)
    except MemoryError:
        clearcache()
        return []","def get_lines_from_cache(filename, module_globals=None):
    """"""Get the lines for a Python source file from the cache.
    Update the cache if it doesn't contain an entry for this file already.""""""

    if filename in cache:
        entry = cache[filename]
        if len(entry) != 1:
            return cache[filename][2]

    try:
        return updatecache(filename, module_globals)
    except MemoryError:
        clearcache()
        return []","def get_line_for_python_source_file_from_cache(filename, lineno, module_globals=None):
    """"""Get a line for a Python source file from the cache.
    Update the cache if it doesn't contain an entry for this file already.""""""

    lines = get_lines_from_cache(filename, module_globals)
    if 1 <= lineno <= len(lines):
        return lines[lineno - 1]
    return ''","{""getlines"": ""get_lines_from_cache""}","def 0(filename, module_globals=None):
    """"""Get the lines for a Python source file from the cache.
    Update the cache if it doesn't contain an entry for this file already.""""""

    if filename in cache:
        entry = cache[filename]
        if len(entry) != 1:
            return cache[filename][2]

    try:
        return updatecache(filename, module_globals)
    except MemoryError:
        clearcache()
        return []","def 1(filename, lineno, module_globals=None):
    """"""Get a line for a Python source file from the cache.
    Update the cache if it doesn't contain an entry for this file already.""""""

    lines = 0(filename, module_globals)
    if 1 <= lineno <= len(lines):
        return lines[lineno - 1]
    return ''","{""getlines"": ""0""}"
93,93,"def getlines(filename, module_globals=None):
    """"""Get the lines for a Python source file from the cache.
    Update the cache if it doesn't contain an entry for this file already.""""""

    if filename in cache:
        entry = cache[filename]
        if len(entry) != 1:
            return cache[filename][2]

    try:
        return updatecache(filename, module_globals)
    except MemoryError:
        clearcache()
        return []","def updatecache(filename, module_globals=None):
    """"""Update a cache entry and return its list of lines.
    If something's wrong, print a message, discard the cache entry,
    and return an empty list.""""""

    if filename in cache:
        if len(cache[filename]) != 1:
            cache.pop(filename, None)
    if not filename or (filename.startswith('<') and filename.endswith('>')):
        return []

    fullname = filename
    try:
        stat = os.stat(fullname)
    except OSError:
        basename = filename

        # Realise a lazy loader based lookup if there is one
        # otherwise try to lookup right now.
        if lazycache(filename, module_globals):
            try:
                data = cache[filename][0]()
            except (ImportError, OSError):
                pass
            else:
                if data is None:
                    # No luck, the PEP302 loader cannot find the source
                    # for this module.
                    return []
                cache[filename] = (
                    len(data),
                    None,
                    [line + '\n' for line in data.splitlines()],
                    fullname
                )
                return cache[filename][2]

        # Try looking through the module search path, which is only useful
        # when handling a relative filename.
        if os.path.isabs(filename):
            return []

        for dirname in sys.path:
            try:
                fullname = os.path.join(dirname, basename)
            except (TypeError, AttributeError):
                # Not sufficiently string-like to do anything useful with.
                continue
            try:
                stat = os.stat(fullname)
                break
            except OSError:
                pass
        else:
            return []
    try:
        with tokenize.open(fullname) as fp:
            lines = fp.readlines()
    except OSError:
        return []
    if lines and not lines[-1].endswith('\n'):
        lines[-1] += '\n'
    size, mtime = stat.st_size, stat.st_mtime
    cache[filename] = size, mtime, lines, fullname
    return lines

def clearcache():
    """"""Clear the cache entirely.""""""
    cache.clear()","def update_cache_entry(filename, module_globals=None):
    """"""Update a cache entry and return its list of lines.
    If something's wrong, print a message, discard the cache entry,
    and return an empty list.""""""

    if filename in cache:
        if len(cache[filename]) != 1:
            cache.pop(filename, None)
    if not filename or (filename.startswith('<') and filename.endswith('>')):
        return []

    fullname = filename
    try:
        stat = os.stat(fullname)
    except OSError:
        basename = filename

        # Realise a lazy loader based lookup if there is one
        # otherwise try to lookup right now.
        if lazycache(filename, module_globals):
            try:
                data = cache[filename][0]()
            except (ImportError, OSError):
                pass
            else:
                if data is None:
                    # No luck, the PEP302 loader cannot find the source
                    # for this module.
                    return []
                cache[filename] = (
                    len(data),
                    None,
                    [line + '\n' for line in data.splitlines()],
                    fullname
                )
                return cache[filename][2]

        # Try looking through the module search path, which is only useful
        # when handling a relative filename.
        if os.path.isabs(filename):
            return []

        for dirname in sys.path:
            try:
                fullname = os.path.join(dirname, basename)
            except (TypeError, AttributeError):
                # Not sufficiently string-like to do anything useful with.
                continue
            try:
                stat = os.stat(fullname)
                break
            except OSError:
                pass
        else:
            return []
    try:
        with tokenize.open(fullname) as fp:
            lines = fp.readlines()
    except OSError:
        return []
    if lines and not lines[-1].endswith('\n'):
        lines[-1] += '\n'
    size, mtime = stat.st_size, stat.st_mtime
    cache[filename] = size, mtime, lines, fullname
    return lines

def clear_entire_cache():
    """"""Clear the cache entirely.""""""
    cache.clear()","def get_lines_from_cache(filename, module_globals=None):
    """"""Get the lines for a Python source file from the cache.
    Update the cache if it doesn't contain an entry for this file already.""""""

    if filename in cache:
        entry = cache[filename]
        if len(entry) != 1:
            return cache[filename][2]

    try:
        return update_cache_entry(filename, module_globals)
    except MemoryError:
        clear_entire_cache()
        return []","{""updatecache"": ""update_cache_entry"", ""clearcache"": ""clear_entire_cache""}","def 0(filename, module_globals=None):
    """"""Update a cache entry and return its list of lines.
    If something's wrong, print a message, discard the cache entry,
    and return an empty list.""""""

    if filename in cache:
        if len(cache[filename]) != 1:
            cache.pop(filename, None)
    if not filename or (filename.startswith('<') and filename.endswith('>')):
        return []

    fullname = filename
    try:
        stat = os.stat(fullname)
    except OSError:
        basename = filename

        # Realise a lazy loader based lookup if there is one
        # otherwise try to lookup right now.
        if lazycache(filename, module_globals):
            try:
                data = cache[filename][0]()
            except (ImportError, OSError):
                pass
            else:
                if data is None:
                    # No luck, the PEP302 loader cannot find the source
                    # for this module.
                    return []
                cache[filename] = (
                    len(data),
                    None,
                    [line + '\n' for line in data.splitlines()],
                    fullname
                )
                return cache[filename][2]

        # Try looking through the module search path, which is only useful
        # when handling a relative filename.
        if os.path.isabs(filename):
            return []

        for dirname in sys.path:
            try:
                fullname = os.path.join(dirname, basename)
            except (TypeError, AttributeError):
                # Not sufficiently string-like to do anything useful with.
                continue
            try:
                stat = os.stat(fullname)
                break
            except OSError:
                pass
        else:
            return []
    try:
        with tokenize.open(fullname) as fp:
            lines = fp.readlines()
    except OSError:
        return []
    if lines and not lines[-1].endswith('\n'):
        lines[-1] += '\n'
    size, mtime = stat.st_size, stat.st_mtime
    cache[filename] = size, mtime, lines, fullname
    return lines

def 1():
    """"""Clear the cache entirely.""""""
    cache.clear()","def 2(filename, module_globals=None):
    """"""Get the lines for a Python source file from the cache.
    Update the cache if it doesn't contain an entry for this file already.""""""

    if filename in cache:
        entry = cache[filename]
        if len(entry) != 1:
            return cache[filename][2]

    try:
        return 0(filename, module_globals)
    except MemoryError:
        1()
        return []","{""updatecache"": ""0"", ""clearcache"": ""1""}"
94,94,"def new_class(name, bases=(), kwds=None, exec_body=None):
    """"""Create a class object dynamically using the appropriate metaclass.""""""
    resolved_bases = resolve_bases(bases)
    meta, ns, kwds = prepare_class(name, resolved_bases, kwds)
    if exec_body is not None:
        exec_body(ns)
    if resolved_bases is not bases:
        ns['__orig_bases__'] = bases
    return meta(name, resolved_bases, ns, **kwds)","def prepare_class(name, bases=(), kwds=None):
    """"""Call the __prepare__ method of the appropriate metaclass.

    Returns (metaclass, namespace, kwds) as a 3-tuple

    *metaclass* is the appropriate metaclass
    *namespace* is the prepared class namespace
    *kwds* is an updated copy of the passed in kwds argument with any
    'metaclass' entry removed. If no kwds argument is passed in, this will
    be an empty dict.
    """"""
    if kwds is None:
        kwds = {}
    else:
        kwds = dict(kwds) # Don't alter the provided mapping
    if 'metaclass' in kwds:
        meta = kwds.pop('metaclass')
    else:
        if bases:
            meta = type(bases[0])
        else:
            meta = type
    if isinstance(meta, type):
        # when meta is a type, we first determine the most-derived metaclass
        # instead of invoking the initial candidate directly
        meta = _calculate_meta(meta, bases)
    if hasattr(meta, '__prepare__'):
        ns = meta.__prepare__(name, bases, **kwds)
    else:
        ns = {}
    return meta, ns, kwds

def resolve_bases(bases):
    """"""Resolve MRO entries dynamically as specified by PEP 560.""""""
    new_bases = list(bases)
    updated = False
    shift = 0
    for i, base in enumerate(bases):
        if isinstance(base, type) and not isinstance(base, GenericAlias):
            continue
        if not hasattr(base, ""__mro_entries__""):
            continue
        new_base = base.__mro_entries__(bases)
        updated = True
        if not isinstance(new_base, tuple):
            raise TypeError(""__mro_entries__ must return a tuple"")
        else:
            new_bases[i+shift:i+shift+1] = new_base
            shift += len(new_base) - 1
    if not updated:
        return bases
    return tuple(new_bases)","def prepare_metaclass_and_namespace(name, bases=(), kwds=None):
    """"""Call the __prepare__ method of the appropriate metaclass.

    Returns (metaclass, namespace, kwds) as a 3-tuple

    *metaclass* is the appropriate metaclass
    *namespace* is the prepared class namespace
    *kwds* is an updated copy of the passed in kwds argument with any
    'metaclass' entry removed. If no kwds argument is passed in, this will
    be an empty dict.
    """"""
    if kwds is None:
        kwds = {}
    else:
        kwds = dict(kwds) # Don't alter the provided mapping
    if 'metaclass' in kwds:
        meta = kwds.pop('metaclass')
    else:
        if bases:
            meta = type(bases[0])
        else:
            meta = type
    if isinstance(meta, type):
        # when meta is a type, we first determine the most-derived metaclass
        # instead of invoking the initial candidate directly
        meta = _calculate_meta(meta, bases)
    if hasattr(meta, '__prepare__'):
        ns = meta.__prepare__(name, bases, **kwds)
    else:
        ns = {}
    return meta, ns, kwds

def resolve_mro_entries_dynamically(bases):
    """"""Resolve MRO entries dynamically as specified by PEP 560.""""""
    new_bases = list(bases)
    updated = False
    shift = 0
    for i, base in enumerate(bases):
        if isinstance(base, type) and not isinstance(base, GenericAlias):
            continue
        if not hasattr(base, ""__mro_entries__""):
            continue
        new_base = base.__mro_entries__(bases)
        updated = True
        if not isinstance(new_base, tuple):
            raise TypeError(""__mro_entries__ must return a tuple"")
        else:
            new_bases[i+shift:i+shift+1] = new_base
            shift += len(new_base) - 1
    if not updated:
        return bases
    return tuple(new_bases)","def create_class_object_dynamically_using_appropriate_metaclass(name, bases=(), kwds=None, exec_body=None):
    """"""Create a class object dynamically using the appropriate metaclass.""""""
    resolved_bases = resolve_mro_entries_dynamically(bases)
    meta, ns, kwds = prepare_metaclass_and_namespace(name, resolved_bases, kwds)
    if exec_body is not None:
        exec_body(ns)
    if resolved_bases is not bases:
        ns['__orig_bases__'] = bases
    return meta(name, resolved_bases, ns, **kwds)","{""prepare_class"": ""prepare_metaclass_and_namespace"", ""resolve_bases"": ""resolve_mro_entries_dynamically""}","def 0(name, bases=(), kwds=None):
    """"""Call the __prepare__ method of the appropriate metaclass.

    Returns (metaclass, namespace, kwds) as a 3-tuple

    *metaclass* is the appropriate metaclass
    *namespace* is the prepared class namespace
    *kwds* is an updated copy of the passed in kwds argument with any
    'metaclass' entry removed. If no kwds argument is passed in, this will
    be an empty dict.
    """"""
    if kwds is None:
        kwds = {}
    else:
        kwds = dict(kwds) # Don't alter the provided mapping
    if 'metaclass' in kwds:
        meta = kwds.pop('metaclass')
    else:
        if bases:
            meta = type(bases[0])
        else:
            meta = type
    if isinstance(meta, type):
        # when meta is a type, we first determine the most-derived metaclass
        # instead of invoking the initial candidate directly
        meta = _calculate_meta(meta, bases)
    if hasattr(meta, '__prepare__'):
        ns = meta.__prepare__(name, bases, **kwds)
    else:
        ns = {}
    return meta, ns, kwds

def 1(bases):
    """"""Resolve MRO entries dynamically as specified by PEP 560.""""""
    new_bases = list(bases)
    updated = False
    shift = 0
    for i, base in enumerate(bases):
        if isinstance(base, type) and not isinstance(base, GenericAlias):
            continue
        if not hasattr(base, ""__mro_entries__""):
            continue
        new_base = base.__mro_entries__(bases)
        updated = True
        if not isinstance(new_base, tuple):
            raise TypeError(""__mro_entries__ must return a tuple"")
        else:
            new_bases[i+shift:i+shift+1] = new_base
            shift += len(new_base) - 1
    if not updated:
        return bases
    return tuple(new_bases)","def 2(name, bases=(), kwds=None, exec_body=None):
    """"""Create a class object dynamically using the appropriate metaclass.""""""
    resolved_bases = 1(bases)
    meta, ns, kwds = 0(name, resolved_bases, kwds)
    if exec_body is not None:
        exec_body(ns)
    if resolved_bases is not bases:
        ns['__orig_bases__'] = bases
    return meta(name, resolved_bases, ns, **kwds)","{""prepare_class"": ""0"", ""resolve_bases"": ""1""}"
95,95,"def resolve_bases(bases):
    """"""Resolve MRO entries dynamically as specified by PEP 560.""""""
    new_bases = list(bases)
    updated = False
    shift = 0
    for i, base in enumerate(bases):
        if isinstance(base, type) and not isinstance(base, GenericAlias):
            continue
        if not hasattr(base, ""__mro_entries__""):
            continue
        new_base = base.__mro_entries__(bases)
        updated = True
        if not isinstance(new_base, tuple):
            raise TypeError(""__mro_entries__ must return a tuple"")
        else:
            new_bases[i+shift:i+shift+1] = new_base
            shift += len(new_base) - 1
    if not updated:
        return bases
    return tuple(new_bases)","def enumerate():
    """"""Return a list of all Thread objects currently alive.

    The list includes daemonic threads, dummy thread objects created by
    current_thread(), and the main thread. It excludes terminated threads and
    threads that have not yet been started.

    """"""
    with _active_limbo_lock:
        return list(_active.values()) + list(_limbo.values())","def get_alive_threads():
    """"""Return a list of all Thread objects currently alive.

    The list includes daemonic threads, dummy thread objects created by
    current_thread(), and the main thread. It excludes terminated threads and
    threads that have not yet been started.

    """"""
    with _active_limbo_lock:
        return list(_active.values()) + list(_limbo.values())","def resolve_mro_entries_dynamically(bases):
    """"""Resolve MRO entries dynamically as specified by PEP 560.""""""
    new_bases = list(bases)
    updated = False
    shift = 0
    for i, base in get_alive_threads(bases):
        if isinstance(base, type) and not isinstance(base, GenericAlias):
            continue
        if not hasattr(base, ""__mro_entries__""):
            continue
        new_base = base.__mro_entries__(bases)
        updated = True
        if not isinstance(new_base, tuple):
            raise TypeError(""__mro_entries__ must return a tuple"")
        else:
            new_bases[i+shift:i+shift+1] = new_base
            shift += len(new_base) - 1
    if not updated:
        return bases
    return tuple(new_bases)","{""enumerate"": ""get_alive_threads""}","def 0():
    """"""Return a list of all Thread objects currently alive.

    The list includes daemonic threads, dummy thread objects created by
    current_thread(), and the main thread. It excludes terminated threads and
    threads that have not yet been started.

    """"""
    with _active_limbo_lock:
        return list(_active.values()) + list(_limbo.values())","def 1(bases):
    """"""Resolve MRO entries dynamically as specified by PEP 560.""""""
    new_bases = list(bases)
    updated = False
    shift = 0
    for i, base in 0(bases):
        if isinstance(base, type) and not isinstance(base, GenericAlias):
            continue
        if not hasattr(base, ""__mro_entries__""):
            continue
        new_base = base.__mro_entries__(bases)
        updated = True
        if not isinstance(new_base, tuple):
            raise TypeError(""__mro_entries__ must return a tuple"")
        else:
            new_bases[i+shift:i+shift+1] = new_base
            shift += len(new_base) - 1
    if not updated:
        return bases
    return tuple(new_bases)","{""enumerate"": ""0""}"
96,96,"def prepare_class(name, bases=(), kwds=None):
    """"""Call the __prepare__ method of the appropriate metaclass.

    Returns (metaclass, namespace, kwds) as a 3-tuple

    *metaclass* is the appropriate metaclass
    *namespace* is the prepared class namespace
    *kwds* is an updated copy of the passed in kwds argument with any
    'metaclass' entry removed. If no kwds argument is passed in, this will
    be an empty dict.
    """"""
    if kwds is None:
        kwds = {}
    else:
        kwds = dict(kwds) # Don't alter the provided mapping
    if 'metaclass' in kwds:
        meta = kwds.pop('metaclass')
    else:
        if bases:
            meta = type(bases[0])
        else:
            meta = type
    if isinstance(meta, type):
        # when meta is a type, we first determine the most-derived metaclass
        # instead of invoking the initial candidate directly
        meta = _calculate_meta(meta, bases)
    if hasattr(meta, '__prepare__'):
        ns = meta.__prepare__(name, bases, **kwds)
    else:
        ns = {}
    return meta, ns, kwds","def _calculate_meta(meta, bases):
    """"""Calculate the most derived metaclass.""""""
    winner = meta
    for base in bases:
        base_meta = type(base)
        if issubclass(winner, base_meta):
            continue
        if issubclass(base_meta, winner):
            winner = base_meta
            continue
        # else:
        raise TypeError(""metaclass conflict: ""
                        ""the metaclass of a derived class ""
                        ""must be a (non-strict) subclass ""
                        ""of the metaclasses of all its bases"")
    return winner","def calculate_most_derived_metaclass(meta, bases):
    """"""Calculate the most derived metaclass.""""""
    winner = meta
    for base in bases:
        base_meta = type(base)
        if issubclass(winner, base_meta):
            continue
        if issubclass(base_meta, winner):
            winner = base_meta
            continue
        # else:
        raise TypeError(""metaclass conflict: ""
                        ""the metaclass of a derived class ""
                        ""must be a (non-strict) subclass ""
                        ""of the metaclasses of all its bases"")
    return winner","def prepare_metaclass_and_namespace(name, bases=(), kwds=None):
    """"""Call the __prepare__ method of the appropriate metaclass.

    Returns (metaclass, namespace, kwds) as a 3-tuple

    *metaclass* is the appropriate metaclass
    *namespace* is the prepared class namespace
    *kwds* is an updated copy of the passed in kwds argument with any
    'metaclass' entry removed. If no kwds argument is passed in, this will
    be an empty dict.
    """"""
    if kwds is None:
        kwds = {}
    else:
        kwds = dict(kwds) # Don't alter the provided mapping
    if 'metaclass' in kwds:
        meta = kwds.pop('metaclass')
    else:
        if bases:
            meta = type(bases[0])
        else:
            meta = type
    if isinstance(meta, type):
        # when meta is a type, we first determine the most-derived metaclass
        # instead of invoking the initial candidate directly
        meta = calculate_most_derived_metaclass(meta, bases)
    if hasattr(meta, '__prepare__'):
        ns = meta.__prepare__(name, bases, **kwds)
    else:
        ns = {}
    return meta, ns, kwds","{""_calculate_meta"": ""calculate_most_derived_metaclass""}","def 0(meta, bases):
    """"""Calculate the most derived metaclass.""""""
    winner = meta
    for base in bases:
        base_meta = type(base)
        if issubclass(winner, base_meta):
            continue
        if issubclass(base_meta, winner):
            winner = base_meta
            continue
        # else:
        raise TypeError(""metaclass conflict: ""
                        ""the metaclass of a derived class ""
                        ""must be a (non-strict) subclass ""
                        ""of the metaclasses of all its bases"")
    return winner","def 1(name, bases=(), kwds=None):
    """"""Call the __prepare__ method of the appropriate metaclass.

    Returns (metaclass, namespace, kwds) as a 3-tuple

    *metaclass* is the appropriate metaclass
    *namespace* is the prepared class namespace
    *kwds* is an updated copy of the passed in kwds argument with any
    'metaclass' entry removed. If no kwds argument is passed in, this will
    be an empty dict.
    """"""
    if kwds is None:
        kwds = {}
    else:
        kwds = dict(kwds) # Don't alter the provided mapping
    if 'metaclass' in kwds:
        meta = kwds.pop('metaclass')
    else:
        if bases:
            meta = type(bases[0])
        else:
            meta = type
    if isinstance(meta, type):
        # when meta is a type, we first determine the most-derived metaclass
        # instead of invoking the initial candidate directly
        meta = 0(meta, bases)
    if hasattr(meta, '__prepare__'):
        ns = meta.__prepare__(name, bases, **kwds)
    else:
        ns = {}
    return meta, ns, kwds","{""_calculate_meta"": ""0""}"
97,97,"def raise_conversion_error(function):
    """""" Wrap any raised struct.errors in a ConversionError. """"""

    @wraps(function)
    def result(self, value):
        try:
            return function(self, value)
        except struct.error as e:
            raise ConversionError(e.args[0]) from None
    return result","def wraps(wrapped,
          assigned = WRAPPER_ASSIGNMENTS,
          updated = WRAPPER_UPDATES):
    """"""Decorator factory to apply update_wrapper() to a wrapper function

       Returns a decorator that invokes update_wrapper() with the decorated
       function as the wrapper argument and the arguments to wraps() as the
       remaining arguments. Default arguments are as for update_wrapper().
       This is a convenience function to simplify applying partial() to
       update_wrapper().
    """"""
    return partial(update_wrapper, wrapped=wrapped,
                   assigned=assigned, updated=updated)

def function(a, b=Foo): pass","def create_decorator_with_update_wrapper(wrapped,
          assigned = WRAPPER_ASSIGNMENTS,
          updated = WRAPPER_UPDATES):
    """"""Decorator factory to apply update_wrapper() to a wrapper function_with_default_argument_Foo

       Returns a decorator that invokes update_wrapper() with the decorated
       function_with_default_argument_Foo as the wrapper argument and the arguments to create_decorator_with_update_wrapper() as the
       remaining arguments. Default arguments are as for update_wrapper().
       This is a convenience function_with_default_argument_Foo to simplify applying partial() to
       update_wrapper().
    """"""
    return partial(update_wrapper, wrapped=wrapped,
                   assigned=assigned, updated=updated)

def function_with_default_argument_Foo(a, b=Foo): pass","def wrap_any_raised_struct_errors_in_conversion_error(function_with_default_argument_Foo):
    """""" Wrap any raised struct.errors in a ConversionError. """"""

    @create_decorator_with_update_wrapper(function_with_default_argument_Foo)
    def handle_result_with_conversion_error(self, value):
        try:
            return function_with_default_argument_Foo(self, value)
        except struct.error as e:
            raise ConversionError(e.args[0]) from None
    return handle_result_with_conversion_error","{""wraps"": ""create_decorator_with_update_wrapper"", ""function"": ""function_with_default_argument_Foo""}","def 0(wrapped,
          assigned = WRAPPER_ASSIGNMENTS,
          updated = WRAPPER_UPDATES):
    """"""Decorator factory to apply update_wrapper() to a wrapper function

       Returns a decorator that invokes update_wrapper() with the decorated
       function as the wrapper argument and the arguments to 0() as the
       remaining arguments. Default arguments are as for update_wrapper().
       This is a convenience function to simplify applying partial() to
       update_wrapper().
    """"""
    return partial(update_wrapper, wrapped=wrapped,
                   assigned=assigned, updated=updated)

def 1(a, b=Foo): pass","def 2(function):
    """""" Wrap any raised struct.errors in a ConversionError. """"""

    @0(function)
    def 3(self, value):
        try:
            return 1(self, value)
        except struct.error as e:
            raise ConversionError(e.args[0]) from None
    return result","{""wraps"": ""0"", ""function"": ""1""}"
98,98,"def _strptime_time(data_string, format=""%a %b %d %H:%M:%S %Y""):
    """"""Return a time struct based on the input string and the
    format string.""""""
    tt = _strptime(data_string, format)[0]
    return time.struct_time(tt[:time._STRUCT_TM_ITEMS])","def _strptime(data_string, format=""%a %b %d %H:%M:%S %Y""):
    """"""Return a 2-tuple consisting of a time struct and an int containing
    the number of microseconds based on the input string and the
    format string.""""""

    for index, arg in enumerate([data_string, format]):
        if not isinstance(arg, str):
            msg = ""strptime() argument {} must be str, not {}""
            raise TypeError(msg.format(index, type(arg)))

    global _TimeRE_cache, _regex_cache
    with _cache_lock:
        locale_time = _TimeRE_cache.locale_time
        if (_getlang() != locale_time.lang or
            time.tzname != locale_time.tzname or
            time.daylight != locale_time.daylight):
            _TimeRE_cache = TimeRE()
            _regex_cache.clear()
            locale_time = _TimeRE_cache.locale_time
        if len(_regex_cache) > _CACHE_MAX_SIZE:
            _regex_cache.clear()
        format_regex = _regex_cache.get(format)
        if not format_regex:
            try:
                format_regex = _TimeRE_cache.compile(format)
            # KeyError raised when a bad format is found; can be specified as
            # \\, in which case it was a stray % but with a space after it
            except KeyError as err:
                bad_directive = err.args[0]
                if bad_directive == ""\\"":
                    bad_directive = ""%""
                del err
                raise ValueError(""'%s' is a bad directive in format '%s'"" %
                                    (bad_directive, format)) from None
            # IndexError only occurs when the format string is ""%""
            except IndexError:
                raise ValueError(""stray %% in format '%s'"" % format) from None
            _regex_cache[format] = format_regex
    found = format_regex.match(data_string)
    if not found:
        raise ValueError(""time data %r does not match format %r"" %
                         (data_string, format))
    if len(data_string) != found.end():
        raise ValueError(""unconverted data remains: %s"" %
                          data_string[found.end():])

    iso_year = year = None
    month = day = 1
    hour = minute = second = fraction = 0
    tz = -1
    gmtoff = None
    gmtoff_fraction = 0
    # Default to -1 to signify that values not known; not critical to have,
    # though
    iso_week = week_of_year = None
    week_of_year_start = None
    # weekday and julian defaulted to None so as to signal need to calculate
    # values
    weekday = julian = None
    found_dict = found.groupdict()
    for group_key in found_dict.keys():
        # Directives not explicitly handled below:
        #   c, x, X
        #      handled by making out of other directives
        #   U, W
        #      worthless without day of the week
        if group_key == 'y':
            year = int(found_dict['y'])
            # Open Group specification for strptime() states that a %y
            #value in the range of [00, 68] is in the century 2000, while
            #[69,99] is in the century 1900
            if year <= 68:
                year += 2000
            else:
                year += 1900
        elif group_key == 'Y':
            year = int(found_dict['Y'])
        elif group_key == 'G':
            iso_year = int(found_dict['G'])
        elif group_key == 'm':
            month = int(found_dict['m'])
        elif group_key == 'B':
            month = locale_time.f_month.index(found_dict['B'].lower())
        elif group_key == 'b':
            month = locale_time.a_month.index(found_dict['b'].lower())
        elif group_key == 'd':
            day = int(found_dict['d'])
        elif group_key == 'H':
            hour = int(found_dict['H'])
        elif group_key == 'I':
            hour = int(found_dict['I'])
            ampm = found_dict.get('p', '').lower()
            # If there was no AM/PM indicator, we'll treat this like AM
            if ampm in ('', locale_time.am_pm[0]):
                # We're in AM so the hour is correct unless we're
                # looking at 12 midnight.
                # 12 midnight == 12 AM == hour 0
                if hour == 12:
                    hour = 0
            elif ampm == locale_time.am_pm[1]:
                # We're in PM so we need to add 12 to the hour unless
                # we're looking at 12 noon.
                # 12 noon == 12 PM == hour 12
                if hour != 12:
                    hour += 12
        elif group_key == 'M':
            minute = int(found_dict['M'])
        elif group_key == 'S':
            second = int(found_dict['S'])
        elif group_key == 'f':
            s = found_dict['f']
            # Pad to always return microseconds.
            s += ""0"" * (6 - len(s))
            fraction = int(s)
        elif group_key == 'A':
            weekday = locale_time.f_weekday.index(found_dict['A'].lower())
        elif group_key == 'a':
            weekday = locale_time.a_weekday.index(found_dict['a'].lower())
        elif group_key == 'w':
            weekday = int(found_dict['w'])
            if weekday == 0:
                weekday = 6
            else:
                weekday -= 1
        elif group_key == 'u':
            weekday = int(found_dict['u'])
            weekday -= 1
        elif group_key == 'j':
            julian = int(found_dict['j'])
        elif group_key in ('U', 'W'):
            week_of_year = int(found_dict[group_key])
            if group_key == 'U':
                # U starts week on Sunday.
                week_of_year_start = 6
            else:
                # W starts week on Monday.
                week_of_year_start = 0
        elif group_key == 'V':
            iso_week = int(found_dict['V'])
        elif group_key == 'z':
            z = found_dict['z']
            if z == 'Z':
                gmtoff = 0
            else:
                if z[3] == ':':
                    z = z[:3] + z[4:]
                    if len(z) > 5:
                        if z[5] != ':':
                            msg = f""Inconsistent use of : in {found_dict['z']}""
                            raise ValueError(msg)
                        z = z[:5] + z[6:]
                hours = int(z[1:3])
                minutes = int(z[3:5])
                seconds = int(z[5:7] or 0)
                gmtoff = (hours * 60 * 60) + (minutes * 60) + seconds
                gmtoff_remainder = z[8:]
                # Pad to always return microseconds.
                gmtoff_remainder_padding = ""0"" * (6 - len(gmtoff_remainder))
                gmtoff_fraction = int(gmtoff_remainder + gmtoff_remainder_padding)
                if z.startswith(""-""):
                    gmtoff = -gmtoff
                    gmtoff_fraction = -gmtoff_fraction
        elif group_key == 'Z':
            # Since -1 is default value only need to worry about setting tz if
            # it can be something other than -1.
            found_zone = found_dict['Z'].lower()
            for value, tz_values in enumerate(locale_time.timezone):
                if found_zone in tz_values:
                    # Deal with bad locale setup where timezone names are the
                    # same and yet time.daylight is true; too ambiguous to
                    # be able to tell what timezone has daylight savings
                    if (time.tzname[0] == time.tzname[1] and
                       time.daylight and found_zone not in (""utc"", ""gmt"")):
                        break
                    else:
                        tz = value
                        break
    # Deal with the cases where ambiguities arize
    # don't assume default values for ISO week/year
    if year is None and iso_year is not None:
        if iso_week is None or weekday is None:
            raise ValueError(""ISO year directive '%G' must be used with ""
                             ""the ISO week directive '%V' and a weekday ""
                             ""directive ('%A', '%a', '%w', or '%u')."")
        if julian is not None:
            raise ValueError(""Day of the year directive '%j' is not ""
                             ""compatible with ISO year directive '%G'. ""
                             ""Use '%Y' instead."")
    elif week_of_year is None and iso_week is not None:
        if weekday is None:
            raise ValueError(""ISO week directive '%V' must be used with ""
                             ""the ISO year directive '%G' and a weekday ""
                             ""directive ('%A', '%a', '%w', or '%u')."")
        else:
            raise ValueError(""ISO week directive '%V' is incompatible with ""
                             ""the year directive '%Y'. Use the ISO year '%G' ""
                             ""instead."")

    leap_year_fix = False
    if year is None and month == 2 and day == 29:
        year = 1904  # 1904 is first leap year of 20th century
        leap_year_fix = True
    elif year is None:
        year = 1900


    # If we know the week of the year and what day of that week, we can figure
    # out the Julian day of the year.
    if julian is None and weekday is not None:
        if week_of_year is not None:
            week_starts_Mon = True if week_of_year_start == 0 else False
            julian = _calc_julian_from_U_or_W(year, week_of_year, weekday,
                                                week_starts_Mon)
        elif iso_year is not None and iso_week is not None:
            year, julian = _calc_julian_from_V(iso_year, iso_week, weekday + 1)
        if julian is not None and julian <= 0:
            year -= 1
            yday = 366 if calendar.isleap(year) else 365
            julian += yday

    if julian is None:
        # Cannot pre-calculate datetime_date() since can change in Julian
        # calculation and thus could have different value for the day of
        # the week calculation.
        # Need to add 1 to result since first day of the year is 1, not 0.
        julian = datetime_date(year, month, day).toordinal() - \
                  datetime_date(year, 1, 1).toordinal() + 1
    else:  # Assume that if they bothered to include Julian day (or if it was
           # calculated above with year/week/weekday) it will be accurate.
        datetime_result = datetime_date.fromordinal(
                            (julian - 1) +
                            datetime_date(year, 1, 1).toordinal())
        year = datetime_result.year
        month = datetime_result.month
        day = datetime_result.day
    if weekday is None:
        weekday = datetime_date(year, month, day).weekday()
    # Add timezone info
    tzname = found_dict.get(""Z"")

    if leap_year_fix:
        # the caller didn't supply a year but asked for Feb 29th. We couldn't
        # use the default of 1900 for computations. We set it back to ensure
        # that February 29th is smaller than March 1st.
        year = 1900

    return (year, month, day,
            hour, minute, second,
            weekday, julian, tz, tzname, gmtoff), fraction, gmtoff_fraction","def _parse_time_from_string_with_format(data_string, format=""%a %b %d %H:%M:%S %Y""):
    """"""Return a 2-tuple consisting of a time struct and an int containing
    the number of microseconds based on the input string and the
    format string.""""""

    for index, arg in enumerate([data_string, format]):
        if not isinstance(arg, str):
            msg = ""strptime() argument {} must be str, not {}""
            raise TypeError(msg.format(index, type(arg)))

    global _TimeRE_cache, _regex_cache
    with _cache_lock:
        locale_time = _TimeRE_cache.locale_time
        if (_getlang() != locale_time.lang or
            time.tzname != locale_time.tzname or
            time.daylight != locale_time.daylight):
            _TimeRE_cache = TimeRE()
            _regex_cache.clear()
            locale_time = _TimeRE_cache.locale_time
        if len(_regex_cache) > _CACHE_MAX_SIZE:
            _regex_cache.clear()
        format_regex = _regex_cache.get(format)
        if not format_regex:
            try:
                format_regex = _TimeRE_cache.compile(format)
            # KeyError raised when a bad format is found; can be specified as
            # \\, in which case it was a stray % but with a space after it
            except KeyError as err:
                bad_directive = err.args[0]
                if bad_directive == ""\\"":
                    bad_directive = ""%""
                del err
                raise ValueError(""'%s' is a bad directive in format '%s'"" %
                                    (bad_directive, format)) from None
            # IndexError only occurs when the format string is ""%""
            except IndexError:
                raise ValueError(""stray %% in format '%s'"" % format) from None
            _regex_cache[format] = format_regex
    found = format_regex.match(data_string)
    if not found:
        raise ValueError(""time data %r does not match format %r"" %
                         (data_string, format))
    if len(data_string) != found.end():
        raise ValueError(""unconverted data remains: %s"" %
                          data_string[found.end():])

    iso_year = year = None
    month = day = 1
    hour = minute = second = fraction = 0
    tz = -1
    gmtoff = None
    gmtoff_fraction = 0
    # Default to -1 to signify that values not known; not critical to have,
    # though
    iso_week = week_of_year = None
    week_of_year_start = None
    # weekday and julian defaulted to None so as to signal need to calculate
    # values
    weekday = julian = None
    found_dict = found.groupdict()
    for group_key in found_dict.keys():
        # Directives not explicitly handled below:
        #   c, x, X
        #      handled by making out of other directives
        #   U, W
        #      worthless without day of the week
        if group_key == 'y':
            year = int(found_dict['y'])
            # Open Group specification for strptime() states that a %y
            #value in the range of [00, 68] is in the century 2000, while
            #[69,99] is in the century 1900
            if year <= 68:
                year += 2000
            else:
                year += 1900
        elif group_key == 'Y':
            year = int(found_dict['Y'])
        elif group_key == 'G':
            iso_year = int(found_dict['G'])
        elif group_key == 'm':
            month = int(found_dict['m'])
        elif group_key == 'B':
            month = locale_time.f_month.index(found_dict['B'].lower())
        elif group_key == 'b':
            month = locale_time.a_month.index(found_dict['b'].lower())
        elif group_key == 'd':
            day = int(found_dict['d'])
        elif group_key == 'H':
            hour = int(found_dict['H'])
        elif group_key == 'I':
            hour = int(found_dict['I'])
            ampm = found_dict.get('p', '').lower()
            # If there was no AM/PM indicator, we'll treat this like AM
            if ampm in ('', locale_time.am_pm[0]):
                # We're in AM so the hour is correct unless we're
                # looking at 12 midnight.
                # 12 midnight == 12 AM == hour 0
                if hour == 12:
                    hour = 0
            elif ampm == locale_time.am_pm[1]:
                # We're in PM so we need to add 12 to the hour unless
                # we're looking at 12 noon.
                # 12 noon == 12 PM == hour 12
                if hour != 12:
                    hour += 12
        elif group_key == 'M':
            minute = int(found_dict['M'])
        elif group_key == 'S':
            second = int(found_dict['S'])
        elif group_key == 'f':
            s = found_dict['f']
            # Pad to always return microseconds.
            s += ""0"" * (6 - len(s))
            fraction = int(s)
        elif group_key == 'A':
            weekday = locale_time.f_weekday.index(found_dict['A'].lower())
        elif group_key == 'a':
            weekday = locale_time.a_weekday.index(found_dict['a'].lower())
        elif group_key == 'w':
            weekday = int(found_dict['w'])
            if weekday == 0:
                weekday = 6
            else:
                weekday -= 1
        elif group_key == 'u':
            weekday = int(found_dict['u'])
            weekday -= 1
        elif group_key == 'j':
            julian = int(found_dict['j'])
        elif group_key in ('U', 'W'):
            week_of_year = int(found_dict[group_key])
            if group_key == 'U':
                # U starts week on Sunday.
                week_of_year_start = 6
            else:
                # W starts week on Monday.
                week_of_year_start = 0
        elif group_key == 'V':
            iso_week = int(found_dict['V'])
        elif group_key == 'z':
            z = found_dict['z']
            if z == 'Z':
                gmtoff = 0
            else:
                if z[3] == ':':
                    z = z[:3] + z[4:]
                    if len(z) > 5:
                        if z[5] != ':':
                            msg = f""Inconsistent use of : in {found_dict['z']}""
                            raise ValueError(msg)
                        z = z[:5] + z[6:]
                hours = int(z[1:3])
                minutes = int(z[3:5])
                seconds = int(z[5:7] or 0)
                gmtoff = (hours * 60 * 60) + (minutes * 60) + seconds
                gmtoff_remainder = z[8:]
                # Pad to always return microseconds.
                gmtoff_remainder_padding = ""0"" * (6 - len(gmtoff_remainder))
                gmtoff_fraction = int(gmtoff_remainder + gmtoff_remainder_padding)
                if z.startswith(""-""):
                    gmtoff = -gmtoff
                    gmtoff_fraction = -gmtoff_fraction
        elif group_key == 'Z':
            # Since -1 is default value only need to worry about setting tz if
            # it can be something other than -1.
            found_zone = found_dict['Z'].lower()
            for value, tz_values in enumerate(locale_time.timezone):
                if found_zone in tz_values:
                    # Deal with bad locale setup where timezone names are the
                    # same and yet time.daylight is true; too ambiguous to
                    # be able to tell what timezone has daylight savings
                    if (time.tzname[0] == time.tzname[1] and
                       time.daylight and found_zone not in (""utc"", ""gmt"")):
                        break
                    else:
                        tz = value
                        break
    # Deal with the cases where ambiguities arize
    # don't assume default values for ISO week/year
    if year is None and iso_year is not None:
        if iso_week is None or weekday is None:
            raise ValueError(""ISO year directive '%G' must be used with ""
                             ""the ISO week directive '%V' and a weekday ""
                             ""directive ('%A', '%a', '%w', or '%u')."")
        if julian is not None:
            raise ValueError(""Day of the year directive '%j' is not ""
                             ""compatible with ISO year directive '%G'. ""
                             ""Use '%Y' instead."")
    elif week_of_year is None and iso_week is not None:
        if weekday is None:
            raise ValueError(""ISO week directive '%V' must be used with ""
                             ""the ISO year directive '%G' and a weekday ""
                             ""directive ('%A', '%a', '%w', or '%u')."")
        else:
            raise ValueError(""ISO week directive '%V' is incompatible with ""
                             ""the year directive '%Y'. Use the ISO year '%G' ""
                             ""instead."")

    leap_year_fix = False
    if year is None and month == 2 and day == 29:
        year = 1904  # 1904 is first leap year of 20th century
        leap_year_fix = True
    elif year is None:
        year = 1900


    # If we know the week of the year and what day of that week, we can figure
    # out the Julian day of the year.
    if julian is None and weekday is not None:
        if week_of_year is not None:
            week_starts_Mon = True if week_of_year_start == 0 else False
            julian = _calc_julian_from_U_or_W(year, week_of_year, weekday,
                                                week_starts_Mon)
        elif iso_year is not None and iso_week is not None:
            year, julian = _calc_julian_from_V(iso_year, iso_week, weekday + 1)
        if julian is not None and julian <= 0:
            year -= 1
            yday = 366 if calendar.isleap(year) else 365
            julian += yday

    if julian is None:
        # Cannot pre-calculate datetime_date() since can change in Julian
        # calculation and thus could have different value for the day of
        # the week calculation.
        # Need to add 1 to result since first day of the year is 1, not 0.
        julian = datetime_date(year, month, day).toordinal() - \
                  datetime_date(year, 1, 1).toordinal() + 1
    else:  # Assume that if they bothered to include Julian day (or if it was
           # calculated above with year/week/weekday) it will be accurate.
        datetime_result = datetime_date.fromordinal(
                            (julian - 1) +
                            datetime_date(year, 1, 1).toordinal())
        year = datetime_result.year
        month = datetime_result.month
        day = datetime_result.day
    if weekday is None:
        weekday = datetime_date(year, month, day).weekday()
    # Add timezone info
    tzname = found_dict.get(""Z"")

    if leap_year_fix:
        # the caller didn't supply a year but asked for Feb 29th. We couldn't
        # use the default of 1900 for computations. We set it back to ensure
        # that February 29th is smaller than March 1st.
        year = 1900

    return (year, month, day,
            hour, minute, second,
            weekday, julian, tz, tzname, gmtoff), fraction, gmtoff_fraction","def _parse_time_from_string_with_format_time(data_string, format=""%a %b %d %H:%M:%S %Y""):
    """"""Return a time struct based on the input string and the
    format string.""""""
    tt = _parse_time_from_string_with_format(data_string, format)[0]
    return time.struct_time(tt[:time._STRUCT_TM_ITEMS])","{""_strptime"": ""_parse_time_from_string_with_format""}","def 0(data_string, format=""%a %b %d %H:%M:%S %Y""):
    """"""Return a 2-tuple consisting of a time struct and an int containing
    the number of microseconds based on the input string and the
    format string.""""""

    for index, arg in enumerate([data_string, format]):
        if not isinstance(arg, str):
            msg = ""strptime() argument {} must be str, not {}""
            raise TypeError(msg.format(index, type(arg)))

    global _TimeRE_cache, _regex_cache
    with _cache_lock:
        locale_time = _TimeRE_cache.locale_time
        if (_getlang() != locale_time.lang or
            time.tzname != locale_time.tzname or
            time.daylight != locale_time.daylight):
            _TimeRE_cache = TimeRE()
            _regex_cache.clear()
            locale_time = _TimeRE_cache.locale_time
        if len(_regex_cache) > _CACHE_MAX_SIZE:
            _regex_cache.clear()
        format_regex = _regex_cache.get(format)
        if not format_regex:
            try:
                format_regex = _TimeRE_cache.compile(format)
            # KeyError raised when a bad format is found; can be specified as
            # \\, in which case it was a stray % but with a space after it
            except KeyError as err:
                bad_directive = err.args[0]
                if bad_directive == ""\\"":
                    bad_directive = ""%""
                del err
                raise ValueError(""'%s' is a bad directive in format '%s'"" %
                                    (bad_directive, format)) from None
            # IndexError only occurs when the format string is ""%""
            except IndexError:
                raise ValueError(""stray %% in format '%s'"" % format) from None
            _regex_cache[format] = format_regex
    found = format_regex.match(data_string)
    if not found:
        raise ValueError(""time data %r does not match format %r"" %
                         (data_string, format))
    if len(data_string) != found.end():
        raise ValueError(""unconverted data remains: %s"" %
                          data_string[found.end():])

    iso_year = year = None
    month = day = 1
    hour = minute = second = fraction = 0
    tz = -1
    gmtoff = None
    gmtoff_fraction = 0
    # Default to -1 to signify that values not known; not critical to have,
    # though
    iso_week = week_of_year = None
    week_of_year_start = None
    # weekday and julian defaulted to None so as to signal need to calculate
    # values
    weekday = julian = None
    found_dict = found.groupdict()
    for group_key in found_dict.keys():
        # Directives not explicitly handled below:
        #   c, x, X
        #      handled by making out of other directives
        #   U, W
        #      worthless without day of the week
        if group_key == 'y':
            year = int(found_dict['y'])
            # Open Group specification for strptime() states that a %y
            #value in the range of [00, 68] is in the century 2000, while
            #[69,99] is in the century 1900
            if year <= 68:
                year += 2000
            else:
                year += 1900
        elif group_key == 'Y':
            year = int(found_dict['Y'])
        elif group_key == 'G':
            iso_year = int(found_dict['G'])
        elif group_key == 'm':
            month = int(found_dict['m'])
        elif group_key == 'B':
            month = locale_time.f_month.index(found_dict['B'].lower())
        elif group_key == 'b':
            month = locale_time.a_month.index(found_dict['b'].lower())
        elif group_key == 'd':
            day = int(found_dict['d'])
        elif group_key == 'H':
            hour = int(found_dict['H'])
        elif group_key == 'I':
            hour = int(found_dict['I'])
            ampm = found_dict.get('p', '').lower()
            # If there was no AM/PM indicator, we'll treat this like AM
            if ampm in ('', locale_time.am_pm[0]):
                # We're in AM so the hour is correct unless we're
                # looking at 12 midnight.
                # 12 midnight == 12 AM == hour 0
                if hour == 12:
                    hour = 0
            elif ampm == locale_time.am_pm[1]:
                # We're in PM so we need to add 12 to the hour unless
                # we're looking at 12 noon.
                # 12 noon == 12 PM == hour 12
                if hour != 12:
                    hour += 12
        elif group_key == 'M':
            minute = int(found_dict['M'])
        elif group_key == 'S':
            second = int(found_dict['S'])
        elif group_key == 'f':
            s = found_dict['f']
            # Pad to always return microseconds.
            s += ""0"" * (6 - len(s))
            fraction = int(s)
        elif group_key == 'A':
            weekday = locale_time.f_weekday.index(found_dict['A'].lower())
        elif group_key == 'a':
            weekday = locale_time.a_weekday.index(found_dict['a'].lower())
        elif group_key == 'w':
            weekday = int(found_dict['w'])
            if weekday == 0:
                weekday = 6
            else:
                weekday -= 1
        elif group_key == 'u':
            weekday = int(found_dict['u'])
            weekday -= 1
        elif group_key == 'j':
            julian = int(found_dict['j'])
        elif group_key in ('U', 'W'):
            week_of_year = int(found_dict[group_key])
            if group_key == 'U':
                # U starts week on Sunday.
                week_of_year_start = 6
            else:
                # W starts week on Monday.
                week_of_year_start = 0
        elif group_key == 'V':
            iso_week = int(found_dict['V'])
        elif group_key == 'z':
            z = found_dict['z']
            if z == 'Z':
                gmtoff = 0
            else:
                if z[3] == ':':
                    z = z[:3] + z[4:]
                    if len(z) > 5:
                        if z[5] != ':':
                            msg = f""Inconsistent use of : in {found_dict['z']}""
                            raise ValueError(msg)
                        z = z[:5] + z[6:]
                hours = int(z[1:3])
                minutes = int(z[3:5])
                seconds = int(z[5:7] or 0)
                gmtoff = (hours * 60 * 60) + (minutes * 60) + seconds
                gmtoff_remainder = z[8:]
                # Pad to always return microseconds.
                gmtoff_remainder_padding = ""0"" * (6 - len(gmtoff_remainder))
                gmtoff_fraction = int(gmtoff_remainder + gmtoff_remainder_padding)
                if z.startswith(""-""):
                    gmtoff = -gmtoff
                    gmtoff_fraction = -gmtoff_fraction
        elif group_key == 'Z':
            # Since -1 is default value only need to worry about setting tz if
            # it can be something other than -1.
            found_zone = found_dict['Z'].lower()
            for value, tz_values in enumerate(locale_time.timezone):
                if found_zone in tz_values:
                    # Deal with bad locale setup where timezone names are the
                    # same and yet time.daylight is true; too ambiguous to
                    # be able to tell what timezone has daylight savings
                    if (time.tzname[0] == time.tzname[1] and
                       time.daylight and found_zone not in (""utc"", ""gmt"")):
                        break
                    else:
                        tz = value
                        break
    # Deal with the cases where ambiguities arize
    # don't assume default values for ISO week/year
    if year is None and iso_year is not None:
        if iso_week is None or weekday is None:
            raise ValueError(""ISO year directive '%G' must be used with ""
                             ""the ISO week directive '%V' and a weekday ""
                             ""directive ('%A', '%a', '%w', or '%u')."")
        if julian is not None:
            raise ValueError(""Day of the year directive '%j' is not ""
                             ""compatible with ISO year directive '%G'. ""
                             ""Use '%Y' instead."")
    elif week_of_year is None and iso_week is not None:
        if weekday is None:
            raise ValueError(""ISO week directive '%V' must be used with ""
                             ""the ISO year directive '%G' and a weekday ""
                             ""directive ('%A', '%a', '%w', or '%u')."")
        else:
            raise ValueError(""ISO week directive '%V' is incompatible with ""
                             ""the year directive '%Y'. Use the ISO year '%G' ""
                             ""instead."")

    leap_year_fix = False
    if year is None and month == 2 and day == 29:
        year = 1904  # 1904 is first leap year of 20th century
        leap_year_fix = True
    elif year is None:
        year = 1900


    # If we know the week of the year and what day of that week, we can figure
    # out the Julian day of the year.
    if julian is None and weekday is not None:
        if week_of_year is not None:
            week_starts_Mon = True if week_of_year_start == 0 else False
            julian = _calc_julian_from_U_or_W(year, week_of_year, weekday,
                                                week_starts_Mon)
        elif iso_year is not None and iso_week is not None:
            year, julian = _calc_julian_from_V(iso_year, iso_week, weekday + 1)
        if julian is not None and julian <= 0:
            year -= 1
            yday = 366 if calendar.isleap(year) else 365
            julian += yday

    if julian is None:
        # Cannot pre-calculate datetime_date() since can change in Julian
        # calculation and thus could have different value for the day of
        # the week calculation.
        # Need to add 1 to result since first day of the year is 1, not 0.
        julian = datetime_date(year, month, day).toordinal() - \
                  datetime_date(year, 1, 1).toordinal() + 1
    else:  # Assume that if they bothered to include Julian day (or if it was
           # calculated above with year/week/weekday) it will be accurate.
        datetime_result = datetime_date.fromordinal(
                            (julian - 1) +
                            datetime_date(year, 1, 1).toordinal())
        year = datetime_result.year
        month = datetime_result.month
        day = datetime_result.day
    if weekday is None:
        weekday = datetime_date(year, month, day).weekday()
    # Add timezone info
    tzname = found_dict.get(""Z"")

    if leap_year_fix:
        # the caller didn't supply a year but asked for Feb 29th. We couldn't
        # use the default of 1900 for computations. We set it back to ensure
        # that February 29th is smaller than March 1st.
        year = 1900

    return (year, month, day,
            hour, minute, second,
            weekday, julian, tz, tzname, gmtoff), fraction, gmtoff_fraction","def 1(data_string, format=""%a %b %d %H:%M:%S %Y""):
    """"""Return a time struct based on the input string and the
    format string.""""""
    tt = 0(data_string, format)[0]
    return time.struct_time(tt[:time._STRUCT_TM_ITEMS])","{""_strptime"": ""0""}"
99,99,"def _strptime_datetime(cls, data_string, format=""%a %b %d %H:%M:%S %Y""):
    """"""Return a class cls instance based on the input string and the
    format string.""""""
    tt, fraction, gmtoff_fraction = _strptime(data_string, format)
    tzname, gmtoff = tt[-2:]
    args = tt[:6] + (fraction,)
    if gmtoff is not None:
        tzdelta = datetime_timedelta(seconds=gmtoff, microseconds=gmtoff_fraction)
        if tzname:
            tz = datetime_timezone(tzdelta, tzname)
        else:
            tz = datetime_timezone(tzdelta)
        args += (tz,)

    return cls(*args)","def _strptime(data_string, format=""%a %b %d %H:%M:%S %Y""):
    """"""Return a 2-tuple consisting of a time struct and an int containing
    the number of microseconds based on the input string and the
    format string.""""""

    for index, arg in enumerate([data_string, format]):
        if not isinstance(arg, str):
            msg = ""strptime() argument {} must be str, not {}""
            raise TypeError(msg.format(index, type(arg)))

    global _TimeRE_cache, _regex_cache
    with _cache_lock:
        locale_time = _TimeRE_cache.locale_time
        if (_getlang() != locale_time.lang or
            time.tzname != locale_time.tzname or
            time.daylight != locale_time.daylight):
            _TimeRE_cache = TimeRE()
            _regex_cache.clear()
            locale_time = _TimeRE_cache.locale_time
        if len(_regex_cache) > _CACHE_MAX_SIZE:
            _regex_cache.clear()
        format_regex = _regex_cache.get(format)
        if not format_regex:
            try:
                format_regex = _TimeRE_cache.compile(format)
            # KeyError raised when a bad format is found; can be specified as
            # \\, in which case it was a stray % but with a space after it
            except KeyError as err:
                bad_directive = err.args[0]
                if bad_directive == ""\\"":
                    bad_directive = ""%""
                del err
                raise ValueError(""'%s' is a bad directive in format '%s'"" %
                                    (bad_directive, format)) from None
            # IndexError only occurs when the format string is ""%""
            except IndexError:
                raise ValueError(""stray %% in format '%s'"" % format) from None
            _regex_cache[format] = format_regex
    found = format_regex.match(data_string)
    if not found:
        raise ValueError(""time data %r does not match format %r"" %
                         (data_string, format))
    if len(data_string) != found.end():
        raise ValueError(""unconverted data remains: %s"" %
                          data_string[found.end():])

    iso_year = year = None
    month = day = 1
    hour = minute = second = fraction = 0
    tz = -1
    gmtoff = None
    gmtoff_fraction = 0
    # Default to -1 to signify that values not known; not critical to have,
    # though
    iso_week = week_of_year = None
    week_of_year_start = None
    # weekday and julian defaulted to None so as to signal need to calculate
    # values
    weekday = julian = None
    found_dict = found.groupdict()
    for group_key in found_dict.keys():
        # Directives not explicitly handled below:
        #   c, x, X
        #      handled by making out of other directives
        #   U, W
        #      worthless without day of the week
        if group_key == 'y':
            year = int(found_dict['y'])
            # Open Group specification for strptime() states that a %y
            #value in the range of [00, 68] is in the century 2000, while
            #[69,99] is in the century 1900
            if year <= 68:
                year += 2000
            else:
                year += 1900
        elif group_key == 'Y':
            year = int(found_dict['Y'])
        elif group_key == 'G':
            iso_year = int(found_dict['G'])
        elif group_key == 'm':
            month = int(found_dict['m'])
        elif group_key == 'B':
            month = locale_time.f_month.index(found_dict['B'].lower())
        elif group_key == 'b':
            month = locale_time.a_month.index(found_dict['b'].lower())
        elif group_key == 'd':
            day = int(found_dict['d'])
        elif group_key == 'H':
            hour = int(found_dict['H'])
        elif group_key == 'I':
            hour = int(found_dict['I'])
            ampm = found_dict.get('p', '').lower()
            # If there was no AM/PM indicator, we'll treat this like AM
            if ampm in ('', locale_time.am_pm[0]):
                # We're in AM so the hour is correct unless we're
                # looking at 12 midnight.
                # 12 midnight == 12 AM == hour 0
                if hour == 12:
                    hour = 0
            elif ampm == locale_time.am_pm[1]:
                # We're in PM so we need to add 12 to the hour unless
                # we're looking at 12 noon.
                # 12 noon == 12 PM == hour 12
                if hour != 12:
                    hour += 12
        elif group_key == 'M':
            minute = int(found_dict['M'])
        elif group_key == 'S':
            second = int(found_dict['S'])
        elif group_key == 'f':
            s = found_dict['f']
            # Pad to always return microseconds.
            s += ""0"" * (6 - len(s))
            fraction = int(s)
        elif group_key == 'A':
            weekday = locale_time.f_weekday.index(found_dict['A'].lower())
        elif group_key == 'a':
            weekday = locale_time.a_weekday.index(found_dict['a'].lower())
        elif group_key == 'w':
            weekday = int(found_dict['w'])
            if weekday == 0:
                weekday = 6
            else:
                weekday -= 1
        elif group_key == 'u':
            weekday = int(found_dict['u'])
            weekday -= 1
        elif group_key == 'j':
            julian = int(found_dict['j'])
        elif group_key in ('U', 'W'):
            week_of_year = int(found_dict[group_key])
            if group_key == 'U':
                # U starts week on Sunday.
                week_of_year_start = 6
            else:
                # W starts week on Monday.
                week_of_year_start = 0
        elif group_key == 'V':
            iso_week = int(found_dict['V'])
        elif group_key == 'z':
            z = found_dict['z']
            if z == 'Z':
                gmtoff = 0
            else:
                if z[3] == ':':
                    z = z[:3] + z[4:]
                    if len(z) > 5:
                        if z[5] != ':':
                            msg = f""Inconsistent use of : in {found_dict['z']}""
                            raise ValueError(msg)
                        z = z[:5] + z[6:]
                hours = int(z[1:3])
                minutes = int(z[3:5])
                seconds = int(z[5:7] or 0)
                gmtoff = (hours * 60 * 60) + (minutes * 60) + seconds
                gmtoff_remainder = z[8:]
                # Pad to always return microseconds.
                gmtoff_remainder_padding = ""0"" * (6 - len(gmtoff_remainder))
                gmtoff_fraction = int(gmtoff_remainder + gmtoff_remainder_padding)
                if z.startswith(""-""):
                    gmtoff = -gmtoff
                    gmtoff_fraction = -gmtoff_fraction
        elif group_key == 'Z':
            # Since -1 is default value only need to worry about setting tz if
            # it can be something other than -1.
            found_zone = found_dict['Z'].lower()
            for value, tz_values in enumerate(locale_time.timezone):
                if found_zone in tz_values:
                    # Deal with bad locale setup where timezone names are the
                    # same and yet time.daylight is true; too ambiguous to
                    # be able to tell what timezone has daylight savings
                    if (time.tzname[0] == time.tzname[1] and
                       time.daylight and found_zone not in (""utc"", ""gmt"")):
                        break
                    else:
                        tz = value
                        break
    # Deal with the cases where ambiguities arize
    # don't assume default values for ISO week/year
    if year is None and iso_year is not None:
        if iso_week is None or weekday is None:
            raise ValueError(""ISO year directive '%G' must be used with ""
                             ""the ISO week directive '%V' and a weekday ""
                             ""directive ('%A', '%a', '%w', or '%u')."")
        if julian is not None:
            raise ValueError(""Day of the year directive '%j' is not ""
                             ""compatible with ISO year directive '%G'. ""
                             ""Use '%Y' instead."")
    elif week_of_year is None and iso_week is not None:
        if weekday is None:
            raise ValueError(""ISO week directive '%V' must be used with ""
                             ""the ISO year directive '%G' and a weekday ""
                             ""directive ('%A', '%a', '%w', or '%u')."")
        else:
            raise ValueError(""ISO week directive '%V' is incompatible with ""
                             ""the year directive '%Y'. Use the ISO year '%G' ""
                             ""instead."")

    leap_year_fix = False
    if year is None and month == 2 and day == 29:
        year = 1904  # 1904 is first leap year of 20th century
        leap_year_fix = True
    elif year is None:
        year = 1900


    # If we know the week of the year and what day of that week, we can figure
    # out the Julian day of the year.
    if julian is None and weekday is not None:
        if week_of_year is not None:
            week_starts_Mon = True if week_of_year_start == 0 else False
            julian = _calc_julian_from_U_or_W(year, week_of_year, weekday,
                                                week_starts_Mon)
        elif iso_year is not None and iso_week is not None:
            year, julian = _calc_julian_from_V(iso_year, iso_week, weekday + 1)
        if julian is not None and julian <= 0:
            year -= 1
            yday = 366 if calendar.isleap(year) else 365
            julian += yday

    if julian is None:
        # Cannot pre-calculate datetime_date() since can change in Julian
        # calculation and thus could have different value for the day of
        # the week calculation.
        # Need to add 1 to result since first day of the year is 1, not 0.
        julian = datetime_date(year, month, day).toordinal() - \
                  datetime_date(year, 1, 1).toordinal() + 1
    else:  # Assume that if they bothered to include Julian day (or if it was
           # calculated above with year/week/weekday) it will be accurate.
        datetime_result = datetime_date.fromordinal(
                            (julian - 1) +
                            datetime_date(year, 1, 1).toordinal())
        year = datetime_result.year
        month = datetime_result.month
        day = datetime_result.day
    if weekday is None:
        weekday = datetime_date(year, month, day).weekday()
    # Add timezone info
    tzname = found_dict.get(""Z"")

    if leap_year_fix:
        # the caller didn't supply a year but asked for Feb 29th. We couldn't
        # use the default of 1900 for computations. We set it back to ensure
        # that February 29th is smaller than March 1st.
        year = 1900

    return (year, month, day,
            hour, minute, second,
            weekday, julian, tz, tzname, gmtoff), fraction, gmtoff_fraction","def _parse_time_from_string_with_format(data_string, format=""%a %b %d %H:%M:%S %Y""):
    """"""Return a 2-tuple consisting of a time struct and an int containing
    the number of microseconds based on the input string and the
    format string.""""""

    for index, arg in enumerate([data_string, format]):
        if not isinstance(arg, str):
            msg = ""strptime() argument {} must be str, not {}""
            raise TypeError(msg.format(index, type(arg)))

    global _TimeRE_cache, _regex_cache
    with _cache_lock:
        locale_time = _TimeRE_cache.locale_time
        if (_getlang() != locale_time.lang or
            time.tzname != locale_time.tzname or
            time.daylight != locale_time.daylight):
            _TimeRE_cache = TimeRE()
            _regex_cache.clear()
            locale_time = _TimeRE_cache.locale_time
        if len(_regex_cache) > _CACHE_MAX_SIZE:
            _regex_cache.clear()
        format_regex = _regex_cache.get(format)
        if not format_regex:
            try:
                format_regex = _TimeRE_cache.compile(format)
            # KeyError raised when a bad format is found; can be specified as
            # \\, in which case it was a stray % but with a space after it
            except KeyError as err:
                bad_directive = err.args[0]
                if bad_directive == ""\\"":
                    bad_directive = ""%""
                del err
                raise ValueError(""'%s' is a bad directive in format '%s'"" %
                                    (bad_directive, format)) from None
            # IndexError only occurs when the format string is ""%""
            except IndexError:
                raise ValueError(""stray %% in format '%s'"" % format) from None
            _regex_cache[format] = format_regex
    found = format_regex.match(data_string)
    if not found:
        raise ValueError(""time data %r does not match format %r"" %
                         (data_string, format))
    if len(data_string) != found.end():
        raise ValueError(""unconverted data remains: %s"" %
                          data_string[found.end():])

    iso_year = year = None
    month = day = 1
    hour = minute = second = fraction = 0
    tz = -1
    gmtoff = None
    gmtoff_fraction = 0
    # Default to -1 to signify that values not known; not critical to have,
    # though
    iso_week = week_of_year = None
    week_of_year_start = None
    # weekday and julian defaulted to None so as to signal need to calculate
    # values
    weekday = julian = None
    found_dict = found.groupdict()
    for group_key in found_dict.keys():
        # Directives not explicitly handled below:
        #   c, x, X
        #      handled by making out of other directives
        #   U, W
        #      worthless without day of the week
        if group_key == 'y':
            year = int(found_dict['y'])
            # Open Group specification for strptime() states that a %y
            #value in the range of [00, 68] is in the century 2000, while
            #[69,99] is in the century 1900
            if year <= 68:
                year += 2000
            else:
                year += 1900
        elif group_key == 'Y':
            year = int(found_dict['Y'])
        elif group_key == 'G':
            iso_year = int(found_dict['G'])
        elif group_key == 'm':
            month = int(found_dict['m'])
        elif group_key == 'B':
            month = locale_time.f_month.index(found_dict['B'].lower())
        elif group_key == 'b':
            month = locale_time.a_month.index(found_dict['b'].lower())
        elif group_key == 'd':
            day = int(found_dict['d'])
        elif group_key == 'H':
            hour = int(found_dict['H'])
        elif group_key == 'I':
            hour = int(found_dict['I'])
            ampm = found_dict.get('p', '').lower()
            # If there was no AM/PM indicator, we'll treat this like AM
            if ampm in ('', locale_time.am_pm[0]):
                # We're in AM so the hour is correct unless we're
                # looking at 12 midnight.
                # 12 midnight == 12 AM == hour 0
                if hour == 12:
                    hour = 0
            elif ampm == locale_time.am_pm[1]:
                # We're in PM so we need to add 12 to the hour unless
                # we're looking at 12 noon.
                # 12 noon == 12 PM == hour 12
                if hour != 12:
                    hour += 12
        elif group_key == 'M':
            minute = int(found_dict['M'])
        elif group_key == 'S':
            second = int(found_dict['S'])
        elif group_key == 'f':
            s = found_dict['f']
            # Pad to always return microseconds.
            s += ""0"" * (6 - len(s))
            fraction = int(s)
        elif group_key == 'A':
            weekday = locale_time.f_weekday.index(found_dict['A'].lower())
        elif group_key == 'a':
            weekday = locale_time.a_weekday.index(found_dict['a'].lower())
        elif group_key == 'w':
            weekday = int(found_dict['w'])
            if weekday == 0:
                weekday = 6
            else:
                weekday -= 1
        elif group_key == 'u':
            weekday = int(found_dict['u'])
            weekday -= 1
        elif group_key == 'j':
            julian = int(found_dict['j'])
        elif group_key in ('U', 'W'):
            week_of_year = int(found_dict[group_key])
            if group_key == 'U':
                # U starts week on Sunday.
                week_of_year_start = 6
            else:
                # W starts week on Monday.
                week_of_year_start = 0
        elif group_key == 'V':
            iso_week = int(found_dict['V'])
        elif group_key == 'z':
            z = found_dict['z']
            if z == 'Z':
                gmtoff = 0
            else:
                if z[3] == ':':
                    z = z[:3] + z[4:]
                    if len(z) > 5:
                        if z[5] != ':':
                            msg = f""Inconsistent use of : in {found_dict['z']}""
                            raise ValueError(msg)
                        z = z[:5] + z[6:]
                hours = int(z[1:3])
                minutes = int(z[3:5])
                seconds = int(z[5:7] or 0)
                gmtoff = (hours * 60 * 60) + (minutes * 60) + seconds
                gmtoff_remainder = z[8:]
                # Pad to always return microseconds.
                gmtoff_remainder_padding = ""0"" * (6 - len(gmtoff_remainder))
                gmtoff_fraction = int(gmtoff_remainder + gmtoff_remainder_padding)
                if z.startswith(""-""):
                    gmtoff = -gmtoff
                    gmtoff_fraction = -gmtoff_fraction
        elif group_key == 'Z':
            # Since -1 is default value only need to worry about setting tz if
            # it can be something other than -1.
            found_zone = found_dict['Z'].lower()
            for value, tz_values in enumerate(locale_time.timezone):
                if found_zone in tz_values:
                    # Deal with bad locale setup where timezone names are the
                    # same and yet time.daylight is true; too ambiguous to
                    # be able to tell what timezone has daylight savings
                    if (time.tzname[0] == time.tzname[1] and
                       time.daylight and found_zone not in (""utc"", ""gmt"")):
                        break
                    else:
                        tz = value
                        break
    # Deal with the cases where ambiguities arize
    # don't assume default values for ISO week/year
    if year is None and iso_year is not None:
        if iso_week is None or weekday is None:
            raise ValueError(""ISO year directive '%G' must be used with ""
                             ""the ISO week directive '%V' and a weekday ""
                             ""directive ('%A', '%a', '%w', or '%u')."")
        if julian is not None:
            raise ValueError(""Day of the year directive '%j' is not ""
                             ""compatible with ISO year directive '%G'. ""
                             ""Use '%Y' instead."")
    elif week_of_year is None and iso_week is not None:
        if weekday is None:
            raise ValueError(""ISO week directive '%V' must be used with ""
                             ""the ISO year directive '%G' and a weekday ""
                             ""directive ('%A', '%a', '%w', or '%u')."")
        else:
            raise ValueError(""ISO week directive '%V' is incompatible with ""
                             ""the year directive '%Y'. Use the ISO year '%G' ""
                             ""instead."")

    leap_year_fix = False
    if year is None and month == 2 and day == 29:
        year = 1904  # 1904 is first leap year of 20th century
        leap_year_fix = True
    elif year is None:
        year = 1900


    # If we know the week of the year and what day of that week, we can figure
    # out the Julian day of the year.
    if julian is None and weekday is not None:
        if week_of_year is not None:
            week_starts_Mon = True if week_of_year_start == 0 else False
            julian = _calc_julian_from_U_or_W(year, week_of_year, weekday,
                                                week_starts_Mon)
        elif iso_year is not None and iso_week is not None:
            year, julian = _calc_julian_from_V(iso_year, iso_week, weekday + 1)
        if julian is not None and julian <= 0:
            year -= 1
            yday = 366 if calendar.isleap(year) else 365
            julian += yday

    if julian is None:
        # Cannot pre-calculate datetime_date() since can change in Julian
        # calculation and thus could have different value for the day of
        # the week calculation.
        # Need to add 1 to result since first day of the year is 1, not 0.
        julian = datetime_date(year, month, day).toordinal() - \
                  datetime_date(year, 1, 1).toordinal() + 1
    else:  # Assume that if they bothered to include Julian day (or if it was
           # calculated above with year/week/weekday) it will be accurate.
        datetime_result = datetime_date.fromordinal(
                            (julian - 1) +
                            datetime_date(year, 1, 1).toordinal())
        year = datetime_result.year
        month = datetime_result.month
        day = datetime_result.day
    if weekday is None:
        weekday = datetime_date(year, month, day).weekday()
    # Add timezone info
    tzname = found_dict.get(""Z"")

    if leap_year_fix:
        # the caller didn't supply a year but asked for Feb 29th. We couldn't
        # use the default of 1900 for computations. We set it back to ensure
        # that February 29th is smaller than March 1st.
        year = 1900

    return (year, month, day,
            hour, minute, second,
            weekday, julian, tz, tzname, gmtoff), fraction, gmtoff_fraction","def _parse_time_from_string_with_format_datetime(cls, data_string, format=""%a %b %d %H:%M:%S %Y""):
    """"""Return a class cls instance based on the input string and the
    format string.""""""
    tt, fraction, gmtoff_fraction = _parse_time_from_string_with_format(data_string, format)
    tzname, gmtoff = tt[-2:]
    args = tt[:6] + (fraction,)
    if gmtoff is not None:
        tzdelta = datetime_timedelta(seconds=gmtoff, microseconds=gmtoff_fraction)
        if tzname:
            tz = datetime_timezone(tzdelta, tzname)
        else:
            tz = datetime_timezone(tzdelta)
        args += (tz,)

    return cls(*args)","{""_strptime"": ""_parse_time_from_string_with_format""}","def 0(data_string, format=""%a %b %d %H:%M:%S %Y""):
    """"""Return a 2-tuple consisting of a time struct and an int containing
    the number of microseconds based on the input string and the
    format string.""""""

    for index, arg in enumerate([data_string, format]):
        if not isinstance(arg, str):
            msg = ""strptime() argument {} must be str, not {}""
            raise TypeError(msg.format(index, type(arg)))

    global _TimeRE_cache, _regex_cache
    with _cache_lock:
        locale_time = _TimeRE_cache.locale_time
        if (_getlang() != locale_time.lang or
            time.tzname != locale_time.tzname or
            time.daylight != locale_time.daylight):
            _TimeRE_cache = TimeRE()
            _regex_cache.clear()
            locale_time = _TimeRE_cache.locale_time
        if len(_regex_cache) > _CACHE_MAX_SIZE:
            _regex_cache.clear()
        format_regex = _regex_cache.get(format)
        if not format_regex:
            try:
                format_regex = _TimeRE_cache.compile(format)
            # KeyError raised when a bad format is found; can be specified as
            # \\, in which case it was a stray % but with a space after it
            except KeyError as err:
                bad_directive = err.args[0]
                if bad_directive == ""\\"":
                    bad_directive = ""%""
                del err
                raise ValueError(""'%s' is a bad directive in format '%s'"" %
                                    (bad_directive, format)) from None
            # IndexError only occurs when the format string is ""%""
            except IndexError:
                raise ValueError(""stray %% in format '%s'"" % format) from None
            _regex_cache[format] = format_regex
    found = format_regex.match(data_string)
    if not found:
        raise ValueError(""time data %r does not match format %r"" %
                         (data_string, format))
    if len(data_string) != found.end():
        raise ValueError(""unconverted data remains: %s"" %
                          data_string[found.end():])

    iso_year = year = None
    month = day = 1
    hour = minute = second = fraction = 0
    tz = -1
    gmtoff = None
    gmtoff_fraction = 0
    # Default to -1 to signify that values not known; not critical to have,
    # though
    iso_week = week_of_year = None
    week_of_year_start = None
    # weekday and julian defaulted to None so as to signal need to calculate
    # values
    weekday = julian = None
    found_dict = found.groupdict()
    for group_key in found_dict.keys():
        # Directives not explicitly handled below:
        #   c, x, X
        #      handled by making out of other directives
        #   U, W
        #      worthless without day of the week
        if group_key == 'y':
            year = int(found_dict['y'])
            # Open Group specification for strptime() states that a %y
            #value in the range of [00, 68] is in the century 2000, while
            #[69,99] is in the century 1900
            if year <= 68:
                year += 2000
            else:
                year += 1900
        elif group_key == 'Y':
            year = int(found_dict['Y'])
        elif group_key == 'G':
            iso_year = int(found_dict['G'])
        elif group_key == 'm':
            month = int(found_dict['m'])
        elif group_key == 'B':
            month = locale_time.f_month.index(found_dict['B'].lower())
        elif group_key == 'b':
            month = locale_time.a_month.index(found_dict['b'].lower())
        elif group_key == 'd':
            day = int(found_dict['d'])
        elif group_key == 'H':
            hour = int(found_dict['H'])
        elif group_key == 'I':
            hour = int(found_dict['I'])
            ampm = found_dict.get('p', '').lower()
            # If there was no AM/PM indicator, we'll treat this like AM
            if ampm in ('', locale_time.am_pm[0]):
                # We're in AM so the hour is correct unless we're
                # looking at 12 midnight.
                # 12 midnight == 12 AM == hour 0
                if hour == 12:
                    hour = 0
            elif ampm == locale_time.am_pm[1]:
                # We're in PM so we need to add 12 to the hour unless
                # we're looking at 12 noon.
                # 12 noon == 12 PM == hour 12
                if hour != 12:
                    hour += 12
        elif group_key == 'M':
            minute = int(found_dict['M'])
        elif group_key == 'S':
            second = int(found_dict['S'])
        elif group_key == 'f':
            s = found_dict['f']
            # Pad to always return microseconds.
            s += ""0"" * (6 - len(s))
            fraction = int(s)
        elif group_key == 'A':
            weekday = locale_time.f_weekday.index(found_dict['A'].lower())
        elif group_key == 'a':
            weekday = locale_time.a_weekday.index(found_dict['a'].lower())
        elif group_key == 'w':
            weekday = int(found_dict['w'])
            if weekday == 0:
                weekday = 6
            else:
                weekday -= 1
        elif group_key == 'u':
            weekday = int(found_dict['u'])
            weekday -= 1
        elif group_key == 'j':
            julian = int(found_dict['j'])
        elif group_key in ('U', 'W'):
            week_of_year = int(found_dict[group_key])
            if group_key == 'U':
                # U starts week on Sunday.
                week_of_year_start = 6
            else:
                # W starts week on Monday.
                week_of_year_start = 0
        elif group_key == 'V':
            iso_week = int(found_dict['V'])
        elif group_key == 'z':
            z = found_dict['z']
            if z == 'Z':
                gmtoff = 0
            else:
                if z[3] == ':':
                    z = z[:3] + z[4:]
                    if len(z) > 5:
                        if z[5] != ':':
                            msg = f""Inconsistent use of : in {found_dict['z']}""
                            raise ValueError(msg)
                        z = z[:5] + z[6:]
                hours = int(z[1:3])
                minutes = int(z[3:5])
                seconds = int(z[5:7] or 0)
                gmtoff = (hours * 60 * 60) + (minutes * 60) + seconds
                gmtoff_remainder = z[8:]
                # Pad to always return microseconds.
                gmtoff_remainder_padding = ""0"" * (6 - len(gmtoff_remainder))
                gmtoff_fraction = int(gmtoff_remainder + gmtoff_remainder_padding)
                if z.startswith(""-""):
                    gmtoff = -gmtoff
                    gmtoff_fraction = -gmtoff_fraction
        elif group_key == 'Z':
            # Since -1 is default value only need to worry about setting tz if
            # it can be something other than -1.
            found_zone = found_dict['Z'].lower()
            for value, tz_values in enumerate(locale_time.timezone):
                if found_zone in tz_values:
                    # Deal with bad locale setup where timezone names are the
                    # same and yet time.daylight is true; too ambiguous to
                    # be able to tell what timezone has daylight savings
                    if (time.tzname[0] == time.tzname[1] and
                       time.daylight and found_zone not in (""utc"", ""gmt"")):
                        break
                    else:
                        tz = value
                        break
    # Deal with the cases where ambiguities arize
    # don't assume default values for ISO week/year
    if year is None and iso_year is not None:
        if iso_week is None or weekday is None:
            raise ValueError(""ISO year directive '%G' must be used with ""
                             ""the ISO week directive '%V' and a weekday ""
                             ""directive ('%A', '%a', '%w', or '%u')."")
        if julian is not None:
            raise ValueError(""Day of the year directive '%j' is not ""
                             ""compatible with ISO year directive '%G'. ""
                             ""Use '%Y' instead."")
    elif week_of_year is None and iso_week is not None:
        if weekday is None:
            raise ValueError(""ISO week directive '%V' must be used with ""
                             ""the ISO year directive '%G' and a weekday ""
                             ""directive ('%A', '%a', '%w', or '%u')."")
        else:
            raise ValueError(""ISO week directive '%V' is incompatible with ""
                             ""the year directive '%Y'. Use the ISO year '%G' ""
                             ""instead."")

    leap_year_fix = False
    if year is None and month == 2 and day == 29:
        year = 1904  # 1904 is first leap year of 20th century
        leap_year_fix = True
    elif year is None:
        year = 1900


    # If we know the week of the year and what day of that week, we can figure
    # out the Julian day of the year.
    if julian is None and weekday is not None:
        if week_of_year is not None:
            week_starts_Mon = True if week_of_year_start == 0 else False
            julian = _calc_julian_from_U_or_W(year, week_of_year, weekday,
                                                week_starts_Mon)
        elif iso_year is not None and iso_week is not None:
            year, julian = _calc_julian_from_V(iso_year, iso_week, weekday + 1)
        if julian is not None and julian <= 0:
            year -= 1
            yday = 366 if calendar.isleap(year) else 365
            julian += yday

    if julian is None:
        # Cannot pre-calculate datetime_date() since can change in Julian
        # calculation and thus could have different value for the day of
        # the week calculation.
        # Need to add 1 to result since first day of the year is 1, not 0.
        julian = datetime_date(year, month, day).toordinal() - \
                  datetime_date(year, 1, 1).toordinal() + 1
    else:  # Assume that if they bothered to include Julian day (or if it was
           # calculated above with year/week/weekday) it will be accurate.
        datetime_result = datetime_date.fromordinal(
                            (julian - 1) +
                            datetime_date(year, 1, 1).toordinal())
        year = datetime_result.year
        month = datetime_result.month
        day = datetime_result.day
    if weekday is None:
        weekday = datetime_date(year, month, day).weekday()
    # Add timezone info
    tzname = found_dict.get(""Z"")

    if leap_year_fix:
        # the caller didn't supply a year but asked for Feb 29th. We couldn't
        # use the default of 1900 for computations. We set it back to ensure
        # that February 29th is smaller than March 1st.
        year = 1900

    return (year, month, day,
            hour, minute, second,
            weekday, julian, tz, tzname, gmtoff), fraction, gmtoff_fraction","def 1(cls, data_string, format=""%a %b %d %H:%M:%S %Y""):
    """"""Return a class cls instance based on the input string and the
    format string.""""""
    tt, fraction, gmtoff_fraction = 0(data_string, format)
    tzname, gmtoff = tt[-2:]
    args = tt[:6] + (fraction,)
    if gmtoff is not None:
        tzdelta = datetime_timedelta(seconds=gmtoff, microseconds=gmtoff_fraction)
        if tzname:
            tz = datetime_timezone(tzdelta, tzname)
        else:
            tz = datetime_timezone(tzdelta)
        args += (tz,)

    return cls(*args)","{""_strptime"": ""0""}"
100,100,"def ftpcp(source, sourcename, target, targetname = '', type = 'I'):
    '''Copy file from one FTP-instance to another.'''
    if not targetname:
        targetname = sourcename
    type = 'TYPE ' + type
    source.voidcmd(type)
    target.voidcmd(type)
    sourcehost, sourceport = parse227(source.sendcmd('PASV'))
    target.sendport(sourcehost, sourceport)
    # RFC 959: the user must ""listen"" [...] BEFORE sending the
    # transfer request.
    # So: STOR before RETR, because here the target is a ""user"".
    treply = target.sendcmd('STOR ' + targetname)
    if treply[:3] not in {'125', '150'}:
        raise error_proto  # RFC 959
    sreply = source.sendcmd('RETR ' + sourcename)
    if sreply[:3] not in {'125', '150'}:
        raise error_proto  # RFC 959
    source.voidresp()
    target.voidresp()","def parse227(resp):
    '''Parse the '227' response for a PASV request.
    Raises error_proto if it does not contain '(h1,h2,h3,h4,p1,p2)'
    Return ('host.addr.as.numbers', port#) tuple.'''
    if resp[:3] != '227':
        raise error_reply(resp)
    global _227_re
    if _227_re is None:
        import re
        _227_re = re.compile(r'(\d+),(\d+),(\d+),(\d+),(\d+),(\d+)', re.ASCII)
    m = _227_re.search(resp)
    if not m:
        raise error_proto(resp)
    numbers = m.groups()
    host = '.'.join(numbers[:4])
    port = (int(numbers[4]) << 8) + int(numbers[5])
    return host, port","def parse_pasv_response(resp):
    '''Parse the '227' response for a PASV request.
    Raises error_proto if it does not contain '(h1,h2,h3,h4,p1,p2)'
    Return ('host.addr.as.numbers', port#) tuple.'''
    if resp[:3] != '227':
        raise error_reply(resp)
    global _227_re
    if _227_re is None:
        import re
        _227_re = re.compile(r'(\d+),(\d+),(\d+),(\d+),(\d+),(\d+)', re.ASCII)
    m = _227_re.search(resp)
    if not m:
        raise error_proto(resp)
    numbers = m.groups()
    host = '.'.join(numbers[:4])
    port = (int(numbers[4]) << 8) + int(numbers[5])
    return host, port","def copy_file_from_one_ftp_instance_to_another(source, sourcename, target, targetname = '', type = 'I'):
    '''Copy file from one FTP-instance to another.'''
    if not targetname:
        targetname = sourcename
    type = 'TYPE ' + type
    source.voidcmd(type)
    target.voidcmd(type)
    sourcehost, sourceport = parse_pasv_response(source.sendcmd('PASV'))
    target.sendport(sourcehost, sourceport)
    # RFC 959: the user must ""listen"" [...] BEFORE sending the
    # transfer request.
    # So: STOR before RETR, because here the target is a ""user"".
    treply = target.sendcmd('STOR ' + targetname)
    if treply[:3] not in {'125', '150'}:
        raise error_proto  # RFC 959
    sreply = source.sendcmd('RETR ' + sourcename)
    if sreply[:3] not in {'125', '150'}:
        raise error_proto  # RFC 959
    source.voidresp()
    target.voidresp()","{""parse227"": ""parse_pasv_response""}","def 0(resp):
    '''Parse the '227' response for a PASV request.
    Raises error_proto if it does not contain '(h1,h2,h3,h4,p1,p2)'
    Return ('host.addr.as.numbers', port#) tuple.'''
    if resp[:3] != '227':
        raise error_reply(resp)
    global _227_re
    if _227_re is None:
        import re
        _227_re = re.compile(r'(\d+),(\d+),(\d+),(\d+),(\d+),(\d+)', re.ASCII)
    m = _227_re.search(resp)
    if not m:
        raise error_proto(resp)
    numbers = m.groups()
    host = '.'.join(numbers[:4])
    port = (int(numbers[4]) << 8) + int(numbers[5])
    return host, port","def 1(source, sourcename, target, targetname = '', type = 'I'):
    '''Copy file from one FTP-instance to another.'''
    if not targetname:
        targetname = sourcename
    type = 'TYPE ' + type
    source.voidcmd(type)
    target.voidcmd(type)
    sourcehost, sourceport = 0(source.sendcmd('PASV'))
    target.sendport(sourcehost, sourceport)
    # RFC 959: the user must ""listen"" [...] BEFORE sending the
    # transfer request.
    # So: STOR before RETR, because here the target is a ""user"".
    treply = target.sendcmd('STOR ' + targetname)
    if treply[:3] not in {'125', '150'}:
        raise error_proto  # RFC 959
    sreply = source.sendcmd('RETR ' + sourcename)
    if sreply[:3] not in {'125', '150'}:
        raise error_proto  # RFC 959
    source.voidresp()
    target.voidresp()","{""parse227"": ""0""}"
101,101,"def currentThread():
    """"""Return the current Thread object, corresponding to the caller's thread of control.

    This function is deprecated, use current_thread() instead.

    """"""
    import warnings
    warnings.warn('currentThread() is deprecated, use current_thread() instead',
                  DeprecationWarning, stacklevel=2)
    return current_thread()","def current_thread():
    """"""Return the current Thread object, corresponding to the caller's thread of control.

    If the caller's thread of control was not created through the threading
    module, a dummy thread object with limited functionality is returned.

    """"""
    try:
        return _active[get_ident()]
    except KeyError:
        return _DummyThread()","def get_current_thread():
    """"""Return the current Thread object, corresponding to the caller's thread of control.

    If the caller's thread of control was not created through the threading
    module, a dummy thread object with limited functionality is returned.

    """"""
    try:
        return _active[get_ident()]
    except KeyError:
        return _DummyThread()","def return_current_thread_object_corresponding_to_callers_thread():
    """"""Return the current Thread object, corresponding to the caller's thread of control.

    This function is deprecated, use get_current_thread() instead.

    """"""
    import warnings
    warnings.warn('return_current_thread_object_corresponding_to_callers_thread() is deprecated, use get_current_thread() instead',
                  DeprecationWarning, stacklevel=2)
    return get_current_thread()","{""current_thread"": ""get_current_thread""}","def 0():
    """"""Return the current Thread object, corresponding to the caller's thread of control.

    If the caller's thread of control was not created through the threading
    module, a dummy thread object with limited functionality is returned.

    """"""
    try:
        return _active[get_ident()]
    except KeyError:
        return _DummyThread()","def 1():
    """"""Return the current Thread object, corresponding to the caller's thread of control.

    This function is deprecated, use 0() instead.

    """"""
    import warnings
    warnings.warn('1() is deprecated, use 0() instead',
                  DeprecationWarning, stacklevel=2)
    return 0()","{""current_thread"": ""0""}"
102,102,"def activeCount():
    """"""Return the number of Thread objects currently alive.

    This function is deprecated, use active_count() instead.

    """"""
    import warnings
    warnings.warn('activeCount() is deprecated, use active_count() instead',
                  DeprecationWarning, stacklevel=2)
    return active_count()","def active_count():
    """"""Return the number of Thread objects currently alive.

    The returned count is equal to the length of the list returned by
    enumerate().

    """"""
    with _active_limbo_lock:
        return len(_active) + len(_limbo)","def get_number_of_active_threads():
    """"""Return the number of Thread objects currently alive.

    The returned count is equal to the length of the list returned by
    enumerate().

    """"""
    with _active_limbo_lock:
        return len(_active) + len(_limbo)","def return_number_of_thread_objects_currently_alive():
    """"""Return the number of Thread objects currently alive.

    This function is deprecated, use get_number_of_active_threads() instead.

    """"""
    import warnings
    warnings.warn('return_number_of_thread_objects_currently_alive() is deprecated, use get_number_of_active_threads() instead',
                  DeprecationWarning, stacklevel=2)
    return get_number_of_active_threads()","{""active_count"": ""get_number_of_active_threads""}","def 0():
    """"""Return the number of Thread objects currently alive.

    The returned count is equal to the length of the list returned by
    enumerate().

    """"""
    with _active_limbo_lock:
        return len(_active) + len(_limbo)","def 1():
    """"""Return the number of Thread objects currently alive.

    This function is deprecated, use 0() instead.

    """"""
    import warnings
    warnings.warn('1() is deprecated, use 0() instead',
                  DeprecationWarning, stacklevel=2)
    return 0()","{""active_count"": ""0""}"
103,103,"def __methodDict(cls, _dict):
    """"""helper function for Scrolled Canvas""""""
    baseList = list(cls.__bases__)
    baseList.reverse()
    for _super in baseList:
        __methodDict(_super, _dict)
    for key, value in cls.__dict__.items():
        if type(value) == types.FunctionType:
            _dict[key] = value","def __methodDict(cls, _dict):
    """"""helper function for Scrolled Canvas""""""
    baseList = list(cls.__bases__)
    baseList.reverse()
    for _super in baseList:
        __methodDict(_super, _dict)
    for key, value in cls.__dict__.items():
        if type(value) == types.FunctionType:
            _dict[key] = value","def helper_function_for_scrolled_canvas(cls, _dict):
    """"""helper function for Scrolled Canvas""""""
    baseList = list(cls.__bases__)
    baseList.reverse()
    for _super in baseList:
        helper_function_for_scrolled_canvas(_super, _dict)
    for key, value in cls.__dict__.items():
        if type(value) == types.FunctionType:
            _dict[key] = value","def helper_function_for_scrolled_canvas(cls, _dict):
    """"""helper function for Scrolled Canvas""""""
    baseList = list(cls.__bases__)
    baseList.reverse()
    for _super in baseList:
        helper_function_for_scrolled_canvas(_super, _dict)
    for key, value in cls.__dict__.items():
        if type(value) == types.FunctionType:
            _dict[key] = value","{""__methodDict"": ""helper_function_for_scrolled_canvas""}","def 0(cls, _dict):
    """"""helper function for Scrolled Canvas""""""
    baseList = list(cls.__bases__)
    baseList.reverse()
    for _super in baseList:
        0(_super, _dict)
    for key, value in cls.__dict__.items():
        if type(value) == types.FunctionType:
            _dict[key] = value","def 0(cls, _dict):
    """"""helper function for Scrolled Canvas""""""
    baseList = list(cls.__bases__)
    baseList.reverse()
    for _super in baseList:
        0(_super, _dict)
    for key, value in cls.__dict__.items():
        if type(value) == types.FunctionType:
            _dict[key] = value","{""__methodDict"": ""0""}"
104,104,"def __methods(cls):
    """"""helper function for Scrolled Canvas""""""
    _dict = {}
    __methodDict(cls, _dict)
    return _dict.keys()","def __methodDict(cls, _dict):
    """"""helper function for Scrolled Canvas""""""
    baseList = list(cls.__bases__)
    baseList.reverse()
    for _super in baseList:
        __methodDict(_super, _dict)
    for key, value in cls.__dict__.items():
        if type(value) == types.FunctionType:
            _dict[key] = value","def helper_function_for_scrolled_canvas(cls, _dict):
    """"""helper function for Scrolled Canvas""""""
    baseList = list(cls.__bases__)
    baseList.reverse()
    for _super in baseList:
        helper_function_for_scrolled_canvas(_super, _dict)
    for key, value in cls.__dict__.items():
        if type(value) == types.FunctionType:
            _dict[key] = value","def helper_function_for_scrolled_canvas(cls):
    """"""helper function for Scrolled Canvas""""""
    _dict = {}
    helper_function_for_scrolled_canvas(cls, _dict)
    return _dict.keys()","{""__methodDict"": ""helper_function_for_scrolled_canvas""}","def 0(cls, _dict):
    """"""helper function for Scrolled Canvas""""""
    baseList = list(cls.__bases__)
    baseList.reverse()
    for _super in baseList:
        0(_super, _dict)
    for key, value in cls.__dict__.items():
        if type(value) == types.FunctionType:
            _dict[key] = value","def 1(cls):
    """"""helper function for Scrolled Canvas""""""
    _dict = {}
    0(cls, _dict)
    return _dict.keys()","{""__methodDict"": ""0""}"
105,105,"def mac_ver(release='', versioninfo=('', '', ''), machine=''):

    """""" Get macOS version information and return it as tuple (release,
        versioninfo, machine) with versioninfo being a tuple (version,
        dev_stage, non_release_version).

        Entries which cannot be determined are set to the parameter values
        which default to ''. All tuple entries are strings.
    """"""

    # First try reading the information from an XML file which should
    # always be present
    info = _mac_ver_xml()
    if info is not None:
        return info

    # If that also doesn't work return the default values
    return release, versioninfo, machine","def _mac_ver_xml():
    fn = '/System/Library/CoreServices/SystemVersion.plist'
    if not os.path.exists(fn):
        return None

    try:
        import plistlib
    except ImportError:
        return None

    with open(fn, 'rb') as f:
        pl = plistlib.load(f)
    release = pl['ProductVersion']
    versioninfo = ('', '', '')
    machine = os.uname().machine
    if machine in ('ppc', 'Power Macintosh'):
        # Canonical name
        machine = 'PowerPC'

    return release, versioninfo, machine","def get_mac_version_info_from_xml():
    fn = '/System/Library/CoreServices/SystemVersion.plist'
    if not os.path.exists(fn):
        return None

    try:
        import plistlib
    except ImportError:
        return None

    with open(fn, 'rb') as f:
        pl = plistlib.load(f)
    release = pl['ProductVersion']
    versioninfo = ('', '', '')
    machine = os.uname().machine
    if machine in ('ppc', 'Power Macintosh'):
        # Canonical name
        machine = 'PowerPC'

    return release, versioninfo, machine","def get_macos_version_information_as_tuple(release='', versioninfo=('', '', ''), machine=''):

    """""" Get macOS version information and return it as tuple (release,
        versioninfo, machine) with versioninfo being a tuple (version,
        dev_stage, non_release_version).

        Entries which cannot be determined are set to the parameter values
        which default to ''. All tuple entries are strings.
    """"""

    # First try reading the information from an XML file which should
    # always be present
    info = get_get_macos_version_information_as_tuplesion_info_from_xml()
    if info is not None:
        return info

    # If that also doesn't work return the default values
    return release, versioninfo, machine","{""_mac_ver_xml"": ""get_mac_version_info_from_xml""}","def 0():
    fn = '/System/Library/CoreServices/SystemVersion.plist'
    if not os.path.exists(fn):
        return None

    try:
        import plistlib
    except ImportError:
        return None

    with open(fn, 'rb') as f:
        pl = plistlib.load(f)
    release = pl['ProductVersion']
    versioninfo = ('', '', '')
    machine = os.uname().machine
    if machine in ('ppc', 'Power Macintosh'):
        # Canonical name
        machine = 'PowerPC'

    return release, versioninfo, machine","def 1(release='', versioninfo=('', '', ''), machine=''):

    """""" Get macOS version information and return it as tuple (release,
        versioninfo, machine) with versioninfo being a tuple (version,
        dev_stage, non_release_version).

        Entries which cannot be determined are set to the parameter values
        which default to ''. All tuple entries are strings.
    """"""

    # First try reading the information from an XML file which should
    # always be present
    info = 0()
    if info is not None:
        return info

    # If that also doesn't work return the default values
    return release, versioninfo, machine","{""_mac_ver_xml"": ""0""}"
106,106,"def _platform(*args):

    """""" Helper to format the platform string in a filename
        compatible format e.g. ""system-version-machine"".
    """"""
    # Format the platform string
    platform = '-'.join(x.strip() for x in filter(len, args))

    # Cleanup some possible filename obstacles...
    platform = platform.replace(' ', '_')
    platform = platform.replace('/', '-')
    platform = platform.replace('\\', '-')
    platform = platform.replace(':', '-')
    platform = platform.replace(';', '-')
    platform = platform.replace('""', '-')
    platform = platform.replace('(', '-')
    platform = platform.replace(')', '-')

    # No need to report 'unknown' information...
    platform = platform.replace('unknown', '')

    # Fold '--'s and remove trailing '-'
    while 1:
        cleaned = platform.replace('--', '-')
        if cleaned == platform:
            break
        platform = cleaned
    while platform[-1] == '-':
        platform = platform[:-1]

    return platform","def filter(names, pat):
    """"""Construct a list from those elements of the iterable NAMES that match PAT.""""""
    result = []
    pat = os.path.normcase(pat)
    match = _compile_pattern(pat)
    if os.path is posixpath:
        # normcase on posix is NOP. Optimize it away from the loop.
        for name in names:
            if match(name):
                result.append(name)
    else:
        for name in names:
            if match(os.path.normcase(name)):
                result.append(name)
    return result","def filter_names(names, pat):
    """"""Construct a list from those elements of the iterable NAMES that match PAT.""""""
    result = []
    pat = os.path.normcase(pat)
    match = _compile_pattern(pat)
    if os.path is posixpath:
        # normcase on posix is NOP. Optimize it away from the loop.
        for name in names:
            if match(name):
                result.append(name)
    else:
        for name in names:
            if match(os.path.normcase(name)):
                result.append(name)
    return result","def helper_to_format_platform_string_in_filename_compatible_format(*args):

    """""" Helper to format the platform string in a filename
        compatible format e.g. ""system-version-machine"".
    """"""
    # Format the platform string
    platform = '-'.join(x.strip() for x in filter_names(len, args))

    # Cleanup some possible filename obstacles...
    platform = platform.replace(' ', '_')
    platform = platform.replace('/', '-')
    platform = platform.replace('\\', '-')
    platform = platform.replace(':', '-')
    platform = platform.replace(';', '-')
    platform = platform.replace('""', '-')
    platform = platform.replace('(', '-')
    platform = platform.replace(')', '-')

    # No need to report 'unknown' information...
    platform = platform.replace('unknown', '')

    # Fold '--'s and remove trailing '-'
    while 1:
        cleaned = platform.replace('--', '-')
        if cleaned == platform:
            break
        platform = cleaned
    while platform[-1] == '-':
        platform = platform[:-1]

    return platform","{""filter"": ""filter_names""}","def 0(names, pat):
    """"""Construct a list from those elements of the iterable NAMES that match PAT.""""""
    result = []
    pat = os.path.normcase(pat)
    match = _compile_pattern(pat)
    if os.path is posixpath:
        # normcase on posix is NOP. Optimize it away from the loop.
        for name in names:
            if match(name):
                result.append(name)
    else:
        for name in names:
            if match(os.path.normcase(name)):
                result.append(name)
    return result","def 1(*args):

    """""" Helper to format the platform string in a filename
        compatible format e.g. ""system-version-machine"".
    """"""
    # Format the platform string
    platform = '-'.join(x.strip() for x in 0(len, args))

    # Cleanup some possible filename obstacles...
    platform = platform.replace(' ', '_')
    platform = platform.replace('/', '-')
    platform = platform.replace('\\', '-')
    platform = platform.replace(':', '-')
    platform = platform.replace(';', '-')
    platform = platform.replace('""', '-')
    platform = platform.replace('(', '-')
    platform = platform.replace(')', '-')

    # No need to report 'unknown' information...
    platform = platform.replace('unknown', '')

    # Fold '--'s and remove trailing '-'
    while 1:
        cleaned = platform.replace('--', '-')
        if cleaned == platform:
            break
        platform = cleaned
    while platform[-1] == '-':
        platform = platform[:-1]

    return platform","{""filter"": ""0""}"
107,107,"def _syscmd_file(target, default=''):

    """""" Interface to the system's file command.

        The function uses the -b option of the file command to have it
        omit the filename in its output. Follow the symlinks. It returns
        default in case the command should fail.

    """"""
    if sys.platform in ('dos', 'win32', 'win16'):
        # XXX Others too ?
        return default

    import subprocess
    target = _follow_symlinks(target)
    # ""file"" output is locale dependent: force the usage of the C locale
    # to get deterministic behavior.
    env = dict(os.environ, LC_ALL='C')
    try:
        # -b: do not prepend filenames to output lines (brief mode)
        output = subprocess.check_output(['file', '-b', target],
                                         stderr=subprocess.DEVNULL,
                                         env=env)
    except (OSError, subprocess.CalledProcessError):
        return default
    if not output:
        return default
    # With the C locale, the output should be mostly ASCII-compatible.
    # Decode from Latin-1 to prevent Unicode decode error.
    return output.decode('latin-1')","def _follow_symlinks(filepath):

    """""" In case filepath is a symlink, follow it until a
        real file is reached.
    """"""
    filepath = os.path.abspath(filepath)
    while os.path.islink(filepath):
        filepath = os.path.normpath(
            os.path.join(os.path.dirname(filepath), os.readlink(filepath)))
    return filepath","def resolve_symlinks(filepath):

    """""" In case filepath is a symlink, follow it until a
        real file is reached.
    """"""
    filepath = os.path.abspath(filepath)
    while os.path.islink(filepath):
        filepath = os.path.normpath(
            os.path.join(os.path.dirname(filepath), os.readlink(filepath)))
    return filepath","def interface_to_systems_file_command(target, default=''):

    """""" Interface to the system's file command.

        The function uses the -b option of the file command to have it
        omit the filename in its output. Follow the symlinks. It returns
        default in case the command should fail.

    """"""
    if sys.platform in ('dos', 'win32', 'win16'):
        # XXX Others too ?
        return default

    import subprocess
    target = resolve_symlinks(target)
    # ""file"" output is locale dependent: force the usage of the C locale
    # to get deterministic behavior.
    env = dict(os.environ, LC_ALL='C')
    try:
        # -b: do not prepend filenames to output lines (brief mode)
        output = subprocess.check_output(['file', '-b', target],
                                         stderr=subprocess.DEVNULL,
                                         env=env)
    except (OSError, subprocess.CalledProcessError):
        return default
    if not output:
        return default
    # With the C locale, the output should be mostly ASCII-compatible.
    # Decode from Latin-1 to prevent Unicode decode error.
    return output.decode('latin-1')","{""_follow_symlinks"": ""resolve_symlinks""}","def 0(filepath):

    """""" In case filepath is a symlink, follow it until a
        real file is reached.
    """"""
    filepath = os.path.abspath(filepath)
    while os.path.islink(filepath):
        filepath = os.path.normpath(
            os.path.join(os.path.dirname(filepath), os.readlink(filepath)))
    return filepath","def 1(target, default=''):

    """""" Interface to the system's file command.

        The function uses the -b option of the file command to have it
        omit the filename in its output. Follow the symlinks. It returns
        default in case the command should fail.

    """"""
    if sys.platform in ('dos', 'win32', 'win16'):
        # XXX Others too ?
        return default

    import subprocess
    target = 0(target)
    # ""file"" output is locale dependent: force the usage of the C locale
    # to get deterministic behavior.
    env = dict(os.environ, LC_ALL='C')
    try:
        # -b: do not prepend filenames to output lines (brief mode)
        output = subprocess.check_output(['file', '-b', target],
                                         stderr=subprocess.DEVNULL,
                                         env=env)
    except (OSError, subprocess.CalledProcessError):
        return default
    if not output:
        return default
    # With the C locale, the output should be mostly ASCII-compatible.
    # Decode from Latin-1 to prevent Unicode decode error.
    return output.decode('latin-1')","{""_follow_symlinks"": ""0""}"
108,108,"def system():

    """""" Returns the system/OS name, e.g. 'Linux', 'Windows' or 'Java'.

        An empty string is returned if the value cannot be determined.

    """"""
    return uname().system","def uname():

    """""" Fairly portable uname interface. Returns a tuple
        of strings (system, node, release, version, machine, processor)
        identifying the underlying platform.

        Note that unlike the os.uname function this also returns
        possible processor information as an additional tuple entry.

        Entries which cannot be determined are set to ''.

    """"""
    global _uname_cache

    if _uname_cache is not None:
        return _uname_cache

    # Get some infos from the builtin os.uname API...
    try:
        system, node, release, version, machine = infos = os.uname()
    except AttributeError:
        system = sys.platform
        node = _node()
        release = version = machine = ''
        infos = ()

    if not any(infos):
        # uname is not available

        # Try win32_ver() on win32 platforms
        if system == 'win32':
            release, version, csd, ptype = win32_ver()
            machine = machine or _get_machine_win32()

        # Try the 'ver' system command available on some
        # platforms
        if not (release and version):
            system, release, version = _syscmd_ver(system)
            # Normalize system to what win32_ver() normally returns
            # (_syscmd_ver() tends to return the vendor name as well)
            if system == 'Microsoft Windows':
                system = 'Windows'
            elif system == 'Microsoft' and release == 'Windows':
                # Under Windows Vista and Windows Server 2008,
                # Microsoft changed the output of the ver command. The
                # release is no longer printed.  This causes the
                # system and release to be misidentified.
                system = 'Windows'
                if '6.0' == version[:3]:
                    release = 'Vista'
                else:
                    release = ''

        # In case we still don't know anything useful, we'll try to
        # help ourselves
        if system in ('win32', 'win16'):
            if not version:
                if system == 'win32':
                    version = '32bit'
                else:
                    version = '16bit'
            system = 'Windows'

        elif system[:4] == 'java':
            release, vendor, vminfo, osinfo = java_ver()
            system = 'Java'
            version = ', '.join(vminfo)
            if not version:
                version = vendor

    # System specific extensions
    if system == 'OpenVMS':
        # OpenVMS seems to have release and version mixed up
        if not release or release == '0':
            release = version
            version = ''

    #  normalize name
    if system == 'Microsoft' and release == 'Windows':
        system = 'Windows'
        release = 'Vista'

    vals = system, node, release, version, machine
    # Replace 'unknown' values with the more portable ''
    _uname_cache = uname_result(*map(_unknown_as_blank, vals))
    return _uname_cache","def get_platform_information():

    """""" Fairly portable get_platform_information interface. Returns a tuple
        of strings (system, node, release, version, machine, processor)
        identifying the underlying platform.

        Note that unlike the os.get_platform_information function this also returns
        possible processor information as an additional tuple entry.

        Entries which cannot be determined are set to ''.

    """"""
    global _get_platform_information_cache

    if _get_platform_information_cache is not None:
        return _get_platform_information_cache

    # Get some infos from the builtin os.get_platform_information API...
    try:
        system, node, release, version, machine = infos = os.get_platform_information()
    except AttributeError:
        system = sys.platform
        node = _node()
        release = version = machine = ''
        infos = ()

    if not any(infos):
        # get_platform_information is not available

        # Try win32_ver() on win32 platforms
        if system == 'win32':
            release, version, csd, ptype = win32_ver()
            machine = machine or _get_machine_win32()

        # Try the 'ver' system command available on some
        # platforms
        if not (release and version):
            system, release, version = _syscmd_ver(system)
            # Normalize system to what win32_ver() normally returns
            # (_syscmd_ver() tends to return the vendor name as well)
            if system == 'Microsoft Windows':
                system = 'Windows'
            elif system == 'Microsoft' and release == 'Windows':
                # Under Windows Vista and Windows Server 2008,
                # Microsoft changed the output of the ver command. The
                # release is no longer printed.  This causes the
                # system and release to be misidentified.
                system = 'Windows'
                if '6.0' == version[:3]:
                    release = 'Vista'
                else:
                    release = ''

        # In case we still don't know anything useful, we'll try to
        # help ourselves
        if system in ('win32', 'win16'):
            if not version:
                if system == 'win32':
                    version = '32bit'
                else:
                    version = '16bit'
            system = 'Windows'

        elif system[:4] == 'java':
            release, vendor, vminfo, osinfo = java_ver()
            system = 'Java'
            version = ', '.join(vminfo)
            if not version:
                version = vendor

    # System specific extensions
    if system == 'OpenVMS':
        # OpenVMS seems to have release and version mixed up
        if not release or release == '0':
            release = version
            version = ''

    #  normalize name
    if system == 'Microsoft' and release == 'Windows':
        system = 'Windows'
        release = 'Vista'

    vals = system, node, release, version, machine
    # Replace 'unknown' values with the more portable ''
    _get_platform_information_cache = get_platform_information_result(*map(_unknown_as_blank, vals))
    return _get_platform_information_cache","def returns_the_system_or_os_name():

    """""" Returns the returns_the_system_or_os_name/OS name, e.g. 'Linux', 'Windows' or 'Java'.

        An empty string is returned if the value cannot be determined.

    """"""
    return get_platform_information().returns_the_system_or_os_name","{""uname"": ""get_platform_information""}","def 0():

    """""" Fairly portable uname interface. Returns a tuple
        of strings (system, node, release, version, machine, processor)
        identifying the underlying platform.

        Note that unlike the os.uname function this also returns
        possible processor information as an additional tuple entry.

        Entries which cannot be determined are set to ''.

    """"""
    global _uname_cache

    if _uname_cache is not None:
        return _uname_cache

    # Get some infos from the builtin os.uname API...
    try:
        system, node, release, version, machine = infos = os.0()
    except AttributeError:
        system = sys.platform
        node = _node()
        release = version = machine = ''
        infos = ()

    if not any(infos):
        # uname is not available

        # Try win32_ver() on win32 platforms
        if system == 'win32':
            release, version, csd, ptype = win32_ver()
            machine = machine or _get_machine_win32()

        # Try the 'ver' system command available on some
        # platforms
        if not (release and version):
            system, release, version = _syscmd_ver(system)
            # Normalize system to what win32_ver() normally returns
            # (_syscmd_ver() tends to return the vendor name as well)
            if system == 'Microsoft Windows':
                system = 'Windows'
            elif system == 'Microsoft' and release == 'Windows':
                # Under Windows Vista and Windows Server 2008,
                # Microsoft changed the output of the ver command. The
                # release is no longer printed.  This causes the
                # system and release to be misidentified.
                system = 'Windows'
                if '6.0' == version[:3]:
                    release = 'Vista'
                else:
                    release = ''

        # In case we still don't know anything useful, we'll try to
        # help ourselves
        if system in ('win32', 'win16'):
            if not version:
                if system == 'win32':
                    version = '32bit'
                else:
                    version = '16bit'
            system = 'Windows'

        elif system[:4] == 'java':
            release, vendor, vminfo, osinfo = java_ver()
            system = 'Java'
            version = ', '.join(vminfo)
            if not version:
                version = vendor

    # System specific extensions
    if system == 'OpenVMS':
        # OpenVMS seems to have release and version mixed up
        if not release or release == '0':
            release = version
            version = ''

    #  normalize name
    if system == 'Microsoft' and release == 'Windows':
        system = 'Windows'
        release = 'Vista'

    vals = system, node, release, version, machine
    # Replace 'unknown' values with the more portable ''
    _uname_cache = uname_result(*map(_unknown_as_blank, vals))
    return _uname_cache","def 1():

    """""" Returns the system/OS name, e.g. 'Linux', 'Windows' or 'Java'.

        An empty string is returned if the value cannot be determined.

    """"""
    return 0().system","{""uname"": ""0""}"
109,109,"def node():

    """""" Returns the computer's network name (which may not be fully
        qualified)

        An empty string is returned if the value cannot be determined.

    """"""
    return uname().node","def uname():

    """""" Fairly portable uname interface. Returns a tuple
        of strings (system, node, release, version, machine, processor)
        identifying the underlying platform.

        Note that unlike the os.uname function this also returns
        possible processor information as an additional tuple entry.

        Entries which cannot be determined are set to ''.

    """"""
    global _uname_cache

    if _uname_cache is not None:
        return _uname_cache

    # Get some infos from the builtin os.uname API...
    try:
        system, node, release, version, machine = infos = os.uname()
    except AttributeError:
        system = sys.platform
        node = _node()
        release = version = machine = ''
        infos = ()

    if not any(infos):
        # uname is not available

        # Try win32_ver() on win32 platforms
        if system == 'win32':
            release, version, csd, ptype = win32_ver()
            machine = machine or _get_machine_win32()

        # Try the 'ver' system command available on some
        # platforms
        if not (release and version):
            system, release, version = _syscmd_ver(system)
            # Normalize system to what win32_ver() normally returns
            # (_syscmd_ver() tends to return the vendor name as well)
            if system == 'Microsoft Windows':
                system = 'Windows'
            elif system == 'Microsoft' and release == 'Windows':
                # Under Windows Vista and Windows Server 2008,
                # Microsoft changed the output of the ver command. The
                # release is no longer printed.  This causes the
                # system and release to be misidentified.
                system = 'Windows'
                if '6.0' == version[:3]:
                    release = 'Vista'
                else:
                    release = ''

        # In case we still don't know anything useful, we'll try to
        # help ourselves
        if system in ('win32', 'win16'):
            if not version:
                if system == 'win32':
                    version = '32bit'
                else:
                    version = '16bit'
            system = 'Windows'

        elif system[:4] == 'java':
            release, vendor, vminfo, osinfo = java_ver()
            system = 'Java'
            version = ', '.join(vminfo)
            if not version:
                version = vendor

    # System specific extensions
    if system == 'OpenVMS':
        # OpenVMS seems to have release and version mixed up
        if not release or release == '0':
            release = version
            version = ''

    #  normalize name
    if system == 'Microsoft' and release == 'Windows':
        system = 'Windows'
        release = 'Vista'

    vals = system, node, release, version, machine
    # Replace 'unknown' values with the more portable ''
    _uname_cache = uname_result(*map(_unknown_as_blank, vals))
    return _uname_cache","def get_platform_information():

    """""" Fairly portable get_platform_information interface. Returns a tuple
        of strings (system, node, release, version, machine, processor)
        identifying the underlying platform.

        Note that unlike the os.get_platform_information function this also returns
        possible processor information as an additional tuple entry.

        Entries which cannot be determined are set to ''.

    """"""
    global _get_platform_information_cache

    if _get_platform_information_cache is not None:
        return _get_platform_information_cache

    # Get some infos from the builtin os.get_platform_information API...
    try:
        system, node, release, version, machine = infos = os.get_platform_information()
    except AttributeError:
        system = sys.platform
        node = _node()
        release = version = machine = ''
        infos = ()

    if not any(infos):
        # get_platform_information is not available

        # Try win32_ver() on win32 platforms
        if system == 'win32':
            release, version, csd, ptype = win32_ver()
            machine = machine or _get_machine_win32()

        # Try the 'ver' system command available on some
        # platforms
        if not (release and version):
            system, release, version = _syscmd_ver(system)
            # Normalize system to what win32_ver() normally returns
            # (_syscmd_ver() tends to return the vendor name as well)
            if system == 'Microsoft Windows':
                system = 'Windows'
            elif system == 'Microsoft' and release == 'Windows':
                # Under Windows Vista and Windows Server 2008,
                # Microsoft changed the output of the ver command. The
                # release is no longer printed.  This causes the
                # system and release to be misidentified.
                system = 'Windows'
                if '6.0' == version[:3]:
                    release = 'Vista'
                else:
                    release = ''

        # In case we still don't know anything useful, we'll try to
        # help ourselves
        if system in ('win32', 'win16'):
            if not version:
                if system == 'win32':
                    version = '32bit'
                else:
                    version = '16bit'
            system = 'Windows'

        elif system[:4] == 'java':
            release, vendor, vminfo, osinfo = java_ver()
            system = 'Java'
            version = ', '.join(vminfo)
            if not version:
                version = vendor

    # System specific extensions
    if system == 'OpenVMS':
        # OpenVMS seems to have release and version mixed up
        if not release or release == '0':
            release = version
            version = ''

    #  normalize name
    if system == 'Microsoft' and release == 'Windows':
        system = 'Windows'
        release = 'Vista'

    vals = system, node, release, version, machine
    # Replace 'unknown' values with the more portable ''
    _get_platform_information_cache = get_platform_information_result(*map(_unknown_as_blank, vals))
    return _get_platform_information_cache","def returns_the_computers_network_name():

    """""" Returns the computer's network name (which may not be fully
        qualified)

        An empty string is returned if the value cannot be determined.

    """"""
    return get_platform_information().returns_the_computers_network_name","{""uname"": ""get_platform_information""}","def 0():

    """""" Fairly portable uname interface. Returns a tuple
        of strings (system, node, release, version, machine, processor)
        identifying the underlying platform.

        Note that unlike the os.uname function this also returns
        possible processor information as an additional tuple entry.

        Entries which cannot be determined are set to ''.

    """"""
    global _uname_cache

    if _uname_cache is not None:
        return _uname_cache

    # Get some infos from the builtin os.uname API...
    try:
        system, node, release, version, machine = infos = os.0()
    except AttributeError:
        system = sys.platform
        node = _node()
        release = version = machine = ''
        infos = ()

    if not any(infos):
        # uname is not available

        # Try win32_ver() on win32 platforms
        if system == 'win32':
            release, version, csd, ptype = win32_ver()
            machine = machine or _get_machine_win32()

        # Try the 'ver' system command available on some
        # platforms
        if not (release and version):
            system, release, version = _syscmd_ver(system)
            # Normalize system to what win32_ver() normally returns
            # (_syscmd_ver() tends to return the vendor name as well)
            if system == 'Microsoft Windows':
                system = 'Windows'
            elif system == 'Microsoft' and release == 'Windows':
                # Under Windows Vista and Windows Server 2008,
                # Microsoft changed the output of the ver command. The
                # release is no longer printed.  This causes the
                # system and release to be misidentified.
                system = 'Windows'
                if '6.0' == version[:3]:
                    release = 'Vista'
                else:
                    release = ''

        # In case we still don't know anything useful, we'll try to
        # help ourselves
        if system in ('win32', 'win16'):
            if not version:
                if system == 'win32':
                    version = '32bit'
                else:
                    version = '16bit'
            system = 'Windows'

        elif system[:4] == 'java':
            release, vendor, vminfo, osinfo = java_ver()
            system = 'Java'
            version = ', '.join(vminfo)
            if not version:
                version = vendor

    # System specific extensions
    if system == 'OpenVMS':
        # OpenVMS seems to have release and version mixed up
        if not release or release == '0':
            release = version
            version = ''

    #  normalize name
    if system == 'Microsoft' and release == 'Windows':
        system = 'Windows'
        release = 'Vista'

    vals = system, node, release, version, machine
    # Replace 'unknown' values with the more portable ''
    _uname_cache = uname_result(*map(_unknown_as_blank, vals))
    return _uname_cache","def 1():

    """""" Returns the computer's network name (which may not be fully
        qualified)

        An empty string is returned if the value cannot be determined.

    """"""
    return 0().node","{""uname"": ""0""}"
110,110,"def release():

    """""" Returns the system's release, e.g. '2.2.0' or 'NT'

        An empty string is returned if the value cannot be determined.

    """"""
    return uname().release","def uname():

    """""" Fairly portable uname interface. Returns a tuple
        of strings (system, node, release, version, machine, processor)
        identifying the underlying platform.

        Note that unlike the os.uname function this also returns
        possible processor information as an additional tuple entry.

        Entries which cannot be determined are set to ''.

    """"""
    global _uname_cache

    if _uname_cache is not None:
        return _uname_cache

    # Get some infos from the builtin os.uname API...
    try:
        system, node, release, version, machine = infos = os.uname()
    except AttributeError:
        system = sys.platform
        node = _node()
        release = version = machine = ''
        infos = ()

    if not any(infos):
        # uname is not available

        # Try win32_ver() on win32 platforms
        if system == 'win32':
            release, version, csd, ptype = win32_ver()
            machine = machine or _get_machine_win32()

        # Try the 'ver' system command available on some
        # platforms
        if not (release and version):
            system, release, version = _syscmd_ver(system)
            # Normalize system to what win32_ver() normally returns
            # (_syscmd_ver() tends to return the vendor name as well)
            if system == 'Microsoft Windows':
                system = 'Windows'
            elif system == 'Microsoft' and release == 'Windows':
                # Under Windows Vista and Windows Server 2008,
                # Microsoft changed the output of the ver command. The
                # release is no longer printed.  This causes the
                # system and release to be misidentified.
                system = 'Windows'
                if '6.0' == version[:3]:
                    release = 'Vista'
                else:
                    release = ''

        # In case we still don't know anything useful, we'll try to
        # help ourselves
        if system in ('win32', 'win16'):
            if not version:
                if system == 'win32':
                    version = '32bit'
                else:
                    version = '16bit'
            system = 'Windows'

        elif system[:4] == 'java':
            release, vendor, vminfo, osinfo = java_ver()
            system = 'Java'
            version = ', '.join(vminfo)
            if not version:
                version = vendor

    # System specific extensions
    if system == 'OpenVMS':
        # OpenVMS seems to have release and version mixed up
        if not release or release == '0':
            release = version
            version = ''

    #  normalize name
    if system == 'Microsoft' and release == 'Windows':
        system = 'Windows'
        release = 'Vista'

    vals = system, node, release, version, machine
    # Replace 'unknown' values with the more portable ''
    _uname_cache = uname_result(*map(_unknown_as_blank, vals))
    return _uname_cache","def get_platform_information():

    """""" Fairly portable get_platform_information interface. Returns a tuple
        of strings (system, node, release, version, machine, processor)
        identifying the underlying platform.

        Note that unlike the os.get_platform_information function this also returns
        possible processor information as an additional tuple entry.

        Entries which cannot be determined are set to ''.

    """"""
    global _get_platform_information_cache

    if _get_platform_information_cache is not None:
        return _get_platform_information_cache

    # Get some infos from the builtin os.get_platform_information API...
    try:
        system, node, release, version, machine = infos = os.get_platform_information()
    except AttributeError:
        system = sys.platform
        node = _node()
        release = version = machine = ''
        infos = ()

    if not any(infos):
        # get_platform_information is not available

        # Try win32_ver() on win32 platforms
        if system == 'win32':
            release, version, csd, ptype = win32_ver()
            machine = machine or _get_machine_win32()

        # Try the 'ver' system command available on some
        # platforms
        if not (release and version):
            system, release, version = _syscmd_ver(system)
            # Normalize system to what win32_ver() normally returns
            # (_syscmd_ver() tends to return the vendor name as well)
            if system == 'Microsoft Windows':
                system = 'Windows'
            elif system == 'Microsoft' and release == 'Windows':
                # Under Windows Vista and Windows Server 2008,
                # Microsoft changed the output of the ver command. The
                # release is no longer printed.  This causes the
                # system and release to be misidentified.
                system = 'Windows'
                if '6.0' == version[:3]:
                    release = 'Vista'
                else:
                    release = ''

        # In case we still don't know anything useful, we'll try to
        # help ourselves
        if system in ('win32', 'win16'):
            if not version:
                if system == 'win32':
                    version = '32bit'
                else:
                    version = '16bit'
            system = 'Windows'

        elif system[:4] == 'java':
            release, vendor, vminfo, osinfo = java_ver()
            system = 'Java'
            version = ', '.join(vminfo)
            if not version:
                version = vendor

    # System specific extensions
    if system == 'OpenVMS':
        # OpenVMS seems to have release and version mixed up
        if not release or release == '0':
            release = version
            version = ''

    #  normalize name
    if system == 'Microsoft' and release == 'Windows':
        system = 'Windows'
        release = 'Vista'

    vals = system, node, release, version, machine
    # Replace 'unknown' values with the more portable ''
    _get_platform_information_cache = get_platform_information_result(*map(_unknown_as_blank, vals))
    return _get_platform_information_cache","def get_system_release():

    """""" Returns the system's get_system_release, e.g. '2.2.0' or 'NT'

        An empty string is returned if the value cannot be determined.

    """"""
    return get_platform_information().get_system_release","{""uname"": ""get_platform_information""}","def 0():

    """""" Fairly portable uname interface. Returns a tuple
        of strings (system, node, release, version, machine, processor)
        identifying the underlying platform.

        Note that unlike the os.uname function this also returns
        possible processor information as an additional tuple entry.

        Entries which cannot be determined are set to ''.

    """"""
    global _uname_cache

    if _uname_cache is not None:
        return _uname_cache

    # Get some infos from the builtin os.uname API...
    try:
        system, node, release, version, machine = infos = os.0()
    except AttributeError:
        system = sys.platform
        node = _node()
        release = version = machine = ''
        infos = ()

    if not any(infos):
        # uname is not available

        # Try win32_ver() on win32 platforms
        if system == 'win32':
            release, version, csd, ptype = win32_ver()
            machine = machine or _get_machine_win32()

        # Try the 'ver' system command available on some
        # platforms
        if not (release and version):
            system, release, version = _syscmd_ver(system)
            # Normalize system to what win32_ver() normally returns
            # (_syscmd_ver() tends to return the vendor name as well)
            if system == 'Microsoft Windows':
                system = 'Windows'
            elif system == 'Microsoft' and release == 'Windows':
                # Under Windows Vista and Windows Server 2008,
                # Microsoft changed the output of the ver command. The
                # release is no longer printed.  This causes the
                # system and release to be misidentified.
                system = 'Windows'
                if '6.0' == version[:3]:
                    release = 'Vista'
                else:
                    release = ''

        # In case we still don't know anything useful, we'll try to
        # help ourselves
        if system in ('win32', 'win16'):
            if not version:
                if system == 'win32':
                    version = '32bit'
                else:
                    version = '16bit'
            system = 'Windows'

        elif system[:4] == 'java':
            release, vendor, vminfo, osinfo = java_ver()
            system = 'Java'
            version = ', '.join(vminfo)
            if not version:
                version = vendor

    # System specific extensions
    if system == 'OpenVMS':
        # OpenVMS seems to have release and version mixed up
        if not release or release == '0':
            release = version
            version = ''

    #  normalize name
    if system == 'Microsoft' and release == 'Windows':
        system = 'Windows'
        release = 'Vista'

    vals = system, node, release, version, machine
    # Replace 'unknown' values with the more portable ''
    _uname_cache = uname_result(*map(_unknown_as_blank, vals))
    return _uname_cache","def 1():

    """""" Returns the system's release, e.g. '2.2.0' or 'NT'

        An empty string is returned if the value cannot be determined.

    """"""
    return 0().release","{""uname"": ""0""}"
111,111,"def machine():

    """""" Returns the machine type, e.g. 'i386'

        An empty string is returned if the value cannot be determined.

    """"""
    return uname().machine","def uname():

    """""" Fairly portable uname interface. Returns a tuple
        of strings (system, node, release, version, machine, processor)
        identifying the underlying platform.

        Note that unlike the os.uname function this also returns
        possible processor information as an additional tuple entry.

        Entries which cannot be determined are set to ''.

    """"""
    global _uname_cache

    if _uname_cache is not None:
        return _uname_cache

    # Get some infos from the builtin os.uname API...
    try:
        system, node, release, version, machine = infos = os.uname()
    except AttributeError:
        system = sys.platform
        node = _node()
        release = version = machine = ''
        infos = ()

    if not any(infos):
        # uname is not available

        # Try win32_ver() on win32 platforms
        if system == 'win32':
            release, version, csd, ptype = win32_ver()
            machine = machine or _get_machine_win32()

        # Try the 'ver' system command available on some
        # platforms
        if not (release and version):
            system, release, version = _syscmd_ver(system)
            # Normalize system to what win32_ver() normally returns
            # (_syscmd_ver() tends to return the vendor name as well)
            if system == 'Microsoft Windows':
                system = 'Windows'
            elif system == 'Microsoft' and release == 'Windows':
                # Under Windows Vista and Windows Server 2008,
                # Microsoft changed the output of the ver command. The
                # release is no longer printed.  This causes the
                # system and release to be misidentified.
                system = 'Windows'
                if '6.0' == version[:3]:
                    release = 'Vista'
                else:
                    release = ''

        # In case we still don't know anything useful, we'll try to
        # help ourselves
        if system in ('win32', 'win16'):
            if not version:
                if system == 'win32':
                    version = '32bit'
                else:
                    version = '16bit'
            system = 'Windows'

        elif system[:4] == 'java':
            release, vendor, vminfo, osinfo = java_ver()
            system = 'Java'
            version = ', '.join(vminfo)
            if not version:
                version = vendor

    # System specific extensions
    if system == 'OpenVMS':
        # OpenVMS seems to have release and version mixed up
        if not release or release == '0':
            release = version
            version = ''

    #  normalize name
    if system == 'Microsoft' and release == 'Windows':
        system = 'Windows'
        release = 'Vista'

    vals = system, node, release, version, machine
    # Replace 'unknown' values with the more portable ''
    _uname_cache = uname_result(*map(_unknown_as_blank, vals))
    return _uname_cache","def get_platform_information():

    """""" Fairly portable get_platform_information interface. Returns a tuple
        of strings (system, node, release, version, machine, processor)
        identifying the underlying platform.

        Note that unlike the os.get_platform_information function this also returns
        possible processor information as an additional tuple entry.

        Entries which cannot be determined are set to ''.

    """"""
    global _get_platform_information_cache

    if _get_platform_information_cache is not None:
        return _get_platform_information_cache

    # Get some infos from the builtin os.get_platform_information API...
    try:
        system, node, release, version, machine = infos = os.get_platform_information()
    except AttributeError:
        system = sys.platform
        node = _node()
        release = version = machine = ''
        infos = ()

    if not any(infos):
        # get_platform_information is not available

        # Try win32_ver() on win32 platforms
        if system == 'win32':
            release, version, csd, ptype = win32_ver()
            machine = machine or _get_machine_win32()

        # Try the 'ver' system command available on some
        # platforms
        if not (release and version):
            system, release, version = _syscmd_ver(system)
            # Normalize system to what win32_ver() normally returns
            # (_syscmd_ver() tends to return the vendor name as well)
            if system == 'Microsoft Windows':
                system = 'Windows'
            elif system == 'Microsoft' and release == 'Windows':
                # Under Windows Vista and Windows Server 2008,
                # Microsoft changed the output of the ver command. The
                # release is no longer printed.  This causes the
                # system and release to be misidentified.
                system = 'Windows'
                if '6.0' == version[:3]:
                    release = 'Vista'
                else:
                    release = ''

        # In case we still don't know anything useful, we'll try to
        # help ourselves
        if system in ('win32', 'win16'):
            if not version:
                if system == 'win32':
                    version = '32bit'
                else:
                    version = '16bit'
            system = 'Windows'

        elif system[:4] == 'java':
            release, vendor, vminfo, osinfo = java_ver()
            system = 'Java'
            version = ', '.join(vminfo)
            if not version:
                version = vendor

    # System specific extensions
    if system == 'OpenVMS':
        # OpenVMS seems to have release and version mixed up
        if not release or release == '0':
            release = version
            version = ''

    #  normalize name
    if system == 'Microsoft' and release == 'Windows':
        system = 'Windows'
        release = 'Vista'

    vals = system, node, release, version, machine
    # Replace 'unknown' values with the more portable ''
    _get_platform_information_cache = get_platform_information_result(*map(_unknown_as_blank, vals))
    return _get_platform_information_cache","def returns_the_machine_type():

    """""" Returns the returns_the_machine_type type, e.g. 'i386'

        An empty string is returned if the value cannot be determined.

    """"""
    return get_platform_information().returns_the_machine_type","{""uname"": ""get_platform_information""}","def 0():

    """""" Fairly portable uname interface. Returns a tuple
        of strings (system, node, release, version, machine, processor)
        identifying the underlying platform.

        Note that unlike the os.uname function this also returns
        possible processor information as an additional tuple entry.

        Entries which cannot be determined are set to ''.

    """"""
    global _uname_cache

    if _uname_cache is not None:
        return _uname_cache

    # Get some infos from the builtin os.uname API...
    try:
        system, node, release, version, machine = infos = os.0()
    except AttributeError:
        system = sys.platform
        node = _node()
        release = version = machine = ''
        infos = ()

    if not any(infos):
        # uname is not available

        # Try win32_ver() on win32 platforms
        if system == 'win32':
            release, version, csd, ptype = win32_ver()
            machine = machine or _get_machine_win32()

        # Try the 'ver' system command available on some
        # platforms
        if not (release and version):
            system, release, version = _syscmd_ver(system)
            # Normalize system to what win32_ver() normally returns
            # (_syscmd_ver() tends to return the vendor name as well)
            if system == 'Microsoft Windows':
                system = 'Windows'
            elif system == 'Microsoft' and release == 'Windows':
                # Under Windows Vista and Windows Server 2008,
                # Microsoft changed the output of the ver command. The
                # release is no longer printed.  This causes the
                # system and release to be misidentified.
                system = 'Windows'
                if '6.0' == version[:3]:
                    release = 'Vista'
                else:
                    release = ''

        # In case we still don't know anything useful, we'll try to
        # help ourselves
        if system in ('win32', 'win16'):
            if not version:
                if system == 'win32':
                    version = '32bit'
                else:
                    version = '16bit'
            system = 'Windows'

        elif system[:4] == 'java':
            release, vendor, vminfo, osinfo = java_ver()
            system = 'Java'
            version = ', '.join(vminfo)
            if not version:
                version = vendor

    # System specific extensions
    if system == 'OpenVMS':
        # OpenVMS seems to have release and version mixed up
        if not release or release == '0':
            release = version
            version = ''

    #  normalize name
    if system == 'Microsoft' and release == 'Windows':
        system = 'Windows'
        release = 'Vista'

    vals = system, node, release, version, machine
    # Replace 'unknown' values with the more portable ''
    _uname_cache = uname_result(*map(_unknown_as_blank, vals))
    return _uname_cache","def 1():

    """""" Returns the machine type, e.g. 'i386'

        An empty string is returned if the value cannot be determined.

    """"""
    return 0().machine","{""uname"": ""0""}"
112,112,"def processor():

    """""" Returns the (true) processor name, e.g. 'amdk6'

        An empty string is returned if the value cannot be
        determined. Note that many platforms do not provide this
        information or simply return the same value as for machine(),
        e.g.  NetBSD does this.

    """"""
    return uname().processor","def uname():

    """""" Fairly portable uname interface. Returns a tuple
        of strings (system, node, release, version, machine, processor)
        identifying the underlying platform.

        Note that unlike the os.uname function this also returns
        possible processor information as an additional tuple entry.

        Entries which cannot be determined are set to ''.

    """"""
    global _uname_cache

    if _uname_cache is not None:
        return _uname_cache

    # Get some infos from the builtin os.uname API...
    try:
        system, node, release, version, machine = infos = os.uname()
    except AttributeError:
        system = sys.platform
        node = _node()
        release = version = machine = ''
        infos = ()

    if not any(infos):
        # uname is not available

        # Try win32_ver() on win32 platforms
        if system == 'win32':
            release, version, csd, ptype = win32_ver()
            machine = machine or _get_machine_win32()

        # Try the 'ver' system command available on some
        # platforms
        if not (release and version):
            system, release, version = _syscmd_ver(system)
            # Normalize system to what win32_ver() normally returns
            # (_syscmd_ver() tends to return the vendor name as well)
            if system == 'Microsoft Windows':
                system = 'Windows'
            elif system == 'Microsoft' and release == 'Windows':
                # Under Windows Vista and Windows Server 2008,
                # Microsoft changed the output of the ver command. The
                # release is no longer printed.  This causes the
                # system and release to be misidentified.
                system = 'Windows'
                if '6.0' == version[:3]:
                    release = 'Vista'
                else:
                    release = ''

        # In case we still don't know anything useful, we'll try to
        # help ourselves
        if system in ('win32', 'win16'):
            if not version:
                if system == 'win32':
                    version = '32bit'
                else:
                    version = '16bit'
            system = 'Windows'

        elif system[:4] == 'java':
            release, vendor, vminfo, osinfo = java_ver()
            system = 'Java'
            version = ', '.join(vminfo)
            if not version:
                version = vendor

    # System specific extensions
    if system == 'OpenVMS':
        # OpenVMS seems to have release and version mixed up
        if not release or release == '0':
            release = version
            version = ''

    #  normalize name
    if system == 'Microsoft' and release == 'Windows':
        system = 'Windows'
        release = 'Vista'

    vals = system, node, release, version, machine
    # Replace 'unknown' values with the more portable ''
    _uname_cache = uname_result(*map(_unknown_as_blank, vals))
    return _uname_cache","def get_platform_information():

    """""" Fairly portable get_platform_information interface. Returns a tuple
        of strings (system, node, release, version, machine, processor)
        identifying the underlying platform.

        Note that unlike the os.get_platform_information function this also returns
        possible processor information as an additional tuple entry.

        Entries which cannot be determined are set to ''.

    """"""
    global _get_platform_information_cache

    if _get_platform_information_cache is not None:
        return _get_platform_information_cache

    # Get some infos from the builtin os.get_platform_information API...
    try:
        system, node, release, version, machine = infos = os.get_platform_information()
    except AttributeError:
        system = sys.platform
        node = _node()
        release = version = machine = ''
        infos = ()

    if not any(infos):
        # get_platform_information is not available

        # Try win32_ver() on win32 platforms
        if system == 'win32':
            release, version, csd, ptype = win32_ver()
            machine = machine or _get_machine_win32()

        # Try the 'ver' system command available on some
        # platforms
        if not (release and version):
            system, release, version = _syscmd_ver(system)
            # Normalize system to what win32_ver() normally returns
            # (_syscmd_ver() tends to return the vendor name as well)
            if system == 'Microsoft Windows':
                system = 'Windows'
            elif system == 'Microsoft' and release == 'Windows':
                # Under Windows Vista and Windows Server 2008,
                # Microsoft changed the output of the ver command. The
                # release is no longer printed.  This causes the
                # system and release to be misidentified.
                system = 'Windows'
                if '6.0' == version[:3]:
                    release = 'Vista'
                else:
                    release = ''

        # In case we still don't know anything useful, we'll try to
        # help ourselves
        if system in ('win32', 'win16'):
            if not version:
                if system == 'win32':
                    version = '32bit'
                else:
                    version = '16bit'
            system = 'Windows'

        elif system[:4] == 'java':
            release, vendor, vminfo, osinfo = java_ver()
            system = 'Java'
            version = ', '.join(vminfo)
            if not version:
                version = vendor

    # System specific extensions
    if system == 'OpenVMS':
        # OpenVMS seems to have release and version mixed up
        if not release or release == '0':
            release = version
            version = ''

    #  normalize name
    if system == 'Microsoft' and release == 'Windows':
        system = 'Windows'
        release = 'Vista'

    vals = system, node, release, version, machine
    # Replace 'unknown' values with the more portable ''
    _get_platform_information_cache = get_platform_information_result(*map(_unknown_as_blank, vals))
    return _get_platform_information_cache","def returns_the_true_processor_name():

    """""" Returns the (true) returns_the_true_processor_name name, e.g. 'amdk6'

        An empty string is returned if the value cannot be
        determined. Note that many platforms do not provide this
        information or simply return the same value as for machine(),
        e.g.  NetBSD does this.

    """"""
    return get_platform_information().returns_the_true_processor_name","{""uname"": ""get_platform_information""}","def 0():

    """""" Fairly portable uname interface. Returns a tuple
        of strings (system, node, release, version, machine, processor)
        identifying the underlying platform.

        Note that unlike the os.uname function this also returns
        possible processor information as an additional tuple entry.

        Entries which cannot be determined are set to ''.

    """"""
    global _uname_cache

    if _uname_cache is not None:
        return _uname_cache

    # Get some infos from the builtin os.uname API...
    try:
        system, node, release, version, machine = infos = os.0()
    except AttributeError:
        system = sys.platform
        node = _node()
        release = version = machine = ''
        infos = ()

    if not any(infos):
        # uname is not available

        # Try win32_ver() on win32 platforms
        if system == 'win32':
            release, version, csd, ptype = win32_ver()
            machine = machine or _get_machine_win32()

        # Try the 'ver' system command available on some
        # platforms
        if not (release and version):
            system, release, version = _syscmd_ver(system)
            # Normalize system to what win32_ver() normally returns
            # (_syscmd_ver() tends to return the vendor name as well)
            if system == 'Microsoft Windows':
                system = 'Windows'
            elif system == 'Microsoft' and release == 'Windows':
                # Under Windows Vista and Windows Server 2008,
                # Microsoft changed the output of the ver command. The
                # release is no longer printed.  This causes the
                # system and release to be misidentified.
                system = 'Windows'
                if '6.0' == version[:3]:
                    release = 'Vista'
                else:
                    release = ''

        # In case we still don't know anything useful, we'll try to
        # help ourselves
        if system in ('win32', 'win16'):
            if not version:
                if system == 'win32':
                    version = '32bit'
                else:
                    version = '16bit'
            system = 'Windows'

        elif system[:4] == 'java':
            release, vendor, vminfo, osinfo = java_ver()
            system = 'Java'
            version = ', '.join(vminfo)
            if not version:
                version = vendor

    # System specific extensions
    if system == 'OpenVMS':
        # OpenVMS seems to have release and version mixed up
        if not release or release == '0':
            release = version
            version = ''

    #  normalize name
    if system == 'Microsoft' and release == 'Windows':
        system = 'Windows'
        release = 'Vista'

    vals = system, node, release, version, machine
    # Replace 'unknown' values with the more portable ''
    _uname_cache = uname_result(*map(_unknown_as_blank, vals))
    return _uname_cache","def 1():

    """""" Returns the (true) processor name, e.g. 'amdk6'

        An empty string is returned if the value cannot be
        determined. Note that many platforms do not provide this
        information or simply return the same value as for machine(),
        e.g.  NetBSD does this.

    """"""
    return 0().processor","{""uname"": ""0""}"
113,113,"def python_implementation():

    """""" Returns a string identifying the Python implementation.

        Currently, the following implementations are identified:
          'CPython' (C implementation of Python),
          'IronPython' (.NET implementation of Python),
          'Jython' (Java implementation of Python),
          'PyPy' (Python implementation of Python).

    """"""
    return _sys_version()[0]","def _sys_version(sys_version=None):

    """""" Returns a parsed version of Python's sys.version as tuple
        (name, version, branch, revision, buildno, builddate, compiler)
        referring to the Python implementation name, version, branch,
        revision, build number, build date/time as string and the compiler
        identification string.

        Note that unlike the Python sys.version, the returned value
        for the Python version will always include the patchlevel (it
        defaults to '.0').

        The function returns empty strings for tuple entries that
        cannot be determined.

        sys_version may be given to parse an alternative version
        string, e.g. if the version was read from a different Python
        interpreter.

    """"""
    # Get the Python version
    if sys_version is None:
        sys_version = sys.version

    # Try the cache first
    result = _sys_version_cache.get(sys_version, None)
    if result is not None:
        return result

    # Parse it
    if 'IronPython' in sys_version:
        # IronPython
        name = 'IronPython'
        if sys_version.startswith('IronPython'):
            match = _ironpython_sys_version_parser.match(sys_version)
        else:
            match = _ironpython26_sys_version_parser.match(sys_version)

        if match is None:
            raise ValueError(
                'failed to parse IronPython sys.version: %s' %
                repr(sys_version))

        version, alt_version, compiler = match.groups()
        buildno = ''
        builddate = ''

    elif sys.platform.startswith('java'):
        # Jython
        name = 'Jython'
        match = _sys_version_parser.match(sys_version)
        if match is None:
            raise ValueError(
                'failed to parse Jython sys.version: %s' %
                repr(sys_version))
        version, buildno, builddate, buildtime, _ = match.groups()
        if builddate is None:
            builddate = ''
        compiler = sys.platform

    elif ""PyPy"" in sys_version:
        # PyPy
        name = ""PyPy""
        match = _pypy_sys_version_parser.match(sys_version)
        if match is None:
            raise ValueError(""failed to parse PyPy sys.version: %s"" %
                             repr(sys_version))
        version, buildno, builddate, buildtime = match.groups()
        compiler = """"

    else:
        # CPython
        match = _sys_version_parser.match(sys_version)
        if match is None:
            raise ValueError(
                'failed to parse CPython sys.version: %s' %
                repr(sys_version))
        version, buildno, builddate, buildtime, compiler = \
              match.groups()
        name = 'CPython'
        if builddate is None:
            builddate = ''
        elif buildtime:
            builddate = builddate + ' ' + buildtime

    if hasattr(sys, '_git'):
        _, branch, revision = sys._git
    elif hasattr(sys, '_mercurial'):
        _, branch, revision = sys._mercurial
    else:
        branch = ''
        revision = ''

    # Add the patchlevel version if missing
    l = version.split('.')
    if len(l) == 2:
        l.append('0')
        version = '.'.join(l)

    # Build and cache the result
    result = (name, version, branch, revision, buildno, builddate, compiler)
    _sys_version_cache[sys_version] = result
    return result","def parse_python_sys_version(sys_version=None):

    """""" Returns a parsed version of Python's sys.version as tuple
        (name, version, branch, revision, buildno, builddate, compiler)
        referring to the Python implementation name, version, branch,
        revision, build number, build date/time as string and the compiler
        identification string.

        Note that unlike the Python sys.version, the returned value
        for the Python version will always include the patchlevel (it
        defaults to '.0').

        The function returns empty strings for tuple entries that
        cannot be determined.

        sys_version may be given to parse an alternative version
        string, e.g. if the version was read from a different Python
        interpreter.

    """"""
    # Get the Python version
    if sys_version is None:
        sys_version = sys.version

    # Try the cache first
    result = parse_python_sys_version_cache.get(sys_version, None)
    if result is not None:
        return result

    # Parse it
    if 'IronPython' in sys_version:
        # IronPython
        name = 'IronPython'
        if sys_version.startswith('IronPython'):
            match = _ironpythonparse_python_sys_version_parser.match(sys_version)
        else:
            match = _ironpython26parse_python_sys_version_parser.match(sys_version)

        if match is None:
            raise ValueError(
                'failed to parse IronPython sys.version: %s' %
                repr(sys_version))

        version, alt_version, compiler = match.groups()
        buildno = ''
        builddate = ''

    elif sys.platform.startswith('java'):
        # Jython
        name = 'Jython'
        match = parse_python_sys_version_parser.match(sys_version)
        if match is None:
            raise ValueError(
                'failed to parse Jython sys.version: %s' %
                repr(sys_version))
        version, buildno, builddate, buildtime, _ = match.groups()
        if builddate is None:
            builddate = ''
        compiler = sys.platform

    elif ""PyPy"" in sys_version:
        # PyPy
        name = ""PyPy""
        match = _pypyparse_python_sys_version_parser.match(sys_version)
        if match is None:
            raise ValueError(""failed to parse PyPy sys.version: %s"" %
                             repr(sys_version))
        version, buildno, builddate, buildtime = match.groups()
        compiler = """"

    else:
        # CPython
        match = parse_python_sys_version_parser.match(sys_version)
        if match is None:
            raise ValueError(
                'failed to parse CPython sys.version: %s' %
                repr(sys_version))
        version, buildno, builddate, buildtime, compiler = \
              match.groups()
        name = 'CPython'
        if builddate is None:
            builddate = ''
        elif buildtime:
            builddate = builddate + ' ' + buildtime

    if hasattr(sys, '_git'):
        _, branch, revision = sys._git
    elif hasattr(sys, '_mercurial'):
        _, branch, revision = sys._mercurial
    else:
        branch = ''
        revision = ''

    # Add the patchlevel version if missing
    l = version.split('.')
    if len(l) == 2:
        l.append('0')
        version = '.'.join(l)

    # Build and cache the result
    result = (name, version, branch, revision, buildno, builddate, compiler)
    parse_python_sys_version_cache[sys_version] = result
    return result","def returns_string_identifying_python_implementation():

    """""" Returns a string identifying the Python implementation.

        Currently, the following implementations are identified:
          'CPython' (C implementation of Python),
          'IronPython' (.NET implementation of Python),
          'Jython' (Java implementation of Python),
          'PyPy' (Python implementation of Python).

    """"""
    return parse_python_sys_version()[0]","{""_sys_version"": ""parse_python_sys_version""}","def 0(sys_version=None):

    """""" Returns a parsed version of Python's sys.version as tuple
        (name, version, branch, revision, buildno, builddate, compiler)
        referring to the Python implementation name, version, branch,
        revision, build number, build date/time as string and the compiler
        identification string.

        Note that unlike the Python sys.version, the returned value
        for the Python version will always include the patchlevel (it
        defaults to '.0').

        The function returns empty strings for tuple entries that
        cannot be determined.

        sys_version may be given to parse an alternative version
        string, e.g. if the version was read from a different Python
        interpreter.

    """"""
    # Get the Python version
    if sys_version is None:
        sys_version = sys.version

    # Try the cache first
    result = _sys_version_cache.get(sys_version, None)
    if result is not None:
        return result

    # Parse it
    if 'IronPython' in sys_version:
        # IronPython
        name = 'IronPython'
        if sys_version.startswith('IronPython'):
            match = _ironpython_sys_version_parser.match(sys_version)
        else:
            match = _ironpython26_sys_version_parser.match(sys_version)

        if match is None:
            raise ValueError(
                'failed to parse IronPython sys.version: %s' %
                repr(sys_version))

        version, alt_version, compiler = match.groups()
        buildno = ''
        builddate = ''

    elif sys.platform.startswith('java'):
        # Jython
        name = 'Jython'
        match = _sys_version_parser.match(sys_version)
        if match is None:
            raise ValueError(
                'failed to parse Jython sys.version: %s' %
                repr(sys_version))
        version, buildno, builddate, buildtime, _ = match.groups()
        if builddate is None:
            builddate = ''
        compiler = sys.platform

    elif ""PyPy"" in sys_version:
        # PyPy
        name = ""PyPy""
        match = _pypy_sys_version_parser.match(sys_version)
        if match is None:
            raise ValueError(""failed to parse PyPy sys.version: %s"" %
                             repr(sys_version))
        version, buildno, builddate, buildtime = match.groups()
        compiler = """"

    else:
        # CPython
        match = _sys_version_parser.match(sys_version)
        if match is None:
            raise ValueError(
                'failed to parse CPython sys.version: %s' %
                repr(sys_version))
        version, buildno, builddate, buildtime, compiler = \
              match.groups()
        name = 'CPython'
        if builddate is None:
            builddate = ''
        elif buildtime:
            builddate = builddate + ' ' + buildtime

    if hasattr(sys, '_git'):
        _, branch, revision = sys._git
    elif hasattr(sys, '_mercurial'):
        _, branch, revision = sys._mercurial
    else:
        branch = ''
        revision = ''

    # Add the patchlevel version if missing
    l = version.split('.')
    if len(l) == 2:
        l.append('0')
        version = '.'.join(l)

    # Build and cache the result
    result = (name, version, branch, revision, buildno, builddate, compiler)
    _sys_version_cache[sys_version] = result
    return result","def 1():

    """""" Returns a string identifying the Python implementation.

        Currently, the following implementations are identified:
          'CPython' (C implementation of Python),
          'IronPython' (.NET implementation of Python),
          'Jython' (Java implementation of Python),
          'PyPy' (Python implementation of Python).

    """"""
    return 0()[0]","{""_sys_version"": ""0""}"
114,114,"def python_version():

    """""" Returns the Python version as string 'major.minor.patchlevel'

        Note that unlike the Python sys.version, the returned value
        will always include the patchlevel (it defaults to 0).

    """"""
    return _sys_version()[1]","def _sys_version(sys_version=None):

    """""" Returns a parsed version of Python's sys.version as tuple
        (name, version, branch, revision, buildno, builddate, compiler)
        referring to the Python implementation name, version, branch,
        revision, build number, build date/time as string and the compiler
        identification string.

        Note that unlike the Python sys.version, the returned value
        for the Python version will always include the patchlevel (it
        defaults to '.0').

        The function returns empty strings for tuple entries that
        cannot be determined.

        sys_version may be given to parse an alternative version
        string, e.g. if the version was read from a different Python
        interpreter.

    """"""
    # Get the Python version
    if sys_version is None:
        sys_version = sys.version

    # Try the cache first
    result = _sys_version_cache.get(sys_version, None)
    if result is not None:
        return result

    # Parse it
    if 'IronPython' in sys_version:
        # IronPython
        name = 'IronPython'
        if sys_version.startswith('IronPython'):
            match = _ironpython_sys_version_parser.match(sys_version)
        else:
            match = _ironpython26_sys_version_parser.match(sys_version)

        if match is None:
            raise ValueError(
                'failed to parse IronPython sys.version: %s' %
                repr(sys_version))

        version, alt_version, compiler = match.groups()
        buildno = ''
        builddate = ''

    elif sys.platform.startswith('java'):
        # Jython
        name = 'Jython'
        match = _sys_version_parser.match(sys_version)
        if match is None:
            raise ValueError(
                'failed to parse Jython sys.version: %s' %
                repr(sys_version))
        version, buildno, builddate, buildtime, _ = match.groups()
        if builddate is None:
            builddate = ''
        compiler = sys.platform

    elif ""PyPy"" in sys_version:
        # PyPy
        name = ""PyPy""
        match = _pypy_sys_version_parser.match(sys_version)
        if match is None:
            raise ValueError(""failed to parse PyPy sys.version: %s"" %
                             repr(sys_version))
        version, buildno, builddate, buildtime = match.groups()
        compiler = """"

    else:
        # CPython
        match = _sys_version_parser.match(sys_version)
        if match is None:
            raise ValueError(
                'failed to parse CPython sys.version: %s' %
                repr(sys_version))
        version, buildno, builddate, buildtime, compiler = \
              match.groups()
        name = 'CPython'
        if builddate is None:
            builddate = ''
        elif buildtime:
            builddate = builddate + ' ' + buildtime

    if hasattr(sys, '_git'):
        _, branch, revision = sys._git
    elif hasattr(sys, '_mercurial'):
        _, branch, revision = sys._mercurial
    else:
        branch = ''
        revision = ''

    # Add the patchlevel version if missing
    l = version.split('.')
    if len(l) == 2:
        l.append('0')
        version = '.'.join(l)

    # Build and cache the result
    result = (name, version, branch, revision, buildno, builddate, compiler)
    _sys_version_cache[sys_version] = result
    return result","def parse_python_sys_version(sys_version=None):

    """""" Returns a parsed version of Python's sys.version as tuple
        (name, version, branch, revision, buildno, builddate, compiler)
        referring to the Python implementation name, version, branch,
        revision, build number, build date/time as string and the compiler
        identification string.

        Note that unlike the Python sys.version, the returned value
        for the Python version will always include the patchlevel (it
        defaults to '.0').

        The function returns empty strings for tuple entries that
        cannot be determined.

        sys_version may be given to parse an alternative version
        string, e.g. if the version was read from a different Python
        interpreter.

    """"""
    # Get the Python version
    if sys_version is None:
        sys_version = sys.version

    # Try the cache first
    result = parse_python_sys_version_cache.get(sys_version, None)
    if result is not None:
        return result

    # Parse it
    if 'IronPython' in sys_version:
        # IronPython
        name = 'IronPython'
        if sys_version.startswith('IronPython'):
            match = _ironpythonparse_python_sys_version_parser.match(sys_version)
        else:
            match = _ironpython26parse_python_sys_version_parser.match(sys_version)

        if match is None:
            raise ValueError(
                'failed to parse IronPython sys.version: %s' %
                repr(sys_version))

        version, alt_version, compiler = match.groups()
        buildno = ''
        builddate = ''

    elif sys.platform.startswith('java'):
        # Jython
        name = 'Jython'
        match = parse_python_sys_version_parser.match(sys_version)
        if match is None:
            raise ValueError(
                'failed to parse Jython sys.version: %s' %
                repr(sys_version))
        version, buildno, builddate, buildtime, _ = match.groups()
        if builddate is None:
            builddate = ''
        compiler = sys.platform

    elif ""PyPy"" in sys_version:
        # PyPy
        name = ""PyPy""
        match = _pypyparse_python_sys_version_parser.match(sys_version)
        if match is None:
            raise ValueError(""failed to parse PyPy sys.version: %s"" %
                             repr(sys_version))
        version, buildno, builddate, buildtime = match.groups()
        compiler = """"

    else:
        # CPython
        match = parse_python_sys_version_parser.match(sys_version)
        if match is None:
            raise ValueError(
                'failed to parse CPython sys.version: %s' %
                repr(sys_version))
        version, buildno, builddate, buildtime, compiler = \
              match.groups()
        name = 'CPython'
        if builddate is None:
            builddate = ''
        elif buildtime:
            builddate = builddate + ' ' + buildtime

    if hasattr(sys, '_git'):
        _, branch, revision = sys._git
    elif hasattr(sys, '_mercurial'):
        _, branch, revision = sys._mercurial
    else:
        branch = ''
        revision = ''

    # Add the patchlevel version if missing
    l = version.split('.')
    if len(l) == 2:
        l.append('0')
        version = '.'.join(l)

    # Build and cache the result
    result = (name, version, branch, revision, buildno, builddate, compiler)
    parse_python_sys_version_cache[sys_version] = result
    return result","def returns_python_version_as_string():

    """""" Returns the Python version as string 'major.minor.patchlevel'

        Note that unlike the Python sys.version, the returned value
        will always include the patchlevel (it defaults to 0).

    """"""
    return parse_python_sys_version()[1]","{""_sys_version"": ""parse_python_sys_version""}","def 0(sys_version=None):

    """""" Returns a parsed version of Python's sys.version as tuple
        (name, version, branch, revision, buildno, builddate, compiler)
        referring to the Python implementation name, version, branch,
        revision, build number, build date/time as string and the compiler
        identification string.

        Note that unlike the Python sys.version, the returned value
        for the Python version will always include the patchlevel (it
        defaults to '.0').

        The function returns empty strings for tuple entries that
        cannot be determined.

        sys_version may be given to parse an alternative version
        string, e.g. if the version was read from a different Python
        interpreter.

    """"""
    # Get the Python version
    if sys_version is None:
        sys_version = sys.version

    # Try the cache first
    result = _sys_version_cache.get(sys_version, None)
    if result is not None:
        return result

    # Parse it
    if 'IronPython' in sys_version:
        # IronPython
        name = 'IronPython'
        if sys_version.startswith('IronPython'):
            match = _ironpython_sys_version_parser.match(sys_version)
        else:
            match = _ironpython26_sys_version_parser.match(sys_version)

        if match is None:
            raise ValueError(
                'failed to parse IronPython sys.version: %s' %
                repr(sys_version))

        version, alt_version, compiler = match.groups()
        buildno = ''
        builddate = ''

    elif sys.platform.startswith('java'):
        # Jython
        name = 'Jython'
        match = _sys_version_parser.match(sys_version)
        if match is None:
            raise ValueError(
                'failed to parse Jython sys.version: %s' %
                repr(sys_version))
        version, buildno, builddate, buildtime, _ = match.groups()
        if builddate is None:
            builddate = ''
        compiler = sys.platform

    elif ""PyPy"" in sys_version:
        # PyPy
        name = ""PyPy""
        match = _pypy_sys_version_parser.match(sys_version)
        if match is None:
            raise ValueError(""failed to parse PyPy sys.version: %s"" %
                             repr(sys_version))
        version, buildno, builddate, buildtime = match.groups()
        compiler = """"

    else:
        # CPython
        match = _sys_version_parser.match(sys_version)
        if match is None:
            raise ValueError(
                'failed to parse CPython sys.version: %s' %
                repr(sys_version))
        version, buildno, builddate, buildtime, compiler = \
              match.groups()
        name = 'CPython'
        if builddate is None:
            builddate = ''
        elif buildtime:
            builddate = builddate + ' ' + buildtime

    if hasattr(sys, '_git'):
        _, branch, revision = sys._git
    elif hasattr(sys, '_mercurial'):
        _, branch, revision = sys._mercurial
    else:
        branch = ''
        revision = ''

    # Add the patchlevel version if missing
    l = version.split('.')
    if len(l) == 2:
        l.append('0')
        version = '.'.join(l)

    # Build and cache the result
    result = (name, version, branch, revision, buildno, builddate, compiler)
    _sys_version_cache[sys_version] = result
    return result","def 1():

    """""" Returns the Python version as string 'major.minor.patchlevel'

        Note that unlike the Python sys.version, the returned value
        will always include the patchlevel (it defaults to 0).

    """"""
    return 0()[1]","{""_sys_version"": ""0""}"
115,115,"def python_version_tuple():

    """""" Returns the Python version as tuple (major, minor, patchlevel)
        of strings.

        Note that unlike the Python sys.version, the returned value
        will always include the patchlevel (it defaults to 0).

    """"""
    return tuple(_sys_version()[1].split('.'))","def _sys_version(sys_version=None):

    """""" Returns a parsed version of Python's sys.version as tuple
        (name, version, branch, revision, buildno, builddate, compiler)
        referring to the Python implementation name, version, branch,
        revision, build number, build date/time as string and the compiler
        identification string.

        Note that unlike the Python sys.version, the returned value
        for the Python version will always include the patchlevel (it
        defaults to '.0').

        The function returns empty strings for tuple entries that
        cannot be determined.

        sys_version may be given to parse an alternative version
        string, e.g. if the version was read from a different Python
        interpreter.

    """"""
    # Get the Python version
    if sys_version is None:
        sys_version = sys.version

    # Try the cache first
    result = _sys_version_cache.get(sys_version, None)
    if result is not None:
        return result

    # Parse it
    if 'IronPython' in sys_version:
        # IronPython
        name = 'IronPython'
        if sys_version.startswith('IronPython'):
            match = _ironpython_sys_version_parser.match(sys_version)
        else:
            match = _ironpython26_sys_version_parser.match(sys_version)

        if match is None:
            raise ValueError(
                'failed to parse IronPython sys.version: %s' %
                repr(sys_version))

        version, alt_version, compiler = match.groups()
        buildno = ''
        builddate = ''

    elif sys.platform.startswith('java'):
        # Jython
        name = 'Jython'
        match = _sys_version_parser.match(sys_version)
        if match is None:
            raise ValueError(
                'failed to parse Jython sys.version: %s' %
                repr(sys_version))
        version, buildno, builddate, buildtime, _ = match.groups()
        if builddate is None:
            builddate = ''
        compiler = sys.platform

    elif ""PyPy"" in sys_version:
        # PyPy
        name = ""PyPy""
        match = _pypy_sys_version_parser.match(sys_version)
        if match is None:
            raise ValueError(""failed to parse PyPy sys.version: %s"" %
                             repr(sys_version))
        version, buildno, builddate, buildtime = match.groups()
        compiler = """"

    else:
        # CPython
        match = _sys_version_parser.match(sys_version)
        if match is None:
            raise ValueError(
                'failed to parse CPython sys.version: %s' %
                repr(sys_version))
        version, buildno, builddate, buildtime, compiler = \
              match.groups()
        name = 'CPython'
        if builddate is None:
            builddate = ''
        elif buildtime:
            builddate = builddate + ' ' + buildtime

    if hasattr(sys, '_git'):
        _, branch, revision = sys._git
    elif hasattr(sys, '_mercurial'):
        _, branch, revision = sys._mercurial
    else:
        branch = ''
        revision = ''

    # Add the patchlevel version if missing
    l = version.split('.')
    if len(l) == 2:
        l.append('0')
        version = '.'.join(l)

    # Build and cache the result
    result = (name, version, branch, revision, buildno, builddate, compiler)
    _sys_version_cache[sys_version] = result
    return result","def parse_python_sys_version(sys_version=None):

    """""" Returns a parsed version of Python's sys.version as tuple
        (name, version, branch, revision, buildno, builddate, compiler)
        referring to the Python implementation name, version, branch,
        revision, build number, build date/time as string and the compiler
        identification string.

        Note that unlike the Python sys.version, the returned value
        for the Python version will always include the patchlevel (it
        defaults to '.0').

        The function returns empty strings for tuple entries that
        cannot be determined.

        sys_version may be given to parse an alternative version
        string, e.g. if the version was read from a different Python
        interpreter.

    """"""
    # Get the Python version
    if sys_version is None:
        sys_version = sys.version

    # Try the cache first
    result = parse_python_sys_version_cache.get(sys_version, None)
    if result is not None:
        return result

    # Parse it
    if 'IronPython' in sys_version:
        # IronPython
        name = 'IronPython'
        if sys_version.startswith('IronPython'):
            match = _ironpythonparse_python_sys_version_parser.match(sys_version)
        else:
            match = _ironpython26parse_python_sys_version_parser.match(sys_version)

        if match is None:
            raise ValueError(
                'failed to parse IronPython sys.version: %s' %
                repr(sys_version))

        version, alt_version, compiler = match.groups()
        buildno = ''
        builddate = ''

    elif sys.platform.startswith('java'):
        # Jython
        name = 'Jython'
        match = parse_python_sys_version_parser.match(sys_version)
        if match is None:
            raise ValueError(
                'failed to parse Jython sys.version: %s' %
                repr(sys_version))
        version, buildno, builddate, buildtime, _ = match.groups()
        if builddate is None:
            builddate = ''
        compiler = sys.platform

    elif ""PyPy"" in sys_version:
        # PyPy
        name = ""PyPy""
        match = _pypyparse_python_sys_version_parser.match(sys_version)
        if match is None:
            raise ValueError(""failed to parse PyPy sys.version: %s"" %
                             repr(sys_version))
        version, buildno, builddate, buildtime = match.groups()
        compiler = """"

    else:
        # CPython
        match = parse_python_sys_version_parser.match(sys_version)
        if match is None:
            raise ValueError(
                'failed to parse CPython sys.version: %s' %
                repr(sys_version))
        version, buildno, builddate, buildtime, compiler = \
              match.groups()
        name = 'CPython'
        if builddate is None:
            builddate = ''
        elif buildtime:
            builddate = builddate + ' ' + buildtime

    if hasattr(sys, '_git'):
        _, branch, revision = sys._git
    elif hasattr(sys, '_mercurial'):
        _, branch, revision = sys._mercurial
    else:
        branch = ''
        revision = ''

    # Add the patchlevel version if missing
    l = version.split('.')
    if len(l) == 2:
        l.append('0')
        version = '.'.join(l)

    # Build and cache the result
    result = (name, version, branch, revision, buildno, builddate, compiler)
    parse_python_sys_version_cache[sys_version] = result
    return result","def retrieve_python_version_as_tuple_of_strings_with_patchlevel():

    """""" Returns the Python version as tuple (major, minor, patchlevel)
        of strings.

        Note that unlike the Python sys.version, the returned value
        will always include the patchlevel (it defaults to 0).

    """"""
    return tuple(parse_python_sys_version()[1].split('.'))","{""_sys_version"": ""parse_python_sys_version""}","def 0(sys_version=None):

    """""" Returns a parsed version of Python's sys.version as tuple
        (name, version, branch, revision, buildno, builddate, compiler)
        referring to the Python implementation name, version, branch,
        revision, build number, build date/time as string and the compiler
        identification string.

        Note that unlike the Python sys.version, the returned value
        for the Python version will always include the patchlevel (it
        defaults to '.0').

        The function returns empty strings for tuple entries that
        cannot be determined.

        sys_version may be given to parse an alternative version
        string, e.g. if the version was read from a different Python
        interpreter.

    """"""
    # Get the Python version
    if sys_version is None:
        sys_version = sys.version

    # Try the cache first
    result = _sys_version_cache.get(sys_version, None)
    if result is not None:
        return result

    # Parse it
    if 'IronPython' in sys_version:
        # IronPython
        name = 'IronPython'
        if sys_version.startswith('IronPython'):
            match = _ironpython_sys_version_parser.match(sys_version)
        else:
            match = _ironpython26_sys_version_parser.match(sys_version)

        if match is None:
            raise ValueError(
                'failed to parse IronPython sys.version: %s' %
                repr(sys_version))

        version, alt_version, compiler = match.groups()
        buildno = ''
        builddate = ''

    elif sys.platform.startswith('java'):
        # Jython
        name = 'Jython'
        match = _sys_version_parser.match(sys_version)
        if match is None:
            raise ValueError(
                'failed to parse Jython sys.version: %s' %
                repr(sys_version))
        version, buildno, builddate, buildtime, _ = match.groups()
        if builddate is None:
            builddate = ''
        compiler = sys.platform

    elif ""PyPy"" in sys_version:
        # PyPy
        name = ""PyPy""
        match = _pypy_sys_version_parser.match(sys_version)
        if match is None:
            raise ValueError(""failed to parse PyPy sys.version: %s"" %
                             repr(sys_version))
        version, buildno, builddate, buildtime = match.groups()
        compiler = """"

    else:
        # CPython
        match = _sys_version_parser.match(sys_version)
        if match is None:
            raise ValueError(
                'failed to parse CPython sys.version: %s' %
                repr(sys_version))
        version, buildno, builddate, buildtime, compiler = \
              match.groups()
        name = 'CPython'
        if builddate is None:
            builddate = ''
        elif buildtime:
            builddate = builddate + ' ' + buildtime

    if hasattr(sys, '_git'):
        _, branch, revision = sys._git
    elif hasattr(sys, '_mercurial'):
        _, branch, revision = sys._mercurial
    else:
        branch = ''
        revision = ''

    # Add the patchlevel version if missing
    l = version.split('.')
    if len(l) == 2:
        l.append('0')
        version = '.'.join(l)

    # Build and cache the result
    result = (name, version, branch, revision, buildno, builddate, compiler)
    _sys_version_cache[sys_version] = result
    return result","def 1():

    """""" Returns the Python version as tuple (major, minor, patchlevel)
        of strings.

        Note that unlike the Python sys.version, the returned value
        will always include the patchlevel (it defaults to 0).

    """"""
    return tuple(0()[1].split('.'))","{""_sys_version"": ""0""}"
116,116,"def python_branch():

    """""" Returns a string identifying the Python implementation
        branch.

        For CPython this is the SCM branch from which the
        Python binary was built.

        If not available, an empty string is returned.

    """"""

    return _sys_version()[2]","def _sys_version(sys_version=None):

    """""" Returns a parsed version of Python's sys.version as tuple
        (name, version, branch, revision, buildno, builddate, compiler)
        referring to the Python implementation name, version, branch,
        revision, build number, build date/time as string and the compiler
        identification string.

        Note that unlike the Python sys.version, the returned value
        for the Python version will always include the patchlevel (it
        defaults to '.0').

        The function returns empty strings for tuple entries that
        cannot be determined.

        sys_version may be given to parse an alternative version
        string, e.g. if the version was read from a different Python
        interpreter.

    """"""
    # Get the Python version
    if sys_version is None:
        sys_version = sys.version

    # Try the cache first
    result = _sys_version_cache.get(sys_version, None)
    if result is not None:
        return result

    # Parse it
    if 'IronPython' in sys_version:
        # IronPython
        name = 'IronPython'
        if sys_version.startswith('IronPython'):
            match = _ironpython_sys_version_parser.match(sys_version)
        else:
            match = _ironpython26_sys_version_parser.match(sys_version)

        if match is None:
            raise ValueError(
                'failed to parse IronPython sys.version: %s' %
                repr(sys_version))

        version, alt_version, compiler = match.groups()
        buildno = ''
        builddate = ''

    elif sys.platform.startswith('java'):
        # Jython
        name = 'Jython'
        match = _sys_version_parser.match(sys_version)
        if match is None:
            raise ValueError(
                'failed to parse Jython sys.version: %s' %
                repr(sys_version))
        version, buildno, builddate, buildtime, _ = match.groups()
        if builddate is None:
            builddate = ''
        compiler = sys.platform

    elif ""PyPy"" in sys_version:
        # PyPy
        name = ""PyPy""
        match = _pypy_sys_version_parser.match(sys_version)
        if match is None:
            raise ValueError(""failed to parse PyPy sys.version: %s"" %
                             repr(sys_version))
        version, buildno, builddate, buildtime = match.groups()
        compiler = """"

    else:
        # CPython
        match = _sys_version_parser.match(sys_version)
        if match is None:
            raise ValueError(
                'failed to parse CPython sys.version: %s' %
                repr(sys_version))
        version, buildno, builddate, buildtime, compiler = \
              match.groups()
        name = 'CPython'
        if builddate is None:
            builddate = ''
        elif buildtime:
            builddate = builddate + ' ' + buildtime

    if hasattr(sys, '_git'):
        _, branch, revision = sys._git
    elif hasattr(sys, '_mercurial'):
        _, branch, revision = sys._mercurial
    else:
        branch = ''
        revision = ''

    # Add the patchlevel version if missing
    l = version.split('.')
    if len(l) == 2:
        l.append('0')
        version = '.'.join(l)

    # Build and cache the result
    result = (name, version, branch, revision, buildno, builddate, compiler)
    _sys_version_cache[sys_version] = result
    return result","def parse_python_sys_version(sys_version=None):

    """""" Returns a parsed version of Python's sys.version as tuple
        (name, version, branch, revision, buildno, builddate, compiler)
        referring to the Python implementation name, version, branch,
        revision, build number, build date/time as string and the compiler
        identification string.

        Note that unlike the Python sys.version, the returned value
        for the Python version will always include the patchlevel (it
        defaults to '.0').

        The function returns empty strings for tuple entries that
        cannot be determined.

        sys_version may be given to parse an alternative version
        string, e.g. if the version was read from a different Python
        interpreter.

    """"""
    # Get the Python version
    if sys_version is None:
        sys_version = sys.version

    # Try the cache first
    result = parse_python_sys_version_cache.get(sys_version, None)
    if result is not None:
        return result

    # Parse it
    if 'IronPython' in sys_version:
        # IronPython
        name = 'IronPython'
        if sys_version.startswith('IronPython'):
            match = _ironpythonparse_python_sys_version_parser.match(sys_version)
        else:
            match = _ironpython26parse_python_sys_version_parser.match(sys_version)

        if match is None:
            raise ValueError(
                'failed to parse IronPython sys.version: %s' %
                repr(sys_version))

        version, alt_version, compiler = match.groups()
        buildno = ''
        builddate = ''

    elif sys.platform.startswith('java'):
        # Jython
        name = 'Jython'
        match = parse_python_sys_version_parser.match(sys_version)
        if match is None:
            raise ValueError(
                'failed to parse Jython sys.version: %s' %
                repr(sys_version))
        version, buildno, builddate, buildtime, _ = match.groups()
        if builddate is None:
            builddate = ''
        compiler = sys.platform

    elif ""PyPy"" in sys_version:
        # PyPy
        name = ""PyPy""
        match = _pypyparse_python_sys_version_parser.match(sys_version)
        if match is None:
            raise ValueError(""failed to parse PyPy sys.version: %s"" %
                             repr(sys_version))
        version, buildno, builddate, buildtime = match.groups()
        compiler = """"

    else:
        # CPython
        match = parse_python_sys_version_parser.match(sys_version)
        if match is None:
            raise ValueError(
                'failed to parse CPython sys.version: %s' %
                repr(sys_version))
        version, buildno, builddate, buildtime, compiler = \
              match.groups()
        name = 'CPython'
        if builddate is None:
            builddate = ''
        elif buildtime:
            builddate = builddate + ' ' + buildtime

    if hasattr(sys, '_git'):
        _, branch, revision = sys._git
    elif hasattr(sys, '_mercurial'):
        _, branch, revision = sys._mercurial
    else:
        branch = ''
        revision = ''

    # Add the patchlevel version if missing
    l = version.split('.')
    if len(l) == 2:
        l.append('0')
        version = '.'.join(l)

    # Build and cache the result
    result = (name, version, branch, revision, buildno, builddate, compiler)
    parse_python_sys_version_cache[sys_version] = result
    return result","def retrieve_python_implementation_branch_string():

    """""" Returns a string identifying the Python implementation
        branch.

        For CPython this is the SCM branch from which the
        Python binary was built.

        If not available, an empty string is returned.

    """"""

    return parse_python_sys_version()[2]","{""_sys_version"": ""parse_python_sys_version""}","def 0(sys_version=None):

    """""" Returns a parsed version of Python's sys.version as tuple
        (name, version, branch, revision, buildno, builddate, compiler)
        referring to the Python implementation name, version, branch,
        revision, build number, build date/time as string and the compiler
        identification string.

        Note that unlike the Python sys.version, the returned value
        for the Python version will always include the patchlevel (it
        defaults to '.0').

        The function returns empty strings for tuple entries that
        cannot be determined.

        sys_version may be given to parse an alternative version
        string, e.g. if the version was read from a different Python
        interpreter.

    """"""
    # Get the Python version
    if sys_version is None:
        sys_version = sys.version

    # Try the cache first
    result = _sys_version_cache.get(sys_version, None)
    if result is not None:
        return result

    # Parse it
    if 'IronPython' in sys_version:
        # IronPython
        name = 'IronPython'
        if sys_version.startswith('IronPython'):
            match = _ironpython_sys_version_parser.match(sys_version)
        else:
            match = _ironpython26_sys_version_parser.match(sys_version)

        if match is None:
            raise ValueError(
                'failed to parse IronPython sys.version: %s' %
                repr(sys_version))

        version, alt_version, compiler = match.groups()
        buildno = ''
        builddate = ''

    elif sys.platform.startswith('java'):
        # Jython
        name = 'Jython'
        match = _sys_version_parser.match(sys_version)
        if match is None:
            raise ValueError(
                'failed to parse Jython sys.version: %s' %
                repr(sys_version))
        version, buildno, builddate, buildtime, _ = match.groups()
        if builddate is None:
            builddate = ''
        compiler = sys.platform

    elif ""PyPy"" in sys_version:
        # PyPy
        name = ""PyPy""
        match = _pypy_sys_version_parser.match(sys_version)
        if match is None:
            raise ValueError(""failed to parse PyPy sys.version: %s"" %
                             repr(sys_version))
        version, buildno, builddate, buildtime = match.groups()
        compiler = """"

    else:
        # CPython
        match = _sys_version_parser.match(sys_version)
        if match is None:
            raise ValueError(
                'failed to parse CPython sys.version: %s' %
                repr(sys_version))
        version, buildno, builddate, buildtime, compiler = \
              match.groups()
        name = 'CPython'
        if builddate is None:
            builddate = ''
        elif buildtime:
            builddate = builddate + ' ' + buildtime

    if hasattr(sys, '_git'):
        _, branch, revision = sys._git
    elif hasattr(sys, '_mercurial'):
        _, branch, revision = sys._mercurial
    else:
        branch = ''
        revision = ''

    # Add the patchlevel version if missing
    l = version.split('.')
    if len(l) == 2:
        l.append('0')
        version = '.'.join(l)

    # Build and cache the result
    result = (name, version, branch, revision, buildno, builddate, compiler)
    _sys_version_cache[sys_version] = result
    return result","def 1():

    """""" Returns a string identifying the Python implementation
        branch.

        For CPython this is the SCM branch from which the
        Python binary was built.

        If not available, an empty string is returned.

    """"""

    return 0()[2]","{""_sys_version"": ""0""}"
117,117,"def python_revision():

    """""" Returns a string identifying the Python implementation
        revision.

        For CPython this is the SCM revision from which the
        Python binary was built.

        If not available, an empty string is returned.

    """"""
    return _sys_version()[3]","def _sys_version(sys_version=None):

    """""" Returns a parsed version of Python's sys.version as tuple
        (name, version, branch, revision, buildno, builddate, compiler)
        referring to the Python implementation name, version, branch,
        revision, build number, build date/time as string and the compiler
        identification string.

        Note that unlike the Python sys.version, the returned value
        for the Python version will always include the patchlevel (it
        defaults to '.0').

        The function returns empty strings for tuple entries that
        cannot be determined.

        sys_version may be given to parse an alternative version
        string, e.g. if the version was read from a different Python
        interpreter.

    """"""
    # Get the Python version
    if sys_version is None:
        sys_version = sys.version

    # Try the cache first
    result = _sys_version_cache.get(sys_version, None)
    if result is not None:
        return result

    # Parse it
    if 'IronPython' in sys_version:
        # IronPython
        name = 'IronPython'
        if sys_version.startswith('IronPython'):
            match = _ironpython_sys_version_parser.match(sys_version)
        else:
            match = _ironpython26_sys_version_parser.match(sys_version)

        if match is None:
            raise ValueError(
                'failed to parse IronPython sys.version: %s' %
                repr(sys_version))

        version, alt_version, compiler = match.groups()
        buildno = ''
        builddate = ''

    elif sys.platform.startswith('java'):
        # Jython
        name = 'Jython'
        match = _sys_version_parser.match(sys_version)
        if match is None:
            raise ValueError(
                'failed to parse Jython sys.version: %s' %
                repr(sys_version))
        version, buildno, builddate, buildtime, _ = match.groups()
        if builddate is None:
            builddate = ''
        compiler = sys.platform

    elif ""PyPy"" in sys_version:
        # PyPy
        name = ""PyPy""
        match = _pypy_sys_version_parser.match(sys_version)
        if match is None:
            raise ValueError(""failed to parse PyPy sys.version: %s"" %
                             repr(sys_version))
        version, buildno, builddate, buildtime = match.groups()
        compiler = """"

    else:
        # CPython
        match = _sys_version_parser.match(sys_version)
        if match is None:
            raise ValueError(
                'failed to parse CPython sys.version: %s' %
                repr(sys_version))
        version, buildno, builddate, buildtime, compiler = \
              match.groups()
        name = 'CPython'
        if builddate is None:
            builddate = ''
        elif buildtime:
            builddate = builddate + ' ' + buildtime

    if hasattr(sys, '_git'):
        _, branch, revision = sys._git
    elif hasattr(sys, '_mercurial'):
        _, branch, revision = sys._mercurial
    else:
        branch = ''
        revision = ''

    # Add the patchlevel version if missing
    l = version.split('.')
    if len(l) == 2:
        l.append('0')
        version = '.'.join(l)

    # Build and cache the result
    result = (name, version, branch, revision, buildno, builddate, compiler)
    _sys_version_cache[sys_version] = result
    return result","def parse_python_sys_version(sys_version=None):

    """""" Returns a parsed version of Python's sys.version as tuple
        (name, version, branch, revision, buildno, builddate, compiler)
        referring to the Python implementation name, version, branch,
        revision, build number, build date/time as string and the compiler
        identification string.

        Note that unlike the Python sys.version, the returned value
        for the Python version will always include the patchlevel (it
        defaults to '.0').

        The function returns empty strings for tuple entries that
        cannot be determined.

        sys_version may be given to parse an alternative version
        string, e.g. if the version was read from a different Python
        interpreter.

    """"""
    # Get the Python version
    if sys_version is None:
        sys_version = sys.version

    # Try the cache first
    result = parse_python_sys_version_cache.get(sys_version, None)
    if result is not None:
        return result

    # Parse it
    if 'IronPython' in sys_version:
        # IronPython
        name = 'IronPython'
        if sys_version.startswith('IronPython'):
            match = _ironpythonparse_python_sys_version_parser.match(sys_version)
        else:
            match = _ironpython26parse_python_sys_version_parser.match(sys_version)

        if match is None:
            raise ValueError(
                'failed to parse IronPython sys.version: %s' %
                repr(sys_version))

        version, alt_version, compiler = match.groups()
        buildno = ''
        builddate = ''

    elif sys.platform.startswith('java'):
        # Jython
        name = 'Jython'
        match = parse_python_sys_version_parser.match(sys_version)
        if match is None:
            raise ValueError(
                'failed to parse Jython sys.version: %s' %
                repr(sys_version))
        version, buildno, builddate, buildtime, _ = match.groups()
        if builddate is None:
            builddate = ''
        compiler = sys.platform

    elif ""PyPy"" in sys_version:
        # PyPy
        name = ""PyPy""
        match = _pypyparse_python_sys_version_parser.match(sys_version)
        if match is None:
            raise ValueError(""failed to parse PyPy sys.version: %s"" %
                             repr(sys_version))
        version, buildno, builddate, buildtime = match.groups()
        compiler = """"

    else:
        # CPython
        match = parse_python_sys_version_parser.match(sys_version)
        if match is None:
            raise ValueError(
                'failed to parse CPython sys.version: %s' %
                repr(sys_version))
        version, buildno, builddate, buildtime, compiler = \
              match.groups()
        name = 'CPython'
        if builddate is None:
            builddate = ''
        elif buildtime:
            builddate = builddate + ' ' + buildtime

    if hasattr(sys, '_git'):
        _, branch, revision = sys._git
    elif hasattr(sys, '_mercurial'):
        _, branch, revision = sys._mercurial
    else:
        branch = ''
        revision = ''

    # Add the patchlevel version if missing
    l = version.split('.')
    if len(l) == 2:
        l.append('0')
        version = '.'.join(l)

    # Build and cache the result
    result = (name, version, branch, revision, buildno, builddate, compiler)
    parse_python_sys_version_cache[sys_version] = result
    return result","def retrieve_python_implementation_revision_string():

    """""" Returns a string identifying the Python implementation
        revision.

        For CPython this is the SCM revision from which the
        Python binary was built.

        If not available, an empty string is returned.

    """"""
    return parse_python_sys_version()[3]","{""_sys_version"": ""parse_python_sys_version""}","def 0(sys_version=None):

    """""" Returns a parsed version of Python's sys.version as tuple
        (name, version, branch, revision, buildno, builddate, compiler)
        referring to the Python implementation name, version, branch,
        revision, build number, build date/time as string and the compiler
        identification string.

        Note that unlike the Python sys.version, the returned value
        for the Python version will always include the patchlevel (it
        defaults to '.0').

        The function returns empty strings for tuple entries that
        cannot be determined.

        sys_version may be given to parse an alternative version
        string, e.g. if the version was read from a different Python
        interpreter.

    """"""
    # Get the Python version
    if sys_version is None:
        sys_version = sys.version

    # Try the cache first
    result = _sys_version_cache.get(sys_version, None)
    if result is not None:
        return result

    # Parse it
    if 'IronPython' in sys_version:
        # IronPython
        name = 'IronPython'
        if sys_version.startswith('IronPython'):
            match = _ironpython_sys_version_parser.match(sys_version)
        else:
            match = _ironpython26_sys_version_parser.match(sys_version)

        if match is None:
            raise ValueError(
                'failed to parse IronPython sys.version: %s' %
                repr(sys_version))

        version, alt_version, compiler = match.groups()
        buildno = ''
        builddate = ''

    elif sys.platform.startswith('java'):
        # Jython
        name = 'Jython'
        match = _sys_version_parser.match(sys_version)
        if match is None:
            raise ValueError(
                'failed to parse Jython sys.version: %s' %
                repr(sys_version))
        version, buildno, builddate, buildtime, _ = match.groups()
        if builddate is None:
            builddate = ''
        compiler = sys.platform

    elif ""PyPy"" in sys_version:
        # PyPy
        name = ""PyPy""
        match = _pypy_sys_version_parser.match(sys_version)
        if match is None:
            raise ValueError(""failed to parse PyPy sys.version: %s"" %
                             repr(sys_version))
        version, buildno, builddate, buildtime = match.groups()
        compiler = """"

    else:
        # CPython
        match = _sys_version_parser.match(sys_version)
        if match is None:
            raise ValueError(
                'failed to parse CPython sys.version: %s' %
                repr(sys_version))
        version, buildno, builddate, buildtime, compiler = \
              match.groups()
        name = 'CPython'
        if builddate is None:
            builddate = ''
        elif buildtime:
            builddate = builddate + ' ' + buildtime

    if hasattr(sys, '_git'):
        _, branch, revision = sys._git
    elif hasattr(sys, '_mercurial'):
        _, branch, revision = sys._mercurial
    else:
        branch = ''
        revision = ''

    # Add the patchlevel version if missing
    l = version.split('.')
    if len(l) == 2:
        l.append('0')
        version = '.'.join(l)

    # Build and cache the result
    result = (name, version, branch, revision, buildno, builddate, compiler)
    _sys_version_cache[sys_version] = result
    return result","def 1():

    """""" Returns a string identifying the Python implementation
        revision.

        For CPython this is the SCM revision from which the
        Python binary was built.

        If not available, an empty string is returned.

    """"""
    return 0()[3]","{""_sys_version"": ""0""}"
118,118,"def python_build():

    """""" Returns a tuple (buildno, builddate) stating the Python
        build number and date as strings.

    """"""
    return _sys_version()[4:6]","def _sys_version(sys_version=None):

    """""" Returns a parsed version of Python's sys.version as tuple
        (name, version, branch, revision, buildno, builddate, compiler)
        referring to the Python implementation name, version, branch,
        revision, build number, build date/time as string and the compiler
        identification string.

        Note that unlike the Python sys.version, the returned value
        for the Python version will always include the patchlevel (it
        defaults to '.0').

        The function returns empty strings for tuple entries that
        cannot be determined.

        sys_version may be given to parse an alternative version
        string, e.g. if the version was read from a different Python
        interpreter.

    """"""
    # Get the Python version
    if sys_version is None:
        sys_version = sys.version

    # Try the cache first
    result = _sys_version_cache.get(sys_version, None)
    if result is not None:
        return result

    # Parse it
    if 'IronPython' in sys_version:
        # IronPython
        name = 'IronPython'
        if sys_version.startswith('IronPython'):
            match = _ironpython_sys_version_parser.match(sys_version)
        else:
            match = _ironpython26_sys_version_parser.match(sys_version)

        if match is None:
            raise ValueError(
                'failed to parse IronPython sys.version: %s' %
                repr(sys_version))

        version, alt_version, compiler = match.groups()
        buildno = ''
        builddate = ''

    elif sys.platform.startswith('java'):
        # Jython
        name = 'Jython'
        match = _sys_version_parser.match(sys_version)
        if match is None:
            raise ValueError(
                'failed to parse Jython sys.version: %s' %
                repr(sys_version))
        version, buildno, builddate, buildtime, _ = match.groups()
        if builddate is None:
            builddate = ''
        compiler = sys.platform

    elif ""PyPy"" in sys_version:
        # PyPy
        name = ""PyPy""
        match = _pypy_sys_version_parser.match(sys_version)
        if match is None:
            raise ValueError(""failed to parse PyPy sys.version: %s"" %
                             repr(sys_version))
        version, buildno, builddate, buildtime = match.groups()
        compiler = """"

    else:
        # CPython
        match = _sys_version_parser.match(sys_version)
        if match is None:
            raise ValueError(
                'failed to parse CPython sys.version: %s' %
                repr(sys_version))
        version, buildno, builddate, buildtime, compiler = \
              match.groups()
        name = 'CPython'
        if builddate is None:
            builddate = ''
        elif buildtime:
            builddate = builddate + ' ' + buildtime

    if hasattr(sys, '_git'):
        _, branch, revision = sys._git
    elif hasattr(sys, '_mercurial'):
        _, branch, revision = sys._mercurial
    else:
        branch = ''
        revision = ''

    # Add the patchlevel version if missing
    l = version.split('.')
    if len(l) == 2:
        l.append('0')
        version = '.'.join(l)

    # Build and cache the result
    result = (name, version, branch, revision, buildno, builddate, compiler)
    _sys_version_cache[sys_version] = result
    return result","def parse_python_sys_version(sys_version=None):

    """""" Returns a parsed version of Python's sys.version as tuple
        (name, version, branch, revision, buildno, builddate, compiler)
        referring to the Python implementation name, version, branch,
        revision, build number, build date/time as string and the compiler
        identification string.

        Note that unlike the Python sys.version, the returned value
        for the Python version will always include the patchlevel (it
        defaults to '.0').

        The function returns empty strings for tuple entries that
        cannot be determined.

        sys_version may be given to parse an alternative version
        string, e.g. if the version was read from a different Python
        interpreter.

    """"""
    # Get the Python version
    if sys_version is None:
        sys_version = sys.version

    # Try the cache first
    result = parse_python_sys_version_cache.get(sys_version, None)
    if result is not None:
        return result

    # Parse it
    if 'IronPython' in sys_version:
        # IronPython
        name = 'IronPython'
        if sys_version.startswith('IronPython'):
            match = _ironpythonparse_python_sys_version_parser.match(sys_version)
        else:
            match = _ironpython26parse_python_sys_version_parser.match(sys_version)

        if match is None:
            raise ValueError(
                'failed to parse IronPython sys.version: %s' %
                repr(sys_version))

        version, alt_version, compiler = match.groups()
        buildno = ''
        builddate = ''

    elif sys.platform.startswith('java'):
        # Jython
        name = 'Jython'
        match = parse_python_sys_version_parser.match(sys_version)
        if match is None:
            raise ValueError(
                'failed to parse Jython sys.version: %s' %
                repr(sys_version))
        version, buildno, builddate, buildtime, _ = match.groups()
        if builddate is None:
            builddate = ''
        compiler = sys.platform

    elif ""PyPy"" in sys_version:
        # PyPy
        name = ""PyPy""
        match = _pypyparse_python_sys_version_parser.match(sys_version)
        if match is None:
            raise ValueError(""failed to parse PyPy sys.version: %s"" %
                             repr(sys_version))
        version, buildno, builddate, buildtime = match.groups()
        compiler = """"

    else:
        # CPython
        match = parse_python_sys_version_parser.match(sys_version)
        if match is None:
            raise ValueError(
                'failed to parse CPython sys.version: %s' %
                repr(sys_version))
        version, buildno, builddate, buildtime, compiler = \
              match.groups()
        name = 'CPython'
        if builddate is None:
            builddate = ''
        elif buildtime:
            builddate = builddate + ' ' + buildtime

    if hasattr(sys, '_git'):
        _, branch, revision = sys._git
    elif hasattr(sys, '_mercurial'):
        _, branch, revision = sys._mercurial
    else:
        branch = ''
        revision = ''

    # Add the patchlevel version if missing
    l = version.split('.')
    if len(l) == 2:
        l.append('0')
        version = '.'.join(l)

    # Build and cache the result
    result = (name, version, branch, revision, buildno, builddate, compiler)
    parse_python_sys_version_cache[sys_version] = result
    return result","def retrieve_python_build_number_and_date_as_strings():

    """""" Returns a tuple (buildno, builddate) stating the Python
        build number and date as strings.

    """"""
    return parse_python_sys_version()[4:6]","{""_sys_version"": ""parse_python_sys_version""}","def 0(sys_version=None):

    """""" Returns a parsed version of Python's sys.version as tuple
        (name, version, branch, revision, buildno, builddate, compiler)
        referring to the Python implementation name, version, branch,
        revision, build number, build date/time as string and the compiler
        identification string.

        Note that unlike the Python sys.version, the returned value
        for the Python version will always include the patchlevel (it
        defaults to '.0').

        The function returns empty strings for tuple entries that
        cannot be determined.

        sys_version may be given to parse an alternative version
        string, e.g. if the version was read from a different Python
        interpreter.

    """"""
    # Get the Python version
    if sys_version is None:
        sys_version = sys.version

    # Try the cache first
    result = _sys_version_cache.get(sys_version, None)
    if result is not None:
        return result

    # Parse it
    if 'IronPython' in sys_version:
        # IronPython
        name = 'IronPython'
        if sys_version.startswith('IronPython'):
            match = _ironpython_sys_version_parser.match(sys_version)
        else:
            match = _ironpython26_sys_version_parser.match(sys_version)

        if match is None:
            raise ValueError(
                'failed to parse IronPython sys.version: %s' %
                repr(sys_version))

        version, alt_version, compiler = match.groups()
        buildno = ''
        builddate = ''

    elif sys.platform.startswith('java'):
        # Jython
        name = 'Jython'
        match = _sys_version_parser.match(sys_version)
        if match is None:
            raise ValueError(
                'failed to parse Jython sys.version: %s' %
                repr(sys_version))
        version, buildno, builddate, buildtime, _ = match.groups()
        if builddate is None:
            builddate = ''
        compiler = sys.platform

    elif ""PyPy"" in sys_version:
        # PyPy
        name = ""PyPy""
        match = _pypy_sys_version_parser.match(sys_version)
        if match is None:
            raise ValueError(""failed to parse PyPy sys.version: %s"" %
                             repr(sys_version))
        version, buildno, builddate, buildtime = match.groups()
        compiler = """"

    else:
        # CPython
        match = _sys_version_parser.match(sys_version)
        if match is None:
            raise ValueError(
                'failed to parse CPython sys.version: %s' %
                repr(sys_version))
        version, buildno, builddate, buildtime, compiler = \
              match.groups()
        name = 'CPython'
        if builddate is None:
            builddate = ''
        elif buildtime:
            builddate = builddate + ' ' + buildtime

    if hasattr(sys, '_git'):
        _, branch, revision = sys._git
    elif hasattr(sys, '_mercurial'):
        _, branch, revision = sys._mercurial
    else:
        branch = ''
        revision = ''

    # Add the patchlevel version if missing
    l = version.split('.')
    if len(l) == 2:
        l.append('0')
        version = '.'.join(l)

    # Build and cache the result
    result = (name, version, branch, revision, buildno, builddate, compiler)
    _sys_version_cache[sys_version] = result
    return result","def 1():

    """""" Returns a tuple (buildno, builddate) stating the Python
        build number and date as strings.

    """"""
    return 0()[4:6]","{""_sys_version"": ""0""}"
119,119,"def python_compiler():

    """""" Returns a string identifying the compiler used for compiling
        Python.

    """"""
    return _sys_version()[6]","def _sys_version(sys_version=None):

    """""" Returns a parsed version of Python's sys.version as tuple
        (name, version, branch, revision, buildno, builddate, compiler)
        referring to the Python implementation name, version, branch,
        revision, build number, build date/time as string and the compiler
        identification string.

        Note that unlike the Python sys.version, the returned value
        for the Python version will always include the patchlevel (it
        defaults to '.0').

        The function returns empty strings for tuple entries that
        cannot be determined.

        sys_version may be given to parse an alternative version
        string, e.g. if the version was read from a different Python
        interpreter.

    """"""
    # Get the Python version
    if sys_version is None:
        sys_version = sys.version

    # Try the cache first
    result = _sys_version_cache.get(sys_version, None)
    if result is not None:
        return result

    # Parse it
    if 'IronPython' in sys_version:
        # IronPython
        name = 'IronPython'
        if sys_version.startswith('IronPython'):
            match = _ironpython_sys_version_parser.match(sys_version)
        else:
            match = _ironpython26_sys_version_parser.match(sys_version)

        if match is None:
            raise ValueError(
                'failed to parse IronPython sys.version: %s' %
                repr(sys_version))

        version, alt_version, compiler = match.groups()
        buildno = ''
        builddate = ''

    elif sys.platform.startswith('java'):
        # Jython
        name = 'Jython'
        match = _sys_version_parser.match(sys_version)
        if match is None:
            raise ValueError(
                'failed to parse Jython sys.version: %s' %
                repr(sys_version))
        version, buildno, builddate, buildtime, _ = match.groups()
        if builddate is None:
            builddate = ''
        compiler = sys.platform

    elif ""PyPy"" in sys_version:
        # PyPy
        name = ""PyPy""
        match = _pypy_sys_version_parser.match(sys_version)
        if match is None:
            raise ValueError(""failed to parse PyPy sys.version: %s"" %
                             repr(sys_version))
        version, buildno, builddate, buildtime = match.groups()
        compiler = """"

    else:
        # CPython
        match = _sys_version_parser.match(sys_version)
        if match is None:
            raise ValueError(
                'failed to parse CPython sys.version: %s' %
                repr(sys_version))
        version, buildno, builddate, buildtime, compiler = \
              match.groups()
        name = 'CPython'
        if builddate is None:
            builddate = ''
        elif buildtime:
            builddate = builddate + ' ' + buildtime

    if hasattr(sys, '_git'):
        _, branch, revision = sys._git
    elif hasattr(sys, '_mercurial'):
        _, branch, revision = sys._mercurial
    else:
        branch = ''
        revision = ''

    # Add the patchlevel version if missing
    l = version.split('.')
    if len(l) == 2:
        l.append('0')
        version = '.'.join(l)

    # Build and cache the result
    result = (name, version, branch, revision, buildno, builddate, compiler)
    _sys_version_cache[sys_version] = result
    return result","def parse_python_sys_version(sys_version=None):

    """""" Returns a parsed version of Python's sys.version as tuple
        (name, version, branch, revision, buildno, builddate, compiler)
        referring to the Python implementation name, version, branch,
        revision, build number, build date/time as string and the compiler
        identification string.

        Note that unlike the Python sys.version, the returned value
        for the Python version will always include the patchlevel (it
        defaults to '.0').

        The function returns empty strings for tuple entries that
        cannot be determined.

        sys_version may be given to parse an alternative version
        string, e.g. if the version was read from a different Python
        interpreter.

    """"""
    # Get the Python version
    if sys_version is None:
        sys_version = sys.version

    # Try the cache first
    result = parse_python_sys_version_cache.get(sys_version, None)
    if result is not None:
        return result

    # Parse it
    if 'IronPython' in sys_version:
        # IronPython
        name = 'IronPython'
        if sys_version.startswith('IronPython'):
            match = _ironpythonparse_python_sys_version_parser.match(sys_version)
        else:
            match = _ironpython26parse_python_sys_version_parser.match(sys_version)

        if match is None:
            raise ValueError(
                'failed to parse IronPython sys.version: %s' %
                repr(sys_version))

        version, alt_version, compiler = match.groups()
        buildno = ''
        builddate = ''

    elif sys.platform.startswith('java'):
        # Jython
        name = 'Jython'
        match = parse_python_sys_version_parser.match(sys_version)
        if match is None:
            raise ValueError(
                'failed to parse Jython sys.version: %s' %
                repr(sys_version))
        version, buildno, builddate, buildtime, _ = match.groups()
        if builddate is None:
            builddate = ''
        compiler = sys.platform

    elif ""PyPy"" in sys_version:
        # PyPy
        name = ""PyPy""
        match = _pypyparse_python_sys_version_parser.match(sys_version)
        if match is None:
            raise ValueError(""failed to parse PyPy sys.version: %s"" %
                             repr(sys_version))
        version, buildno, builddate, buildtime = match.groups()
        compiler = """"

    else:
        # CPython
        match = parse_python_sys_version_parser.match(sys_version)
        if match is None:
            raise ValueError(
                'failed to parse CPython sys.version: %s' %
                repr(sys_version))
        version, buildno, builddate, buildtime, compiler = \
              match.groups()
        name = 'CPython'
        if builddate is None:
            builddate = ''
        elif buildtime:
            builddate = builddate + ' ' + buildtime

    if hasattr(sys, '_git'):
        _, branch, revision = sys._git
    elif hasattr(sys, '_mercurial'):
        _, branch, revision = sys._mercurial
    else:
        branch = ''
        revision = ''

    # Add the patchlevel version if missing
    l = version.split('.')
    if len(l) == 2:
        l.append('0')
        version = '.'.join(l)

    # Build and cache the result
    result = (name, version, branch, revision, buildno, builddate, compiler)
    parse_python_sys_version_cache[sys_version] = result
    return result","def retrieve_compiler_used_for_compiling_python():

    """""" Returns a string identifying the compiler used for compiling
        Python.

    """"""
    return parse_python_sys_version()[6]","{""_sys_version"": ""parse_python_sys_version""}","def 0(sys_version=None):

    """""" Returns a parsed version of Python's sys.version as tuple
        (name, version, branch, revision, buildno, builddate, compiler)
        referring to the Python implementation name, version, branch,
        revision, build number, build date/time as string and the compiler
        identification string.

        Note that unlike the Python sys.version, the returned value
        for the Python version will always include the patchlevel (it
        defaults to '.0').

        The function returns empty strings for tuple entries that
        cannot be determined.

        sys_version may be given to parse an alternative version
        string, e.g. if the version was read from a different Python
        interpreter.

    """"""
    # Get the Python version
    if sys_version is None:
        sys_version = sys.version

    # Try the cache first
    result = _sys_version_cache.get(sys_version, None)
    if result is not None:
        return result

    # Parse it
    if 'IronPython' in sys_version:
        # IronPython
        name = 'IronPython'
        if sys_version.startswith('IronPython'):
            match = _ironpython_sys_version_parser.match(sys_version)
        else:
            match = _ironpython26_sys_version_parser.match(sys_version)

        if match is None:
            raise ValueError(
                'failed to parse IronPython sys.version: %s' %
                repr(sys_version))

        version, alt_version, compiler = match.groups()
        buildno = ''
        builddate = ''

    elif sys.platform.startswith('java'):
        # Jython
        name = 'Jython'
        match = _sys_version_parser.match(sys_version)
        if match is None:
            raise ValueError(
                'failed to parse Jython sys.version: %s' %
                repr(sys_version))
        version, buildno, builddate, buildtime, _ = match.groups()
        if builddate is None:
            builddate = ''
        compiler = sys.platform

    elif ""PyPy"" in sys_version:
        # PyPy
        name = ""PyPy""
        match = _pypy_sys_version_parser.match(sys_version)
        if match is None:
            raise ValueError(""failed to parse PyPy sys.version: %s"" %
                             repr(sys_version))
        version, buildno, builddate, buildtime = match.groups()
        compiler = """"

    else:
        # CPython
        match = _sys_version_parser.match(sys_version)
        if match is None:
            raise ValueError(
                'failed to parse CPython sys.version: %s' %
                repr(sys_version))
        version, buildno, builddate, buildtime, compiler = \
              match.groups()
        name = 'CPython'
        if builddate is None:
            builddate = ''
        elif buildtime:
            builddate = builddate + ' ' + buildtime

    if hasattr(sys, '_git'):
        _, branch, revision = sys._git
    elif hasattr(sys, '_mercurial'):
        _, branch, revision = sys._mercurial
    else:
        branch = ''
        revision = ''

    # Add the patchlevel version if missing
    l = version.split('.')
    if len(l) == 2:
        l.append('0')
        version = '.'.join(l)

    # Build and cache the result
    result = (name, version, branch, revision, buildno, builddate, compiler)
    _sys_version_cache[sys_version] = result
    return result","def 1():

    """""" Returns a string identifying the compiler used for compiling
        Python.

    """"""
    return 0()[6]","{""_sys_version"": ""0""}"
120,120,"def add_func_stats(target, source):
    """"""Add together all the stats for two profile entries.""""""
    cc, nc, tt, ct, callers = source
    t_cc, t_nc, t_tt, t_ct, t_callers = target
    return (cc+t_cc, nc+t_nc, tt+t_tt, ct+t_ct,
              add_callers(t_callers, callers))","def add_callers(target, source):
    """"""Combine two caller lists in a single list.""""""
    new_callers = {}
    for func, caller in target.items():
        new_callers[func] = caller
    for func, caller in source.items():
        if func in new_callers:
            if isinstance(caller, tuple):
                # format used by cProfile
                new_callers[func] = tuple(i + j for i, j in zip(caller, new_callers[func]))
            else:
                # format used by profile
                new_callers[func] += caller
        else:
            new_callers[func] = caller
    return new_callers","def combine_caller_lists(target, source):
    """"""Combine two caller lists in a single list.""""""
    new_callers = {}
    for func, caller in target.items():
        new_callers[func] = caller
    for func, caller in source.items():
        if func in new_callers:
            if isinstance(caller, tuple):
                # format used by cProfile
                new_callers[func] = tuple(i + j for i, j in zip(caller, new_callers[func]))
            else:
                # format used by profile
                new_callers[func] += caller
        else:
            new_callers[func] = caller
    return new_callers","def combine_stats_for_two_profile_entries_with_callers(target, source):
    """"""Add together all the stats for two profile entries.""""""
    cc, nc, tt, ct, callers = source
    t_cc, t_nc, t_tt, t_ct, t_callers = target
    return (cc+t_cc, nc+t_nc, tt+t_tt, ct+t_ct,
              combine_caller_lists(t_callers, callers))","{""add_callers"": ""combine_caller_lists""}","def 0(target, source):
    """"""Combine two caller lists in a single list.""""""
    new_callers = {}
    for func, caller in target.items():
        new_callers[func] = caller
    for func, caller in source.items():
        if func in new_callers:
            if isinstance(caller, tuple):
                # format used by cProfile
                new_callers[func] = tuple(i + j for i, j in zip(caller, new_callers[func]))
            else:
                # format used by profile
                new_callers[func] += caller
        else:
            new_callers[func] = caller
    return new_callers","def 1(target, source):
    """"""Add together all the stats for two profile entries.""""""
    cc, nc, tt, ct, callers = source
    t_cc, t_nc, t_tt, t_ct, t_callers = target
    return (cc+t_cc, nc+t_nc, tt+t_tt, ct+t_ct,
              0(t_callers, callers))","{""add_callers"": ""0""}"
121,121,"def monthrange(year, month):
    """"""Return weekday (0-6 ~ Mon-Sun) and number of days (28-31) for
       year, month.""""""
    if not 1 <= month <= 12:
        raise IllegalMonthError(month)
    day1 = weekday(year, month, 1)
    ndays = mdays[month] + (month == February and isleap(year))
    return day1, ndays","def isleap(year):
    """"""Return True for leap years, False for non-leap years.""""""
    return year % 4 == 0 and (year % 100 != 0 or year % 400 == 0)

def weekday(year, month, day):
    """"""Return weekday (0-6 ~ Mon-Sun) for year, month (1-12), day (1-31).""""""
    if not datetime.MINYEAR <= year <= datetime.MAXYEAR:
        year = 2000 + year % 400
    return datetime.date(year, month, day).weekday()","def is_leap_year(year):
    """"""Return True for leap years, False for non-leap years.""""""
    return year % 4 == 0 and (year % 100 != 0 or year % 400 == 0)

def get_weekday(year, month, day):
    """"""Return get_weekday (0-6 ~ Mon-Sun) for year, month (1-12), day (1-31).""""""
    if not datetime.MINYEAR <= year <= datetime.MAXYEAR:
        year = 2000 + year % 400
    return datetime.date(year, month, day).get_weekday()","def return_weekday_and_number_of_days_for_year_and_month(year, month):
    """"""Return get_weekday (0-6 ~ Mon-Sun) and number of days (28-31) for
       year, month.""""""
    if not 1 <= month <= 12:
        raise IllegalMonthError(month)
    day1 = get_weekday(year, month, 1)
    ndays = mdays[month] + (month == February and is_leap_year(year))
    return day1, ndays","{""isleap"": ""is_leap_year"", ""weekday"": ""get_weekday""}","def 0(year):
    """"""Return True for leap years, False for non-leap years.""""""
    return year % 4 == 0 and (year % 100 != 0 or year % 400 == 0)

def 1(year, month, day):
    """"""Return weekday (0-6 ~ Mon-Sun) for year, month (1-12), day (1-31).""""""
    if not datetime.MINYEAR <= year <= datetime.MAXYEAR:
        year = 2000 + year % 400
    return datetime.date(year, month, day).1()","def 2(year, month):
    """"""Return weekday (0-6 ~ Mon-Sun) and number of days (28-31) for
       year, month.""""""
    if not 1 <= month <= 12:
        raise IllegalMonthError(month)
    day1 = 1(year, month, 1)
    ndays = mdays[month] + (month == February and 0(year))
    return day1, ndays","{""isleap"": ""0"", ""weekday"": ""1""}"
122,122,"def ismethoddescriptor(object):
    """"""Return true if the object is a method descriptor.

    But not if ismethod() or isclass() or isfunction() are true.

    This is new in Python 2.2, and, for example, is true of int.__add__.
    An object passing this test has a __get__ attribute but not a __set__
    attribute, but beyond that the set of attributes varies.  __name__ is
    usually sensible, and __doc__ often is.

    Methods implemented via descriptors that also pass one of the other
    tests return false from the ismethoddescriptor() test, simply because
    the other tests promise more -- you can, e.g., count on having the
    __func__ attribute (etc) when an object passes ismethod().""""""
    if isclass(object) or ismethod(object) or isfunction(object):
        # mutual exclusion
        return False
    tp = type(object)
    return hasattr(tp, ""__get__"") and not hasattr(tp, ""__set__"")","def isclass(object):
    """"""Return true if the object is a class.

    Class objects provide these attributes:
        __doc__         documentation string
        __module__      name of module in which this class was defined""""""
    return isinstance(object, type)

def isfunction(object):
    """"""Return true if the object is a user-defined function.

    Function objects provide these attributes:
        __doc__         documentation string
        __name__        name with which this function was defined
        __code__        code object containing compiled function bytecode
        __defaults__    tuple of any default values for arguments
        __globals__     global namespace in which this function was defined
        __annotations__ dict of parameter annotations
        __kwdefaults__  dict of keyword only parameters with defaults""""""
    return isinstance(object, types.FunctionType)

def ismethod(object):
    """"""Return true if the object is an instance method.

    Instance method objects provide these attributes:
        __doc__         documentation string
        __name__        name with which this method was defined
        __func__        function object containing implementation of method
        __self__        instance to which this method is bound""""""
    return isinstance(object, types.MethodType)","def check_if_object_is_class(object):
    """"""Return true if the object is a class.

    Class objects provide these attributes:
        __doc__         documentation string
        __module__      name of module in which this class was defined""""""
    return isinstance(object, type)

def determine_if_object_is_user_function(object):
    """"""Return true if the object is a user-defined function.

    Function objects provide these attributes:
        __doc__         documentation string
        __name__        name with which this function was defined
        __code__        code object containing compiled function bytecode
        __defaults__    tuple of any default values for arguments
        __globals__     global namespace in which this function was defined
        __annotations__ dict of parameter annotations
        __kwdefaults__  dict of keyword only parameters with defaults""""""
    return isinstance(object, types.FunctionType)

def verify_if_object_is_instance_method(object):
    """"""Return true if the object is an instance method.

    Instance method objects provide these attributes:
        __doc__         documentation string
        __name__        name with which this method was defined
        __func__        function object containing implementation of method
        __self__        instance to which this method is bound""""""
    return isinstance(object, types.MethodType)","def verify_if_object_is_instance_methoddescriptor(object):
    """"""Return true if the object is a method descriptor.

    But not if verify_if_object_is_instance_method() or check_if_object_is_class() or determine_if_object_is_user_function() are true.

    This is new in Python 2.2, and, for example, is true of int.__add__.
    An object passing this test has a __get__ attribute but not a __set__
    attribute, but beyond that the set of attributes varies.  __name__ is
    usually sensible, and __doc__ often is.

    Methods implemented via descriptors that also pass one of the other
    tests return false from the verify_if_object_is_instance_methoddescriptor() test, simply because
    the other tests promise more -- you can, e.g., count on having the
    __func__ attribute (etc) when an object passes verify_if_object_is_instance_method().""""""
    if check_if_object_is_class(object) or verify_if_object_is_instance_method(object) or determine_if_object_is_user_function(object):
        # mutual exclusion
        return False
    tp = type(object)
    return hasattr(tp, ""__get__"") and not hasattr(tp, ""__set__"")","{""isclass"": ""check_if_object_is_class"", ""isfunction"": ""determine_if_object_is_user_function"", ""ismethod"": ""verify_if_object_is_instance_method""}","def 0(object):
    """"""Return true if the object is a class.

    Class objects provide these attributes:
        __doc__         documentation string
        __module__      name of module in which this class was defined""""""
    return isinstance(object, type)

def 1(object):
    """"""Return true if the object is a user-defined function.

    Function objects provide these attributes:
        __doc__         documentation string
        __name__        name with which this function was defined
        __code__        code object containing compiled function bytecode
        __defaults__    tuple of any default values for arguments
        __globals__     global namespace in which this function was defined
        __annotations__ dict of parameter annotations
        __kwdefaults__  dict of keyword only parameters with defaults""""""
    return isinstance(object, types.FunctionType)

def 2(object):
    """"""Return true if the object is an instance method.

    Instance method objects provide these attributes:
        __doc__         documentation string
        __name__        name with which this method was defined
        __func__        function object containing implementation of method
        __self__        instance to which this method is bound""""""
    return isinstance(object, types.MethodType)","def 3(object):
    """"""Return true if the object is a method descriptor.

    But not if 2() or 0() or 1() are true.

    This is new in Python 2.2, and, for example, is true of int.__add__.
    An object passing this test has a __get__ attribute but not a __set__
    attribute, but beyond that the set of attributes varies.  __name__ is
    usually sensible, and __doc__ often is.

    Methods implemented via descriptors that also pass one of the other
    tests return false from the 3() test, simply because
    the other tests promise more -- you can, e.g., count on having the
    __func__ attribute (etc) when an object passes 2().""""""
    if 0(object) or 2(object) or 1(object):
        # mutual exclusion
        return False
    tp = type(object)
    return hasattr(tp, ""__get__"") and not hasattr(tp, ""__set__"")","{""isclass"": ""0"", ""isfunction"": ""1"", ""ismethod"": ""2""}"
123,123,"def isdatadescriptor(object):
    """"""Return true if the object is a data descriptor.

    Data descriptors have a __set__ or a __delete__ attribute.  Examples are
    properties (defined in Python) and getsets and members (defined in C).
    Typically, data descriptors will also have __name__ and __doc__ attributes
    (properties, getsets, and members have both of these attributes), but this
    is not guaranteed.""""""
    if isclass(object) or ismethod(object) or isfunction(object):
        # mutual exclusion
        return False
    tp = type(object)
    return hasattr(tp, ""__set__"") or hasattr(tp, ""__delete__"")","def isclass(object):
    """"""Return true if the object is a class.

    Class objects provide these attributes:
        __doc__         documentation string
        __module__      name of module in which this class was defined""""""
    return isinstance(object, type)

def isfunction(object):
    """"""Return true if the object is a user-defined function.

    Function objects provide these attributes:
        __doc__         documentation string
        __name__        name with which this function was defined
        __code__        code object containing compiled function bytecode
        __defaults__    tuple of any default values for arguments
        __globals__     global namespace in which this function was defined
        __annotations__ dict of parameter annotations
        __kwdefaults__  dict of keyword only parameters with defaults""""""
    return isinstance(object, types.FunctionType)

def ismethod(object):
    """"""Return true if the object is an instance method.

    Instance method objects provide these attributes:
        __doc__         documentation string
        __name__        name with which this method was defined
        __func__        function object containing implementation of method
        __self__        instance to which this method is bound""""""
    return isinstance(object, types.MethodType)","def check_if_object_is_class(object):
    """"""Return true if the object is a class.

    Class objects provide these attributes:
        __doc__         documentation string
        __module__      name of module in which this class was defined""""""
    return isinstance(object, type)

def determine_if_object_is_user_function(object):
    """"""Return true if the object is a user-defined function.

    Function objects provide these attributes:
        __doc__         documentation string
        __name__        name with which this function was defined
        __code__        code object containing compiled function bytecode
        __defaults__    tuple of any default values for arguments
        __globals__     global namespace in which this function was defined
        __annotations__ dict of parameter annotations
        __kwdefaults__  dict of keyword only parameters with defaults""""""
    return isinstance(object, types.FunctionType)

def verify_if_object_is_instance_method(object):
    """"""Return true if the object is an instance method.

    Instance method objects provide these attributes:
        __doc__         documentation string
        __name__        name with which this method was defined
        __func__        function object containing implementation of method
        __self__        instance to which this method is bound""""""
    return isinstance(object, types.MethodType)","def check_if_object_is_data_descriptor_return_true_or_false(object):
    """"""Return true if the object is a data descriptor.

    Data descriptors have a __set__ or a __delete__ attribute.  Examples are
    properties (defined in Python) and getsets and members (defined in C).
    Typically, data descriptors will also have __name__ and __doc__ attributes
    (properties, getsets, and members have both of these attributes), but this
    is not guaranteed.""""""
    if check_if_object_is_class(object) or verify_if_object_is_instance_method(object) or determine_if_object_is_user_function(object):
        # mutual exclusion
        return False
    tp = type(object)
    return hasattr(tp, ""__set__"") or hasattr(tp, ""__delete__"")","{""isclass"": ""check_if_object_is_class"", ""isfunction"": ""determine_if_object_is_user_function"", ""ismethod"": ""verify_if_object_is_instance_method""}","def 0(object):
    """"""Return true if the object is a class.

    Class objects provide these attributes:
        __doc__         documentation string
        __module__      name of module in which this class was defined""""""
    return isinstance(object, type)

def 1(object):
    """"""Return true if the object is a user-defined function.

    Function objects provide these attributes:
        __doc__         documentation string
        __name__        name with which this function was defined
        __code__        code object containing compiled function bytecode
        __defaults__    tuple of any default values for arguments
        __globals__     global namespace in which this function was defined
        __annotations__ dict of parameter annotations
        __kwdefaults__  dict of keyword only parameters with defaults""""""
    return isinstance(object, types.FunctionType)

def 2(object):
    """"""Return true if the object is an instance method.

    Instance method objects provide these attributes:
        __doc__         documentation string
        __name__        name with which this method was defined
        __func__        function object containing implementation of method
        __self__        instance to which this method is bound""""""
    return isinstance(object, types.MethodType)","def 3(object):
    """"""Return true if the object is a data descriptor.

    Data descriptors have a __set__ or a __delete__ attribute.  Examples are
    properties (defined in Python) and getsets and members (defined in C).
    Typically, data descriptors will also have __name__ and __doc__ attributes
    (properties, getsets, and members have both of these attributes), but this
    is not guaranteed.""""""
    if 0(object) or 2(object) or 1(object):
        # mutual exclusion
        return False
    tp = type(object)
    return hasattr(tp, ""__set__"") or hasattr(tp, ""__delete__"")","{""isclass"": ""0"", ""isfunction"": ""1"", ""ismethod"": ""2""}"
124,124,"def _has_code_flag(f, flag):
    """"""Return true if ``f`` is a function (or a method or functools.partial
    wrapper wrapping a function) whose code object has the given ``flag``
    set in its flags.""""""
    while ismethod(f):
        f = f.__func__
    f = functools._unwrap_partial(f)
    if not isfunction(f):
        return False
    return bool(f.__code__.co_flags & flag)","def isfunction(object):
    """"""Return true if the object is a user-defined function.

    Function objects provide these attributes:
        __doc__         documentation string
        __name__        name with which this function was defined
        __code__        code object containing compiled function bytecode
        __defaults__    tuple of any default values for arguments
        __globals__     global namespace in which this function was defined
        __annotations__ dict of parameter annotations
        __kwdefaults__  dict of keyword only parameters with defaults""""""
    return isinstance(object, types.FunctionType)

def ismethod(object):
    """"""Return true if the object is an instance method.

    Instance method objects provide these attributes:
        __doc__         documentation string
        __name__        name with which this method was defined
        __func__        function object containing implementation of method
        __self__        instance to which this method is bound""""""
    return isinstance(object, types.MethodType)","def determine_if_object_is_user_function(object):
    """"""Return true if the object is a user-defined function.

    Function objects provide these attributes:
        __doc__         documentation string
        __name__        name with which this function was defined
        __code__        code object containing compiled function bytecode
        __defaults__    tuple of any default values for arguments
        __globals__     global namespace in which this function was defined
        __annotations__ dict of parameter annotations
        __kwdefaults__  dict of keyword only parameters with defaults""""""
    return isinstance(object, types.FunctionType)

def verify_if_object_is_instance_method(object):
    """"""Return true if the object is an instance method.

    Instance method objects provide these attributes:
        __doc__         documentation string
        __name__        name with which this method was defined
        __func__        function object containing implementation of method
        __self__        instance to which this method is bound""""""
    return isinstance(object, types.MethodType)","def has_code_flag(f, flag):
    """"""Return true if ``f`` is a function (or a method or functools.partial
    wrapper wrapping a function) whose code object has the given ``flag``
    set in its flags.""""""
    while verify_if_object_is_instance_method(f):
        f = f.__func__
    f = functools._unwrap_partial(f)
    if not determine_if_object_is_user_function(f):
        return False
    return bool(f.__code__.co_flags & flag)","{""isfunction"": ""determine_if_object_is_user_function"", ""ismethod"": ""verify_if_object_is_instance_method""}","def 0(object):
    """"""Return true if the object is a user-defined function.

    Function objects provide these attributes:
        __doc__         documentation string
        __name__        name with which this function was defined
        __code__        code object containing compiled function bytecode
        __defaults__    tuple of any default values for arguments
        __globals__     global namespace in which this function was defined
        __annotations__ dict of parameter annotations
        __kwdefaults__  dict of keyword only parameters with defaults""""""
    return isinstance(object, types.FunctionType)

def 1(object):
    """"""Return true if the object is an instance method.

    Instance method objects provide these attributes:
        __doc__         documentation string
        __name__        name with which this method was defined
        __func__        function object containing implementation of method
        __self__        instance to which this method is bound""""""
    return isinstance(object, types.MethodType)","def 2(f, flag):
    """"""Return true if ``f`` is a function (or a method or functools.partial
    wrapper wrapping a function) whose code object has the given ``flag``
    set in its flags.""""""
    while 1(f):
        f = f.__func__
    f = functools._unwrap_partial(f)
    if not 0(f):
        return False
    return bool(f.__code__.co_flags & flag)","{""isfunction"": ""0"", ""ismethod"": ""1""}"
125,125,"def isgeneratorfunction(obj):
    """"""Return true if the object is a user-defined generator function.

    Generator function objects provide the same attributes as functions.
    See help(isfunction) for a list of attributes.""""""
    return _has_code_flag(obj, CO_GENERATOR)","def _has_code_flag(f, flag):
    """"""Return true if ``f`` is a function (or a method or functools.partial
    wrapper wrapping a function) whose code object has the given ``flag``
    set in its flags.""""""
    while ismethod(f):
        f = f.__func__
    f = functools._unwrap_partial(f)
    if not isfunction(f):
        return False
    return bool(f.__code__.co_flags & flag)","def has_code_flag(f, flag):
    """"""Return true if ``f`` is a function (or a method or functools.partial
    wrapper wrapping a function) whose code object has the given ``flag``
    set in its flags.""""""
    while ismethod(f):
        f = f.__func__
    f = functools._unwrap_partial(f)
    if not isfunction(f):
        return False
    return bool(f.__code__.co_flags & flag)","def return_true_if_object_is_user_defined_generator_function(obj):
    """"""Return true if the object is a user-defined generator function.

    Generator function objects provide the same attributes as functions.
    See help(isfunction) for a list of attributes.""""""
    return has_code_flag(obj, CO_GENERATOR)","{""_has_code_flag"": ""has_code_flag""}","def 0(f, flag):
    """"""Return true if ``f`` is a function (or a method or functools.partial
    wrapper wrapping a function) whose code object has the given ``flag``
    set in its flags.""""""
    while ismethod(f):
        f = f.__func__
    f = functools._unwrap_partial(f)
    if not isfunction(f):
        return False
    return bool(f.__code__.co_flags & flag)","def 1(obj):
    """"""Return true if the object is a user-defined generator function.

    Generator function objects provide the same attributes as functions.
    See help(isfunction) for a list of attributes.""""""
    return 0(obj, CO_GENERATOR)","{""_has_code_flag"": ""0""}"
126,126,"def isasyncgenfunction(obj):
    """"""Return true if the object is an asynchronous generator function.

    Asynchronous generator functions are defined with ""async def""
    syntax and have ""yield"" expressions in their body.
    """"""
    return _has_code_flag(obj, CO_ASYNC_GENERATOR)","def _has_code_flag(f, flag):
    """"""Return true if ``f`` is a function (or a method or functools.partial
    wrapper wrapping a function) whose code object has the given ``flag``
    set in its flags.""""""
    while ismethod(f):
        f = f.__func__
    f = functools._unwrap_partial(f)
    if not isfunction(f):
        return False
    return bool(f.__code__.co_flags & flag)","def has_code_flag(f, flag):
    """"""Return true if ``f`` is a function (or a method or functools.partial
    wrapper wrapping a function) whose code object has the given ``flag``
    set in its flags.""""""
    while ismethod(f):
        f = f.__func__
    f = functools._unwrap_partial(f)
    if not isfunction(f):
        return False
    return bool(f.__code__.co_flags & flag)","def return_true_if_object_is_asynchronous_generator_function(obj):
    """"""Return true if the object is an asynchronous generator function.

    Asynchronous generator functions are defined with ""async def""
    syntax and have ""yield"" expressions in their body.
    """"""
    return has_code_flag(obj, CO_ASYNC_GENERATOR)","{""_has_code_flag"": ""has_code_flag""}","def 0(f, flag):
    """"""Return true if ``f`` is a function (or a method or functools.partial
    wrapper wrapping a function) whose code object has the given ``flag``
    set in its flags.""""""
    while ismethod(f):
        f = f.__func__
    f = functools._unwrap_partial(f)
    if not isfunction(f):
        return False
    return bool(f.__code__.co_flags & flag)","def 1(obj):
    """"""Return true if the object is an asynchronous generator function.

    Asynchronous generator functions are defined with ""async def""
    syntax and have ""yield"" expressions in their body.
    """"""
    return 0(obj, CO_ASYNC_GENERATOR)","{""_has_code_flag"": ""0""}"
127,127,"def isroutine(object):
    """"""Return true if the object is any kind of function or method.""""""
    return (isbuiltin(object)
            or isfunction(object)
            or ismethod(object)
            or ismethoddescriptor(object))","def isbuiltin(object):
    """"""Return true if the object is a built-in function or method.

    Built-in functions and methods provide these attributes:
        __doc__         documentation string
        __name__        original name of this function or method
        __self__        instance to which a method is bound, or None""""""
    return isinstance(object, types.BuiltinFunctionType)

def isfunction(object):
    """"""Return true if the object is a user-defined function.

    Function objects provide these attributes:
        __doc__         documentation string
        __name__        name with which this function was defined
        __code__        code object containing compiled function bytecode
        __defaults__    tuple of any default values for arguments
        __globals__     global namespace in which this function was defined
        __annotations__ dict of parameter annotations
        __kwdefaults__  dict of keyword only parameters with defaults""""""
    return isinstance(object, types.FunctionType)

def ismethoddescriptor(object):
    """"""Return true if the object is a method descriptor.

    But not if ismethod() or isclass() or isfunction() are true.

    This is new in Python 2.2, and, for example, is true of int.__add__.
    An object passing this test has a __get__ attribute but not a __set__
    attribute, but beyond that the set of attributes varies.  __name__ is
    usually sensible, and __doc__ often is.

    Methods implemented via descriptors that also pass one of the other
    tests return false from the ismethoddescriptor() test, simply because
    the other tests promise more -- you can, e.g., count on having the
    __func__ attribute (etc) when an object passes ismethod().""""""
    if isclass(object) or ismethod(object) or isfunction(object):
        # mutual exclusion
        return False
    tp = type(object)
    return hasattr(tp, ""__get__"") and not hasattr(tp, ""__set__"")

def ismethod(object):
    """"""Return true if the object is an instance method.

    Instance method objects provide these attributes:
        __doc__         documentation string
        __name__        name with which this method was defined
        __func__        function object containing implementation of method
        __self__        instance to which this method is bound""""""
    return isinstance(object, types.MethodType)","def is_builtin_function_or_method(object):
    """"""Return true if the object is a built-in function or method.

    Built-in functions and methods provide these attributes:
        __doc__         documentation string
        __name__        original name of this function or method
        __self__        instance to which a method is bound, or None""""""
    return isinstance(object, types.BuiltinFunctionType)

def determine_if_object_is_user_function(object):
    """"""Return true if the object is a user-defined function.

    Function objects provide these attributes:
        __doc__         documentation string
        __name__        name with which this function was defined
        __code__        code object containing compiled function bytecode
        __defaults__    tuple of any default values for arguments
        __globals__     global namespace in which this function was defined
        __annotations__ dict of parameter annotations
        __kwdefaults__  dict of keyword only parameters with defaults""""""
    return isinstance(object, types.FunctionType)

def is_method_descriptor(object):
    """"""Return true if the object is a method descriptor.

    But not if verify_if_object_is_instance_method() or isclass() or determine_if_object_is_user_function() are true.

    This is new in Python 2.2, and, for example, is true of int.__add__.
    An object passing this test has a __get__ attribute but not a __set__
    attribute, but beyond that the set of attributes varies.  __name__ is
    usually sensible, and __doc__ often is.

    Methods implemented via descriptors that also pass one of the other
    tests return false from the is_method_descriptor() test, simply because
    the other tests promise more -- you can, e.g., count on having the
    __func__ attribute (etc) when an object passes verify_if_object_is_instance_method().""""""
    if isclass(object) or verify_if_object_is_instance_method(object) or determine_if_object_is_user_function(object):
        # mutual exclusion
        return False
    tp = type(object)
    return hasattr(tp, ""__get__"") and not hasattr(tp, ""__set__"")

def verify_if_object_is_instance_method(object):
    """"""Return true if the object is an instance method.

    Instance method objects provide these attributes:
        __doc__         documentation string
        __name__        name with which this method was defined
        __func__        function object containing implementation of method
        __self__        instance to which this method is bound""""""
    return isinstance(object, types.MethodType)","def return_true_if_object_is_any_kind_of_function_or_method(object):
    """"""Return true if the object is any kind of function or method.""""""
    return (is_builtin_function_or_method(object)
            or determine_if_object_is_user_function(object)
            or verify_if_object_is_instance_method(object)
            or is_method_descriptor(object))","{""isbuiltin"": ""is_builtin_function_or_method"", ""isfunction"": ""determine_if_object_is_user_function"", ""ismethoddescriptor"": ""is_method_descriptor"", ""ismethod"": ""verify_if_object_is_instance_method""}","def 0(object):
    """"""Return true if the object is a built-in function or method.

    Built-in functions and methods provide these attributes:
        __doc__         documentation string
        __name__        original name of this function or method
        __self__        instance to which a method is bound, or None""""""
    return isinstance(object, types.BuiltinFunctionType)

def 1(object):
    """"""Return true if the object is a user-defined function.

    Function objects provide these attributes:
        __doc__         documentation string
        __name__        name with which this function was defined
        __code__        code object containing compiled function bytecode
        __defaults__    tuple of any default values for arguments
        __globals__     global namespace in which this function was defined
        __annotations__ dict of parameter annotations
        __kwdefaults__  dict of keyword only parameters with defaults""""""
    return isinstance(object, types.FunctionType)

def 2(object):
    """"""Return true if the object is a method descriptor.

    But not if 3() or isclass() or 1() are true.

    This is new in Python 2.2, and, for example, is true of int.__add__.
    An object passing this test has a __get__ attribute but not a __set__
    attribute, but beyond that the set of attributes varies.  __name__ is
    usually sensible, and __doc__ often is.

    Methods implemented via descriptors that also pass one of the other
    tests return false from the 2() test, simply because
    the other tests promise more -- you can, e.g., count on having the
    __func__ attribute (etc) when an object passes 3().""""""
    if isclass(object) or 3(object) or 1(object):
        # mutual exclusion
        return False
    tp = type(object)
    return hasattr(tp, ""__get__"") and not hasattr(tp, ""__set__"")

def 3(object):
    """"""Return true if the object is an instance method.

    Instance method objects provide these attributes:
        __doc__         documentation string
        __name__        name with which this method was defined
        __func__        function object containing implementation of method
        __self__        instance to which this method is bound""""""
    return isinstance(object, types.MethodType)","def 4(object):
    """"""Return true if the object is any kind of function or method.""""""
    return (0(object)
            or 1(object)
            or 3(object)
            or 2(object))","{""isbuiltin"": ""0"", ""isfunction"": ""1"", ""ismethoddescriptor"": ""2"", ""ismethod"": ""3""}"
128,128,"def getfile(object):
    """"""Work out which source or compiled file an object was defined in.""""""
    if ismodule(object):
        if getattr(object, '__file__', None):
            return object.__file__
        raise TypeError('{!r} is a built-in module'.format(object))
    if isclass(object):
        if hasattr(object, '__module__'):
            module = sys.modules.get(object.__module__)
            if getattr(module, '__file__', None):
                return module.__file__
            if object.__module__ == '__main__':
                raise OSError('source code not available')
        raise TypeError('{!r} is a built-in class'.format(object))
    if ismethod(object):
        object = object.__func__
    if isfunction(object):
        object = object.__code__
    if istraceback(object):
        object = object.tb_frame
    if isframe(object):
        object = object.f_code
    if iscode(object):
        return object.co_filename
    raise TypeError('module, class, method, function, traceback, frame, or '
                    'code object was expected, got {}'.format(
                    type(object).__name__))","def istraceback(object):
    """"""Return true if the object is a traceback.

    Traceback objects provide these attributes:
        tb_frame        frame object at this level
        tb_lasti        index of last attempted instruction in bytecode
        tb_lineno       current line number in Python source code
        tb_next         next inner traceback object (called by this level)""""""
    return isinstance(object, types.TracebackType)

def isframe(object):
    """"""Return true if the object is a frame object.

    Frame objects provide these attributes:
        f_back          next outer frame object (this frame's caller)
        f_builtins      built-in namespace seen by this frame
        f_code          code object being executed in this frame
        f_globals       global namespace seen by this frame
        f_lasti         index of last attempted instruction in bytecode
        f_lineno        current line number in Python source code
        f_locals        local namespace seen by this frame
        f_trace         tracing function for this frame, or None""""""
    return isinstance(object, types.FrameType)

def ismodule(object):
    """"""Return true if the object is a module.

    Module objects provide these attributes:
        __cached__      pathname to byte compiled file
        __doc__         documentation string
        __file__        filename (missing for built-in modules)""""""
    return isinstance(object, types.ModuleType)

def isclass(object):
    """"""Return true if the object is a class.

    Class objects provide these attributes:
        __doc__         documentation string
        __module__      name of module in which this class was defined""""""
    return isinstance(object, type)

def isfunction(object):
    """"""Return true if the object is a user-defined function.

    Function objects provide these attributes:
        __doc__         documentation string
        __name__        name with which this function was defined
        __code__        code object containing compiled function bytecode
        __defaults__    tuple of any default values for arguments
        __globals__     global namespace in which this function was defined
        __annotations__ dict of parameter annotations
        __kwdefaults__  dict of keyword only parameters with defaults""""""
    return isinstance(object, types.FunctionType)

def iscode(object):
    """"""Return true if the object is a code object.

    Code objects provide these attributes:
        co_argcount         number of arguments (not including *, ** args
                            or keyword only arguments)
        co_code             string of raw compiled bytecode
        co_cellvars         tuple of names of cell variables
        co_consts           tuple of constants used in the bytecode
        co_filename         name of file in which this code object was created
        co_firstlineno      number of first line in Python source code
        co_flags            bitmap: 1=optimized | 2=newlocals | 4=*arg | 8=**arg
                            | 16=nested | 32=generator | 64=nofree | 128=coroutine
                            | 256=iterable_coroutine | 512=async_generator
        co_freevars         tuple of names of free variables
        co_posonlyargcount  number of positional only arguments
        co_kwonlyargcount   number of keyword only arguments (not including ** arg)
        co_lnotab           encoded mapping of line numbers to bytecode indices
        co_name             name with which this code object was defined
        co_names            tuple of names other than arguments and function locals
        co_nlocals          number of local variables
        co_stacksize        virtual machine stack space required
        co_varnames         tuple of names of arguments and local variables""""""
    return isinstance(object, types.CodeType)

def ismethod(object):
    """"""Return true if the object is an instance method.

    Instance method objects provide these attributes:
        __doc__         documentation string
        __name__        name with which this method was defined
        __func__        function object containing implementation of method
        __self__        instance to which this method is bound""""""
    return isinstance(object, types.MethodType)","def is_traceback(object):
    """"""Return true if the object is a traceback.

    Traceback objects provide these attributes:
        tb_frame        frame object at this level
        tb_lasti        index of last attempted instruction in bytecode
        tb_lineno       current line number in Python source code
        tb_next         next inner traceback object (called by this level)""""""
    return isinstance(object, types.TracebackType)

def is_frame(object):
    """"""Return true if the object is a frame object.

    Frame objects provide these attributes:
        f_back          next outer frame object (this frame's caller)
        f_builtins      built-in namespace seen by this frame
        f_code          code object being executed in this frame
        f_globals       global namespace seen by this frame
        f_lasti         index of last attempted instruction in bytecode
        f_lineno        current line number in Python source code
        f_locals        local namespace seen by this frame
        f_trace         tracing function for this frame, or None""""""
    return isinstance(object, types.FrameType)

def is_module(object):
    """"""Return true if the object is a module.

    Module objects provide these attributes:
        __cached__      pathname to byte compiled file
        __doc__         documentation string
        __file__        filename (missing for built-in modules)""""""
    return isinstance(object, types.ModuleType)

def check_if_object_is_class(object):
    """"""Return true if the object is a class.

    Class objects provide these attributes:
        __doc__         documentation string
        __module__      name of module in which this class was defined""""""
    return isinstance(object, type)

def determine_if_object_is_user_function(object):
    """"""Return true if the object is a user-defined function.

    Function objects provide these attributes:
        __doc__         documentation string
        __name__        name with which this function was defined
        __code__        code object containing compiled function bytecode
        __defaults__    tuple of any default values for arguments
        __globals__     global namespace in which this function was defined
        __annotations__ dict of parameter annotations
        __kwdefaults__  dict of keyword only parameters with defaults""""""
    return isinstance(object, types.FunctionType)

def check_if_object_is_code_object(object):
    """"""Return true if the object is a code object.

    Code objects provide these attributes:
        co_argcount         number of arguments (not including *, ** args
                            or keyword only arguments)
        co_code             string of raw compiled bytecode
        co_cellvars         tuple of names of cell variables
        co_consts           tuple of constants used in the bytecode
        co_filename         name of file in which this code object was created
        co_firstlineno      number of first line in Python source code
        co_flags            bitmap: 1=optimized | 2=newlocals | 4=*arg | 8=**arg
                            | 16=nested | 32=generator | 64=nofree | 128=coroutine
                            | 256=iterable_coroutine | 512=async_generator
        co_freevars         tuple of names of free variables
        co_posonlyargcount  number of positional only arguments
        co_kwonlyargcount   number of keyword only arguments (not including ** arg)
        co_lnotab           encoded mapping of line numbers to bytecode indices
        co_name             name with which this code object was defined
        co_names            tuple of names other than arguments and function locals
        co_nlocals          number of local variables
        co_stacksize        virtual machine stack space required
        co_varnames         tuple of names of arguments and local variables""""""
    return isinstance(object, types.CodeType)

def verify_if_object_is_instance_method(object):
    """"""Return true if the object is an instance method.

    Instance method objects provide these attributes:
        __doc__         documentation string
        __name__        name with which this method was defined
        __func__        function object containing implementation of method
        __self__        instance to which this method is bound""""""
    return isinstance(object, types.MethodType)","def identify_source_or_compiled_file_of_object(object):
    """"""Work out which source or compiled file an object was defined in.""""""
    if is_module(object):
        if getattr(object, '__file__', None):
            return object.__file__
        raise TypeError('{!r} is a built-in module'.format(object))
    if check_if_object_is_class(object):
        if hasattr(object, '__module__'):
            module = sys.modules.get(object.__module__)
            if getattr(module, '__file__', None):
                return module.__file__
            if object.__module__ == '__main__':
                raise OSError('source code not available')
        raise TypeError('{!r} is a built-in class'.format(object))
    if verify_if_object_is_instance_method(object):
        object = object.__func__
    if determine_if_object_is_user_function(object):
        object = object.__code__
    if is_traceback(object):
        object = object.tb_frame
    if is_frame(object):
        object = object.f_code
    if check_if_object_is_code_object(object):
        return object.co_filename
    raise TypeError('module, class, method, function, traceback, frame, or '
                    'code object was expected, got {}'.format(
                    type(object).__name__))","{""istraceback"": ""is_traceback"", ""isframe"": ""is_frame"", ""ismodule"": ""is_module"", ""isclass"": ""check_if_object_is_class"", ""isfunction"": ""determine_if_object_is_user_function"", ""iscode"": ""check_if_object_is_code_object"", ""ismethod"": ""verify_if_object_is_instance_method""}","def 0(object):
    """"""Return true if the object is a traceback.

    Traceback objects provide these attributes:
        tb_frame        frame object at this level
        tb_lasti        index of last attempted instruction in bytecode
        tb_lineno       current line number in Python source code
        tb_next         next inner traceback object (called by this level)""""""
    return isinstance(object, types.TracebackType)

def 1(object):
    """"""Return true if the object is a frame object.

    Frame objects provide these attributes:
        f_back          next outer frame object (this frame's caller)
        f_builtins      built-in namespace seen by this frame
        f_code          code object being executed in this frame
        f_globals       global namespace seen by this frame
        f_lasti         index of last attempted instruction in bytecode
        f_lineno        current line number in Python source code
        f_locals        local namespace seen by this frame
        f_trace         tracing function for this frame, or None""""""
    return isinstance(object, types.FrameType)

def 2(object):
    """"""Return true if the object is a module.

    Module objects provide these attributes:
        __cached__      pathname to byte compiled file
        __doc__         documentation string
        __file__        filename (missing for built-in modules)""""""
    return isinstance(object, types.ModuleType)

def 3(object):
    """"""Return true if the object is a class.

    Class objects provide these attributes:
        __doc__         documentation string
        __module__      name of module in which this class was defined""""""
    return isinstance(object, type)

def 4(object):
    """"""Return true if the object is a user-defined function.

    Function objects provide these attributes:
        __doc__         documentation string
        __name__        name with which this function was defined
        __code__        code object containing compiled function bytecode
        __defaults__    tuple of any default values for arguments
        __globals__     global namespace in which this function was defined
        __annotations__ dict of parameter annotations
        __kwdefaults__  dict of keyword only parameters with defaults""""""
    return isinstance(object, types.FunctionType)

def 5(object):
    """"""Return true if the object is a code object.

    Code objects provide these attributes:
        co_argcount         number of arguments (not including *, ** args
                            or keyword only arguments)
        co_code             string of raw compiled bytecode
        co_cellvars         tuple of names of cell variables
        co_consts           tuple of constants used in the bytecode
        co_filename         name of file in which this code object was created
        co_firstlineno      number of first line in Python source code
        co_flags            bitmap: 1=optimized | 2=newlocals | 4=*arg | 8=**arg
                            | 16=nested | 32=generator | 64=nofree | 128=coroutine
                            | 256=iterable_coroutine | 512=async_generator
        co_freevars         tuple of names of free variables
        co_posonlyargcount  number of positional only arguments
        co_kwonlyargcount   number of keyword only arguments (not including ** arg)
        co_lnotab           encoded mapping of line numbers to bytecode indices
        co_name             name with which this code object was defined
        co_names            tuple of names other than arguments and function locals
        co_nlocals          number of local variables
        co_stacksize        virtual machine stack space required
        co_varnames         tuple of names of arguments and local variables""""""
    return isinstance(object, types.CodeType)

def 6(object):
    """"""Return true if the object is an instance method.

    Instance method objects provide these attributes:
        __doc__         documentation string
        __name__        name with which this method was defined
        __func__        function object containing implementation of method
        __self__        instance to which this method is bound""""""
    return isinstance(object, types.MethodType)","def 7(object):
    """"""Work out which source or compiled file an object was defined in.""""""
    if 2(object):
        if getattr(object, '__file__', None):
            return object.__file__
        raise TypeError('{!r} is a built-in module'.format(object))
    if 3(object):
        if hasattr(object, '__module__'):
            module = sys.modules.get(object.__module__)
            if getattr(module, '__file__', None):
                return module.__file__
            if object.__module__ == '__main__':
                raise OSError('source code not available')
        raise TypeError('{!r} is a built-in class'.format(object))
    if 6(object):
        object = object.__func__
    if 4(object):
        object = object.__code__
    if 0(object):
        object = object.tb_frame
    if 1(object):
        object = object.f_code
    if 5(object):
        return object.co_filename
    raise TypeError('module, class, method, function, traceback, frame, or '
                    'code object was expected, got {}'.format(
                    type(object).__name__))","{""istraceback"": ""0"", ""isframe"": ""1"", ""ismodule"": ""2"", ""isclass"": ""3"", ""isfunction"": ""4"", ""iscode"": ""5"", ""ismethod"": ""6""}"
129,129,"def getabsfile(object, _filename=None):
    """"""Return an absolute path to the source or compiled file for an object.

    The idea is for each object to have a unique origin, so this routine
    normalizes the result as much as possible.""""""
    if _filename is None:
        _filename = getsourcefile(object) or getfile(object)
    return os.path.normcase(os.path.abspath(_filename))","def getfile(object):
    """"""Work out which source or compiled file an object was defined in.""""""
    if ismodule(object):
        if getattr(object, '__file__', None):
            return object.__file__
        raise TypeError('{!r} is a built-in module'.format(object))
    if isclass(object):
        if hasattr(object, '__module__'):
            module = sys.modules.get(object.__module__)
            if getattr(module, '__file__', None):
                return module.__file__
            if object.__module__ == '__main__':
                raise OSError('source code not available')
        raise TypeError('{!r} is a built-in class'.format(object))
    if ismethod(object):
        object = object.__func__
    if isfunction(object):
        object = object.__code__
    if istraceback(object):
        object = object.tb_frame
    if isframe(object):
        object = object.f_code
    if iscode(object):
        return object.co_filename
    raise TypeError('module, class, method, function, traceback, frame, or '
                    'code object was expected, got {}'.format(
                    type(object).__name__))

def getsourcefile(object):
    """"""Return the filename that can be used to locate an object's source.
    Return None if no way can be identified to get the source.
    """"""
    filename = getfile(object)
    all_bytecode_suffixes = importlib.machinery.DEBUG_BYTECODE_SUFFIXES[:]
    all_bytecode_suffixes += importlib.machinery.OPTIMIZED_BYTECODE_SUFFIXES[:]
    if any(filename.endswith(s) for s in all_bytecode_suffixes):
        filename = (os.path.splitext(filename)[0] +
                    importlib.machinery.SOURCE_SUFFIXES[0])
    elif any(filename.endswith(s) for s in
                 importlib.machinery.EXTENSION_SUFFIXES):
        return None
    if os.path.exists(filename):
        return filename
    # only return a non-existent filename if the module has a PEP 302 loader
    module = getmodule(object, filename)
    if getattr(module, '__loader__', None) is not None:
        return filename
    elif getattr(getattr(module, ""__spec__"", None), ""loader"", None) is not None:
        return filename
    # or it is in the linecache
    elif filename in linecache.cache:
        return filename","def identify_source_or_compiled_file_of_object(object):
    """"""Work out which source or compiled file an object was defined in.""""""
    if ismodule(object):
        if getattr(object, '__file__', None):
            return object.__file__
        raise TypeError('{!r} is a built-in module'.format(object))
    if isclass(object):
        if hasattr(object, '__module__'):
            module = sys.modules.get(object.__module__)
            if getattr(module, '__file__', None):
                return module.__file__
            if object.__module__ == '__main__':
                raise OSError('source code not available')
        raise TypeError('{!r} is a built-in class'.format(object))
    if ismethod(object):
        object = object.__func__
    if isfunction(object):
        object = object.__code__
    if istraceback(object):
        object = object.tb_frame
    if isframe(object):
        object = object.f_code
    if iscode(object):
        return object.co_filename
    raise TypeError('module, class, method, function, traceback, frame, or '
                    'code object was expected, got {}'.format(
                    type(object).__name__))

def get_source_file(object):
    """"""Return the filename that can be used to locate an object's source.
    Return None if no way can be identified to get the source.
    """"""
    filename = identify_source_or_compiled_file_of_object(object)
    all_bytecode_suffixes = importlib.machinery.DEBUG_BYTECODE_SUFFIXES[:]
    all_bytecode_suffixes += importlib.machinery.OPTIMIZED_BYTECODE_SUFFIXES[:]
    if any(filename.endswith(s) for s in all_bytecode_suffixes):
        filename = (os.path.splitext(filename)[0] +
                    importlib.machinery.SOURCE_SUFFIXES[0])
    elif any(filename.endswith(s) for s in
                 importlib.machinery.EXTENSION_SUFFIXES):
        return None
    if os.path.exists(filename):
        return filename
    # only return a non-existent filename if the module has a PEP 302 loader
    module = getmodule(object, filename)
    if getattr(module, '__loader__', None) is not None:
        return filename
    elif getattr(getattr(module, ""__spec__"", None), ""loader"", None) is not None:
        return filename
    # or it is in the linecache
    elif filename in linecache.cache:
        return filename","def return_absolute_path_to_source_or_compiled_file_for_object(object, _filename=None):
    """"""Return an absolute path to the source or compiled file for an object.

    The idea is for each object to have a unique origin, so this routine
    normalizes the result as much as possible.""""""
    if _filename is None:
        _filename = get_source_file(object) or identify_source_or_compiled_file_of_object(object)
    return os.path.normcase(os.path.abspath(_filename))","{""getfile"": ""identify_source_or_compiled_file_of_object"", ""getsourcefile"": ""get_source_file""}","def 0(object):
    """"""Work out which source or compiled file an object was defined in.""""""
    if ismodule(object):
        if getattr(object, '__file__', None):
            return object.__file__
        raise TypeError('{!r} is a built-in module'.format(object))
    if isclass(object):
        if hasattr(object, '__module__'):
            module = sys.modules.get(object.__module__)
            if getattr(module, '__file__', None):
                return module.__file__
            if object.__module__ == '__main__':
                raise OSError('source code not available')
        raise TypeError('{!r} is a built-in class'.format(object))
    if ismethod(object):
        object = object.__func__
    if isfunction(object):
        object = object.__code__
    if istraceback(object):
        object = object.tb_frame
    if isframe(object):
        object = object.f_code
    if iscode(object):
        return object.co_filename
    raise TypeError('module, class, method, function, traceback, frame, or '
                    'code object was expected, got {}'.format(
                    type(object).__name__))

def 1(object):
    """"""Return the filename that can be used to locate an object's source.
    Return None if no way can be identified to get the source.
    """"""
    filename = 0(object)
    all_bytecode_suffixes = importlib.machinery.DEBUG_BYTECODE_SUFFIXES[:]
    all_bytecode_suffixes += importlib.machinery.OPTIMIZED_BYTECODE_SUFFIXES[:]
    if any(filename.endswith(s) for s in all_bytecode_suffixes):
        filename = (os.path.splitext(filename)[0] +
                    importlib.machinery.SOURCE_SUFFIXES[0])
    elif any(filename.endswith(s) for s in
                 importlib.machinery.EXTENSION_SUFFIXES):
        return None
    if os.path.exists(filename):
        return filename
    # only return a non-existent filename if the module has a PEP 302 loader
    module = getmodule(object, filename)
    if getattr(module, '__loader__', None) is not None:
        return filename
    elif getattr(getattr(module, ""__spec__"", None), ""loader"", None) is not None:
        return filename
    # or it is in the linecache
    elif filename in linecache.cache:
        return filename","def 2(object, _filename=None):
    """"""Return an absolute path to the source or compiled file for an object.

    The idea is for each object to have a unique origin, so this routine
    normalizes the result as much as possible.""""""
    if _filename is None:
        _filename = 1(object) or 0(object)
    return os.path.normcase(os.path.abspath(_filename))","{""getfile"": ""0"", ""getsourcefile"": ""1""}"
130,130,"def walktree(classes, children, parent):
    """"""Recursive helper function for getclasstree().""""""
    results = []
    classes.sort(key=attrgetter('__module__', '__name__'))
    for c in classes:
        results.append((c, c.__bases__))
        if c in children:
            results.append(walktree(children[c], children, c))
    return results","def walktree(classes, children, parent):
    """"""Recursive helper function for getclasstree().""""""
    results = []
    classes.sort(key=attrgetter('__module__', '__name__'))
    for c in classes:
        results.append((c, c.__bases__))
        if c in children:
            results.append(walktree(children[c], children, c))
    return results","def walk_tree(classes, children, parent):
    """"""Recursive helper function for getclasstree().""""""
    results = []
    classes.sort(key=attrgetter('__module__', '__name__'))
    for c in classes:
        results.append((c, c.__bases__))
        if c in children:
            results.append(walk_tree(children[c], children, c))
    return results","def walk_tree(classes, children, parent):
    """"""Recursive helper function for getclasstree().""""""
    results = []
    classes.sort(key=attrgetter('__module__', '__name__'))
    for c in classes:
        results.append((c, c.__bases__))
        if c in children:
            results.append(walk_tree(children[c], children, c))
    return results","{""walktree"": ""walk_tree""}","def 0(classes, children, parent):
    """"""Recursive helper function for getclasstree().""""""
    results = []
    classes.sort(key=attrgetter('__module__', '__name__'))
    for c in classes:
        results.append((c, c.__bases__))
        if c in children:
            results.append(0(children[c], children, c))
    return results","def 0(classes, children, parent):
    """"""Recursive helper function for getclasstree().""""""
    results = []
    classes.sort(key=attrgetter('__module__', '__name__'))
    for c in classes:
        results.append((c, c.__bases__))
        if c in children:
            results.append(0(children[c], children, c))
    return results","{""walktree"": ""0""}"
131,131,"def getclasstree(classes, unique=False):
    """"""Arrange the given list of classes into a hierarchy of nested lists.

    Where a nested list appears, it contains classes derived from the class
    whose entry immediately precedes the list.  Each entry is a 2-tuple
    containing a class and a tuple of its base classes.  If the 'unique'
    argument is true, exactly one entry appears in the returned structure
    for each class in the given list.  Otherwise, classes using multiple
    inheritance and their descendants will appear multiple times.""""""
    children = {}
    roots = []
    for c in classes:
        if c.__bases__:
            for parent in c.__bases__:
                if parent not in children:
                    children[parent] = []
                if c not in children[parent]:
                    children[parent].append(c)
                if unique and parent in classes: break
        elif c not in roots:
            roots.append(c)
    for parent in children:
        if parent not in classes:
            roots.append(parent)
    return walktree(roots, children, None)","def walktree(classes, children, parent):
    """"""Recursive helper function for getclasstree().""""""
    results = []
    classes.sort(key=attrgetter('__module__', '__name__'))
    for c in classes:
        results.append((c, c.__bases__))
        if c in children:
            results.append(walktree(children[c], children, c))
    return results","def walk_tree(classes, children, parent):
    """"""Recursive helper function for getclasstree().""""""
    results = []
    classes.sort(key=attrgetter('__module__', '__name__'))
    for c in classes:
        results.append((c, c.__bases__))
        if c in children:
            results.append(walk_tree(children[c], children, c))
    return results","def arrange_list_of_classes_into_nested_hierarchy_of_lists(classes, unique=False):
    """"""Arrange the given list of classes into a hierarchy of nested lists.

    Where a nested list appears, it contains classes derived from the class
    whose entry immediately precedes the list.  Each entry is a 2-tuple
    containing a class and a tuple of its base classes.  If the 'unique'
    argument is true, exactly one entry appears in the returned structure
    for each class in the given list.  Otherwise, classes using multiple
    inheritance and their descendants will appear multiple times.""""""
    children = {}
    roots = []
    for c in classes:
        if c.__bases__:
            for parent in c.__bases__:
                if parent not in children:
                    children[parent] = []
                if c not in children[parent]:
                    children[parent].append(c)
                if unique and parent in classes: break
        elif c not in roots:
            roots.append(c)
    for parent in children:
        if parent not in classes:
            roots.append(parent)
    return walk_tree(roots, children, None)","{""walktree"": ""walk_tree""}","def 0(classes, children, parent):
    """"""Recursive helper function for getclasstree().""""""
    results = []
    classes.sort(key=attrgetter('__module__', '__name__'))
    for c in classes:
        results.append((c, c.__bases__))
        if c in children:
            results.append(0(children[c], children, c))
    return results","def 1(classes, unique=False):
    """"""Arrange the given list of classes into a hierarchy of nested lists.

    Where a nested list appears, it contains classes derived from the class
    whose entry immediately precedes the list.  Each entry is a 2-tuple
    containing a class and a tuple of its base classes.  If the 'unique'
    argument is true, exactly one entry appears in the returned structure
    for each class in the given list.  Otherwise, classes using multiple
    inheritance and their descendants will appear multiple times.""""""
    children = {}
    roots = []
    for c in classes:
        if c.__bases__:
            for parent in c.__bases__:
                if parent not in children:
                    children[parent] = []
                if c not in children[parent]:
                    children[parent].append(c)
                if unique and parent in classes: break
        elif c not in roots:
            roots.append(c)
    for parent in children:
        if parent not in classes:
            roots.append(parent)
    return 0(roots, children, None)","{""walktree"": ""0""}"
132,132,"def getargs(co):
    """"""Get information about the arguments accepted by a code object.

    Three things are returned: (args, varargs, varkw), where
    'args' is the list of argument names. Keyword-only arguments are
    appended. 'varargs' and 'varkw' are the names of the * and **
    arguments or None.""""""
    if not iscode(co):
        raise TypeError('{!r} is not a code object'.format(co))

    names = co.co_varnames
    nargs = co.co_argcount
    nkwargs = co.co_kwonlyargcount
    args = list(names[:nargs])
    kwonlyargs = list(names[nargs:nargs+nkwargs])
    step = 0

    nargs += nkwargs
    varargs = None
    if co.co_flags & CO_VARARGS:
        varargs = co.co_varnames[nargs]
        nargs = nargs + 1
    varkw = None
    if co.co_flags & CO_VARKEYWORDS:
        varkw = co.co_varnames[nargs]
    return Arguments(args + kwonlyargs, varargs, varkw)","def iscode(object):
    """"""Return true if the object is a code object.

    Code objects provide these attributes:
        co_argcount         number of arguments (not including *, ** args
                            or keyword only arguments)
        co_code             string of raw compiled bytecode
        co_cellvars         tuple of names of cell variables
        co_consts           tuple of constants used in the bytecode
        co_filename         name of file in which this code object was created
        co_firstlineno      number of first line in Python source code
        co_flags            bitmap: 1=optimized | 2=newlocals | 4=*arg | 8=**arg
                            | 16=nested | 32=generator | 64=nofree | 128=coroutine
                            | 256=iterable_coroutine | 512=async_generator
        co_freevars         tuple of names of free variables
        co_posonlyargcount  number of positional only arguments
        co_kwonlyargcount   number of keyword only arguments (not including ** arg)
        co_lnotab           encoded mapping of line numbers to bytecode indices
        co_name             name with which this code object was defined
        co_names            tuple of names other than arguments and function locals
        co_nlocals          number of local variables
        co_stacksize        virtual machine stack space required
        co_varnames         tuple of names of arguments and local variables""""""
    return isinstance(object, types.CodeType)","def check_if_object_is_code_object(object):
    """"""Return true if the object is a code object.

    Code objects provide these attributes:
        co_argcount         number of arguments (not including *, ** args
                            or keyword only arguments)
        co_code             string of raw compiled bytecode
        co_cellvars         tuple of names of cell variables
        co_consts           tuple of constants used in the bytecode
        co_filename         name of file in which this code object was created
        co_firstlineno      number of first line in Python source code
        co_flags            bitmap: 1=optimized | 2=newlocals | 4=*arg | 8=**arg
                            | 16=nested | 32=generator | 64=nofree | 128=coroutine
                            | 256=iterable_coroutine | 512=async_generator
        co_freevars         tuple of names of free variables
        co_posonlyargcount  number of positional only arguments
        co_kwonlyargcount   number of keyword only arguments (not including ** arg)
        co_lnotab           encoded mapping of line numbers to bytecode indices
        co_name             name with which this code object was defined
        co_names            tuple of names other than arguments and function locals
        co_nlocals          number of local variables
        co_stacksize        virtual machine stack space required
        co_varnames         tuple of names of arguments and local variables""""""
    return isinstance(object, types.CodeType)","def get_arguments(co):
    """"""Get information about the arguments accepted by a code object.

    Three things are returned: (args, varargs, varkw), where
    'args' is the list of argument names. Keyword-only arguments are
    appended. 'varargs' and 'varkw' are the names of the * and **
    arguments or None.""""""
    if not check_if_object_is_code_object(co):
        raise TypeError('{!r} is not a code object'.format(co))

    names = co.co_varnames
    nargs = co.co_argcount
    nkwargs = co.co_kwonlyargcount
    args = list(names[:nargs])
    kwonlyargs = list(names[nargs:nargs+nkwargs])
    step = 0

    nargs += nkwargs
    varargs = None
    if co.co_flags & CO_VARARGS:
        varargs = co.co_varnames[nargs]
        nargs = nargs + 1
    varkw = None
    if co.co_flags & CO_VARKEYWORDS:
        varkw = co.co_varnames[nargs]
    return Arguments(args + kwonlyargs, varargs, varkw)","{""iscode"": ""check_if_object_is_code_object""}","def 0(object):
    """"""Return true if the object is a code object.

    Code objects provide these attributes:
        co_argcount         number of arguments (not including *, ** args
                            or keyword only arguments)
        co_code             string of raw compiled bytecode
        co_cellvars         tuple of names of cell variables
        co_consts           tuple of constants used in the bytecode
        co_filename         name of file in which this code object was created
        co_firstlineno      number of first line in Python source code
        co_flags            bitmap: 1=optimized | 2=newlocals | 4=*arg | 8=**arg
                            | 16=nested | 32=generator | 64=nofree | 128=coroutine
                            | 256=iterable_coroutine | 512=async_generator
        co_freevars         tuple of names of free variables
        co_posonlyargcount  number of positional only arguments
        co_kwonlyargcount   number of keyword only arguments (not including ** arg)
        co_lnotab           encoded mapping of line numbers to bytecode indices
        co_name             name with which this code object was defined
        co_names            tuple of names other than arguments and function locals
        co_nlocals          number of local variables
        co_stacksize        virtual machine stack space required
        co_varnames         tuple of names of arguments and local variables""""""
    return isinstance(object, types.CodeType)","def 1(co):
    """"""Get information about the arguments accepted by a code object.

    Three things are returned: (args, varargs, varkw), where
    'args' is the list of argument names. Keyword-only arguments are
    appended. 'varargs' and 'varkw' are the names of the * and **
    arguments or None.""""""
    if not 0(co):
        raise TypeError('{!r} is not a code object'.format(co))

    names = co.co_varnames
    nargs = co.co_argcount
    nkwargs = co.co_kwonlyargcount
    args = list(names[:nargs])
    kwonlyargs = list(names[nargs:nargs+nkwargs])
    step = 0

    nargs += nkwargs
    varargs = None
    if co.co_flags & CO_VARARGS:
        varargs = co.co_varnames[nargs]
        nargs = nargs + 1
    varkw = None
    if co.co_flags & CO_VARKEYWORDS:
        varkw = co.co_varnames[nargs]
    return Arguments(args + kwonlyargs, varargs, varkw)","{""iscode"": ""0""}"
133,133,"def getargvalues(frame):
    """"""Get information about arguments passed into a particular frame.

    A tuple of four things is returned: (args, varargs, varkw, locals).
    'args' is a list of the argument names.
    'varargs' and 'varkw' are the names of the * and ** arguments or None.
    'locals' is the locals dictionary of the given frame.""""""
    args, varargs, varkw = getargs(frame.f_code)
    return ArgInfo(args, varargs, varkw, frame.f_locals)","def getargs(co):
    """"""Get information about the arguments accepted by a code object.

    Three things are returned: (args, varargs, varkw), where
    'args' is the list of argument names. Keyword-only arguments are
    appended. 'varargs' and 'varkw' are the names of the * and **
    arguments or None.""""""
    if not iscode(co):
        raise TypeError('{!r} is not a code object'.format(co))

    names = co.co_varnames
    nargs = co.co_argcount
    nkwargs = co.co_kwonlyargcount
    args = list(names[:nargs])
    kwonlyargs = list(names[nargs:nargs+nkwargs])
    step = 0

    nargs += nkwargs
    varargs = None
    if co.co_flags & CO_VARARGS:
        varargs = co.co_varnames[nargs]
        nargs = nargs + 1
    varkw = None
    if co.co_flags & CO_VARKEYWORDS:
        varkw = co.co_varnames[nargs]
    return Arguments(args + kwonlyargs, varargs, varkw)","def get_arguments(co):
    """"""Get information about the arguments accepted by a code object.

    Three things are returned: (args, varargs, varkw), where
    'args' is the list of argument names. Keyword-only arguments are
    appended. 'varargs' and 'varkw' are the names of the * and **
    arguments or None.""""""
    if not iscode(co):
        raise TypeError('{!r} is not a code object'.format(co))

    names = co.co_varnames
    nargs = co.co_argcount
    nkwargs = co.co_kwonlyargcount
    args = list(names[:nargs])
    kwonlyargs = list(names[nargs:nargs+nkwargs])
    step = 0

    nargs += nkwargs
    varargs = None
    if co.co_flags & CO_VARARGS:
        varargs = co.co_varnames[nargs]
        nargs = nargs + 1
    varkw = None
    if co.co_flags & CO_VARKEYWORDS:
        varkw = co.co_varnames[nargs]
    return Arguments(args + kwonlyargs, varargs, varkw)","def get_information_about_arguments_passed_into_frame(frame):
    """"""Get information about arguments passed into a particular frame.

    A tuple of four things is returned: (args, varargs, varkw, locals).
    'args' is a list of the argument names.
    'varargs' and 'varkw' are the names of the * and ** arguments or None.
    'locals' is the locals dictionary of the given frame.""""""
    args, varargs, varkw = get_arguments(frame.f_code)
    return ArgInfo(args, varargs, varkw, frame.f_locals)","{""getargs"": ""get_arguments""}","def 0(co):
    """"""Get information about the arguments accepted by a code object.

    Three things are returned: (args, varargs, varkw), where
    'args' is the list of argument names. Keyword-only arguments are
    appended. 'varargs' and 'varkw' are the names of the * and **
    arguments or None.""""""
    if not iscode(co):
        raise TypeError('{!r} is not a code object'.format(co))

    names = co.co_varnames
    nargs = co.co_argcount
    nkwargs = co.co_kwonlyargcount
    args = list(names[:nargs])
    kwonlyargs = list(names[nargs:nargs+nkwargs])
    step = 0

    nargs += nkwargs
    varargs = None
    if co.co_flags & CO_VARARGS:
        varargs = co.co_varnames[nargs]
        nargs = nargs + 1
    varkw = None
    if co.co_flags & CO_VARKEYWORDS:
        varkw = co.co_varnames[nargs]
    return Arguments(args + kwonlyargs, varargs, varkw)","def 1(frame):
    """"""Get information about arguments passed into a particular frame.

    A tuple of four things is returned: (args, varargs, varkw, locals).
    'args' is a list of the argument names.
    'varargs' and 'varkw' are the names of the * and ** arguments or None.
    'locals' is the locals dictionary of the given frame.""""""
    args, varargs, varkw = 0(frame.f_code)
    return ArgInfo(args, varargs, varkw, frame.f_locals)","{""getargs"": ""0""}"
134,134,"def getouterframes(frame, context=1):
    """"""Get a list of records for a frame and all higher (calling) frames.

    Each record contains a frame object, filename, line number, function
    name, a list of lines of context, and index within the context.""""""
    framelist = []
    while frame:
        frameinfo = (frame,) + getframeinfo(frame, context)
        framelist.append(FrameInfo(*frameinfo))
        frame = frame.f_back
    return framelist","def getframeinfo(frame, context=1):
    """"""Get information about a frame or traceback object.

    A tuple of five things is returned: the filename, the line number of
    the current line, the function name, a list of lines of context from
    the source code, and the index of the current line within that list.
    The optional second argument specifies the number of lines of context
    to return, which are centered around the current line.""""""
    if istraceback(frame):
        lineno = frame.tb_lineno
        frame = frame.tb_frame
    else:
        lineno = frame.f_lineno
    if not isframe(frame):
        raise TypeError('{!r} is not a frame or traceback object'.format(frame))

    filename = getsourcefile(frame) or getfile(frame)
    if context > 0:
        start = lineno - 1 - context//2
        try:
            lines, lnum = findsource(frame)
        except OSError:
            lines = index = None
        else:
            start = max(0, min(start, len(lines) - context))
            lines = lines[start:start+context]
            index = lineno - 1 - start
    else:
        lines = index = None

    return Traceback(filename, lineno, frame.f_code.co_name, lines, index)","def get_frame_info(frame, context=1):
    """"""Get information about a frame or traceback object.

    A tuple of five things is returned: the filename, the line number of
    the current line, the function name, a list of lines of context from
    the source code, and the index of the current line within that list.
    The optional second argument specifies the number of lines of context
    to return, which are centered around the current line.""""""
    if istraceback(frame):
        lineno = frame.tb_lineno
        frame = frame.tb_frame
    else:
        lineno = frame.f_lineno
    if not isframe(frame):
        raise TypeError('{!r} is not a frame or traceback object'.format(frame))

    filename = getsourcefile(frame) or getfile(frame)
    if context > 0:
        start = lineno - 1 - context//2
        try:
            lines, lnum = findsource(frame)
        except OSError:
            lines = index = None
        else:
            start = max(0, min(start, len(lines) - context))
            lines = lines[start:start+context]
            index = lineno - 1 - start
    else:
        lines = index = None

    return Traceback(filename, lineno, frame.f_code.co_name, lines, index)","def get_outer_frames(frame, context=1):
    """"""Get a list of records for a frame and all higher (calling) frames.

    Each record contains a frame object, filename, line number, function
    name, a list of lines of context, and index within the context.""""""
    framelist = []
    while frame:
        frameinfo = (frame,) + get_frame_info(frame, context)
        framelist.append(FrameInfo(*frameinfo))
        frame = frame.f_back
    return framelist","{""getframeinfo"": ""get_frame_info""}","def 0(frame, context=1):
    """"""Get information about a frame or traceback object.

    A tuple of five things is returned: the filename, the line number of
    the current line, the function name, a list of lines of context from
    the source code, and the index of the current line within that list.
    The optional second argument specifies the number of lines of context
    to return, which are centered around the current line.""""""
    if istraceback(frame):
        lineno = frame.tb_lineno
        frame = frame.tb_frame
    else:
        lineno = frame.f_lineno
    if not isframe(frame):
        raise TypeError('{!r} is not a frame or traceback object'.format(frame))

    filename = getsourcefile(frame) or getfile(frame)
    if context > 0:
        start = lineno - 1 - context//2
        try:
            lines, lnum = findsource(frame)
        except OSError:
            lines = index = None
        else:
            start = max(0, min(start, len(lines) - context))
            lines = lines[start:start+context]
            index = lineno - 1 - start
    else:
        lines = index = None

    return Traceback(filename, lineno, frame.f_code.co_name, lines, index)","def 1(frame, context=1):
    """"""Get a list of records for a frame and all higher (calling) frames.

    Each record contains a frame object, filename, line number, function
    name, a list of lines of context, and index within the context.""""""
    framelist = []
    while frame:
        frameinfo = (frame,) + 0(frame, context)
        framelist.append(FrameInfo(*frameinfo))
        frame = frame.f_back
    return framelist","{""getframeinfo"": ""0""}"
135,135,"def getinnerframes(tb, context=1):
    """"""Get a list of records for a traceback's frame and all lower frames.

    Each record contains a frame object, filename, line number, function
    name, a list of lines of context, and index within the context.""""""
    framelist = []
    while tb:
        frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)
        framelist.append(FrameInfo(*frameinfo))
        tb = tb.tb_next
    return framelist","def getframeinfo(frame, context=1):
    """"""Get information about a frame or traceback object.

    A tuple of five things is returned: the filename, the line number of
    the current line, the function name, a list of lines of context from
    the source code, and the index of the current line within that list.
    The optional second argument specifies the number of lines of context
    to return, which are centered around the current line.""""""
    if istraceback(frame):
        lineno = frame.tb_lineno
        frame = frame.tb_frame
    else:
        lineno = frame.f_lineno
    if not isframe(frame):
        raise TypeError('{!r} is not a frame or traceback object'.format(frame))

    filename = getsourcefile(frame) or getfile(frame)
    if context > 0:
        start = lineno - 1 - context//2
        try:
            lines, lnum = findsource(frame)
        except OSError:
            lines = index = None
        else:
            start = max(0, min(start, len(lines) - context))
            lines = lines[start:start+context]
            index = lineno - 1 - start
    else:
        lines = index = None

    return Traceback(filename, lineno, frame.f_code.co_name, lines, index)","def get_frame_info(frame, context=1):
    """"""Get information about a frame or traceback object.

    A tuple of five things is returned: the filename, the line number of
    the current line, the function name, a list of lines of context from
    the source code, and the index of the current line within that list.
    The optional second argument specifies the number of lines of context
    to return, which are centered around the current line.""""""
    if istraceback(frame):
        lineno = frame.tb_lineno
        frame = frame.tb_frame
    else:
        lineno = frame.f_lineno
    if not isframe(frame):
        raise TypeError('{!r} is not a frame or traceback object'.format(frame))

    filename = getsourcefile(frame) or getfile(frame)
    if context > 0:
        start = lineno - 1 - context//2
        try:
            lines, lnum = findsource(frame)
        except OSError:
            lines = index = None
        else:
            start = max(0, min(start, len(lines) - context))
            lines = lines[start:start+context]
            index = lineno - 1 - start
    else:
        lines = index = None

    return Traceback(filename, lineno, frame.f_code.co_name, lines, index)","def get_inner_frames(tb, context=1):
    """"""Get a list of records for a traceback's frame and all lower frames.

    Each record contains a frame object, filename, line number, function
    name, a list of lines of context, and index within the context.""""""
    framelist = []
    while tb:
        frameinfo = (tb.tb_frame,) + get_frame_info(tb, context)
        framelist.append(FrameInfo(*frameinfo))
        tb = tb.tb_next
    return framelist","{""getframeinfo"": ""get_frame_info""}","def 0(frame, context=1):
    """"""Get information about a frame or traceback object.

    A tuple of five things is returned: the filename, the line number of
    the current line, the function name, a list of lines of context from
    the source code, and the index of the current line within that list.
    The optional second argument specifies the number of lines of context
    to return, which are centered around the current line.""""""
    if istraceback(frame):
        lineno = frame.tb_lineno
        frame = frame.tb_frame
    else:
        lineno = frame.f_lineno
    if not isframe(frame):
        raise TypeError('{!r} is not a frame or traceback object'.format(frame))

    filename = getsourcefile(frame) or getfile(frame)
    if context > 0:
        start = lineno - 1 - context//2
        try:
            lines, lnum = findsource(frame)
        except OSError:
            lines = index = None
        else:
            start = max(0, min(start, len(lines) - context))
            lines = lines[start:start+context]
            index = lineno - 1 - start
    else:
        lines = index = None

    return Traceback(filename, lineno, frame.f_code.co_name, lines, index)","def 1(tb, context=1):
    """"""Get a list of records for a traceback's frame and all lower frames.

    Each record contains a frame object, filename, line number, function
    name, a list of lines of context, and index within the context.""""""
    framelist = []
    while tb:
        frameinfo = (tb.tb_frame,) + 0(tb, context)
        framelist.append(FrameInfo(*frameinfo))
        tb = tb.tb_next
    return framelist","{""getframeinfo"": ""0""}"
136,136,"def stack(context=1):
    """"""Return a list of records for the stack above the caller's frame.""""""
    return getouterframes(sys._getframe(1), context)","def getouterframes(frame, context=1):
    """"""Get a list of records for a frame and all higher (calling) frames.

    Each record contains a frame object, filename, line number, function
    name, a list of lines of context, and index within the context.""""""
    framelist = []
    while frame:
        frameinfo = (frame,) + getframeinfo(frame, context)
        framelist.append(FrameInfo(*frameinfo))
        frame = frame.f_back
    return framelist","def get_outer_frames(frame, context=1):
    """"""Get a list of records for a frame and all higher (calling) frames.

    Each record contains a frame object, filename, line number, function
    name, a list of lines of context, and index within the context.""""""
    framelist = []
    while frame:
        frameinfo = (frame,) + getframeinfo(frame, context)
        framelist.append(FrameInfo(*frameinfo))
        frame = frame.f_back
    return framelist","def return_list_of_records_for_stack_above_callers_frame(context=1):
    """"""Return a list of records for the return_list_of_records_for_stack_above_callers_frame above the caller's frame.""""""
    return get_outer_frames(sys._getframe(1), context)","{""getouterframes"": ""get_outer_frames""}","def 0(frame, context=1):
    """"""Get a list of records for a frame and all higher (calling) frames.

    Each record contains a frame object, filename, line number, function
    name, a list of lines of context, and index within the context.""""""
    framelist = []
    while frame:
        frameinfo = (frame,) + getframeinfo(frame, context)
        framelist.append(FrameInfo(*frameinfo))
        frame = frame.f_back
    return framelist","def 1(context=1):
    """"""Return a list of records for the stack above the caller's frame.""""""
    return 0(sys._getframe(1), context)","{""getouterframes"": ""0""}"
137,137,"def trace(context=1):
    """"""Return a list of records for the stack below the current exception.""""""
    return getinnerframes(sys.exc_info()[2], context)","def getinnerframes(tb, context=1):
    """"""Get a list of records for a traceback's frame and all lower frames.

    Each record contains a frame object, filename, line number, function
    name, a list of lines of context, and index within the context.""""""
    framelist = []
    while tb:
        frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)
        framelist.append(FrameInfo(*frameinfo))
        tb = tb.tb_next
    return framelist","def get_inner_frames(tb, context=1):
    """"""Get a list of records for a traceback's frame and all lower frames.

    Each record contains a frame object, filename, line number, function
    name, a list of lines of context, and index within the context.""""""
    framelist = []
    while tb:
        frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)
        framelist.append(FrameInfo(*frameinfo))
        tb = tb.tb_next
    return framelist","def return_list_of_records_for_stack_below_current_exception(context=1):
    """"""Return a list of records for the stack below the current exception.""""""
    return get_inner_frames(sys.exc_info()[2], context)","{""getinnerframes"": ""get_inner_frames""}","def 0(tb, context=1):
    """"""Get a list of records for a traceback's frame and all lower frames.

    Each record contains a frame object, filename, line number, function
    name, a list of lines of context, and index within the context.""""""
    framelist = []
    while tb:
        frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)
        framelist.append(FrameInfo(*frameinfo))
        tb = tb.tb_next
    return framelist","def 1(context=1):
    """"""Return a list of records for the stack below the current exception.""""""
    return 0(sys.exc_info()[2], context)","{""getinnerframes"": ""0""}"
138,138,"def getgeneratorlocals(generator):
    """"""
    Get the mapping of generator local variables to their current values.

    A dict is returned, with the keys the local variable names and values the
    bound values.""""""

    if not isgenerator(generator):
        raise TypeError(""{!r} is not a Python generator"".format(generator))

    frame = getattr(generator, ""gi_frame"", None)
    if frame is not None:
        return generator.gi_frame.f_locals
    else:
        return {}","def isgenerator(object):
    """"""Return true if the object is a generator.

    Generator objects provide these attributes:
        __iter__        defined to support iteration over container
        close           raises a new GeneratorExit exception inside the
                        generator to terminate the iteration
        gi_code         code object
        gi_frame        frame object or possibly None once the generator has
                        been exhausted
        gi_running      set to 1 when generator is executing, 0 otherwise
        next            return the next item from the container
        send            resumes the generator and ""sends"" a value that becomes
                        the result of the current yield-expression
        throw           used to raise an exception inside the generator""""""
    return isinstance(object, types.GeneratorType)","def validate_if_object_is_generator(object):
    """"""Return true if the object is a generator.

    Generator objects provide these attributes:
        __iter__        defined to support iteration over container
        close           raises a new GeneratorExit exception inside the
                        generator to terminate the iteration
        gi_code         code object
        gi_frame        frame object or possibly None once the generator has
                        been exhausted
        gi_running      set to 1 when generator is executing, 0 otherwise
        next            return the next item from the container
        send            resumes the generator and ""sends"" a value that becomes
                        the result of the current yield-expression
        throw           used to raise an exception inside the generator""""""
    return isinstance(object, types.GeneratorType)","def get_mapping_of_generator_local_variables_to_values(generator):
    """"""
    Get the mapping of generator local variables to their current values.

    A dict is returned, with the keys the local variable names and values the
    bound values.""""""

    if not validate_if_object_is_generator(generator):
        raise TypeError(""{!r} is not a Python generator"".format(generator))

    frame = getattr(generator, ""gi_frame"", None)
    if frame is not None:
        return generator.gi_frame.f_locals
    else:
        return {}","{""isgenerator"": ""validate_if_object_is_generator""}","def 0(object):
    """"""Return true if the object is a generator.

    Generator objects provide these attributes:
        __iter__        defined to support iteration over container
        close           raises a new GeneratorExit exception inside the
                        generator to terminate the iteration
        gi_code         code object
        gi_frame        frame object or possibly None once the generator has
                        been exhausted
        gi_running      set to 1 when generator is executing, 0 otherwise
        next            return the next item from the container
        send            resumes the generator and ""sends"" a value that becomes
                        the result of the current yield-expression
        throw           used to raise an exception inside the generator""""""
    return isinstance(object, types.GeneratorType)","def 1(generator):
    """"""
    Get the mapping of generator local variables to their current values.

    A dict is returned, with the keys the local variable names and values the
    bound values.""""""

    if not 0(generator):
        raise TypeError(""{!r} is not a Python generator"".format(generator))

    frame = getattr(generator, ""gi_frame"", None)
    if frame is not None:
        return generator.gi_frame.f_locals
    else:
        return {}","{""isgenerator"": ""0""}"
139,139,"def _signature_is_builtin(obj):
    """"""Private helper to test if `obj` is a callable that might
    support Argument Clinic's __text_signature__ protocol.
    """"""
    return (isbuiltin(obj) or
            ismethoddescriptor(obj) or
            isinstance(obj, _NonUserDefinedCallables) or
            # Can't test 'isinstance(type)' here, as it would
            # also be True for regular python classes
            obj in (type, object))","def isbuiltin(object):
    """"""Return true if the object is a built-in function or method.

    Built-in functions and methods provide these attributes:
        __doc__         documentation string
        __name__        original name of this function or method
        __self__        instance to which a method is bound, or None""""""
    return isinstance(object, types.BuiltinFunctionType)

def ismethoddescriptor(object):
    """"""Return true if the object is a method descriptor.

    But not if ismethod() or isclass() or isfunction() are true.

    This is new in Python 2.2, and, for example, is true of int.__add__.
    An object passing this test has a __get__ attribute but not a __set__
    attribute, but beyond that the set of attributes varies.  __name__ is
    usually sensible, and __doc__ often is.

    Methods implemented via descriptors that also pass one of the other
    tests return false from the ismethoddescriptor() test, simply because
    the other tests promise more -- you can, e.g., count on having the
    __func__ attribute (etc) when an object passes ismethod().""""""
    if isclass(object) or ismethod(object) or isfunction(object):
        # mutual exclusion
        return False
    tp = type(object)
    return hasattr(tp, ""__get__"") and not hasattr(tp, ""__set__"")","def is_builtin_function_or_method(object):
    """"""Return true if the object is a built-in function or method.

    Built-in functions and methods provide these attributes:
        __doc__         documentation string
        __name__        original name of this function or method
        __self__        instance to which a method is bound, or None""""""
    return isinstance(object, types.BuiltinFunctionType)

def is_method_descriptor(object):
    """"""Return true if the object is a method descriptor.

    But not if ismethod() or isclass() or isfunction() are true.

    This is new in Python 2.2, and, for example, is true of int.__add__.
    An object passing this test has a __get__ attribute but not a __set__
    attribute, but beyond that the set of attributes varies.  __name__ is
    usually sensible, and __doc__ often is.

    Methods implemented via descriptors that also pass one of the other
    tests return false from the is_method_descriptor() test, simply because
    the other tests promise more -- you can, e.g., count on having the
    __func__ attribute (etc) when an object passes ismethod().""""""
    if isclass(object) or ismethod(object) or isfunction(object):
        # mutual exclusion
        return False
    tp = type(object)
    return hasattr(tp, ""__get__"") and not hasattr(tp, ""__set__"")","def is_builtin_signature(obj):
    """"""Private helper to test if `obj` is a callable that might
    support Argument Clinic's __text_signature__ protocol.
    """"""
    return (is_builtin_function_or_method(obj) or
            is_method_descriptor(obj) or
            isinstance(obj, _NonUserDefinedCallables) or
            # Can't test 'isinstance(type)' here, as it would
            # also be True for regular python classes
            obj in (type, object))","{""isbuiltin"": ""is_builtin_function_or_method"", ""ismethoddescriptor"": ""is_method_descriptor""}","def 0(object):
    """"""Return true if the object is a built-in function or method.

    Built-in functions and methods provide these attributes:
        __doc__         documentation string
        __name__        original name of this function or method
        __self__        instance to which a method is bound, or None""""""
    return isinstance(object, types.BuiltinFunctionType)

def 1(object):
    """"""Return true if the object is a method descriptor.

    But not if ismethod() or isclass() or isfunction() are true.

    This is new in Python 2.2, and, for example, is true of int.__add__.
    An object passing this test has a __get__ attribute but not a __set__
    attribute, but beyond that the set of attributes varies.  __name__ is
    usually sensible, and __doc__ often is.

    Methods implemented via descriptors that also pass one of the other
    tests return false from the 1() test, simply because
    the other tests promise more -- you can, e.g., count on having the
    __func__ attribute (etc) when an object passes ismethod().""""""
    if isclass(object) or ismethod(object) or isfunction(object):
        # mutual exclusion
        return False
    tp = type(object)
    return hasattr(tp, ""__get__"") and not hasattr(tp, ""__set__"")","def 2(obj):
    """"""Private helper to test if `obj` is a callable that might
    support Argument Clinic's __text_signature__ protocol.
    """"""
    return (0(obj) or
            1(obj) or
            isinstance(obj, _NonUserDefinedCallables) or
            # Can't test 'isinstance(type)' here, as it would
            # also be True for regular python classes
            obj in (type, object))","{""isbuiltin"": ""0"", ""ismethoddescriptor"": ""1""}"
140,140,"def _signature_is_functionlike(obj):
    """"""Private helper to test if `obj` is a duck type of FunctionType.
    A good example of such objects are functions compiled with
    Cython, which have all attributes that a pure Python function
    would have, but have their code statically compiled.
    """"""

    if not callable(obj) or isclass(obj):
        # All function-like objects are obviously callables,
        # and not classes.
        return False

    name = getattr(obj, '__name__', None)
    code = getattr(obj, '__code__', None)
    defaults = getattr(obj, '__defaults__', _void) # Important to use _void ...
    kwdefaults = getattr(obj, '__kwdefaults__', _void) # ... and not None here
    annotations = getattr(obj, '__annotations__', None)

    return (isinstance(code, types.CodeType) and
            isinstance(name, str) and
            (defaults is None or isinstance(defaults, tuple)) and
            (kwdefaults is None or isinstance(kwdefaults, dict)) and
            (isinstance(annotations, (dict)) or annotations is None) )","def isclass(object):
    """"""Return true if the object is a class.

    Class objects provide these attributes:
        __doc__         documentation string
        __module__      name of module in which this class was defined""""""
    return isinstance(object, type)","def check_if_object_is_class(object):
    """"""Return true if the object is a class.

    Class objects provide these attributes:
        __doc__         documentation string
        __module__      name of module in which this class was defined""""""
    return isinstance(object, type)","def test_if_object_is_duck_type_of_function_type(obj):
    """"""Private helper to test if `obj` is a duck type of FunctionType.
    A good example of such objects are functions compiled with
    Cython, which have all attributes that a pure Python function
    would have, but have their code statically compiled.
    """"""

    if not callable(obj) or check_if_object_is_class(obj):
        # All function-like objects are obviously callables,
        # and not classes.
        return False

    name = getattr(obj, '__name__', None)
    code = getattr(obj, '__code__', None)
    defaults = getattr(obj, '__defaults__', _void) # Important to use _void ...
    kwdefaults = getattr(obj, '__kwdefaults__', _void) # ... and not None here
    annotations = getattr(obj, '__annotations__', None)

    return (isinstance(code, types.CodeType) and
            isinstance(name, str) and
            (defaults is None or isinstance(defaults, tuple)) and
            (kwdefaults is None or isinstance(kwdefaults, dict)) and
            (isinstance(annotations, (dict)) or annotations is None) )","{""isclass"": ""check_if_object_is_class""}","def 0(object):
    """"""Return true if the object is a class.

    Class objects provide these attributes:
        __doc__         documentation string
        __module__      name of module in which this class was defined""""""
    return isinstance(object, type)","def 1(obj):
    """"""Private helper to test if `obj` is a duck type of FunctionType.
    A good example of such objects are functions compiled with
    Cython, which have all attributes that a pure Python function
    would have, but have their code statically compiled.
    """"""

    if not callable(obj) or 0(obj):
        # All function-like objects are obviously callables,
        # and not classes.
        return False

    name = getattr(obj, '__name__', None)
    code = getattr(obj, '__code__', None)
    defaults = getattr(obj, '__defaults__', _void) # Important to use _void ...
    kwdefaults = getattr(obj, '__kwdefaults__', _void) # ... and not None here
    annotations = getattr(obj, '__annotations__', None)

    return (isinstance(code, types.CodeType) and
            isinstance(name, str) and
            (defaults is None or isinstance(defaults, tuple)) and
            (kwdefaults is None or isinstance(kwdefaults, dict)) and
            (isinstance(annotations, (dict)) or annotations is None) )","{""isclass"": ""0""}"
141,141,"def _signature_from_builtin(cls, func, skip_bound_arg=True):
    """"""Private helper function to get signature for
    builtin callables.
    """"""

    if not _signature_is_builtin(func):
        raise TypeError(""{!r} is not a Python builtin ""
                        ""function"".format(func))

    s = getattr(func, ""__text_signature__"", None)
    if not s:
        raise ValueError(""no signature found for builtin {!r}"".format(func))

    return _signature_fromstr(cls, func, s, skip_bound_arg)","def _signature_is_builtin(obj):
    """"""Private helper to test if `obj` is a callable that might
    support Argument Clinic's __text_signature__ protocol.
    """"""
    return (isbuiltin(obj) or
            ismethoddescriptor(obj) or
            isinstance(obj, _NonUserDefinedCallables) or
            # Can't test 'isinstance(type)' here, as it would
            # also be True for regular python classes
            obj in (type, object))

def _signature_fromstr(cls, obj, s, skip_bound_arg=True):
    """"""Private helper to parse content of '__text_signature__'
    and return a Signature based on it.
    """"""
    # Lazy import ast because it's relatively heavy and
    # it's not used for other than this function.
    import ast

    Parameter = cls._parameter_cls

    clean_signature, self_parameter, last_positional_only = \
        _signature_strip_non_python_syntax(s)

    program = ""def foo"" + clean_signature + "": pass""

    try:
        module = ast.parse(program)
    except SyntaxError:
        module = None

    if not isinstance(module, ast.Module):
        raise ValueError(""{!r} builtin has invalid signature"".format(obj))

    f = module.body[0]

    parameters = []
    empty = Parameter.empty
    invalid = object()

    module = None
    module_dict = {}
    module_name = getattr(obj, '__module__', None)
    if module_name:
        module = sys.modules.get(module_name, None)
        if module:
            module_dict = module.__dict__
    sys_module_dict = sys.modules.copy()

    def parse_name(node):
        assert isinstance(node, ast.arg)
        if node.annotation is not None:
            raise ValueError(""Annotations are not currently supported"")
        return node.arg

    def wrap_value(s):
        try:
            value = eval(s, module_dict)
        except NameError:
            try:
                value = eval(s, sys_module_dict)
            except NameError:
                raise RuntimeError()

        if isinstance(value, (str, int, float, bytes, bool, type(None))):
            return ast.Constant(value)
        raise RuntimeError()

    class RewriteSymbolics(ast.NodeTransformer):
        def visit_Attribute(self, node):
            a = []
            n = node
            while isinstance(n, ast.Attribute):
                a.append(n.attr)
                n = n.value
            if not isinstance(n, ast.Name):
                raise RuntimeError()
            a.append(n.id)
            value = ""."".join(reversed(a))
            return wrap_value(value)

        def visit_Name(self, node):
            if not isinstance(node.ctx, ast.Load):
                raise ValueError()
            return wrap_value(node.id)

    def p(name_node, default_node, default=empty):
        name = parse_name(name_node)
        if name is invalid:
            return None
        if default_node and default_node is not _empty:
            try:
                default_node = RewriteSymbolics().visit(default_node)
                o = ast.literal_eval(default_node)
            except ValueError:
                o = invalid
            if o is invalid:
                return None
            default = o if o is not invalid else default
        parameters.append(Parameter(name, kind, default=default, annotation=empty))

    # non-keyword-only parameters
    args = reversed(f.args.args)
    defaults = reversed(f.args.defaults)
    iter = itertools.zip_longest(args, defaults, fillvalue=None)
    if last_positional_only is not None:
        kind = Parameter.POSITIONAL_ONLY
    else:
        kind = Parameter.POSITIONAL_OR_KEYWORD
    for i, (name, default) in enumerate(reversed(list(iter))):
        p(name, default)
        if i == last_positional_only:
            kind = Parameter.POSITIONAL_OR_KEYWORD

    # *args
    if f.args.vararg:
        kind = Parameter.VAR_POSITIONAL
        p(f.args.vararg, empty)

    # keyword-only arguments
    kind = Parameter.KEYWORD_ONLY
    for name, default in zip(f.args.kwonlyargs, f.args.kw_defaults):
        p(name, default)

    # **kwargs
    if f.args.kwarg:
        kind = Parameter.VAR_KEYWORD
        p(f.args.kwarg, empty)

    if self_parameter is not None:
        # Possibly strip the bound argument:
        #    - We *always* strip first bound argument if
        #      it is a module.
        #    - We don't strip first bound argument if
        #      skip_bound_arg is False.
        assert parameters
        _self = getattr(obj, '__self__', None)
        self_isbound = _self is not None
        self_ismodule = ismodule(_self)
        if self_isbound and (self_ismodule or skip_bound_arg):
            parameters.pop(0)
        else:
            # for builtins, self parameter is always positional-only!
            p = parameters[0].replace(kind=Parameter.POSITIONAL_ONLY)
            parameters[0] = p

    return cls(parameters, return_annotation=cls.empty)","def is_builtin_signature(obj):
    """"""Private helparse_default_parameterer to test if `obj` is a callable that might
    suparse_default_parameterparse_default_parameterort Argument Clinic's __text_signature__ parse_default_parameterrotocol.
    """"""
    return (isbuiltin(obj) or
            ismethoddescriparse_default_parametertor(obj) or
            isinstance(obj, _NonUserDefinedCallables) or
            # Can't test 'isinstance(typarse_default_parametere)' here, as it would
            # also be True for regular parse_default_parameterython classes
            obj in (typarse_default_parametere, object))

def parse_default_parameterarse_signature_from_string(cls, obj, s, skiparse_default_parameter_bound_arg=True):
    """"""Private helparse_default_parameterer to parse_default_parameterarse content of '__text_signature__'
    and return a Signature based on it.
    """"""
    # Lazy imparse_default_parameterort ast because it's relatively heavy and
    # it's not used for other than this function.
    imparse_default_parameterort ast

    Parameter = cls._parse_default_parameterarameter_cls

    clean_signature, self_parse_default_parameterarameter, last_parse_default_parameterositional_only = \
        _signature_striparse_default_parameter_non_parse_default_parameterython_syntax(s)

    parse_default_parameterrogram = ""def foo"" + clean_signature + "": parse_default_parameterass""

    try:
        module = ast.parse_default_parameterarse(parse_default_parameterrogram)
    exceparse_default_parametert SyntaxError:
        module = None

    if not isinstance(module, ast.Module):
        raise ValueError(""{!r} builtin has invalid signature"".format(obj))

    f = module.body[0]

    parse_default_parameterarameters = []
    emparse_default_parameterty = Parameter.emparse_default_parameterty
    invalid = object()

    module = None
    module_dict = {}
    module_name = getattr(obj, '__module__', None)
    if module_name:
        module = sys.modules.get(module_name, None)
        if module:
            module_dict = module.__dict__
    sys_module_dict = sys.modules.coparse_default_parametery()

    def parse_default_parameterarse_name(node):
        assert isinstance(node, ast.arg)
        if node.annotation is not None:
            raise ValueError(""Annotations are not currently suparse_default_parameterparse_default_parameterorted"")
        return node.arg

    def wraparse_default_parameter_value(s):
        try:
            value = eval(s, module_dict)
        exceparse_default_parametert NameError:
            try:
                value = eval(s, sys_module_dict)
            exceparse_default_parametert NameError:
                raise RuntimeError()

        if isinstance(value, (str, int, float, bytes, bool, typarse_default_parametere(None))):
            return ast.Constant(value)
        raise RuntimeError()

    class RewriteSymbolics(ast.NodeTransformer):
        def visit_Attribute(self, node):
            a = []
            n = node
            while isinstance(n, ast.Attribute):
                a.aparse_default_parameterparse_default_parameterend(n.attr)
                n = n.value
            if not isinstance(n, ast.Name):
                raise RuntimeError()
            a.aparse_default_parameterparse_default_parameterend(n.id)
            value = ""."".join(reversed(a))
            return wraparse_default_parameter_value(value)

        def visit_Name(self, node):
            if not isinstance(node.ctx, ast.Load):
                raise ValueError()
            return wraparse_default_parameter_value(node.id)

    def parse_default_parameter(name_node, default_node, default=emparse_default_parameterty):
        name = parse_default_parameterarse_name(name_node)
        if name is invalid:
            return None
        if default_node and default_node is not _emparse_default_parameterty:
            try:
                default_node = RewriteSymbolics().visit(default_node)
                o = ast.literal_eval(default_node)
            exceparse_default_parametert ValueError:
                o = invalid
            if o is invalid:
                return None
            default = o if o is not invalid else default
        parse_default_parameterarameters.aparse_default_parameterparse_default_parameterend(Parameter(name, kind, default=default, annotation=emparse_default_parameterty))

    # non-keyword-only parse_default_parameterarameters
    args = reversed(f.args.args)
    defaults = reversed(f.args.defaults)
    iter = itertools.ziparse_default_parameter_longest(args, defaults, fillvalue=None)
    if last_parse_default_parameterositional_only is not None:
        kind = Parameter.POSITIONAL_ONLY
    else:
        kind = Parameter.POSITIONAL_OR_KEYWORD
    for i, (name, default) in enumerate(reversed(list(iter))):
        parse_default_parameter(name, default)
        if i == last_parse_default_parameterositional_only:
            kind = Parameter.POSITIONAL_OR_KEYWORD

    # *args
    if f.args.vararg:
        kind = Parameter.VAR_POSITIONAL
        parse_default_parameter(f.args.vararg, emparse_default_parameterty)

    # keyword-only arguments
    kind = Parameter.KEYWORD_ONLY
    for name, default in ziparse_default_parameter(f.args.kwonlyargs, f.args.kw_defaults):
        parse_default_parameter(name, default)

    # **kwargs
    if f.args.kwarg:
        kind = Parameter.VAR_KEYWORD
        parse_default_parameter(f.args.kwarg, emparse_default_parameterty)

    if self_parse_default_parameterarameter is not None:
        # Possibly striparse_default_parameter the bound argument:
        #    - We *always* striparse_default_parameter first bound argument if
        #      it is a module.
        #    - We don't striparse_default_parameter first bound argument if
        #      skiparse_default_parameter_bound_arg is False.
        assert parse_default_parameterarameters
        _self = getattr(obj, '__self__', None)
        self_isbound = _self is not None
        self_ismodule = ismodule(_self)
        if self_isbound and (self_ismodule or skiparse_default_parameter_bound_arg):
            parse_default_parameterarameters.parse_default_parameteroparse_default_parameter(0)
        else:
            # for builtins, self parse_default_parameterarameter is always parse_default_parameterositional-only!
            parse_default_parameter = parse_default_parameterarameters[0].reparse_default_parameterlace(kind=Parameter.POSITIONAL_ONLY)
            parse_default_parameterarameters[0] = parse_default_parameter

    return cls(parse_default_parameterarameters, return_annotation=cls.emparse_default_parameterty)","def get_signature_for_builtin_callables(cls, func, skiparse_default_parameter_bound_arg=True):
    """"""Private helparse_default_parameterer function to get signature for
    builtin callables.
    """"""

    if not is_builtin_signature(func):
        raise Typarse_default_parametereError(""{!r} is not a Python builtin ""
                        ""function"".format(func))

    s = getattr(func, ""__text_signature__"", None)
    if not s:
        raise ValueError(""no signature found for builtin {!r}"".format(func))

    return parse_default_parameterarse_signature_from_string(cls, func, s, skiparse_default_parameter_bound_arg)","{""_signature_is_builtin"": ""is_builtin_signature"", ""_signature_fromstr"": ""parse_signature_from_string"", ""p"": ""parse_default_parameter""}","def 0(obj):
    """"""Private helper to test if `obj` is a callable that might
    support Argument Clinic's __text_signature__ protocol.
    """"""
    return (isbuiltin(obj) or
            ismethoddescriptor(obj) or
            isinstance(obj, _NonUserDefinedCallables) or
            # Can't test 'isinstance(type)' here, as it would
            # also be True for regular python classes
            obj in (type, object))

def 1(cls, obj, s, skip_bound_arg=True):
    """"""Private helper to parse content of '__text_signature__'
    and return a Signature based on it.
    """"""
    # Lazy import ast because it's relatively heavy and
    # it's not used for other than this function.
    import ast

    Parameter = cls._parameter_cls

    clean_signature, self_parameter, last_positional_only = \
        _signature_strip_non_python_syntax(s)

    program = ""def foo"" + clean_signature + "": pass""

    try:
        module = ast.parse(program)
    except SyntaxError:
        module = None

    if not isinstance(module, ast.Module):
        raise ValueError(""{!r} builtin has invalid signature"".format(obj))

    f = module.body[0]

    parameters = []
    empty = Parameter.empty
    invalid = object()

    module = None
    module_dict = {}
    module_name = getattr(obj, '__module__', None)
    if module_name:
        module = sys.modules.get(module_name, None)
        if module:
            module_dict = module.__dict__
    sys_module_dict = sys.modules.copy()

    def parse_name(node):
        assert isinstance(node, ast.arg)
        if node.annotation is not None:
            raise ValueError(""Annotations are not currently supported"")
        return node.arg

    def wrap_value(s):
        try:
            value = eval(s, module_dict)
        except NameError:
            try:
                value = eval(s, sys_module_dict)
            except NameError:
                raise RuntimeError()

        if isinstance(value, (str, int, float, bytes, bool, type(None))):
            return ast.Constant(value)
        raise RuntimeError()

    class RewriteSymbolics(ast.NodeTransformer):
        def visit_Attribute(self, node):
            a = []
            n = node
            while isinstance(n, ast.Attribute):
                a.append(n.attr)
                n = n.value
            if not isinstance(n, ast.Name):
                raise RuntimeError()
            a.append(n.id)
            value = ""."".join(reversed(a))
            return wrap_value(value)

        def visit_Name(self, node):
            if not isinstance(node.ctx, ast.Load):
                raise ValueError()
            return wrap_value(node.id)

    def 6(name_node, default_node, default=empty):
        name = parse_name(name_node)
        if name is invalid:
            return None
        if default_node and default_node is not _empty:
            try:
                default_node = RewriteSymbolics().visit(default_node)
                o = ast.literal_eval(default_node)
            except ValueError:
                o = invalid
            if o is invalid:
                return None
            default = o if o is not invalid else default
        parameters.append(Parameter(name, kind, default=default, annotation=empty))

    # non-keyword-only parameters
    args = reversed(f.args.args)
    defaults = reversed(f.args.defaults)
    iter = itertools.zip_longest(args, defaults, fillvalue=None)
    if last_positional_only is not None:
        kind = Parameter.POSITIONAL_ONLY
    else:
        kind = Parameter.POSITIONAL_OR_KEYWORD
    for i, (name, default) in enumerate(reversed(list(iter))):
        6(name, default)
        if i == last_positional_only:
            kind = Parameter.POSITIONAL_OR_KEYWORD

    # *args
    if f.args.vararg:
        kind = Parameter.VAR_POSITIONAL
        6(f.args.vararg, empty)

    # keyword-only arguments
    kind = Parameter.KEYWORD_ONLY
    for name, default in zi6(f.args.kwonlyargs, f.args.kw_defaults):
        6(name, default)

    # **kwargs
    if f.args.kwarg:
        kind = Parameter.VAR_KEYWORD
        6(f.args.kwarg, empty)

    if self_parameter is not None:
        # Possibly strip the bound argument:
        #    - We *always* strip first bound argument if
        #      it is a module.
        #    - We don't strip first bound argument if
        #      skip_bound_arg is False.
        assert parameters
        _self = getattr(obj, '__self__', None)
        self_isbound = _self is not None
        self_ismodule = ismodule(_self)
        if self_isbound and (self_ismodule or skip_bound_arg):
            parameters.po6(0)
        else:
            # for builtins, self parameter is always positional-only!
            p = parameters[0].replace(kind=Parameter.POSITIONAL_ONLY)
            parameters[0] = p

    return cls(parameters, return_annotation=cls.empty)","def 7(cls, func, skip_bound_arg=True):
    """"""Private helper function to get signature for
    builtin callables.
    """"""

    if not 0(func):
        raise TypeError(""{!r} is not a Python builtin ""
                        ""function"".format(func))

    s = getattr(func, ""__text_signature__"", None)
    if not s:
        raise ValueError(""no signature found for builtin {!r}"".format(func))

    return 1(cls, func, s, skip_bound_arg)","{""_signature_is_builtin"": ""0"", ""_signature_fromstr"": ""1"", ""p"": ""6""}"
142,142,"def binhex(inp, out):
    """"""binhex(infilename, outfilename): create binhex-encoded copy of a file""""""
    finfo = getfileinfo(inp)
    ofp = BinHex(finfo, out)

    with io.open(inp, 'rb') as ifp:
        # XXXX Do textfile translation on non-mac systems
        while True:
            d = ifp.read(128000)
            if not d: break
            ofp.write(d)
        ofp.close_data()

    ifp = openrsrc(inp, 'rb')
    while True:
        d = ifp.read(128000)
        if not d: break
        ofp.write_rsrc(d)
    ofp.close()
    ifp.close()","def getfileinfo(name):
    finfo = FInfo()
    with io.open(name, 'rb') as fp:
        # Quick check for textfile
        data = fp.read(512)
        if 0 not in data:
            finfo.Type = 'TEXT'
        fp.seek(0, 2)
        dsize = fp.tell()
    dir, file = os.path.split(name)
    file = file.replace(':', '-', 1)
    return file, finfo, dsize, 0","def get_file_info(name):
    finfo = FInfo()
    with io.open(name, 'rb') as fp:
        # Quick check for textfile
        data = fp.read(512)
        if 0 not in data:
            finfo.Type = 'TEXT'
        fp.seek(0, 2)
        dsize = fp.tell()
    dir, file = os.path.split(name)
    file = file.replace(':', '-', 1)
    return file, finfo, dsize, 0","def create_binhex_encoded_copy_of_file(inp, out):
    """"""create_binhex_encoded_copy_of_file(infilename, outfilename): create create_binhex_encoded_copy_of_file-encoded copy of a file""""""
    finfo = get_file_info(inp)
    ofp = BinHex(finfo, out)

    with io.open(inp, 'rb') as ifp:
        # XXXX Do textfile translation on non-mac systems
        while True:
            d = ifp.read(128000)
            if not d: break
            ofp.write(d)
        ofp.close_data()

    ifp = openrsrc(inp, 'rb')
    while True:
        d = ifp.read(128000)
        if not d: break
        ofp.write_rsrc(d)
    ofp.close()
    ifp.close()","{""getfileinfo"": ""get_file_info""}","def 0(name):
    finfo = FInfo()
    with io.open(name, 'rb') as fp:
        # Quick check for textfile
        data = fp.read(512)
        if 0 not in data:
            finfo.Type = 'TEXT'
        fp.seek(0, 2)
        dsize = fp.tell()
    dir, file = os.path.split(name)
    file = file.replace(':', '-', 1)
    return file, finfo, dsize, 0","def 1(inp, out):
    """"""1(infilename, outfilename): create binhex-encoded copy of a file""""""
    finfo = 0(inp)
    ofp = BinHex(finfo, out)

    with io.open(inp, 'rb') as ifp:
        # XXXX Do textfile translation on non-mac systems
        while True:
            d = ifp.read(128000)
            if not d: break
            ofp.write(d)
        ofp.close_data()

    ifp = openrsrc(inp, 'rb')
    while True:
        d = ifp.read(128000)
        if not d: break
        ofp.write_rsrc(d)
    ofp.close()
    ifp.close()","{""getfileinfo"": ""0""}"
143,143,"def read_stringnl_noescape_pair(f):
    r""""""
    >>> import io
    >>> read_stringnl_noescape_pair(io.BytesIO(b""Queue\nEmpty\njunk""))
    'Queue Empty'
    """"""

    return ""%s %s"" % (read_stringnl_noescape(f), read_stringnl_noescape(f))","def read_stringnl_noescape(f):
    return read_stringnl(f, stripquotes=False)","def read_string_no_escape_nl(f):
    return read_stringnl(f, stripquotes=False)","def read_string_no_escape_nl_pair(f):
    r""""""
    >>> import io
    >>> read_string_no_escape_nl_pair(io.BytesIO(b""Queue\nEmpty\njunk""))
    'Queue Empty'
    """"""

    return ""%s %s"" % (read_string_no_escape_nl(f), read_string_no_escape_nl(f))","{""read_stringnl_noescape"": ""read_string_no_escape_nl""}","def 0(f):
    return read_stringnl(f, stripquotes=False)","def 1(f):
    r""""""
    >>> import io
    >>> 1(io.BytesIO(b""Queue\nEmpty\njunk""))
    'Queue Empty'
    """"""

    return ""%s %s"" % (0(f), 0(f))","{""read_stringnl_noescape"": ""0""}"
144,144,"def read_string1(f):
    r""""""
    >>> import io
    >>> read_string1(io.BytesIO(b""\x00""))
    ''
    >>> read_string1(io.BytesIO(b""\x03abcdef""))
    'abc'
    """"""

    n = read_uint1(f)
    assert n >= 0
    data = f.read(n)
    if len(data) == n:
        return data.decode(""latin-1"")
    raise ValueError(""expected %d bytes in a string1, but only %d remain"" %
                     (n, len(data)))","def read_uint1(f):
    r""""""
    >>> import io
    >>> read_uint1(io.BytesIO(b'\xff'))
    255
    """"""

    data = f.read(1)
    if data:
        return data[0]
    raise ValueError(""not enough data in stream to read uint1"")","def read_uint_1_byte(f):
    r""""""
    >>> import io
    >>> read_uint_1_byte(io.BytesIO(b'\xff'))
    255
    """"""

    data = f.read(1)
    if data:
        return data[0]
    raise ValueError(""not enough data in stream to read uint1"")","def read_string1_with_bytes_input(f):
    r""""""
    >>> import io
    >>> read_string1_with_bytes_input(io.BytesIO(b""\x00""))
    ''
    >>> read_string1_with_bytes_input(io.BytesIO(b""\x03abcdef""))
    'abc'
    """"""

    n = read_uint_1_byte(f)
    assert n >= 0
    data = f.read(n)
    if len(data) == n:
        return data.decode(""latin-1"")
    raise ValueError(""expected %d bytes in a string1, but only %d remain"" %
                     (n, len(data)))","{""read_uint1"": ""read_uint_1_byte""}","def 0(f):
    r""""""
    >>> import io
    >>> 0(io.BytesIO(b'\xff'))
    255
    """"""

    data = f.read(1)
    if data:
        return data[0]
    raise ValueError(""not enough data in stream to read uint1"")","def 1(f):
    r""""""
    >>> import io
    >>> 1(io.BytesIO(b""\x00""))
    ''
    >>> 1(io.BytesIO(b""\x03abcdef""))
    'abc'
    """"""

    n = 0(f)
    assert n >= 0
    data = f.read(n)
    if len(data) == n:
        return data.decode(""latin-1"")
    raise ValueError(""expected %d bytes in a string1, but only %d remain"" %
                     (n, len(data)))","{""read_uint1"": ""0""}"
145,145,"def read_string4(f):
    r""""""
    >>> import io
    >>> read_string4(io.BytesIO(b""\x00\x00\x00\x00abc""))
    ''
    >>> read_string4(io.BytesIO(b""\x03\x00\x00\x00abcdef""))
    'abc'
    >>> read_string4(io.BytesIO(b""\x00\x00\x00\x03abcdef""))
    Traceback (most recent call last):
    ...
    ValueError: expected 50331648 bytes in a string4, but only 6 remain
    """"""

    n = read_int4(f)
    if n < 0:
        raise ValueError(""string4 byte count < 0: %d"" % n)
    data = f.read(n)
    if len(data) == n:
        return data.decode(""latin-1"")
    raise ValueError(""expected %d bytes in a string4, but only %d remain"" %
                     (n, len(data)))","def read_int4(f):
    r""""""
    >>> import io
    >>> read_int4(io.BytesIO(b'\xff\x00\x00\x00'))
    255
    >>> read_int4(io.BytesIO(b'\x00\x00\x00\x80')) == -(2**31)
    True
    """"""

    data = f.read(4)
    if len(data) == 4:
        return _unpack(""<i"", data)[0]
    raise ValueError(""not enough data in stream to read int4"")","def read_int_4_bytes(f):
    r""""""
    >>> import io
    >>> read_int_4_bytes(io.BytesIO(b'\xff\x00\x00\x00'))
    255
    >>> read_int_4_bytes(io.BytesIO(b'\x00\x00\x00\x80')) == -(2**31)
    True
    """"""

    data = f.read(4)
    if len(data) == 4:
        return _unpack(""<i"", data)[0]
    raise ValueError(""not enough data in stream to read int4"")","def read_string4_with_bytes_input(f):
    r""""""
    >>> import io
    >>> read_string4_with_bytes_input(io.BytesIO(b""\x00\x00\x00\x00abc""))
    ''
    >>> read_string4_with_bytes_input(io.BytesIO(b""\x03\x00\x00\x00abcdef""))
    'abc'
    >>> read_string4_with_bytes_input(io.BytesIO(b""\x00\x00\x00\x03abcdef""))
    Traceback (most recent call last):
    ...
    ValueError: expected 50331648 bytes in a string4, but only 6 remain
    """"""

    n = read_int_4_bytes(f)
    if n < 0:
        raise ValueError(""string4 byte count < 0: %d"" % n)
    data = f.read(n)
    if len(data) == n:
        return data.decode(""latin-1"")
    raise ValueError(""expected %d bytes in a string4, but only %d remain"" %
                     (n, len(data)))","{""read_int4"": ""read_int_4_bytes""}","def 0(f):
    r""""""
    >>> import io
    >>> 0(io.BytesIO(b'\xff\x00\x00\x00'))
    255
    >>> 0(io.BytesIO(b'\x00\x00\x00\x80')) == -(2**31)
    True
    """"""

    data = f.read(4)
    if len(data) == 4:
        return _unpack(""<i"", data)[0]
    raise ValueError(""not enough data in stream to read int4"")","def 1(f):
    r""""""
    >>> import io
    >>> 1(io.BytesIO(b""\x00\x00\x00\x00abc""))
    ''
    >>> 1(io.BytesIO(b""\x03\x00\x00\x00abcdef""))
    'abc'
    >>> 1(io.BytesIO(b""\x00\x00\x00\x03abcdef""))
    Traceback (most recent call last):
    ...
    ValueError: expected 50331648 bytes in a string4, but only 6 remain
    """"""

    n = 0(f)
    if n < 0:
        raise ValueError(""string4 byte count < 0: %d"" % n)
    data = f.read(n)
    if len(data) == n:
        return data.decode(""latin-1"")
    raise ValueError(""expected %d bytes in a string4, but only %d remain"" %
                     (n, len(data)))","{""read_int4"": ""0""}"
146,146,"def read_bytes1(f):
    r""""""
    >>> import io
    >>> read_bytes1(io.BytesIO(b""\x00""))
    b''
    >>> read_bytes1(io.BytesIO(b""\x03abcdef""))
    b'abc'
    """"""

    n = read_uint1(f)
    assert n >= 0
    data = f.read(n)
    if len(data) == n:
        return data
    raise ValueError(""expected %d bytes in a bytes1, but only %d remain"" %
                     (n, len(data)))","def read_uint1(f):
    r""""""
    >>> import io
    >>> read_uint1(io.BytesIO(b'\xff'))
    255
    """"""

    data = f.read(1)
    if data:
        return data[0]
    raise ValueError(""not enough data in stream to read uint1"")","def read_uint_1_byte(f):
    r""""""
    >>> import io
    >>> read_uint_1_byte(io.BytesIO(b'\xff'))
    255
    """"""

    data = f.read(1)
    if data:
        return data[0]
    raise ValueError(""not enough data in stream to read uint1"")","def read_bytes1_with_bytes_input(f):
    r""""""
    >>> import io
    >>> read_bytes1_with_bytes_input(io.BytesIO(b""\x00""))
    b''
    >>> read_bytes1_with_bytes_input(io.BytesIO(b""\x03abcdef""))
    b'abc'
    """"""

    n = read_uint_1_byte(f)
    assert n >= 0
    data = f.read(n)
    if len(data) == n:
        return data
    raise ValueError(""expected %d bytes in a bytes1, but only %d remain"" %
                     (n, len(data)))","{""read_uint1"": ""read_uint_1_byte""}","def 0(f):
    r""""""
    >>> import io
    >>> 0(io.BytesIO(b'\xff'))
    255
    """"""

    data = f.read(1)
    if data:
        return data[0]
    raise ValueError(""not enough data in stream to read uint1"")","def 1(f):
    r""""""
    >>> import io
    >>> 1(io.BytesIO(b""\x00""))
    b''
    >>> 1(io.BytesIO(b""\x03abcdef""))
    b'abc'
    """"""

    n = 0(f)
    assert n >= 0
    data = f.read(n)
    if len(data) == n:
        return data
    raise ValueError(""expected %d bytes in a bytes1, but only %d remain"" %
                     (n, len(data)))","{""read_uint1"": ""0""}"
147,147,"def read_bytes4(f):
    r""""""
    >>> import io
    >>> read_bytes4(io.BytesIO(b""\x00\x00\x00\x00abc""))
    b''
    >>> read_bytes4(io.BytesIO(b""\x03\x00\x00\x00abcdef""))
    b'abc'
    >>> read_bytes4(io.BytesIO(b""\x00\x00\x00\x03abcdef""))
    Traceback (most recent call last):
    ...
    ValueError: expected 50331648 bytes in a bytes4, but only 6 remain
    """"""

    n = read_uint4(f)
    assert n >= 0
    if n > sys.maxsize:
        raise ValueError(""bytes4 byte count > sys.maxsize: %d"" % n)
    data = f.read(n)
    if len(data) == n:
        return data
    raise ValueError(""expected %d bytes in a bytes4, but only %d remain"" %
                     (n, len(data)))","def read_uint4(f):
    r""""""
    >>> import io
    >>> read_uint4(io.BytesIO(b'\xff\x00\x00\x00'))
    255
    >>> read_uint4(io.BytesIO(b'\x00\x00\x00\x80')) == 2**31
    True
    """"""

    data = f.read(4)
    if len(data) == 4:
        return _unpack(""<I"", data)[0]
    raise ValueError(""not enough data in stream to read uint4"")","def read_uint_4_bytes(f):
    r""""""
    >>> import io
    >>> read_uint_4_bytes(io.BytesIO(b'\xff\x00\x00\x00'))
    255
    >>> read_uint_4_bytes(io.BytesIO(b'\x00\x00\x00\x80')) == 2**31
    True
    """"""

    data = f.read(4)
    if len(data) == 4:
        return _unpack(""<I"", data)[0]
    raise ValueError(""not enough data in stream to read uint4"")","def read_bytes4_with_bytes_input(f):
    r""""""
    >>> import io
    >>> read_bytes4_with_bytes_input(io.BytesIO(b""\x00\x00\x00\x00abc""))
    b''
    >>> read_bytes4_with_bytes_input(io.BytesIO(b""\x03\x00\x00\x00abcdef""))
    b'abc'
    >>> read_bytes4_with_bytes_input(io.BytesIO(b""\x00\x00\x00\x03abcdef""))
    Traceback (most recent call last):
    ...
    ValueError: expected 50331648 bytes in a bytes4, but only 6 remain
    """"""

    n = read_uint_4_bytes(f)
    assert n >= 0
    if n > sys.maxsize:
        raise ValueError(""bytes4 byte count > sys.maxsize: %d"" % n)
    data = f.read(n)
    if len(data) == n:
        return data
    raise ValueError(""expected %d bytes in a bytes4, but only %d remain"" %
                     (n, len(data)))","{""read_uint4"": ""read_uint_4_bytes""}","def 0(f):
    r""""""
    >>> import io
    >>> 0(io.BytesIO(b'\xff\x00\x00\x00'))
    255
    >>> 0(io.BytesIO(b'\x00\x00\x00\x80')) == 2**31
    True
    """"""

    data = f.read(4)
    if len(data) == 4:
        return _unpack(""<I"", data)[0]
    raise ValueError(""not enough data in stream to read uint4"")","def 1(f):
    r""""""
    >>> import io
    >>> 1(io.BytesIO(b""\x00\x00\x00\x00abc""))
    b''
    >>> 1(io.BytesIO(b""\x03\x00\x00\x00abcdef""))
    b'abc'
    >>> 1(io.BytesIO(b""\x00\x00\x00\x03abcdef""))
    Traceback (most recent call last):
    ...
    ValueError: expected 50331648 bytes in a bytes4, but only 6 remain
    """"""

    n = 0(f)
    assert n >= 0
    if n > sys.maxsize:
        raise ValueError(""bytes4 byte count > sys.maxsize: %d"" % n)
    data = f.read(n)
    if len(data) == n:
        return data
    raise ValueError(""expected %d bytes in a bytes4, but only %d remain"" %
                     (n, len(data)))","{""read_uint4"": ""0""}"
148,148,"def read_unicodestringnl(f):
    r""""""
    >>> import io
    >>> read_unicodestringnl(io.BytesIO(b""abc\\uabcd\njunk"")) == 'abc\uabcd'
    True
    """"""

    data = f.readline()
    if not data.endswith(b'\n'):
        raise ValueError(""no newline found when trying to read ""
                         ""unicodestringnl"")
    data = data[:-1]    # lose the newline
    return str(data, 'raw-unicode-escape')","def str(val):
    """"""Convert float to string, taking the locale into account.""""""
    return _format(""%.12g"", val)","def locale_aware_float_to_string(val):
    """"""Convert float to locale_aware_float_to_stringing, taking the locale into account.""""""
    return _format(""%.12g"", val)","def read_unicodelocale_aware_float_to_stringingnl(f):
    r""""""
    >>> import io
    >>> read_unicodelocale_aware_float_to_stringingnl(io.BytesIO(b""abc\\uabcd\njunk"")) == 'abc\uabcd'
    True
    """"""

    data = f.readline()
    if not data.endswith(b'\n'):
        raise ValueError(""no newline found when trying to read ""
                         ""unicodelocale_aware_float_to_stringingnl"")
    data = data[:-1]    # lose the newline
    return locale_aware_float_to_string(data, 'raw-unicode-escape')","{""str"": ""locale_aware_float_to_string""}","def 0(val):
    """"""Convert float to string, taking the locale into account.""""""
    return _format(""%.12g"", val)","def 1(f):
    r""""""
    >>> import io
    >>> 1(io.BytesIO(b""abc\\uabcd\njunk"")) == 'abc\uabcd'
    True
    """"""

    data = f.readline()
    if not data.endswith(b'\n'):
        raise ValueError(""no newline found when trying to read ""
                         ""unicodestringnl"")
    data = data[:-1]    # lose the newline
    return 0(data, 'raw-unicode-escape')","{""str"": ""0""}"
149,149,"def read_unicodestring1(f):
    r""""""
    >>> import io
    >>> s = 'abcd\uabcd'
    >>> enc = s.encode('utf-8')
    >>> enc
    b'abcd\xea\xaf\x8d'
    >>> n = bytes([len(enc)])  # little-endian 1-byte length
    >>> t = read_unicodestring1(io.BytesIO(n + enc + b'junk'))
    >>> s == t
    True

    >>> read_unicodestring1(io.BytesIO(n + enc[:-1]))
    Traceback (most recent call last):
    ...
    ValueError: expected 7 bytes in a unicodestring1, but only 6 remain
    """"""

    n = read_uint1(f)
    assert n >= 0
    data = f.read(n)
    if len(data) == n:
        return str(data, 'utf-8', 'surrogatepass')
    raise ValueError(""expected %d bytes in a unicodestring1, but only %d ""
                     ""remain"" % (n, len(data)))","def read_uint1(f):
    r""""""
    >>> import io
    >>> read_uint1(io.BytesIO(b'\xff'))
    255
    """"""

    data = f.read(1)
    if data:
        return data[0]
    raise ValueError(""not enough data in stream to read uint1"")

def str(val):
    """"""Convert float to string, taking the locale into account.""""""
    return _format(""%.12g"", val)","def read_uint_1_byte(f):
    r""""""
    >>> import io
    >>> read_uint_1_byte(io.BytesIO(b'\xff'))
    255
    """"""

    data = f.read(1)
    if data:
        return data[0]
    raise ValueError(""not enough data in locale_aware_float_to_stringeam to read uint1"")

def locale_aware_float_to_string(val):
    """"""Convert float to locale_aware_float_to_stringing, taking the locale into account.""""""
    return _format(""%.12g"", val)","def read_unicodelocale_aware_float_to_stringing1(f):
    r""""""
    >>> import io
    >>> s = 'abcd\uabcd'
    >>> enc = s.encode('utf-8')
    >>> enc
    b'abcd\xea\xaf\x8d'
    >>> n = bytes([len(enc)])  # little-endian 1-byte length
    >>> t = read_unicodelocale_aware_float_to_stringing1(io.BytesIO(n + enc + b'junk'))
    >>> s == t
    True

    >>> read_unicodelocale_aware_float_to_stringing1(io.BytesIO(n + enc[:-1]))
    Traceback (most recent call last):
    ...
    ValueError: expected 7 bytes in a unicodelocale_aware_float_to_stringing1, but only 6 remain
    """"""

    n = read_uint_1_byte(f)
    assert n >= 0
    data = f.read(n)
    if len(data) == n:
        return locale_aware_float_to_string(data, 'utf-8', 'surrogatepass')
    raise ValueError(""expected %d bytes in a unicodelocale_aware_float_to_stringing1, but only %d ""
                     ""remain"" % (n, len(data)))","{""read_uint1"": ""read_uint_1_byte"", ""str"": ""locale_aware_float_to_string""}","def 0(f):
    r""""""
    >>> import io
    >>> 0(io.BytesIO(b'\xff'))
    255
    """"""

    data = f.read(1)
    if data:
        return data[0]
    raise ValueError(""not enough data in stream to read uint1"")

def 1(val):
    """"""Convert float to string, taking the locale into account.""""""
    return _format(""%.12g"", val)","def 2(f):
    r""""""
    >>> import io
    >>> s = 'abcd\uabcd'
    >>> enc = s.encode('utf-8')
    >>> enc
    b'abcd\xea\xaf\x8d'
    >>> n = bytes([len(enc)])  # little-endian 1-byte length
    >>> t = 2(io.BytesIO(n + enc + b'junk'))
    >>> s == t
    True

    >>> 2(io.BytesIO(n + enc[:-1]))
    Traceback (most recent call last):
    ...
    ValueError: expected 7 bytes in a unicodestring1, but only 6 remain
    """"""

    n = 0(f)
    assert n >= 0
    data = f.read(n)
    if len(data) == n:
        return 1(data, 'utf-8', 'surrogatepass')
    raise ValueError(""expected %d bytes in a unicodestring1, but only %d ""
                     ""remain"" % (n, len(data)))","{""read_uint1"": ""0"", ""str"": ""1""}"
150,150,"def read_decimalnl_short(f):
    r""""""
    >>> import io
    >>> read_decimalnl_short(io.BytesIO(b""1234\n56""))
    1234

    >>> read_decimalnl_short(io.BytesIO(b""1234L\n56""))
    Traceback (most recent call last):
    ...
    ValueError: invalid literal for int() with base 10: b'1234L'
    """"""

    s = read_stringnl(f, decode=False, stripquotes=False)

    # There's a hack for True and False here.
    if s == b""00"":
        return False
    elif s == b""01"":
        return True

    return int(s)","def read_stringnl(f, decode=True, stripquotes=True):
    r""""""
    >>> import io
    >>> read_stringnl(io.BytesIO(b""'abcd'\nefg\n""))
    'abcd'

    >>> read_stringnl(io.BytesIO(b""\n""))
    Traceback (most recent call last):
    ...
    ValueError: no string quotes around b''

    >>> read_stringnl(io.BytesIO(b""\n""), stripquotes=False)
    ''

    >>> read_stringnl(io.BytesIO(b""''\n""))
    ''

    >>> read_stringnl(io.BytesIO(b'""abcd""'))
    Traceback (most recent call last):
    ...
    ValueError: no newline found when trying to read stringnl

    Embedded escapes are undone in the result.
    >>> read_stringnl(io.BytesIO(br""'a\n\\b\x00c\td'"" + b""\n'e'""))
    'a\n\\b\x00c\td'
    """"""

    data = f.readline()
    if not data.endswith(b'\n'):
        raise ValueError(""no newline found when trying to read stringnl"")
    data = data[:-1]    # lose the newline

    if stripquotes:
        for q in (b'""', b""'""):
            if data.startswith(q):
                if not data.endswith(q):
                    raise ValueError(""strinq quote %r not found at both ""
                                     ""ends of %r"" % (q, data))
                data = data[1:-1]
                break
        else:
            raise ValueError(""no string quotes around %r"" % data)

    if decode:
        data = codecs.escape_decode(data)[0].decode(""ascii"")
    return data","def read_string_nl(f, decode=True, stripquotes=True):
    r""""""
    >>> import io
    >>> read_string_nl(io.BytesIO(b""'abcd'\nefg\n""))
    'abcd'

    >>> read_string_nl(io.BytesIO(b""\n""))
    Traceback (most recent call last):
    ...
    ValueError: no string quotes around b''

    >>> read_string_nl(io.BytesIO(b""\n""), stripquotes=False)
    ''

    >>> read_string_nl(io.BytesIO(b""''\n""))
    ''

    >>> read_string_nl(io.BytesIO(b'""abcd""'))
    Traceback (most recent call last):
    ...
    ValueError: no newline found when trying to read stringnl

    Embedded escapes are undone in the result.
    >>> read_string_nl(io.BytesIO(br""'a\n\\b\x00c\td'"" + b""\n'e'""))
    'a\n\\b\x00c\td'
    """"""

    data = f.readline()
    if not data.endswith(b'\n'):
        raise ValueError(""no newline found when trying to read stringnl"")
    data = data[:-1]    # lose the newline

    if stripquotes:
        for q in (b'""', b""'""):
            if data.startswith(q):
                if not data.endswith(q):
                    raise ValueError(""strinq quote %r not found at both ""
                                     ""ends of %r"" % (q, data))
                data = data[1:-1]
                break
        else:
            raise ValueError(""no string quotes around %r"" % data)

    if decode:
        data = codecs.escape_decode(data)[0].decode(""ascii"")
    return data","def read_decimalnl_short_with_bytes_input(f):
    r""""""
    >>> import io
    >>> read_decimalnl_short_with_bytes_input(io.BytesIO(b""1234\n56""))
    1234

    >>> read_decimalnl_short_with_bytes_input(io.BytesIO(b""1234L\n56""))
    Traceback (most recent call last):
    ...
    ValueError: invalid literal for int() with base 10: b'1234L'
    """"""

    s = read_string_nl(f, decode=False, stripquotes=False)

    # There's a hack for True and False here.
    if s == b""00"":
        return False
    elif s == b""01"":
        return True

    return int(s)","{""read_stringnl"": ""read_string_nl""}","def 0(f, decode=True, stripquotes=True):
    r""""""
    >>> import io
    >>> 0(io.BytesIO(b""'abcd'\nefg\n""))
    'abcd'

    >>> 0(io.BytesIO(b""\n""))
    Traceback (most recent call last):
    ...
    ValueError: no string quotes around b''

    >>> 0(io.BytesIO(b""\n""), stripquotes=False)
    ''

    >>> 0(io.BytesIO(b""''\n""))
    ''

    >>> 0(io.BytesIO(b'""abcd""'))
    Traceback (most recent call last):
    ...
    ValueError: no newline found when trying to read stringnl

    Embedded escapes are undone in the result.
    >>> 0(io.BytesIO(br""'a\n\\b\x00c\td'"" + b""\n'e'""))
    'a\n\\b\x00c\td'
    """"""

    data = f.readline()
    if not data.endswith(b'\n'):
        raise ValueError(""no newline found when trying to read stringnl"")
    data = data[:-1]    # lose the newline

    if stripquotes:
        for q in (b'""', b""'""):
            if data.startswith(q):
                if not data.endswith(q):
                    raise ValueError(""strinq quote %r not found at both ""
                                     ""ends of %r"" % (q, data))
                data = data[1:-1]
                break
        else:
            raise ValueError(""no string quotes around %r"" % data)

    if decode:
        data = codecs.escape_decode(data)[0].decode(""ascii"")
    return data","def 1(f):
    r""""""
    >>> import io
    >>> 1(io.BytesIO(b""1234\n56""))
    1234

    >>> 1(io.BytesIO(b""1234L\n56""))
    Traceback (most recent call last):
    ...
    ValueError: invalid literal for int() with base 10: b'1234L'
    """"""

    s = 0(f, decode=False, stripquotes=False)

    # There's a hack for True and False here.
    if s == b""00"":
        return False
    elif s == b""01"":
        return True

    return int(s)","{""read_stringnl"": ""0""}"
151,151,"def read_decimalnl_long(f):
    r""""""
    >>> import io

    >>> read_decimalnl_long(io.BytesIO(b""1234L\n56""))
    1234

    >>> read_decimalnl_long(io.BytesIO(b""123456789012345678901234L\n6""))
    123456789012345678901234
    """"""

    s = read_stringnl(f, decode=False, stripquotes=False)
    if s[-1:] == b'L':
        s = s[:-1]
    return int(s)","def read_stringnl(f, decode=True, stripquotes=True):
    r""""""
    >>> import io
    >>> read_stringnl(io.BytesIO(b""'abcd'\nefg\n""))
    'abcd'

    >>> read_stringnl(io.BytesIO(b""\n""))
    Traceback (most recent call last):
    ...
    ValueError: no string quotes around b''

    >>> read_stringnl(io.BytesIO(b""\n""), stripquotes=False)
    ''

    >>> read_stringnl(io.BytesIO(b""''\n""))
    ''

    >>> read_stringnl(io.BytesIO(b'""abcd""'))
    Traceback (most recent call last):
    ...
    ValueError: no newline found when trying to read stringnl

    Embedded escapes are undone in the result.
    >>> read_stringnl(io.BytesIO(br""'a\n\\b\x00c\td'"" + b""\n'e'""))
    'a\n\\b\x00c\td'
    """"""

    data = f.readline()
    if not data.endswith(b'\n'):
        raise ValueError(""no newline found when trying to read stringnl"")
    data = data[:-1]    # lose the newline

    if stripquotes:
        for q in (b'""', b""'""):
            if data.startswith(q):
                if not data.endswith(q):
                    raise ValueError(""strinq quote %r not found at both ""
                                     ""ends of %r"" % (q, data))
                data = data[1:-1]
                break
        else:
            raise ValueError(""no string quotes around %r"" % data)

    if decode:
        data = codecs.escape_decode(data)[0].decode(""ascii"")
    return data","def read_string_nl(f, decode=True, stripquotes=True):
    r""""""
    >>> import io
    >>> read_string_nl(io.BytesIO(b""'abcd'\nefg\n""))
    'abcd'

    >>> read_string_nl(io.BytesIO(b""\n""))
    Traceback (most recent call last):
    ...
    ValueError: no string quotes around b''

    >>> read_string_nl(io.BytesIO(b""\n""), stripquotes=False)
    ''

    >>> read_string_nl(io.BytesIO(b""''\n""))
    ''

    >>> read_string_nl(io.BytesIO(b'""abcd""'))
    Traceback (most recent call last):
    ...
    ValueError: no newline found when trying to read stringnl

    Embedded escapes are undone in the result.
    >>> read_string_nl(io.BytesIO(br""'a\n\\b\x00c\td'"" + b""\n'e'""))
    'a\n\\b\x00c\td'
    """"""

    data = f.readline()
    if not data.endswith(b'\n'):
        raise ValueError(""no newline found when trying to read stringnl"")
    data = data[:-1]    # lose the newline

    if stripquotes:
        for q in (b'""', b""'""):
            if data.startswith(q):
                if not data.endswith(q):
                    raise ValueError(""strinq quote %r not found at both ""
                                     ""ends of %r"" % (q, data))
                data = data[1:-1]
                break
        else:
            raise ValueError(""no string quotes around %r"" % data)

    if decode:
        data = codecs.escape_decode(data)[0].decode(""ascii"")
    return data","def read_decimalnl_long_with_bytes_input(f):
    r""""""
    >>> import io

    >>> read_decimalnl_long_with_bytes_input(io.BytesIO(b""1234L\n56""))
    1234

    >>> read_decimalnl_long_with_bytes_input(io.BytesIO(b""123456789012345678901234L\n6""))
    123456789012345678901234
    """"""

    s = read_string_nl(f, decode=False, stripquotes=False)
    if s[-1:] == b'L':
        s = s[:-1]
    return int(s)","{""read_stringnl"": ""read_string_nl""}","def 0(f, decode=True, stripquotes=True):
    r""""""
    >>> import io
    >>> 0(io.BytesIO(b""'abcd'\nefg\n""))
    'abcd'

    >>> 0(io.BytesIO(b""\n""))
    Traceback (most recent call last):
    ...
    ValueError: no string quotes around b''

    >>> 0(io.BytesIO(b""\n""), stripquotes=False)
    ''

    >>> 0(io.BytesIO(b""''\n""))
    ''

    >>> 0(io.BytesIO(b'""abcd""'))
    Traceback (most recent call last):
    ...
    ValueError: no newline found when trying to read stringnl

    Embedded escapes are undone in the result.
    >>> 0(io.BytesIO(br""'a\n\\b\x00c\td'"" + b""\n'e'""))
    'a\n\\b\x00c\td'
    """"""

    data = f.readline()
    if not data.endswith(b'\n'):
        raise ValueError(""no newline found when trying to read stringnl"")
    data = data[:-1]    # lose the newline

    if stripquotes:
        for q in (b'""', b""'""):
            if data.startswith(q):
                if not data.endswith(q):
                    raise ValueError(""strinq quote %r not found at both ""
                                     ""ends of %r"" % (q, data))
                data = data[1:-1]
                break
        else:
            raise ValueError(""no string quotes around %r"" % data)

    if decode:
        data = codecs.escape_decode(data)[0].decode(""ascii"")
    return data","def 1(f):
    r""""""
    >>> import io

    >>> 1(io.BytesIO(b""1234L\n56""))
    1234

    >>> 1(io.BytesIO(b""123456789012345678901234L\n6""))
    123456789012345678901234
    """"""

    s = 0(f, decode=False, stripquotes=False)
    if s[-1:] == b'L':
        s = s[:-1]
    return int(s)","{""read_stringnl"": ""0""}"
152,152,"def read_floatnl(f):
    r""""""
    >>> import io
    >>> read_floatnl(io.BytesIO(b""-1.25\n6""))
    -1.25
    """"""
    s = read_stringnl(f, decode=False, stripquotes=False)
    return float(s)","def read_stringnl(f, decode=True, stripquotes=True):
    r""""""
    >>> import io
    >>> read_stringnl(io.BytesIO(b""'abcd'\nefg\n""))
    'abcd'

    >>> read_stringnl(io.BytesIO(b""\n""))
    Traceback (most recent call last):
    ...
    ValueError: no string quotes around b''

    >>> read_stringnl(io.BytesIO(b""\n""), stripquotes=False)
    ''

    >>> read_stringnl(io.BytesIO(b""''\n""))
    ''

    >>> read_stringnl(io.BytesIO(b'""abcd""'))
    Traceback (most recent call last):
    ...
    ValueError: no newline found when trying to read stringnl

    Embedded escapes are undone in the result.
    >>> read_stringnl(io.BytesIO(br""'a\n\\b\x00c\td'"" + b""\n'e'""))
    'a\n\\b\x00c\td'
    """"""

    data = f.readline()
    if not data.endswith(b'\n'):
        raise ValueError(""no newline found when trying to read stringnl"")
    data = data[:-1]    # lose the newline

    if stripquotes:
        for q in (b'""', b""'""):
            if data.startswith(q):
                if not data.endswith(q):
                    raise ValueError(""strinq quote %r not found at both ""
                                     ""ends of %r"" % (q, data))
                data = data[1:-1]
                break
        else:
            raise ValueError(""no string quotes around %r"" % data)

    if decode:
        data = codecs.escape_decode(data)[0].decode(""ascii"")
    return data","def read_string_nl(f, decode=True, stripquotes=True):
    r""""""
    >>> import io
    >>> read_string_nl(io.BytesIO(b""'abcd'\nefg\n""))
    'abcd'

    >>> read_string_nl(io.BytesIO(b""\n""))
    Traceback (most recent call last):
    ...
    ValueError: no string quotes around b''

    >>> read_string_nl(io.BytesIO(b""\n""), stripquotes=False)
    ''

    >>> read_string_nl(io.BytesIO(b""''\n""))
    ''

    >>> read_string_nl(io.BytesIO(b'""abcd""'))
    Traceback (most recent call last):
    ...
    ValueError: no newline found when trying to read stringnl

    Embedded escapes are undone in the result.
    >>> read_string_nl(io.BytesIO(br""'a\n\\b\x00c\td'"" + b""\n'e'""))
    'a\n\\b\x00c\td'
    """"""

    data = f.readline()
    if not data.endswith(b'\n'):
        raise ValueError(""no newline found when trying to read stringnl"")
    data = data[:-1]    # lose the newline

    if stripquotes:
        for q in (b'""', b""'""):
            if data.startswith(q):
                if not data.endswith(q):
                    raise ValueError(""strinq quote %r not found at both ""
                                     ""ends of %r"" % (q, data))
                data = data[1:-1]
                break
        else:
            raise ValueError(""no string quotes around %r"" % data)

    if decode:
        data = codecs.escape_decode(data)[0].decode(""ascii"")
    return data","def read_floatnl_with_bytes_input(f):
    r""""""
    >>> import io
    >>> read_floatnl_with_bytes_input(io.BytesIO(b""-1.25\n6""))
    -1.25
    """"""
    s = read_string_nl(f, decode=False, stripquotes=False)
    return float(s)","{""read_stringnl"": ""read_string_nl""}","def 0(f, decode=True, stripquotes=True):
    r""""""
    >>> import io
    >>> 0(io.BytesIO(b""'abcd'\nefg\n""))
    'abcd'

    >>> 0(io.BytesIO(b""\n""))
    Traceback (most recent call last):
    ...
    ValueError: no string quotes around b''

    >>> 0(io.BytesIO(b""\n""), stripquotes=False)
    ''

    >>> 0(io.BytesIO(b""''\n""))
    ''

    >>> 0(io.BytesIO(b'""abcd""'))
    Traceback (most recent call last):
    ...
    ValueError: no newline found when trying to read stringnl

    Embedded escapes are undone in the result.
    >>> 0(io.BytesIO(br""'a\n\\b\x00c\td'"" + b""\n'e'""))
    'a\n\\b\x00c\td'
    """"""

    data = f.readline()
    if not data.endswith(b'\n'):
        raise ValueError(""no newline found when trying to read stringnl"")
    data = data[:-1]    # lose the newline

    if stripquotes:
        for q in (b'""', b""'""):
            if data.startswith(q):
                if not data.endswith(q):
                    raise ValueError(""strinq quote %r not found at both ""
                                     ""ends of %r"" % (q, data))
                data = data[1:-1]
                break
        else:
            raise ValueError(""no string quotes around %r"" % data)

    if decode:
        data = codecs.escape_decode(data)[0].decode(""ascii"")
    return data","def 1(f):
    r""""""
    >>> import io
    >>> 1(io.BytesIO(b""-1.25\n6""))
    -1.25
    """"""
    s = 0(f, decode=False, stripquotes=False)
    return float(s)","{""read_stringnl"": ""0""}"
153,153,"def read_long1(f):
    r""""""
    >>> import io
    >>> read_long1(io.BytesIO(b""\x00""))
    0
    >>> read_long1(io.BytesIO(b""\x02\xff\x00""))
    255
    >>> read_long1(io.BytesIO(b""\x02\xff\x7f""))
    32767
    >>> read_long1(io.BytesIO(b""\x02\x00\xff""))
    -256
    >>> read_long1(io.BytesIO(b""\x02\x00\x80""))
    -32768
    """"""

    n = read_uint1(f)
    data = f.read(n)
    if len(data) != n:
        raise ValueError(""not enough data in stream to read long1"")
    return decode_long(data)","def decode_long(data):
    r""""""Decode a long from a two's complement little-endian binary string.

    >>> decode_long(b'')
    0
    >>> decode_long(b""\xff\x00"")
    255
    >>> decode_long(b""\xff\x7f"")
    32767
    >>> decode_long(b""\x00\xff"")
    -256
    >>> decode_long(b""\x00\x80"")
    -32768
    >>> decode_long(b""\x80"")
    -128
    >>> decode_long(b""\x7f"")
    127
    """"""
    return int.from_bytes(data, byteorder='little', signed=True)

def read_uint1(f):
    r""""""
    >>> import io
    >>> read_uint1(io.BytesIO(b'\xff'))
    255
    """"""

    data = f.read(1)
    if data:
        return data[0]
    raise ValueError(""not enough data in stream to read uint1"")","def decode_twos_complement_long(data):
    r""""""Decode a long from a two's complement little-endian binary string.

    >>> decode_twos_complement_long(b'')
    0
    >>> decode_twos_complement_long(b""\xff\x00"")
    255
    >>> decode_twos_complement_long(b""\xff\x7f"")
    32767
    >>> decode_twos_complement_long(b""\x00\xff"")
    -256
    >>> decode_twos_complement_long(b""\x00\x80"")
    -32768
    >>> decode_twos_complement_long(b""\x80"")
    -128
    >>> decode_twos_complement_long(b""\x7f"")
    127
    """"""
    return int.from_bytes(data, byteorder='little', signed=True)

def read_uint_1_byte(f):
    r""""""
    >>> import io
    >>> read_uint_1_byte(io.BytesIO(b'\xff'))
    255
    """"""

    data = f.read(1)
    if data:
        return data[0]
    raise ValueError(""not enough data in stream to read uint1"")","def read_long1_and_return_decoded_value_from_bytes_input(f):
    r""""""
    >>> import io
    >>> read_long1_and_return_decoded_value_from_bytes_input(io.BytesIO(b""\x00""))
    0
    >>> read_long1_and_return_decoded_value_from_bytes_input(io.BytesIO(b""\x02\xff\x00""))
    255
    >>> read_long1_and_return_decoded_value_from_bytes_input(io.BytesIO(b""\x02\xff\x7f""))
    32767
    >>> read_long1_and_return_decoded_value_from_bytes_input(io.BytesIO(b""\x02\x00\xff""))
    -256
    >>> read_long1_and_return_decoded_value_from_bytes_input(io.BytesIO(b""\x02\x00\x80""))
    -32768
    """"""

    n = read_uint_1_byte(f)
    data = f.read(n)
    if len(data) != n:
        raise ValueError(""not enough data in stream to read long1"")
    return decode_twos_complement_long(data)","{""decode_long"": ""decode_twos_complement_long"", ""read_uint1"": ""read_uint_1_byte""}","def 0(data):
    r""""""Decode a long from a two's complement little-endian binary string.

    >>> 0(b'')
    0
    >>> 0(b""\xff\x00"")
    255
    >>> 0(b""\xff\x7f"")
    32767
    >>> 0(b""\x00\xff"")
    -256
    >>> 0(b""\x00\x80"")
    -32768
    >>> 0(b""\x80"")
    -128
    >>> 0(b""\x7f"")
    127
    """"""
    return int.from_bytes(data, byteorder='little', signed=True)

def 1(f):
    r""""""
    >>> import io
    >>> 1(io.BytesIO(b'\xff'))
    255
    """"""

    data = f.read(1)
    if data:
        return data[0]
    raise ValueError(""not enough data in stream to read uint1"")","def 2(f):
    r""""""
    >>> import io
    >>> 2(io.BytesIO(b""\x00""))
    0
    >>> 2(io.BytesIO(b""\x02\xff\x00""))
    255
    >>> 2(io.BytesIO(b""\x02\xff\x7f""))
    32767
    >>> 2(io.BytesIO(b""\x02\x00\xff""))
    -256
    >>> 2(io.BytesIO(b""\x02\x00\x80""))
    -32768
    """"""

    n = 1(f)
    data = f.read(n)
    if len(data) != n:
        raise ValueError(""not enough data in stream to read long1"")
    return 0(data)","{""decode_long"": ""0"", ""read_uint1"": ""1""}"
154,154,"def read_long4(f):
    r""""""
    >>> import io
    >>> read_long4(io.BytesIO(b""\x02\x00\x00\x00\xff\x00""))
    255
    >>> read_long4(io.BytesIO(b""\x02\x00\x00\x00\xff\x7f""))
    32767
    >>> read_long4(io.BytesIO(b""\x02\x00\x00\x00\x00\xff""))
    -256
    >>> read_long4(io.BytesIO(b""\x02\x00\x00\x00\x00\x80""))
    -32768
    >>> read_long1(io.BytesIO(b""\x00\x00\x00\x00""))
    0
    """"""

    n = read_int4(f)
    if n < 0:
        raise ValueError(""long4 byte count < 0: %d"" % n)
    data = f.read(n)
    if len(data) != n:
        raise ValueError(""not enough data in stream to read long4"")
    return decode_long(data)","def decode_long(data):
    r""""""Decode a long from a two's complement little-endian binary string.

    >>> decode_long(b'')
    0
    >>> decode_long(b""\xff\x00"")
    255
    >>> decode_long(b""\xff\x7f"")
    32767
    >>> decode_long(b""\x00\xff"")
    -256
    >>> decode_long(b""\x00\x80"")
    -32768
    >>> decode_long(b""\x80"")
    -128
    >>> decode_long(b""\x7f"")
    127
    """"""
    return int.from_bytes(data, byteorder='little', signed=True)

def read_int4(f):
    r""""""
    >>> import io
    >>> read_int4(io.BytesIO(b'\xff\x00\x00\x00'))
    255
    >>> read_int4(io.BytesIO(b'\x00\x00\x00\x80')) == -(2**31)
    True
    """"""

    data = f.read(4)
    if len(data) == 4:
        return _unpack(""<i"", data)[0]
    raise ValueError(""not enough data in stream to read int4"")","def decode_twos_complement_long(data):
    r""""""Decode a long from a two's complement little-endian binary string.

    >>> decode_twos_complement_long(b'')
    0
    >>> decode_twos_complement_long(b""\xff\x00"")
    255
    >>> decode_twos_complement_long(b""\xff\x7f"")
    32767
    >>> decode_twos_complement_long(b""\x00\xff"")
    -256
    >>> decode_twos_complement_long(b""\x00\x80"")
    -32768
    >>> decode_twos_complement_long(b""\x80"")
    -128
    >>> decode_twos_complement_long(b""\x7f"")
    127
    """"""
    return int.from_bytes(data, byteorder='little', signed=True)

def read_int_4_bytes(f):
    r""""""
    >>> import io
    >>> read_int_4_bytes(io.BytesIO(b'\xff\x00\x00\x00'))
    255
    >>> read_int_4_bytes(io.BytesIO(b'\x00\x00\x00\x80')) == -(2**31)
    True
    """"""

    data = f.read(4)
    if len(data) == 4:
        return _unpack(""<i"", data)[0]
    raise ValueError(""not enough data in stream to read int4"")","def read_long4_and_return_decoded_value_from_bytes_input(f):
    r""""""
    >>> import io
    >>> read_long4_and_return_decoded_value_from_bytes_input(io.BytesIO(b""\x02\x00\x00\x00\xff\x00""))
    255
    >>> read_long4_and_return_decoded_value_from_bytes_input(io.BytesIO(b""\x02\x00\x00\x00\xff\x7f""))
    32767
    >>> read_long4_and_return_decoded_value_from_bytes_input(io.BytesIO(b""\x02\x00\x00\x00\x00\xff""))
    -256
    >>> read_long4_and_return_decoded_value_from_bytes_input(io.BytesIO(b""\x02\x00\x00\x00\x00\x80""))
    -32768
    >>> read_long1(io.BytesIO(b""\x00\x00\x00\x00""))
    0
    """"""

    n = read_int_4_bytes(f)
    if n < 0:
        raise ValueError(""long4 byte count < 0: %d"" % n)
    data = f.read(n)
    if len(data) != n:
        raise ValueError(""not enough data in stream to read long4"")
    return decode_twos_complement_long(data)","{""decode_long"": ""decode_twos_complement_long"", ""read_int4"": ""read_int_4_bytes""}","def 0(data):
    r""""""Decode a long from a two's complement little-endian binary string.

    >>> 0(b'')
    0
    >>> 0(b""\xff\x00"")
    255
    >>> 0(b""\xff\x7f"")
    32767
    >>> 0(b""\x00\xff"")
    -256
    >>> 0(b""\x00\x80"")
    -32768
    >>> 0(b""\x80"")
    -128
    >>> 0(b""\x7f"")
    127
    """"""
    return int.from_bytes(data, byteorder='little', signed=True)

def 1(f):
    r""""""
    >>> import io
    >>> 1(io.BytesIO(b'\xff\x00\x00\x00'))
    255
    >>> 1(io.BytesIO(b'\x00\x00\x00\x80')) == -(2**31)
    True
    """"""

    data = f.read(4)
    if len(data) == 4:
        return _unpack(""<i"", data)[0]
    raise ValueError(""not enough data in stream to read int4"")","def 2(f):
    r""""""
    >>> import io
    >>> 2(io.BytesIO(b""\x02\x00\x00\x00\xff\x00""))
    255
    >>> 2(io.BytesIO(b""\x02\x00\x00\x00\xff\x7f""))
    32767
    >>> 2(io.BytesIO(b""\x02\x00\x00\x00\x00\xff""))
    -256
    >>> 2(io.BytesIO(b""\x02\x00\x00\x00\x00\x80""))
    -32768
    >>> read_long1(io.BytesIO(b""\x00\x00\x00\x00""))
    0
    """"""

    n = 1(f)
    if n < 0:
        raise ValueError(""long4 byte count < 0: %d"" % n)
    data = f.read(n)
    if len(data) != n:
        raise ValueError(""not enough data in stream to read long4"")
    return 0(data)","{""decode_long"": ""0"", ""read_int4"": ""1""}"
155,155,"def genops(pickle):
    """"""Generate all the opcodes in a pickle.

    'pickle' is a file-like object, or string, containing the pickle.

    Each opcode in the pickle is generated, from the current pickle position,
    stopping after a STOP opcode is delivered.  A triple is generated for
    each opcode:

        opcode, arg, pos

    opcode is an OpcodeInfo record, describing the current opcode.

    If the opcode has an argument embedded in the pickle, arg is its decoded
    value, as a Python object.  If the opcode doesn't have an argument, arg
    is None.

    If the pickle has a tell() method, pos was the value of pickle.tell()
    before reading the current opcode.  If the pickle is a bytes object,
    it's wrapped in a BytesIO object, and the latter's tell() result is
    used.  Else (the pickle doesn't have a tell(), and it's not obvious how
    to query its current position) pos is None.
    """"""
    return _genops(pickle)","def _genops(data, yield_end_pos=False):
    if isinstance(data, bytes_types):
        data = io.BytesIO(data)

    if hasattr(data, ""tell""):
        getpos = data.tell
    else:
        getpos = lambda: None

    while True:
        pos = getpos()
        code = data.read(1)
        opcode = code2op.get(code.decode(""latin-1""))
        if opcode is None:
            if code == b"""":
                raise ValueError(""pickle exhausted before seeing STOP"")
            else:
                raise ValueError(""at position %s, opcode %r unknown"" % (
                                 ""<unknown>"" if pos is None else pos,
                                 code))
        if opcode.arg is None:
            arg = None
        else:
            arg = opcode.arg.reader(data)
        if yield_end_pos:
            yield opcode, arg, pos, getpos()
        else:
            yield opcode, arg, pos
        if code == b'.':
            assert opcode.name == 'STOP'
            break","def generate_operations_from_data(data, yield_end_pos=False):
    if isinstance(data, bytes_types):
        data = io.BytesIO(data)

    if hasattr(data, ""tell""):
        getpos = data.tell
    else:
        getpos = lambda: None

    while True:
        pos = getpos()
        code = data.read(1)
        opcode = code2op.get(code.decode(""latin-1""))
        if opcode is None:
            if code == b"""":
                raise ValueError(""pickle exhausted before seeing STOP"")
            else:
                raise ValueError(""at position %s, opcode %r unknown"" % (
                                 ""<unknown>"" if pos is None else pos,
                                 code))
        if opcode.arg is None:
            arg = None
        else:
            arg = opcode.arg.reader(data)
        if yield_end_pos:
            yield opcode, arg, pos, getpos()
        else:
            yield opcode, arg, pos
        if code == b'.':
            assert opcode.name == 'STOP'
            break","def generate_all_opcodes_in_pickle_and_return_opcode_info(pickle):
    """"""Generate all the opcodes in a pickle.

    'pickle' is a file-like object, or string, containing the pickle.

    Each opcode in the pickle is generated, from the current pickle position,
    stopping after a STOP opcode is delivered.  A triple is generated for
    each opcode:

        opcode, arg, pos

    opcode is an OpcodeInfo record, describing the current opcode.

    If the opcode has an argument embedded in the pickle, arg is its decoded
    value, as a Python object.  If the opcode doesn't have an argument, arg
    is None.

    If the pickle has a tell() method, pos was the value of pickle.tell()
    before reading the current opcode.  If the pickle is a bytes object,
    it's wrapped in a BytesIO object, and the latter's tell() result is
    used.  Else (the pickle doesn't have a tell(), and it's not obvious how
    to query its current position) pos is None.
    """"""
    return generate_operations_from_data(pickle)","{""_genops"": ""generate_operations_from_data""}","def 0(data, yield_end_pos=False):
    if isinstance(data, bytes_types):
        data = io.BytesIO(data)

    if hasattr(data, ""tell""):
        getpos = data.tell
    else:
        getpos = lambda: None

    while True:
        pos = getpos()
        code = data.read(1)
        opcode = code2op.get(code.decode(""latin-1""))
        if opcode is None:
            if code == b"""":
                raise ValueError(""pickle exhausted before seeing STOP"")
            else:
                raise ValueError(""at position %s, opcode %r unknown"" % (
                                 ""<unknown>"" if pos is None else pos,
                                 code))
        if opcode.arg is None:
            arg = None
        else:
            arg = opcode.arg.reader(data)
        if yield_end_pos:
            yield opcode, arg, pos, getpos()
        else:
            yield opcode, arg, pos
        if code == b'.':
            assert opcode.name == 'STOP'
            break","def 1(pickle):
    """"""Generate all the opcodes in a pickle.

    'pickle' is a file-like object, or string, containing the pickle.

    Each opcode in the pickle is generated, from the current pickle position,
    stopping after a STOP opcode is delivered.  A triple is generated for
    each opcode:

        opcode, arg, pos

    opcode is an OpcodeInfo record, describing the current opcode.

    If the opcode has an argument embedded in the pickle, arg is its decoded
    value, as a Python object.  If the opcode doesn't have an argument, arg
    is None.

    If the pickle has a tell() method, pos was the value of pickle.tell()
    before reading the current opcode.  If the pickle is a bytes object,
    it's wrapped in a BytesIO object, and the latter's tell() result is
    used.  Else (the pickle doesn't have a tell(), and it's not obvious how
    to query its current position) pos is None.
    """"""
    return 0(pickle)","{""_genops"": ""0""}"
156,156,"def removeduppaths():
    """""" Remove duplicate entries from sys.path along with making them
    absolute""""""
    # This ensures that the initial path provided by the interpreter contains
    # only absolute pathnames, even if we're running from the build directory.
    L = []
    known_paths = set()
    for dir in sys.path:
        # Filter out duplicate paths (on case-insensitive file systems also
        # if they only differ in case); turn relative paths into absolute
        # paths.
        dir, dircase = makepath(dir)
        if dircase not in known_paths:
            L.append(dir)
            known_paths.add(dircase)
    sys.path[:] = L
    return known_paths","def makepath(*paths):
    dir = os.path.join(*paths)
    try:
        dir = os.path.abspath(dir)
    except OSError:
        pass
    return dir, os.path.normcase(dir)","def normalize_and_absolute_path(*paths):
    dir = os.path.join(*paths)
    try:
        dir = os.path.abspath(dir)
    except OSError:
        pass
    return dir, os.path.normcase(dir)","def remove_duplicate_entries_from_sys_path_and_make_absolute():
    """""" Remove duplicate entries from sys.path along with making them
    absolute""""""
    # This ensures that the initial path provided by the interpreter contains
    # only absolute pathnames, even if we're running from the build directory.
    L = []
    known_paths = set()
    for dir in sys.path:
        # Filter out duplicate paths (on case-insensitive file systems also
        # if they only differ in case); turn relative paths into absolute
        # paths.
        dir, dircase = normalize_and_absolute_path(dir)
        if dircase not in known_paths:
            L.append(dir)
            known_paths.add(dircase)
    sys.path[:] = L
    return known_paths","{""makepath"": ""normalize_and_absolute_path""}","def 0(*paths):
    dir = os.path.join(*paths)
    try:
        dir = os.path.abspath(dir)
    except OSError:
        pass
    return dir, os.path.normcase(dir)","def 1():
    """""" Remove duplicate entries from sys.path along with making them
    absolute""""""
    # This ensures that the initial path provided by the interpreter contains
    # only absolute pathnames, even if we're running from the build directory.
    L = []
    known_paths = set()
    for dir in sys.path:
        # Filter out duplicate paths (on case-insensitive file systems also
        # if they only differ in case); turn relative paths into absolute
        # paths.
        dir, dircase = 0(dir)
        if dircase not in known_paths:
            L.append(dir)
            known_paths.add(dircase)
    sys.path[:] = L
    return known_paths","{""makepath"": ""0""}"
157,157,"def _init_pathinfo():
    """"""Return a set containing all existing file system items from sys.path.""""""
    d = set()
    for item in sys.path:
        try:
            if os.path.exists(item):
                _, itemcase = makepath(item)
                d.add(itemcase)
        except TypeError:
            continue
    return d","def makepath(*paths):
    dir = os.path.join(*paths)
    try:
        dir = os.path.abspath(dir)
    except OSError:
        pass
    return dir, os.path.normcase(dir)","def normalize_and_absolute_path(*paths):
    dir = os.path.join(*paths)
    try:
        dir = os.path.abspath(dir)
    except OSError:
        pass
    return dir, os.path.normcase(dir)","def initialize_path_info():
    """"""Return a set containing all existing file system items from sys.path.""""""
    d = set()
    for item in sys.path:
        try:
            if os.path.exists(item):
                _, itemcase = normalize_and_absolute_path(item)
                d.add(itemcase)
        except TypeError:
            continue
    return d","{""makepath"": ""normalize_and_absolute_path""}","def 0(*paths):
    dir = os.path.join(*paths)
    try:
        dir = os.path.abspath(dir)
    except OSError:
        pass
    return dir, os.path.normcase(dir)","def 1():
    """"""Return a set containing all existing file system items from sys.path.""""""
    d = set()
    for item in sys.path:
        try:
            if os.path.exists(item):
                _, itemcase = 0(item)
                d.add(itemcase)
        except TypeError:
            continue
    return d","{""makepath"": ""0""}"
158,158,"def addsitedir(sitedir, known_paths=None):
    """"""Add 'sitedir' argument to sys.path if missing and handle .pth files in
    'sitedir'""""""
    _trace(f""Adding directory: {sitedir!r}"")
    if known_paths is None:
        known_paths = _init_pathinfo()
        reset = True
    else:
        reset = False
    sitedir, sitedircase = makepath(sitedir)
    if not sitedircase in known_paths:
        sys.path.append(sitedir)        # Add path component
        known_paths.add(sitedircase)
    try:
        names = os.listdir(sitedir)
    except OSError:
        return
    names = [name for name in names if name.endswith("".pth"")]
    for name in sorted(names):
        addpackage(sitedir, name, known_paths)
    if reset:
        known_paths = None
    return known_paths","def makepath(*paths):
    dir = os.path.join(*paths)
    try:
        dir = os.path.abspath(dir)
    except OSError:
        pass
    return dir, os.path.normcase(dir)

def _init_pathinfo():
    """"""Return a set containing all existing file system items from sys.path.""""""
    d = set()
    for item in sys.path:
        try:
            if os.path.exists(item):
                _, itemcase = makepath(item)
                d.add(itemcase)
        except TypeError:
            continue
    return d

def addpackage(sitedir, name, known_paths):
    """"""Process a .pth file within the site-packages directory:
       For each line in the file, either combine it with sitedir to a path
       and add that to known_paths, or execute it if it starts with 'import '.
    """"""
    if known_paths is None:
        known_paths = _init_pathinfo()
        reset = True
    else:
        reset = False
    fullname = os.path.join(sitedir, name)
    _trace(f""Processing .pth file: {fullname!r}"")
    try:
        # locale encoding is not ideal especially on Windows. But we have used
        # it for a long time. setuptools uses the locale encoding too.
        f = io.TextIOWrapper(io.open_code(fullname), encoding=""locale"")
    except OSError:
        return
    with f:
        for n, line in enumerate(f):
            if line.startswith(""#""):
                continue
            if line.strip() == """":
                continue
            try:
                if line.startswith((""import "", ""import\t"")):
                    exec(line)
                    continue
                line = line.rstrip()
                dir, dircase = makepath(sitedir, line)
                if not dircase in known_paths and os.path.exists(dir):
                    sys.path.append(dir)
                    known_paths.add(dircase)
            except Exception:
                print(""Error processing line {:d} of {}:\n"".format(n+1, fullname),
                      file=sys.stderr)
                import traceback
                for record in traceback.format_exception(*sys.exc_info()):
                    for line in record.splitlines():
                        print('  '+line, file=sys.stderr)
                print(""\nRemainder of file ignored"", file=sys.stderr)
                break
    if reset:
        known_paths = None
    return known_paths

def _trace(message):
    if sys.flags.verbose:
        print(message, file=sys.stderr)","def normalize_and_absolute_path(*paths):
    dir = os.path.join(*paths)
    try:
        dir = os.path.abspath(dir)
    except OSError:
        pass
    return dir, os.path.normcase(dir)

def initialize_path_info():
    """"""Return a set containing all existing file system items from sys.path.""""""
    d = set()
    for item in sys.path:
        try:
            if os.path.exists(item):
                _, itemcase = normalize_and_absolute_path(item)
                d.add(itemcase)
        except TypeError:
            continue
    return d

def process_site_packages_pth(sitedir, name, known_paths):
    """"""Process a .pth file within the site-packages directory:
       For each line in the file, either combine it with sitedir to a path
       and add that to known_paths, or execute it if it starts with 'import '.
    """"""
    if known_paths is None:
        known_paths = initialize_path_info()
        reset = True
    else:
        reset = False
    fullname = os.path.join(sitedir, name)
    log_trace_message_if_verbose(f""Processing .pth file: {fullname!r}"")
    try:
        # locale encoding is not ideal especially on Windows. But we have used
        # it for a long time. setuptools uses the locale encoding too.
        f = io.TextIOWrapper(io.open_code(fullname), encoding=""locale"")
    except OSError:
        return
    with f:
        for n, line in enumerate(f):
            if line.startswith(""#""):
                continue
            if line.strip() == """":
                continue
            try:
                if line.startswith((""import "", ""import\t"")):
                    exec(line)
                    continue
                line = line.rstrip()
                dir, dircase = normalize_and_absolute_path(sitedir, line)
                if not dircase in known_paths and os.path.exists(dir):
                    sys.path.append(dir)
                    known_paths.add(dircase)
            except Exception:
                print(""Error processing line {:d} of {}:\n"".format(n+1, fullname),
                      file=sys.stderr)
                import traceback
                for record in traceback.format_exception(*sys.exc_info()):
                    for line in record.splitlines():
                        print('  '+line, file=sys.stderr)
                print(""\nRemainder of file ignored"", file=sys.stderr)
                break
    if reset:
        known_paths = None
    return known_paths

def log_trace_message_if_verbose(message):
    if sys.flags.verbose:
        print(message, file=sys.stderr)","def add_site_directory(sitedir, known_paths=None):
    """"""Add 'sitedir' argument to sys.path if missing and handle .pth files in
    'sitedir'""""""
    log_trace_message_if_verbose(f""Adding directory: {sitedir!r}"")
    if known_paths is None:
        known_paths = initialize_path_info()
        reset = True
    else:
        reset = False
    sitedir, sitedircase = normalize_and_absolute_path(sitedir)
    if not sitedircase in known_paths:
        sys.path.append(sitedir)        # Add path component
        known_paths.add(sitedircase)
    try:
        names = os.listdir(sitedir)
    except OSError:
        return
    names = [name for name in names if name.endswith("".pth"")]
    for name in sorted(names):
        process_site_packages_pth(sitedir, name, known_paths)
    if reset:
        known_paths = None
    return known_paths","{""makepath"": ""normalize_and_absolute_path"", ""_init_pathinfo"": ""initialize_path_info"", ""addpackage"": ""process_site_packages_pth"", ""_trace"": ""log_trace_message_if_verbose""}","def 0(*paths):
    dir = os.path.join(*paths)
    try:
        dir = os.path.abspath(dir)
    except OSError:
        pass
    return dir, os.path.normcase(dir)

def 1():
    """"""Return a set containing all existing file system items from sys.path.""""""
    d = set()
    for item in sys.path:
        try:
            if os.path.exists(item):
                _, itemcase = 0(item)
                d.add(itemcase)
        except TypeError:
            continue
    return d

def 2(sitedir, name, known_paths):
    """"""Process a .pth file within the site-packages directory:
       For each line in the file, either combine it with sitedir to a path
       and add that to known_paths, or execute it if it starts with 'import '.
    """"""
    if known_paths is None:
        known_paths = 1()
        reset = True
    else:
        reset = False
    fullname = os.path.join(sitedir, name)
    3(f""Processing .pth file: {fullname!r}"")
    try:
        # locale encoding is not ideal especially on Windows. But we have used
        # it for a long time. setuptools uses the locale encoding too.
        f = io.TextIOWrapper(io.open_code(fullname), encoding=""locale"")
    except OSError:
        return
    with f:
        for n, line in enumerate(f):
            if line.startswith(""#""):
                continue
            if line.strip() == """":
                continue
            try:
                if line.startswith((""import "", ""import\t"")):
                    exec(line)
                    continue
                line = line.rstrip()
                dir, dircase = 0(sitedir, line)
                if not dircase in known_paths and os.path.exists(dir):
                    sys.path.append(dir)
                    known_paths.add(dircase)
            except Exception:
                print(""Error processing line {:d} of {}:\n"".format(n+1, fullname),
                      file=sys.stderr)
                import traceback
                for record in traceback.format_exception(*sys.exc_info()):
                    for line in record.splitlines():
                        print('  '+line, file=sys.stderr)
                print(""\nRemainder of file ignored"", file=sys.stderr)
                break
    if reset:
        known_paths = None
    return known_paths

def 3(message):
    if sys.flags.verbose:
        print(message, file=sys.stderr)","def 4(sitedir, known_paths=None):
    """"""Add 'sitedir' argument to sys.path if missing and handle .pth files in
    'sitedir'""""""
    3(f""Adding directory: {sitedir!r}"")
    if known_paths is None:
        known_paths = 1()
        reset = True
    else:
        reset = False
    sitedir, sitedircase = 0(sitedir)
    if not sitedircase in known_paths:
        sys.path.append(sitedir)        # Add path component
        known_paths.add(sitedircase)
    try:
        names = os.listdir(sitedir)
    except OSError:
        return
    names = [name for name in names if name.endswith("".pth"")]
    for name in sorted(names):
        2(sitedir, name, known_paths)
    if reset:
        known_paths = None
    return known_paths","{""makepath"": ""0"", ""_init_pathinfo"": ""1"", ""addpackage"": ""2"", ""_trace"": ""3""}"
159,159,"def getusersitepackages():
    """"""Returns the user-specific site-packages directory path.

    If the global variable ``USER_SITE`` is not initialized yet, this
    function will also set it.
    """"""
    global USER_SITE, ENABLE_USER_SITE
    userbase = getuserbase() # this will also set USER_BASE

    if USER_SITE is None:
        if userbase is None:
            ENABLE_USER_SITE = False # disable user site and return None
        else:
            USER_SITE = _get_path(userbase)

    return USER_SITE","def _get_path(userbase):
    version = sys.version_info

    if os.name == 'nt':
        ver_nodot = sys.winver.replace('.', '')
        return f'{userbase}\\Python{ver_nodot}\\site-packages'

    if sys.platform == 'darwin' and sys._framework:
        return f'{userbase}/lib/python/site-packages'

    return f'{userbase}/lib/python{version[0]}.{version[1]}/site-packages'

def getuserbase():
    """"""Returns the `user base` directory path.

    The `user base` directory can be used to store data. If the global
    variable ``USER_BASE`` is not initialized yet, this function will also set
    it.
    """"""
    global USER_BASE
    if USER_BASE is None:
        USER_BASE = _getuserbase()
    return USER_BASE","def get_site_packages_path(userbase):
    version = sys.version_info

    if os.name == 'nt':
        ver_nodot = sys.winver.replace('.', '')
        return f'{userbase}\\Python{ver_nodot}\\site-packages'

    if sys.platform == 'darwin' and sys._framework:
        return f'{userbase}/lib/python/site-packages'

    return f'{userbase}/lib/python{version[0]}.{version[1]}/site-packages'

def get_user_base():
    """"""Returns the `user base` directory path.

    The `user base` directory can be used to store data. If the global
    variable ``USER_BASE`` is not initialized yet, this function will also set
    it.
    """"""
    global USER_BASE
    if USER_BASE is None:
        USER_BASE = _get_user_base()
    return USER_BASE","def get_user_site_packages():
    """"""Returns the user-specific site-packages directory path.

    If the global variable ``USER_SITE`` is not initialized yet, this
    function will also set it.
    """"""
    global USER_SITE, ENABLE_USER_SITE
    userbase = get_user_base() # this will also set USER_BASE

    if USER_SITE is None:
        if userbase is None:
            ENABLE_USER_SITE = False # disable user site and return None
        else:
            USER_SITE = get_site_packages_path(userbase)

    return USER_SITE","{""_get_path"": ""get_site_packages_path"", ""getuserbase"": ""get_user_base""}","def 0(userbase):
    version = sys.version_info

    if os.name == 'nt':
        ver_nodot = sys.winver.replace('.', '')
        return f'{userbase}\\Python{ver_nodot}\\site-packages'

    if sys.platform == 'darwin' and sys._framework:
        return f'{userbase}/lib/python/site-packages'

    return f'{userbase}/lib/python{version[0]}.{version[1]}/site-packages'

def 1():
    """"""Returns the `user base` directory path.

    The `user base` directory can be used to store data. If the global
    variable ``USER_BASE`` is not initialized yet, this function will also set
    it.
    """"""
    global USER_BASE
    if USER_BASE is None:
        USER_BASE = _1()
    return USER_BASE","def 2():
    """"""Returns the user-specific site-packages directory path.

    If the global variable ``USER_SITE`` is not initialized yet, this
    function will also set it.
    """"""
    global USER_SITE, ENABLE_USER_SITE
    userbase = 1() # this will also set USER_BASE

    if USER_SITE is None:
        if userbase is None:
            ENABLE_USER_SITE = False # disable user site and return None
        else:
            USER_SITE = 0(userbase)

    return USER_SITE","{""_get_path"": ""0"", ""getuserbase"": ""1""}"
160,160,"def addusersitepackages(known_paths):
    """"""Add a per user site-package to sys.path

    Each user has its own python directory with site-packages in the
    home directory.
    """"""
    # get the per user site-package path
    # this call will also make sure USER_BASE and USER_SITE are set
    _trace(""Processing user site-packages"")
    user_site = getusersitepackages()

    if ENABLE_USER_SITE and os.path.isdir(user_site):
        addsitedir(user_site, known_paths)
    return known_paths","def getusersitepackages():
    """"""Returns the user-specific site-packages directory path.

    If the global variable ``USER_SITE`` is not initialized yet, this
    function will also set it.
    """"""
    global USER_SITE, ENABLE_USER_SITE
    userbase = getuserbase() # this will also set USER_BASE

    if USER_SITE is None:
        if userbase is None:
            ENABLE_USER_SITE = False # disable user site and return None
        else:
            USER_SITE = _get_path(userbase)

    return USER_SITE

def addsitedir(sitedir, known_paths=None):
    """"""Add 'sitedir' argument to sys.path if missing and handle .pth files in
    'sitedir'""""""
    _trace(f""Adding directory: {sitedir!r}"")
    if known_paths is None:
        known_paths = _init_pathinfo()
        reset = True
    else:
        reset = False
    sitedir, sitedircase = makepath(sitedir)
    if not sitedircase in known_paths:
        sys.path.append(sitedir)        # Add path component
        known_paths.add(sitedircase)
    try:
        names = os.listdir(sitedir)
    except OSError:
        return
    names = [name for name in names if name.endswith("".pth"")]
    for name in sorted(names):
        addpackage(sitedir, name, known_paths)
    if reset:
        known_paths = None
    return known_paths

def _trace(message):
    if sys.flags.verbose:
        print(message, file=sys.stderr)","def get_user_site_packages():
    """"""Returns the user-specific site-packages directory path.

    If the global variable ``USER_SITE`` is not initialized yet, this
    function will also set it.
    """"""
    global USER_SITE, ENABLE_USER_SITE
    userbase = getuserbase() # this will also set USER_BASE

    if USER_SITE is None:
        if userbase is None:
            ENABLE_USER_SITE = False # disable user site and return None
        else:
            USER_SITE = _get_path(userbase)

    return USER_SITE

def add_site_directory(sitedir, known_paths=None):
    """"""Add 'sitedir' argument to sys.path if missing and handle .pth files in
    'sitedir'""""""
    log_trace_message_if_verbose(f""Adding directory: {sitedir!r}"")
    if known_paths is None:
        known_paths = _init_pathinfo()
        reset = True
    else:
        reset = False
    sitedir, sitedircase = makepath(sitedir)
    if not sitedircase in known_paths:
        sys.path.append(sitedir)        # Add path component
        known_paths.add(sitedircase)
    try:
        names = os.listdir(sitedir)
    except OSError:
        return
    names = [name for name in names if name.endswith("".pth"")]
    for name in sorted(names):
        addpackage(sitedir, name, known_paths)
    if reset:
        known_paths = None
    return known_paths

def log_trace_message_if_verbose(message):
    if sys.flags.verbose:
        print(message, file=sys.stderr)","def add_per_user_site_package_to_sys_path(known_paths):
    """"""Add a per user site-package to sys.path

    Each user has its own python directory with site-packages in the
    home directory.
    """"""
    # get the per user site-package path
    # this call will also make sure USER_BASE and USER_SITE are set
    log_trace_message_if_verbose(""Processing user site-packages"")
    user_site = get_user_site_packages()

    if ENABLE_USER_SITE and os.path.isdir(user_site):
        add_site_directory(user_site, known_paths)
    return known_paths","{""getusersitepackages"": ""get_user_site_packages"", ""addsitedir"": ""add_site_directory"", ""_trace"": ""log_trace_message_if_verbose""}","def 0():
    """"""Returns the user-specific site-packages directory path.

    If the global variable ``USER_SITE`` is not initialized yet, this
    function will also set it.
    """"""
    global USER_SITE, ENABLE_USER_SITE
    userbase = getuserbase() # this will also set USER_BASE

    if USER_SITE is None:
        if userbase is None:
            ENABLE_USER_SITE = False # disable user site and return None
        else:
            USER_SITE = _get_path(userbase)

    return USER_SITE

def 1(sitedir, known_paths=None):
    """"""Add 'sitedir' argument to sys.path if missing and handle .pth files in
    'sitedir'""""""
    2(f""Adding directory: {sitedir!r}"")
    if known_paths is None:
        known_paths = _init_pathinfo()
        reset = True
    else:
        reset = False
    sitedir, sitedircase = makepath(sitedir)
    if not sitedircase in known_paths:
        sys.path.append(sitedir)        # Add path component
        known_paths.add(sitedircase)
    try:
        names = os.listdir(sitedir)
    except OSError:
        return
    names = [name for name in names if name.endswith("".pth"")]
    for name in sorted(names):
        addpackage(sitedir, name, known_paths)
    if reset:
        known_paths = None
    return known_paths

def 2(message):
    if sys.flags.verbose:
        print(message, file=sys.stderr)","def 3(known_paths):
    """"""Add a per user site-package to sys.path

    Each user has its own python directory with site-packages in the
    home directory.
    """"""
    # get the per user site-package path
    # this call will also make sure USER_BASE and USER_SITE are set
    2(""Processing user site-packages"")
    user_site = 0()

    if ENABLE_USER_SITE and os.path.isdir(user_site):
        1(user_site, known_paths)
    return known_paths","{""getusersitepackages"": ""0"", ""addsitedir"": ""1"", ""_trace"": ""2""}"
161,161,"def addsitepackages(known_paths, prefixes=None):
    """"""Add site-packages to sys.path""""""
    _trace(""Processing global site-packages"")
    for sitedir in getsitepackages(prefixes):
        if os.path.isdir(sitedir):
            addsitedir(sitedir, known_paths)

    return known_paths","def getsitepackages(prefixes=None):
    """"""Returns a list containing all global site-packages directories.

    For each directory present in ``prefixes`` (or the global ``PREFIXES``),
    this function will find its `site-packages` subdirectory depending on the
    system environment, and will return a list of full paths.
    """"""
    sitepackages = []
    seen = set()

    if prefixes is None:
        prefixes = PREFIXES

    for prefix in prefixes:
        if not prefix or prefix in seen:
            continue
        seen.add(prefix)

        libdirs = [sys.platlibdir]
        if sys.platlibdir != ""lib"":
            libdirs.append(""lib"")

        if os.sep == '/':
            for libdir in libdirs:
                path = os.path.join(prefix, libdir,
                                    ""python%d.%d"" % sys.version_info[:2],
                                    ""site-packages"")
                sitepackages.append(path)
        else:
            sitepackages.append(prefix)

            for libdir in libdirs:
                path = os.path.join(prefix, libdir, ""site-packages"")
                sitepackages.append(path)
    return sitepackages

def addsitedir(sitedir, known_paths=None):
    """"""Add 'sitedir' argument to sys.path if missing and handle .pth files in
    'sitedir'""""""
    _trace(f""Adding directory: {sitedir!r}"")
    if known_paths is None:
        known_paths = _init_pathinfo()
        reset = True
    else:
        reset = False
    sitedir, sitedircase = makepath(sitedir)
    if not sitedircase in known_paths:
        sys.path.append(sitedir)        # Add path component
        known_paths.add(sitedircase)
    try:
        names = os.listdir(sitedir)
    except OSError:
        return
    names = [name for name in names if name.endswith("".pth"")]
    for name in sorted(names):
        addpackage(sitedir, name, known_paths)
    if reset:
        known_paths = None
    return known_paths

def _trace(message):
    if sys.flags.verbose:
        print(message, file=sys.stderr)","def get_site_packages_directories(prefixes=None):
    """"""Returns a list containing all global site-packages directories.

    For each directory present in ``prefixes`` (or the global ``PREFIXES``),
    this function will find its `site-packages` subdirectory depending on the
    system environment, and will return a list of full paths.
    """"""
    sitepackages = []
    seen = set()

    if prefixes is None:
        prefixes = PREFIXES

    for prefix in prefixes:
        if not prefix or prefix in seen:
            continue
        seen.add(prefix)

        libdirs = [sys.platlibdir]
        if sys.platlibdir != ""lib"":
            libdirs.append(""lib"")

        if os.sep == '/':
            for libdir in libdirs:
                path = os.path.join(prefix, libdir,
                                    ""python%d.%d"" % sys.version_info[:2],
                                    ""site-packages"")
                sitepackages.append(path)
        else:
            sitepackages.append(prefix)

            for libdir in libdirs:
                path = os.path.join(prefix, libdir, ""site-packages"")
                sitepackages.append(path)
    return sitepackages

def add_site_directory(sitedir, known_paths=None):
    """"""Add 'sitedir' argument to sys.path if missing and handle .pth files in
    'sitedir'""""""
    log_trace_message_if_verbose(f""Adding directory: {sitedir!r}"")
    if known_paths is None:
        known_paths = _init_pathinfo()
        reset = True
    else:
        reset = False
    sitedir, sitedircase = makepath(sitedir)
    if not sitedircase in known_paths:
        sys.path.append(sitedir)        # Add path component
        known_paths.add(sitedircase)
    try:
        names = os.listdir(sitedir)
    except OSError:
        return
    names = [name for name in names if name.endswith("".pth"")]
    for name in sorted(names):
        addpackage(sitedir, name, known_paths)
    if reset:
        known_paths = None
    return known_paths

def log_trace_message_if_verbose(message):
    if sys.flags.verbose:
        print(message, file=sys.stderr)","def add_site_packages_to_sys_path(known_paths, prefixes=None):
    """"""Add site-packages to sys.path""""""
    log_trace_message_if_verbose(""Processing global site-packages"")
    for sitedir in get_site_packages_directories(prefixes):
        if os.path.isdir(sitedir):
            add_site_directory(sitedir, known_paths)

    return known_paths","{""getsitepackages"": ""get_site_packages_directories"", ""addsitedir"": ""add_site_directory"", ""_trace"": ""log_trace_message_if_verbose""}","def 0(prefixes=None):
    """"""Returns a list containing all global site-packages directories.

    For each directory present in ``prefixes`` (or the global ``PREFIXES``),
    this function will find its `site-packages` subdirectory depending on the
    system environment, and will return a list of full paths.
    """"""
    sitepackages = []
    seen = set()

    if prefixes is None:
        prefixes = PREFIXES

    for prefix in prefixes:
        if not prefix or prefix in seen:
            continue
        seen.add(prefix)

        libdirs = [sys.platlibdir]
        if sys.platlibdir != ""lib"":
            libdirs.append(""lib"")

        if os.sep == '/':
            for libdir in libdirs:
                path = os.path.join(prefix, libdir,
                                    ""python%d.%d"" % sys.version_info[:2],
                                    ""site-packages"")
                sitepackages.append(path)
        else:
            sitepackages.append(prefix)

            for libdir in libdirs:
                path = os.path.join(prefix, libdir, ""site-packages"")
                sitepackages.append(path)
    return sitepackages

def 1(sitedir, known_paths=None):
    """"""Add 'sitedir' argument to sys.path if missing and handle .pth files in
    'sitedir'""""""
    2(f""Adding directory: {sitedir!r}"")
    if known_paths is None:
        known_paths = _init_pathinfo()
        reset = True
    else:
        reset = False
    sitedir, sitedircase = makepath(sitedir)
    if not sitedircase in known_paths:
        sys.path.append(sitedir)        # Add path component
        known_paths.add(sitedircase)
    try:
        names = os.listdir(sitedir)
    except OSError:
        return
    names = [name for name in names if name.endswith("".pth"")]
    for name in sorted(names):
        addpackage(sitedir, name, known_paths)
    if reset:
        known_paths = None
    return known_paths

def 2(message):
    if sys.flags.verbose:
        print(message, file=sys.stderr)","def 3(known_paths, prefixes=None):
    """"""Add site-packages to sys.path""""""
    2(""Processing global site-packages"")
    for sitedir in 0(prefixes):
        if os.path.isdir(sitedir):
            1(sitedir, known_paths)

    return known_paths","{""getsitepackages"": ""0"", ""addsitedir"": ""1"", ""_trace"": ""2""}"
162,162,"def recursive_repr(fillvalue='...'):
    'Decorator to make a repr function return fillvalue for a recursive call'

    def decorating_function(user_function):
        repr_running = set()

        def wrapper(self):
            key = id(self), get_ident()
            if key in repr_running:
                return fillvalue
            repr_running.add(key)
            try:
                result = user_function(self)
            finally:
                repr_running.discard(key)
            return result

        # Can't use functools.wraps() here because of bootstrap issues
        wrapper.__module__ = getattr(user_function, '__module__')
        wrapper.__doc__ = getattr(user_function, '__doc__')
        wrapper.__name__ = getattr(user_function, '__name__')
        wrapper.__qualname__ = getattr(user_function, '__qualname__')
        wrapper.__annotations__ = getattr(user_function, '__annotations__', {})
        return wrapper

    return decorating_function","def id() -> str:
    """"""
    Return the distro ID of the current distribution, as a
    machine-readable string.

    For a number of OS distributions, the returned distro ID value is
    *reliable*, in the sense that it is documented and that it does not change
    across releases of the distribution.

    This package maintains the following reliable distro ID values:

    ==============  =========================================
    Distro ID       Distribution
    ==============  =========================================
    ""ubuntu""        Ubuntu
    ""debian""        Debian
    ""rhel""          RedHat Enterprise Linux
    ""centos""        CentOS
    ""fedora""        Fedora
    ""sles""          SUSE Linux Enterprise Server
    ""opensuse""      openSUSE
    ""amzn""          Amazon Linux
    ""arch""          Arch Linux
    ""cloudlinux""    CloudLinux OS
    ""exherbo""       Exherbo Linux
    ""gentoo""        GenToo Linux
    ""ibm_powerkvm""  IBM PowerKVM
    ""kvmibm""        KVM for IBM z Systems
    ""linuxmint""     Linux Mint
    ""mageia""        Mageia
    ""mandriva""      Mandriva Linux
    ""parallels""     Parallels
    ""pidora""        Pidora
    ""raspbian""      Raspbian
    ""oracle""        Oracle Linux (and Oracle Enterprise Linux)
    ""scientific""    Scientific Linux
    ""slackware""     Slackware
    ""xenserver""     XenServer
    ""openbsd""       OpenBSD
    ""netbsd""        NetBSD
    ""freebsd""       FreeBSD
    ""midnightbsd""   MidnightBSD
    ""rocky""         Rocky Linux
    ""aix""           AIX
    ==============  =========================================

    If you have a need to get distros for reliable IDs added into this set,
    or if you find that the :func:`distro.id` function returns a different
    distro ID for one of the listed distros, please create an issue in the
    `distro issue tracker`_.

    **Lookup hierarchy and transformations:**

    First, the ID is obtained from the following sources, in the specified
    order. The first available and non-empty value is used:

    * the value of the ""ID"" attribute of the os-release file,

    * the value of the ""Distributor ID"" attribute returned by the lsb_release
      command,

    * the first part of the file name of the distro release file,

    The so determined ID value then passes the following transformations,
    before it is returned by this method:

    * it is translated to lower case,

    * blanks (which should not be there anyway) are translated to underscores,

    * a normalization of the ID is performed, based upon
      `normalization tables`_. The purpose of this normalization is to ensure
      that the ID is as reliable as possible, even across incompatible changes
      in the OS distributions. A common reason for an incompatible change is
      the addition of an os-release file, or the addition of the lsb_release
      command, with ID values that differ from what was previously determined
      from the distro release file name.
    """"""
    return _distro.id()","def id() -> str:
    """"""
    Return the distro ID of the current distribution, as a
    machine-readable string.

    For a number of OS distributions, the returned distro ID value is
    *reliable*, in the sense that it is documented and that it does not change
    across releases of the distribution.

    This package maintains the following reliable distro ID values:

    ==============  =========================================
    Distro ID       Distribution
    ==============  =========================================
    ""ubuntu""        Ubuntu
    ""debian""        Debian
    ""rhel""          RedHat Enterprise Linux
    ""centos""        CentOS
    ""fedora""        Fedora
    ""sles""          SUSE Linux Enterprise Server
    ""opensuse""      openSUSE
    ""amzn""          Amazon Linux
    ""arch""          Arch Linux
    ""cloudlinux""    CloudLinux OS
    ""exherbo""       Exherbo Linux
    ""gentoo""        GenToo Linux
    ""ibm_powerkvm""  IBM PowerKVM
    ""kvmibm""        KVM for IBM z Systems
    ""linuxmint""     Linux Mint
    ""mageia""        Mageia
    ""mandriva""      Mandriva Linux
    ""parallels""     Parallels
    ""pidora""        Pidora
    ""raspbian""      Raspbian
    ""oracle""        Oracle Linux (and Oracle Enterprise Linux)
    ""scientific""    Scientific Linux
    ""slackware""     Slackware
    ""xenserver""     XenServer
    ""openbsd""       OpenBSD
    ""netbsd""        NetBSD
    ""freebsd""       FreeBSD
    ""midnightbsd""   MidnightBSD
    ""rocky""         Rocky Linux
    ""aix""           AIX
    ==============  =========================================

    If you have a need to get distros for reliable IDs added into this set,
    or if you find that the :func:`distro.id` function returns a different
    distro ID for one of the listed distros, please create an issue in the
    `distro issue tracker`_.

    **Lookup hierarchy and transformations:**

    First, the ID is obtained from the following sources, in the specified
    order. The first available and non-empty value is used:

    * the value of the ""ID"" attribute of the os-release file,

    * the value of the ""Distributor ID"" attribute returned by the lsb_release
      command,

    * the first part of the file name of the distro release file,

    The so determined ID value then passes the following transformations,
    before it is returned by this method:

    * it is translated to lower case,

    * blanks (which should not be there anyway) are translated to underscores,

    * a normalization of the ID is performed, based upon
      `normalization tables`_. The purpose of this normalization is to ensure
      that the ID is as reliable as possible, even across incompatible changes
      in the OS distributions. A common reason for an incompatible change is
      the addition of an os-release file, or the addition of the lsb_release
      command, with ID values that differ from what was previously determined
      from the distro release file name.
    """"""
    return _distro.id()","def decorator_to_make_repr_return_fillvalue_for_recursive_call(fillvalue='...'):
    'Decorator to make a repr function return fillvalue for a recursive call'

    def function_decorated_to_handle_recursive_repr(user_function):
        repr_running = set()

        def initialize_curses_and_call_function(self):
            key = id(self), get_ident()
            if key in repr_running:
                return fillvalue
            repr_running.add(key)
            try:
                result = user_function(self)
            finally:
                repr_running.discard(key)
            return result

        # Can't use functools.wraps() here because of bootstrap issues
        initialize_curses_and_call_function.__module__ = getattr(user_function, '__module__')
        initialize_curses_and_call_function.__doc__ = getattr(user_function, '__doc__')
        initialize_curses_and_call_function.__name__ = getattr(user_function, '__name__')
        initialize_curses_and_call_function.__qualname__ = getattr(user_function, '__qualname__')
        initialize_curses_and_call_function.__annotations__ = getattr(user_function, '__annotations__', {})
        return initialize_curses_and_call_function

    return function_decorated_to_handle_recursive_repr",{},"def id() -> str:
    """"""
    Return the distro ID of the current distribution, as a
    machine-readable string.

    For a number of OS distributions, the returned distro ID value is
    *reliable*, in the sense that it is documented and that it does not change
    across releases of the distribution.

    This package maintains the following reliable distro ID values:

    ==============  =========================================
    Distro ID       Distribution
    ==============  =========================================
    ""ubuntu""        Ubuntu
    ""debian""        Debian
    ""rhel""          RedHat Enterprise Linux
    ""centos""        CentOS
    ""fedora""        Fedora
    ""sles""          SUSE Linux Enterprise Server
    ""opensuse""      openSUSE
    ""amzn""          Amazon Linux
    ""arch""          Arch Linux
    ""cloudlinux""    CloudLinux OS
    ""exherbo""       Exherbo Linux
    ""gentoo""        GenToo Linux
    ""ibm_powerkvm""  IBM PowerKVM
    ""kvmibm""        KVM for IBM z Systems
    ""linuxmint""     Linux Mint
    ""mageia""        Mageia
    ""mandriva""      Mandriva Linux
    ""parallels""     Parallels
    ""pidora""        Pidora
    ""raspbian""      Raspbian
    ""oracle""        Oracle Linux (and Oracle Enterprise Linux)
    ""scientific""    Scientific Linux
    ""slackware""     Slackware
    ""xenserver""     XenServer
    ""openbsd""       OpenBSD
    ""netbsd""        NetBSD
    ""freebsd""       FreeBSD
    ""midnightbsd""   MidnightBSD
    ""rocky""         Rocky Linux
    ""aix""           AIX
    ==============  =========================================

    If you have a need to get distros for reliable IDs added into this set,
    or if you find that the :func:`distro.id` function returns a different
    distro ID for one of the listed distros, please create an issue in the
    `distro issue tracker`_.

    **Lookup hierarchy and transformations:**

    First, the ID is obtained from the following sources, in the specified
    order. The first available and non-empty value is used:

    * the value of the ""ID"" attribute of the os-release file,

    * the value of the ""Distributor ID"" attribute returned by the lsb_release
      command,

    * the first part of the file name of the distro release file,

    The so determined ID value then passes the following transformations,
    before it is returned by this method:

    * it is translated to lower case,

    * blanks (which should not be there anyway) are translated to underscores,

    * a normalization of the ID is performed, based upon
      `normalization tables`_. The purpose of this normalization is to ensure
      that the ID is as reliable as possible, even across incompatible changes
      in the OS distributions. A common reason for an incompatible change is
      the addition of an os-release file, or the addition of the lsb_release
      command, with ID values that differ from what was previously determined
      from the distro release file name.
    """"""
    return _distro.id()","def 3(fillvalue='...'):
    'Decorator to make a repr function return fillvalue for a recursive call'

    def 4(user_function):
        repr_running = set()

        def 5(self):
            key = id(self), get_ident()
            if key in repr_running:
                return fillvalue
            repr_running.add(key)
            try:
                result = user_function(self)
            finally:
                repr_running.discard(key)
            return result

        # Can't use functools.wraps() here because of bootstrap issues
        wrapper.__module__ = getattr(user_function, '__module__')
        wrapper.__doc__ = getattr(user_function, '__doc__')
        wrapper.__name__ = getattr(user_function, '__name__')
        wrapper.__qualname__ = getattr(user_function, '__qualname__')
        wrapper.__annotations__ = getattr(user_function, '__annotations__', {})
        return wrapper

    return decorating_function",{}
163,163,"def token_hex(nbytes=None):
    """"""Return a random text string, in hexadecimal.

    The string has *nbytes* random bytes, each byte converted to two
    hex digits.  If *nbytes* is ``None`` or not supplied, a reasonable
    default is used.

    >>> token_hex(16)  #doctest:+SKIP
    'f9bf78b9a18ce6d46a0cd2b0b86df9da'

    """"""
    return binascii.hexlify(token_bytes(nbytes)).decode('ascii')","def token_bytes(nbytes=None):
    """"""Return a random byte string containing *nbytes* bytes.

    If *nbytes* is ``None`` or not supplied, a reasonable
    default is used.

    >>> token_bytes(16)  #doctest:+SKIP
    b'\\xebr\\x17D*t\\xae\\xd4\\xe3S\\xb6\\xe2\\xebP1\\x8b'

    """"""
    if nbytes is None:
        nbytes = DEFAULT_ENTROPY
    return _sysrand.randbytes(nbytes)","def generate_random_byte_string_of_length(nbytes=None):
    """"""Return a random byte string containing *nbytes* bytes.

    If *nbytes* is ``None`` or not supplied, a reasonable
    default is used.

    >>> generate_random_byte_string_of_length(16)  #doctest:+SKIP
    b'\\xebr\\x17D*t\\xae\\xd4\\xe3S\\xb6\\xe2\\xebP1\\x8b'

    """"""
    if nbytes is None:
        nbytes = DEFAULT_ENTROPY
    return _sysrand.randbytes(nbytes)","def return_random_text_string_in_hexadecimal_with_nbytes(nbytes=None):
    """"""Return a random text string, in hexadecimal.

    The string has *nbytes* random bytes, each byte converted to two
    hex digits.  If *nbytes* is ``None`` or not supplied, a reasonable
    default is used.

    >>> return_random_text_string_in_hexadecimal_with_nbytes(16)  #doctest:+SKIP
    'f9bf78b9a18ce6d46a0cd2b0b86df9da'

    """"""
    return binascii.hexlify(generate_random_byte_string_of_length(nbytes)).decode('ascii')","{""token_bytes"": ""generate_random_byte_string_of_length""}","def 0(nbytes=None):
    """"""Return a random byte string containing *nbytes* bytes.

    If *nbytes* is ``None`` or not supplied, a reasonable
    default is used.

    >>> 0(16)  #doctest:+SKIP
    b'\\xebr\\x17D*t\\xae\\xd4\\xe3S\\xb6\\xe2\\xebP1\\x8b'

    """"""
    if nbytes is None:
        nbytes = DEFAULT_ENTROPY
    return _sysrand.randbytes(nbytes)","def 1(nbytes=None):
    """"""Return a random text string, in hexadecimal.

    The string has *nbytes* random bytes, each byte converted to two
    hex digits.  If *nbytes* is ``None`` or not supplied, a reasonable
    default is used.

    >>> 1(16)  #doctest:+SKIP
    'f9bf78b9a18ce6d46a0cd2b0b86df9da'

    """"""
    return binascii.hexlify(0(nbytes)).decode('ascii')","{""token_bytes"": ""0""}"
164,164,"def token_urlsafe(nbytes=None):
    """"""Return a random URL-safe text string, in Base64 encoding.

    The string has *nbytes* random bytes.  If *nbytes* is ``None``
    or not supplied, a reasonable default is used.

    >>> token_urlsafe(16)  #doctest:+SKIP
    'Drmhze6EPcv0fN_81Bj-nA'

    """"""
    tok = token_bytes(nbytes)
    return base64.urlsafe_b64encode(tok).rstrip(b'=').decode('ascii')","def token_bytes(nbytes=None):
    """"""Return a random byte string containing *nbytes* bytes.

    If *nbytes* is ``None`` or not supplied, a reasonable
    default is used.

    >>> token_bytes(16)  #doctest:+SKIP
    b'\\xebr\\x17D*t\\xae\\xd4\\xe3S\\xb6\\xe2\\xebP1\\x8b'

    """"""
    if nbytes is None:
        nbytes = DEFAULT_ENTROPY
    return _sysrand.randbytes(nbytes)","def generate_random_byte_string_of_length(nbytes=None):
    """"""Return a random byte string containing *nbytes* bytes.

    If *nbytes* is ``None`` or not supplied, a reasonable
    default is used.

    >>> generate_random_byte_string_of_length(16)  #doctest:+SKIP
    b'\\xebr\\x17D*t\\xae\\xd4\\xe3S\\xb6\\xe2\\xebP1\\x8b'

    """"""
    if nbytes is None:
        nbytes = DEFAULT_ENTROPY
    return _sysrand.randbytes(nbytes)","def return_random_URL_safe_text_string_in_Base64_encoding_with_nbytes(nbytes=None):
    """"""Return a random URL-safe text string, in Base64 encoding.

    The string has *nbytes* random bytes.  If *nbytes* is ``None``
    or not supplied, a reasonable default is used.

    >>> return_random_URL_safe_text_string_in_Base64_encoding_with_nbytes(16)  #doctest:+SKIP
    'Drmhze6EPcv0fN_81Bj-nA'

    """"""
    tok = generate_random_byte_string_of_length(nbytes)
    return base64.urlsafe_b64encode(tok).rstrip(b'=').decode('ascii')","{""token_bytes"": ""generate_random_byte_string_of_length""}","def 0(nbytes=None):
    """"""Return a random byte string containing *nbytes* bytes.

    If *nbytes* is ``None`` or not supplied, a reasonable
    default is used.

    >>> 0(16)  #doctest:+SKIP
    b'\\xebr\\x17D*t\\xae\\xd4\\xe3S\\xb6\\xe2\\xebP1\\x8b'

    """"""
    if nbytes is None:
        nbytes = DEFAULT_ENTROPY
    return _sysrand.randbytes(nbytes)","def 1(nbytes=None):
    """"""Return a random URL-safe text string, in Base64 encoding.

    The string has *nbytes* random bytes.  If *nbytes* is ``None``
    or not supplied, a reasonable default is used.

    >>> 1(16)  #doctest:+SKIP
    'Drmhze6EPcv0fN_81Bj-nA'

    """"""
    tok = 0(nbytes)
    return base64.urlsafe_b64encode(tok).rstrip(b'=').decode('ascii')","{""token_bytes"": ""0""}"
165,165,"def realpath(filename, *, strict=False):
    """"""Return the canonical path of the specified filename, eliminating any
symbolic links encountered in the path.""""""
    filename = os.fspath(filename)
    path, ok = _joinrealpath(filename[:0], filename, strict, {})
    return abspath(path)","def abspath(path):
    """"""Return an absolute path.""""""
    path = os.fspath(path)
    if not isabs(path):
        if isinstance(path, bytes):
            cwd = os.getcwdb()
        else:
            cwd = os.getcwd()
        path = join(cwd, path)
    return normpath(path)

def _joinrealpath(path, rest, strict, seen):
    if isinstance(path, bytes):
        sep = b'/'
        curdir = b'.'
        pardir = b'..'
    else:
        sep = '/'
        curdir = '.'
        pardir = '..'

    if isabs(rest):
        rest = rest[1:]
        path = sep

    while rest:
        name, _, rest = rest.partition(sep)
        if not name or name == curdir:
            # current dir
            continue
        if name == pardir:
            # parent dir
            if path:
                path, name = split(path)
                if name == pardir:
                    path = join(path, pardir, pardir)
            else:
                path = pardir
            continue
        newpath = join(path, name)
        try:
            st = os.lstat(newpath)
        except OSError:
            if strict:
                raise
            is_link = False
        else:
            is_link = stat.S_ISLNK(st.st_mode)
        if not is_link:
            path = newpath
            continue
        # Resolve the symbolic link
        if newpath in seen:
            # Already seen this path
            path = seen[newpath]
            if path is not None:
                # use cached value
                continue
            # The symlink is not resolved, so we must have a symlink loop.
            if strict:
                # Raise OSError(errno.ELOOP)
                os.stat(newpath)
            else:
                # Return already resolved part + rest of the path unchanged.
                return join(newpath, rest), False
        seen[newpath] = None # not resolved symlink
        path, ok = _joinrealpath(path, os.readlink(newpath), strict, seen)
        if not ok:
            return join(path, rest), False
        seen[newpath] = path # resolved symlink

    return path, True","def get_absolute_path(path):
    """"""Return an absolute path.""""""
    path = os.fspath(path)
    if not isabs(path):
        if isinstance(path, bytes):
            cwd = os.getcwdb()
        else:
            cwd = os.getcwd()
        path = join(cwd, path)
    return normpath(path)

def join_real_path(path, rest, strict, seen):
    if isinstance(path, bytes):
        sep = b'/'
        curdir = b'.'
        pardir = b'..'
    else:
        sep = '/'
        curdir = '.'
        pardir = '..'

    if isabs(rest):
        rest = rest[1:]
        path = sep

    while rest:
        name, _, rest = rest.partition(sep)
        if not name or name == curdir:
            # current dir
            continue
        if name == pardir:
            # parent dir
            if path:
                path, name = split(path)
                if name == pardir:
                    path = join(path, pardir, pardir)
            else:
                path = pardir
            continue
        newpath = join(path, name)
        try:
            st = os.lstat(newpath)
        except OSError:
            if strict:
                raise
            is_link = False
        else:
            is_link = stat.S_ISLNK(st.st_mode)
        if not is_link:
            path = newpath
            continue
        # Resolve the symbolic link
        if newpath in seen:
            # Already seen this path
            path = seen[newpath]
            if path is not None:
                # use cached value
                continue
            # The symlink is not resolved, so we must have a symlink loop.
            if strict:
                # Raise OSError(errno.ELOOP)
                os.stat(newpath)
            else:
                # Return already resolved part + rest of the path unchanged.
                return join(newpath, rest), False
        seen[newpath] = None # not resolved symlink
        path, ok = join_real_path(path, os.readlink(newpath), strict, seen)
        if not ok:
            return join(path, rest), False
        seen[newpath] = path # resolved symlink

    return path, True","def return_canonical_path_of_specified_filename_eliminating_symbolic_links(filename, *, strict=False):
    """"""Return the canonical path of the specified filename, eliminating any
symbolic links encountered in the path.""""""
    filename = os.fspath(filename)
    path, ok = join_real_path(filename[:0], filename, strict, {})
    return get_absolute_path(path)","{""abspath"": ""get_absolute_path"", ""_joinrealpath"": ""join_real_path""}","def 0(path):
    """"""Return an absolute path.""""""
    path = os.fspath(path)
    if not isabs(path):
        if isinstance(path, bytes):
            cwd = os.getcwdb()
        else:
            cwd = os.getcwd()
        path = join(cwd, path)
    return normpath(path)

def 1(path, rest, strict, seen):
    if isinstance(path, bytes):
        sep = b'/'
        curdir = b'.'
        pardir = b'..'
    else:
        sep = '/'
        curdir = '.'
        pardir = '..'

    if isabs(rest):
        rest = rest[1:]
        path = sep

    while rest:
        name, _, rest = rest.partition(sep)
        if not name or name == curdir:
            # current dir
            continue
        if name == pardir:
            # parent dir
            if path:
                path, name = split(path)
                if name == pardir:
                    path = join(path, pardir, pardir)
            else:
                path = pardir
            continue
        newpath = join(path, name)
        try:
            st = os.lstat(newpath)
        except OSError:
            if strict:
                raise
            is_link = False
        else:
            is_link = stat.S_ISLNK(st.st_mode)
        if not is_link:
            path = newpath
            continue
        # Resolve the symbolic link
        if newpath in seen:
            # Already seen this path
            path = seen[newpath]
            if path is not None:
                # use cached value
                continue
            # The symlink is not resolved, so we must have a symlink loop.
            if strict:
                # Raise OSError(errno.ELOOP)
                os.stat(newpath)
            else:
                # Return already resolved part + rest of the path unchanged.
                return join(newpath, rest), False
        seen[newpath] = None # not resolved symlink
        path, ok = 1(path, os.readlink(newpath), strict, seen)
        if not ok:
            return join(path, rest), False
        seen[newpath] = path # resolved symlink

    return path, True","def 2(filename, *, strict=False):
    """"""Return the canonical path of the specified filename, eliminating any
symbolic links encountered in the path.""""""
    filename = os.fspath(filename)
    path, ok = 1(filename[:0], filename, strict, {})
    return 0(path)","{""abspath"": ""0"", ""_joinrealpath"": ""1""}"
166,166,"def _find_build_tool(toolname):
    """"""Find a build tool on current path or using xcrun""""""
    return (_find_executable(toolname)
                or _read_output(""/usr/bin/xcrun -find %s"" % (toolname,))
                or ''
            )","def _read_output(commandstring, capture_stderr=False):
    """"""Output from successful command execution or None""""""
    # Similar to os.popen(commandstring, ""r"").read(),
    # but without actually using os.popen because that
    # function is not usable during python bootstrap.
    # tempfile is also not available then.
    import contextlib
    try:
        import tempfile
        fp = tempfile.NamedTemporaryFile()
    except ImportError:
        fp = open(""/tmp/_osx_support.%s""%(
            os.getpid(),), ""w+b"")

    with contextlib.closing(fp) as fp:
        if capture_stderr:
            cmd = ""%s >'%s' 2>&1"" % (commandstring, fp.name)
        else:
            cmd = ""%s 2>/dev/null >'%s'"" % (commandstring, fp.name)
        return fp.read().decode('utf-8').strip() if not os.system(cmd) else None

def _find_executable(executable, path=None):
    """"""Tries to find 'executable' in the directories listed in 'path'.

    A string listing directories separated by 'os.pathsep'; defaults to
    os.environ['PATH'].  Returns the complete filename or None if not found.
    """"""
    if path is None:
        path = os.environ['PATH']

    paths = path.split(os.pathsep)
    base, ext = os.path.splitext(executable)

    if (sys.platform == 'win32') and (ext != '.exe'):
        executable = executable + '.exe'

    if not os.path.isfile(executable):
        for p in paths:
            f = os.path.join(p, executable)
            if os.path.isfile(f):
                # the file exists, we have a shot at spawn working
                return f
        return None
    else:
        return executable","def read_command_output(commandstring, capture_stderr=False):
    """"""Output from successful command execution or None""""""
    # Similar to os.popen(commandstring, ""r"").read(),
    # but without actually using os.popen because that
    # function is not usable during python bootstrap.
    # tempfile is also not available then.
    import contextlib
    try:
        import tempfile
        fp = tempfile.NamedTemporaryFile()
    except ImportError:
        fp = open(""/tmp/_osx_support.%s""%(
            os.getpid(),), ""w+b"")

    with contextlib.closing(fp) as fp:
        if capture_stderr:
            cmd = ""%s >'%s' 2>&1"" % (commandstring, fp.name)
        else:
            cmd = ""%s 2>/dev/null >'%s'"" % (commandstring, fp.name)
        return fp.read().decode('utf-8').strip() if not os.system(cmd) else None

def search_for_executable_in_path_directories(executable, path=None):
    """"""Tries to find 'executable' in the directories listed in 'path'.

    A string listing directories separated by 'os.pathsep'; defaults to
    os.environ['PATH'].  Returns the complete filename or None if not found.
    """"""
    if path is None:
        path = os.environ['PATH']

    paths = path.split(os.pathsep)
    base, ext = os.path.splitext(executable)

    if (sys.platform == 'win32') and (ext != '.exe'):
        executable = executable + '.exe'

    if not os.path.isfile(executable):
        for p in paths:
            f = os.path.join(p, executable)
            if os.path.isfile(f):
                # the file exists, we have a shot at spawn working
                return f
        return None
    else:
        return executable","def find_build_tool_on_current_path_or_using_xcrun(toolname):
    """"""Find a build tool on current path or using xcrun""""""
    return (search_for_executable_in_path_directories(toolname)
                or read_command_output(""/usr/bin/xcrun -find %s"" % (toolname,))
                or ''
            )","{""_read_output"": ""read_command_output"", ""_find_executable"": ""search_for_executable_in_path_directories""}","def 0(commandstring, capture_stderr=False):
    """"""Output from successful command execution or None""""""
    # Similar to os.popen(commandstring, ""r"").read(),
    # but without actually using os.popen because that
    # function is not usable during python bootstrap.
    # tempfile is also not available then.
    import contextlib
    try:
        import tempfile
        fp = tempfile.NamedTemporaryFile()
    except ImportError:
        fp = open(""/tmp/_osx_support.%s""%(
            os.getpid(),), ""w+b"")

    with contextlib.closing(fp) as fp:
        if capture_stderr:
            cmd = ""%s >'%s' 2>&1"" % (commandstring, fp.name)
        else:
            cmd = ""%s 2>/dev/null >'%s'"" % (commandstring, fp.name)
        return fp.read().decode('utf-8').strip() if not os.system(cmd) else None

def 1(executable, path=None):
    """"""Tries to find 'executable' in the directories listed in 'path'.

    A string listing directories separated by 'os.pathsep'; defaults to
    os.environ['PATH'].  Returns the complete filename or None if not found.
    """"""
    if path is None:
        path = os.environ['PATH']

    paths = path.split(os.pathsep)
    base, ext = os.path.splitext(executable)

    if (sys.platform == 'win32') and (ext != '.exe'):
        executable = executable + '.exe'

    if not os.path.isfile(executable):
        for p in paths:
            f = os.path.join(p, executable)
            if os.path.isfile(f):
                # the file exists, we have a shot at spawn working
                return f
        return None
    else:
        return executable","def 2(toolname):
    """"""Find a build tool on current path or using xcrun""""""
    return (1(toolname)
                or 0(""/usr/bin/xcrun -find %s"" % (toolname,))
                or ''
            )","{""_read_output"": ""0"", ""_find_executable"": ""1""}"
167,167,"def _get_system_version_tuple():
    """"""
    Return the macOS system version as a tuple

    The return value is safe to use to compare
    two version numbers.
    """"""
    global _SYSTEM_VERSION_TUPLE
    if _SYSTEM_VERSION_TUPLE is None:
        osx_version = _get_system_version()
        if osx_version:
            try:
                _SYSTEM_VERSION_TUPLE = tuple(int(i) for i in osx_version.split('.'))
            except ValueError:
                _SYSTEM_VERSION_TUPLE = ()

    return _SYSTEM_VERSION_TUPLE","def _get_system_version():
    """"""Return the OS X system version as a string""""""
    # Reading this plist is a documented way to get the system
    # version (see the documentation for the Gestalt Manager)
    # We avoid using platform.mac_ver to avoid possible bootstrap issues during
    # the build of Python itself (distutils is used to build standard library
    # extensions).

    global _SYSTEM_VERSION

    if _SYSTEM_VERSION is None:
        _SYSTEM_VERSION = ''
        try:
            f = open('/System/Library/CoreServices/SystemVersion.plist', encoding=""utf-8"")
        except OSError:
            # We're on a plain darwin box, fall back to the default
            # behaviour.
            pass
        else:
            try:
                m = re.search(r'<key>ProductUserVisibleVersion</key>\s*'
                              r'<string>(.*?)</string>', f.read())
            finally:
                f.close()
            if m is not None:
                _SYSTEM_VERSION = '.'.join(m.group(1).split('.')[:2])
            # else: fall back to the default behaviour

    return _SYSTEM_VERSION","def retrieve_os_x_system_version_as_string():
    """"""Return the OS X system version as a string""""""
    # Reading this plist is a documented way to get the system
    # version (see the documentation for the Gestalt Manager)
    # We avoid using platform.mac_ver to avoid possible bootstrap issues during
    # the build of Python itself (distutils is used to build standard library
    # extensions).

    global _SYSTEM_VERSION

    if _SYSTEM_VERSION is None:
        _SYSTEM_VERSION = ''
        try:
            f = open('/System/Library/CoreServices/SystemVersion.plist', encoding=""utf-8"")
        except OSError:
            # We're on a plain darwin box, fall back to the default
            # behaviour.
            pass
        else:
            try:
                m = re.search(r'<key>ProductUserVisibleVersion</key>\s*'
                              r'<string>(.*?)</string>', f.read())
            finally:
                f.close()
            if m is not None:
                _SYSTEM_VERSION = '.'.join(m.group(1).split('.')[:2])
            # else: fall back to the default behaviour

    return _SYSTEM_VERSION","def retrieve_os_x_system_version_as_string_tuple():
    """"""
    Return the macOS system version as a tuple

    The return value is safe to use to compare
    two version numbers.
    """"""
    global _SYSTEM_VERSION_TUPLE
    if _SYSTEM_VERSION_TUPLE is None:
        osx_version = retrieve_os_x_system_version_as_string()
        if osx_version:
            try:
                _SYSTEM_VERSION_TUPLE = tuple(int(i) for i in osx_version.split('.'))
            except ValueError:
                _SYSTEM_VERSION_TUPLE = ()

    return _SYSTEM_VERSION_TUPLE","{""_get_system_version"": ""retrieve_os_x_system_version_as_string""}","def 0():
    """"""Return the OS X system version as a string""""""
    # Reading this plist is a documented way to get the system
    # version (see the documentation for the Gestalt Manager)
    # We avoid using platform.mac_ver to avoid possible bootstrap issues during
    # the build of Python itself (distutils is used to build standard library
    # extensions).

    global _SYSTEM_VERSION

    if _SYSTEM_VERSION is None:
        _SYSTEM_VERSION = ''
        try:
            f = open('/System/Library/CoreServices/SystemVersion.plist', encoding=""utf-8"")
        except OSError:
            # We're on a plain darwin box, fall back to the default
            # behaviour.
            pass
        else:
            try:
                m = re.search(r'<key>ProductUserVisibleVersion</key>\s*'
                              r'<string>(.*?)</string>', f.read())
            finally:
                f.close()
            if m is not None:
                _SYSTEM_VERSION = '.'.join(m.group(1).split('.')[:2])
            # else: fall back to the default behaviour

    return _SYSTEM_VERSION","def 1():
    """"""
    Return the macOS system version as a tuple

    The return value is safe to use to compare
    two version numbers.
    """"""
    global _SYSTEM_VERSION_TUPLE
    if _SYSTEM_VERSION_TUPLE is None:
        osx_version = 0()
        if osx_version:
            try:
                _SYSTEM_VERSION_TUPLE = tuple(int(i) for i in osx_version.split('.'))
            except ValueError:
                _SYSTEM_VERSION_TUPLE = ()

    return _SYSTEM_VERSION_TUPLE","{""_get_system_version"": ""0""}"
168,168,"def _default_sysroot(cc):
    """""" Returns the root of the default SDK for this system, or '/' """"""
    global _cache_default_sysroot

    if _cache_default_sysroot is not None:
        return _cache_default_sysroot

    contents = _read_output('%s -c -E -v - </dev/null' % (cc,), True)
    in_incdirs = False
    for line in contents.splitlines():
        if line.startswith(""#include <...>""):
            in_incdirs = True
        elif line.startswith(""End of search list""):
            in_incdirs = False
        elif in_incdirs:
            line = line.strip()
            if line == '/usr/include':
                _cache_default_sysroot = '/'
            elif line.endswith("".sdk/usr/include""):
                _cache_default_sysroot = line[:-12]
    if _cache_default_sysroot is None:
        _cache_default_sysroot = '/'

    return _cache_default_sysroot","def _read_output(commandstring, capture_stderr=False):
    """"""Output from successful command execution or None""""""
    # Similar to os.popen(commandstring, ""r"").read(),
    # but without actually using os.popen because that
    # function is not usable during python bootstrap.
    # tempfile is also not available then.
    import contextlib
    try:
        import tempfile
        fp = tempfile.NamedTemporaryFile()
    except ImportError:
        fp = open(""/tmp/_osx_support.%s""%(
            os.getpid(),), ""w+b"")

    with contextlib.closing(fp) as fp:
        if capture_stderr:
            cmd = ""%s >'%s' 2>&1"" % (commandstring, fp.name)
        else:
            cmd = ""%s 2>/dev/null >'%s'"" % (commandstring, fp.name)
        return fp.read().decode('utf-8').strip() if not os.system(cmd) else None","def read_command_output(commandstring, capture_stderr=False):
    """"""Output from successful command execution or None""""""
    # Similar to os.popen(commandstring, ""r"").read(),
    # but without actually using os.popen because that
    # function is not usable during python bootstrap.
    # tempfile is also not available then.
    import contextlib
    try:
        import tempfile
        fp = tempfile.NamedTemporaryFile()
    except ImportError:
        fp = open(""/tmp/_osx_support.%s""%(
            os.getpid(),), ""w+b"")

    with contextlib.closing(fp) as fp:
        if capture_stderr:
            cmd = ""%s >'%s' 2>&1"" % (commandstring, fp.name)
        else:
            cmd = ""%s 2>/dev/null >'%s'"" % (commandstring, fp.name)
        return fp.read().decode('utf-8').strip() if not os.system(cmd) else None","def return_the_root_of_the_default_sysroot_for_compiler(cc):
    """""" Returns the root of the default SDK for this system, or '/' """"""
    global _cachereturn_the_root_of_the_default_sysroot_for_compiler

    if _cachereturn_the_root_of_the_default_sysroot_for_compiler is not None:
        return _cachereturn_the_root_of_the_default_sysroot_for_compiler

    contents = read_command_output('%s -c -E -v - </dev/null' % (cc,), True)
    in_incdirs = False
    for line in contents.splitlines():
        if line.startswith(""#include <...>""):
            in_incdirs = True
        elif line.startswith(""End of search list""):
            in_incdirs = False
        elif in_incdirs:
            line = line.strip()
            if line == '/usr/include':
                _cachereturn_the_root_of_the_default_sysroot_for_compiler = '/'
            elif line.endswith("".sdk/usr/include""):
                _cachereturn_the_root_of_the_default_sysroot_for_compiler = line[:-12]
    if _cachereturn_the_root_of_the_default_sysroot_for_compiler is None:
        _cachereturn_the_root_of_the_default_sysroot_for_compiler = '/'

    return _cachereturn_the_root_of_the_default_sysroot_for_compiler","{""_read_output"": ""read_command_output""}","def 0(commandstring, capture_stderr=False):
    """"""Output from successful command execution or None""""""
    # Similar to os.popen(commandstring, ""r"").read(),
    # but without actually using os.popen because that
    # function is not usable during python bootstrap.
    # tempfile is also not available then.
    import contextlib
    try:
        import tempfile
        fp = tempfile.NamedTemporaryFile()
    except ImportError:
        fp = open(""/tmp/_osx_support.%s""%(
            os.getpid(),), ""w+b"")

    with contextlib.closing(fp) as fp:
        if capture_stderr:
            cmd = ""%s >'%s' 2>&1"" % (commandstring, fp.name)
        else:
            cmd = ""%s 2>/dev/null >'%s'"" % (commandstring, fp.name)
        return fp.read().decode('utf-8').strip() if not os.system(cmd) else None","def 1(cc):
    """""" Returns the root of the default SDK for this system, or '/' """"""
    global _cache_default_sysroot

    if _cache_default_sysroot is not None:
        return _cache_default_sysroot

    contents = 0('%s -c -E -v - </dev/null' % (cc,), True)
    in_incdirs = False
    for line in contents.splitlines():
        if line.startswith(""#include <...>""):
            in_incdirs = True
        elif line.startswith(""End of search list""):
            in_incdirs = False
        elif in_incdirs:
            line = line.strip()
            if line == '/usr/include':
                _cache_default_sysroot = '/'
            elif line.endswith("".sdk/usr/include""):
                _cache_default_sysroot = line[:-12]
    if _cache_default_sysroot is None:
        _cache_default_sysroot = '/'

    return _cache_default_sysroot","{""_read_output"": ""0""}"
169,169,"def _supports_universal_builds():
    """"""Returns True if universal builds are supported on this system""""""
    # As an approximation, we assume that if we are running on 10.4 or above,
    # then we are running with an Xcode environment that supports universal
    # builds, in particular -isysroot and -arch arguments to the compiler. This
    # is in support of allowing 10.4 universal builds to run on 10.3.x systems.

    osx_version = _get_system_version_tuple()
    return bool(osx_version >= (10, 4)) if osx_version else False","def _get_system_version_tuple():
    """"""
    Return the macOS system version as a tuple

    The return value is safe to use to compare
    two version numbers.
    """"""
    global _SYSTEM_VERSION_TUPLE
    if _SYSTEM_VERSION_TUPLE is None:
        osx_version = _get_system_version()
        if osx_version:
            try:
                _SYSTEM_VERSION_TUPLE = tuple(int(i) for i in osx_version.split('.'))
            except ValueError:
                _SYSTEM_VERSION_TUPLE = ()

    return _SYSTEM_VERSION_TUPLE","def get_system_version_tuple():
    """"""
    Return the macOS system version as a tuple

    The return value is safe to use to compare
    two version numbers.
    """"""
    global _SYSTEM_VERSION_TUPLE
    if _SYSTEM_VERSION_TUPLE is None:
        osx_version = _get_system_version()
        if osx_version:
            try:
                _SYSTEM_VERSION_TUPLE = tuple(int(i) for i in osx_version.split('.'))
            except ValueError:
                _SYSTEM_VERSION_TUPLE = ()

    return _SYSTEM_VERSION_TUPLE","def return_true_if_universal_builds_are_supported_on_this_system():
    """"""Returns True if universal builds are supported on this system""""""
    # As an approximation, we assume that if we are running on 10.4 or above,
    # then we are running with an Xcode environment that supports universal
    # builds, in particular -isysroot and -arch arguments to the compiler. This
    # is in support of allowing 10.4 universal builds to run on 10.3.x systems.

    osx_version = get_system_version_tuple()
    return bool(osx_version >= (10, 4)) if osx_version else False","{""_get_system_version_tuple"": ""get_system_version_tuple""}","def 0():
    """"""
    Return the macOS system version as a tuple

    The return value is safe to use to compare
    two version numbers.
    """"""
    global _SYSTEM_VERSION_TUPLE
    if _SYSTEM_VERSION_TUPLE is None:
        osx_version = _get_system_version()
        if osx_version:
            try:
                _SYSTEM_VERSION_TUPLE = tuple(int(i) for i in osx_version.split('.'))
            except ValueError:
                _SYSTEM_VERSION_TUPLE = ()

    return _SYSTEM_VERSION_TUPLE","def 1():
    """"""Returns True if universal builds are supported on this system""""""
    # As an approximation, we assume that if we are running on 10.4 or above,
    # then we are running with an Xcode environment that supports universal
    # builds, in particular -isysroot and -arch arguments to the compiler. This
    # is in support of allowing 10.4 universal builds to run on 10.3.x systems.

    osx_version = 0()
    return bool(osx_version >= (10, 4)) if osx_version else False","{""_get_system_version_tuple"": ""0""}"
170,170,"def _supports_arm64_builds():
    """"""Returns True if arm64 builds are supported on this system""""""
    # There are two sets of systems supporting macOS/arm64 builds:
    # 1. macOS 11 and later, unconditionally
    # 2. macOS 10.15 with Xcode 12.2 or later
    # For now the second category is ignored.
    osx_version = _get_system_version_tuple()
    return osx_version >= (11, 0) if osx_version else False","def _get_system_version_tuple():
    """"""
    Return the macOS system version as a tuple

    The return value is safe to use to compare
    two version numbers.
    """"""
    global _SYSTEM_VERSION_TUPLE
    if _SYSTEM_VERSION_TUPLE is None:
        osx_version = _get_system_version()
        if osx_version:
            try:
                _SYSTEM_VERSION_TUPLE = tuple(int(i) for i in osx_version.split('.'))
            except ValueError:
                _SYSTEM_VERSION_TUPLE = ()

    return _SYSTEM_VERSION_TUPLE","def get_system_version_tuple():
    """"""
    Return the macOS system version as a tuple

    The return value is safe to use to compare
    two version numbers.
    """"""
    global _SYSTEM_VERSION_TUPLE
    if _SYSTEM_VERSION_TUPLE is None:
        osx_version = _get_system_version()
        if osx_version:
            try:
                _SYSTEM_VERSION_TUPLE = tuple(int(i) for i in osx_version.split('.'))
            except ValueError:
                _SYSTEM_VERSION_TUPLE = ()

    return _SYSTEM_VERSION_TUPLE","def return_true_if_arm64_builds_are_supported_on_this_system():
    """"""Returns True if arm64 builds are supported on this system""""""
    # There are two sets of systems supporting macOS/arm64 builds:
    # 1. macOS 11 and later, unconditionally
    # 2. macOS 10.15 with Xcode 12.2 or later
    # For now the second category is ignored.
    osx_version = get_system_version_tuple()
    return osx_version >= (11, 0) if osx_version else False","{""_get_system_version_tuple"": ""get_system_version_tuple""}","def 0():
    """"""
    Return the macOS system version as a tuple

    The return value is safe to use to compare
    two version numbers.
    """"""
    global _SYSTEM_VERSION_TUPLE
    if _SYSTEM_VERSION_TUPLE is None:
        osx_version = _get_system_version()
        if osx_version:
            try:
                _SYSTEM_VERSION_TUPLE = tuple(int(i) for i in osx_version.split('.'))
            except ValueError:
                _SYSTEM_VERSION_TUPLE = ()

    return _SYSTEM_VERSION_TUPLE","def 1():
    """"""Returns True if arm64 builds are supported on this system""""""
    # There are two sets of systems supporting macOS/arm64 builds:
    # 1. macOS 11 and later, unconditionally
    # 2. macOS 10.15 with Xcode 12.2 or later
    # For now the second category is ignored.
    osx_version = 0()
    return osx_version >= (11, 0) if osx_version else False","{""_get_system_version_tuple"": ""0""}"
171,171,"def _remove_universal_flags(_config_vars):
    """"""Remove all universal build arguments from config vars""""""

    for cv in _UNIVERSAL_CONFIG_VARS:
        # Do not alter a config var explicitly overridden by env var
        if cv in _config_vars and cv not in os.environ:
            flags = _config_vars[cv]
            flags = re.sub(r'-arch\s+\w+\s', ' ', flags, flags=re.ASCII)
            flags = re.sub(r'-isysroot\s*\S+', ' ', flags)
            _save_modified_value(_config_vars, cv, flags)

    return _config_vars","def _save_modified_value(_config_vars, cv, newvalue):
    """"""Save modified and original unmodified value of configuration var""""""

    oldvalue = _config_vars.get(cv, '')
    if (oldvalue != newvalue) and (_INITPRE + cv not in _config_vars):
        _config_vars[_INITPRE + cv] = oldvalue
    _config_vars[cv] = newvalue","def persist_modified_and_original_values_of_configuration_variable(_config_vars, cv, newvalue):
    """"""Save modified and original unmodified value of configuration var""""""

    oldvalue = _config_vars.get(cv, '')
    if (oldvalue != newvalue) and (_INITPRE + cv not in _config_vars):
        _config_vars[_INITPRE + cv] = oldvalue
    _config_vars[cv] = newvalue","def remove_all_universal_build_arguments_from_config_vars(_config_vars):
    """"""Remove all universal build arguments from config vars""""""

    for cv in _UNIVERSAL_CONFIG_VARS:
        # Do not alter a config var explicitly overridden by env var
        if cv in _config_vars and cv not in os.environ:
            flags = _config_vars[cv]
            flags = re.sub(r'-arch\s+\w+\s', ' ', flags, flags=re.ASCII)
            flags = re.sub(r'-isysroot\s*\S+', ' ', flags)
            persist_modified_and_original_values_of_configuration_variable(_config_vars, cv, flags)

    return _config_vars","{""_save_modified_value"": ""persist_modified_and_original_values_of_configuration_variable""}","def 0(_config_vars, cv, newvalue):
    """"""Save modified and original unmodified value of configuration var""""""

    oldvalue = _config_vars.get(cv, '')
    if (oldvalue != newvalue) and (_INITPRE + cv not in _config_vars):
        _config_vars[_INITPRE + cv] = oldvalue
    _config_vars[cv] = newvalue","def 1(_config_vars):
    """"""Remove all universal build arguments from config vars""""""

    for cv in _UNIVERSAL_CONFIG_VARS:
        # Do not alter a config var explicitly overridden by env var
        if cv in _config_vars and cv not in os.environ:
            flags = _config_vars[cv]
            flags = re.sub(r'-arch\s+\w+\s', ' ', flags, flags=re.ASCII)
            flags = re.sub(r'-isysroot\s*\S+', ' ', flags)
            0(_config_vars, cv, flags)

    return _config_vars","{""_save_modified_value"": ""0""}"
172,172,"def _override_all_archs(_config_vars):
    """"""Allow override of all archs with ARCHFLAGS env var""""""
    # NOTE: This name was introduced by Apple in OSX 10.5 and
    # is used by several scripting languages distributed with
    # that OS release.
    if 'ARCHFLAGS' in os.environ:
        arch = os.environ['ARCHFLAGS']
        for cv in _UNIVERSAL_CONFIG_VARS:
            if cv in _config_vars and '-arch' in _config_vars[cv]:
                flags = _config_vars[cv]
                flags = re.sub(r'-arch\s+\w+\s', ' ', flags)
                flags = flags + ' ' + arch
                _save_modified_value(_config_vars, cv, flags)

    return _config_vars","def _save_modified_value(_config_vars, cv, newvalue):
    """"""Save modified and original unmodified value of configuration var""""""

    oldvalue = _config_vars.get(cv, '')
    if (oldvalue != newvalue) and (_INITPRE + cv not in _config_vars):
        _config_vars[_INITPRE + cv] = oldvalue
    _config_vars[cv] = newvalue","def persist_modified_and_original_values_of_configuration_variable(_config_vars, cv, newvalue):
    """"""Save modified and original unmodified value of configuration var""""""

    oldvalue = _config_vars.get(cv, '')
    if (oldvalue != newvalue) and (_INITPRE + cv not in _config_vars):
        _config_vars[_INITPRE + cv] = oldvalue
    _config_vars[cv] = newvalue","def allow_override_of_all_archs_with_ARCHFLAGS_env_var(_config_vars):
    """"""Allow override of all archs with ARCHFLAGS env var""""""
    # NOTE: This name was introduced by Apple in OSX 10.5 and
    # is used by several scripting languages distributed with
    # that OS release.
    if 'ARCHFLAGS' in os.environ:
        arch = os.environ['ARCHFLAGS']
        for cv in _UNIVERSAL_CONFIG_VARS:
            if cv in _config_vars and '-arch' in _config_vars[cv]:
                flags = _config_vars[cv]
                flags = re.sub(r'-arch\s+\w+\s', ' ', flags)
                flags = flags + ' ' + arch
                persist_modified_and_original_values_of_configuration_variable(_config_vars, cv, flags)

    return _config_vars","{""_save_modified_value"": ""persist_modified_and_original_values_of_configuration_variable""}","def 0(_config_vars, cv, newvalue):
    """"""Save modified and original unmodified value of configuration var""""""

    oldvalue = _config_vars.get(cv, '')
    if (oldvalue != newvalue) and (_INITPRE + cv not in _config_vars):
        _config_vars[_INITPRE + cv] = oldvalue
    _config_vars[cv] = newvalue","def 1(_config_vars):
    """"""Allow override of all archs with ARCHFLAGS env var""""""
    # NOTE: This name was introduced by Apple in OSX 10.5 and
    # is used by several scripting languages distributed with
    # that OS release.
    if 'ARCHFLAGS' in os.environ:
        arch = os.environ['ARCHFLAGS']
        for cv in _UNIVERSAL_CONFIG_VARS:
            if cv in _config_vars and '-arch' in _config_vars[cv]:
                flags = _config_vars[cv]
                flags = re.sub(r'-arch\s+\w+\s', ' ', flags)
                flags = flags + ' ' + arch
                0(_config_vars, cv, flags)

    return _config_vars","{""_save_modified_value"": ""0""}"
173,173,"def S_ISDIR(mode):
    """"""Return True if mode is from a directory.""""""
    return S_IFMT(mode) == S_IFDIR","def S_IFMT(mode):
    """"""Return the portion of the file's mode that describes the
    file type.
    """"""
    return mode & 0o170000","def get_file_mode_type(mode):
    """"""Return the portion of the file's mode that describes the
    file type.
    """"""
    return mode & 0o170000","def return_true_if_mode_is_from_a_directory(mode):
    """"""Return True if mode is from a directory.""""""
    return get_file_mode_type(mode) == S_IFDIR","{""S_IFMT"": ""get_file_mode_type""}","def 0(mode):
    """"""Return the portion of the file's mode that describes the
    file type.
    """"""
    return mode & 0o170000","def 1(mode):
    """"""Return True if mode is from a directory.""""""
    return 0(mode) == S_IFDIR","{""S_IFMT"": ""0""}"
174,174,"def S_ISCHR(mode):
    """"""Return True if mode is from a character special device file.""""""
    return S_IFMT(mode) == S_IFCHR","def S_IFMT(mode):
    """"""Return the portion of the file's mode that describes the
    file type.
    """"""
    return mode & 0o170000","def get_file_mode_type(mode):
    """"""Return the portion of the file's mode that describes the
    file type.
    """"""
    return mode & 0o170000","def return_true_if_mode_is_from_a_character_special_device_file(mode):
    """"""Return True if mode is from a character special device file.""""""
    return get_file_mode_type(mode) == S_IFCHR","{""S_IFMT"": ""get_file_mode_type""}","def 0(mode):
    """"""Return the portion of the file's mode that describes the
    file type.
    """"""
    return mode & 0o170000","def 1(mode):
    """"""Return True if mode is from a character special device file.""""""
    return 0(mode) == S_IFCHR","{""S_IFMT"": ""0""}"
175,175,"def S_ISBLK(mode):
    """"""Return True if mode is from a block special device file.""""""
    return S_IFMT(mode) == S_IFBLK","def S_IFMT(mode):
    """"""Return the portion of the file's mode that describes the
    file type.
    """"""
    return mode & 0o170000","def get_file_mode_type(mode):
    """"""Return the portion of the file's mode that describes the
    file type.
    """"""
    return mode & 0o170000","def return_true_if_mode_is_from_a_block_special_device_file(mode):
    """"""Return True if mode is from a block special device file.""""""
    return get_file_mode_type(mode) == S_IFBLK","{""S_IFMT"": ""get_file_mode_type""}","def 0(mode):
    """"""Return the portion of the file's mode that describes the
    file type.
    """"""
    return mode & 0o170000","def 1(mode):
    """"""Return True if mode is from a block special device file.""""""
    return 0(mode) == S_IFBLK","{""S_IFMT"": ""0""}"
176,176,"def S_ISREG(mode):
    """"""Return True if mode is from a regular file.""""""
    return S_IFMT(mode) == S_IFREG","def S_IFMT(mode):
    """"""Return the portion of the file's mode that describes the
    file type.
    """"""
    return mode & 0o170000","def get_file_mode_type(mode):
    """"""Return the portion of the file's mode that describes the
    file type.
    """"""
    return mode & 0o170000","def return_true_if_mode_is_from_a_regular_file(mode):
    """"""Return True if mode is from a regular file.""""""
    return get_file_mode_type(mode) == S_IFREG","{""S_IFMT"": ""get_file_mode_type""}","def 0(mode):
    """"""Return the portion of the file's mode that describes the
    file type.
    """"""
    return mode & 0o170000","def 1(mode):
    """"""Return True if mode is from a regular file.""""""
    return 0(mode) == S_IFREG","{""S_IFMT"": ""0""}"
177,177,"def S_ISFIFO(mode):
    """"""Return True if mode is from a FIFO (named pipe).""""""
    return S_IFMT(mode) == S_IFIFO","def S_IFMT(mode):
    """"""Return the portion of the file's mode that describes the
    file type.
    """"""
    return mode & 0o170000","def get_file_mode_type(mode):
    """"""Return the portion of the file's mode that describes the
    file type.
    """"""
    return mode & 0o170000","def return_true_if_mode_is_from_a_FIFO_named_pipe(mode):
    """"""Return True if mode is from a FIFO (named pipe).""""""
    return get_file_mode_type(mode) == S_IFIFO","{""S_IFMT"": ""get_file_mode_type""}","def 0(mode):
    """"""Return the portion of the file's mode that describes the
    file type.
    """"""
    return mode & 0o170000","def 1(mode):
    """"""Return True if mode is from a FIFO (named pipe).""""""
    return 0(mode) == S_IFIFO","{""S_IFMT"": ""0""}"
178,178,"def S_ISLNK(mode):
    """"""Return True if mode is from a symbolic link.""""""
    return S_IFMT(mode) == S_IFLNK","def S_IFMT(mode):
    """"""Return the portion of the file's mode that describes the
    file type.
    """"""
    return mode & 0o170000","def get_file_mode_type(mode):
    """"""Return the portion of the file's mode that describes the
    file type.
    """"""
    return mode & 0o170000","def return_true_if_mode_is_from_a_symbolic_link(mode):
    """"""Return True if mode is from a symbolic link.""""""
    return get_file_mode_type(mode) == S_IFLNK","{""S_IFMT"": ""get_file_mode_type""}","def 0(mode):
    """"""Return the portion of the file's mode that describes the
    file type.
    """"""
    return mode & 0o170000","def 1(mode):
    """"""Return True if mode is from a symbolic link.""""""
    return 0(mode) == S_IFLNK","{""S_IFMT"": ""0""}"
179,179,"def S_ISSOCK(mode):
    """"""Return True if mode is from a socket.""""""
    return S_IFMT(mode) == S_IFSOCK","def S_IFMT(mode):
    """"""Return the portion of the file's mode that describes the
    file type.
    """"""
    return mode & 0o170000","def get_file_mode_type(mode):
    """"""Return the portion of the file's mode that describes the
    file type.
    """"""
    return mode & 0o170000","def return_true_if_mode_is_from_a_socket(mode):
    """"""Return True if mode is from a socket.""""""
    return get_file_mode_type(mode) == S_IFSOCK","{""S_IFMT"": ""get_file_mode_type""}","def 0(mode):
    """"""Return the portion of the file's mode that describes the
    file type.
    """"""
    return mode & 0o170000","def 1(mode):
    """"""Return True if mode is from a socket.""""""
    return 0(mode) == S_IFSOCK","{""S_IFMT"": ""0""}"
180,180,"def Int2AP(num):

    """"""Convert integer to A-P string representation.""""""

    val = b''; AP = b'ABCDEFGHIJKLMNOP'
    num = int(abs(num))
    while num:
        num, mod = divmod(num, 16)
        val = AP[mod:mod+1] + val
    return val","def abs(a):
    ""Same as abs(a).""
    return _abs(a)","def calculate_absolute_value(a):
    ""Same as calculate_absolute_value(a).""
    return _calculate_absolute_value(a)","def convert_integer_to_A_P_string_representation(num):

    """"""Convert integer to A-P string representation.""""""

    val = b''; AP = b'ABCDEFGHIJKLMNOP'
    num = int(calculate_absolute_value(num))
    while num:
        num, mod = divmod(num, 16)
        val = AP[mod:mod+1] + val
    return val","{""abs"": ""calculate_absolute_value""}","def 0(a):
    ""Same as 0(a).""
    return _0(a)","def 1(num):

    """"""Convert integer to A-P string representation.""""""

    val = b''; AP = b'ABCDEFGHIJKLMNOP'
    num = int(0(num))
    while num:
        num, mod = divmod(num, 16)
        val = AP[mod:mod+1] + val
    return val","{""abs"": ""0""}"
181,181,"def _create_temporary(path):
    """"""Create a temp file based on path and open for reading and writing.""""""
    return _create_carefully('%s.%s.%s.%s' % (path, int(time.time()),
                                              socket.gethostname(),
                                              os.getpid()))","def _create_carefully(path):
    """"""Create a file if it doesn't exist and open for reading and writing.""""""
    fd = os.open(path, os.O_CREAT | os.O_EXCL | os.O_RDWR, 0o666)
    try:
        return open(path, 'rb+')
    finally:
        os.close(fd)","def create_carefully_and_open(path):
    """"""Create a file if it doesn't exist and open for reading and writing.""""""
    fd = os.open(path, os.O_CREAT | os.O_EXCL | os.O_RDWR, 0o666)
    try:
        return open(path, 'rb+')
    finally:
        os.close(fd)","def create_a_temp_file_based_on_path_and_open_for_reading_and_writing(path):
    """"""Create a temp file based on path and open for reading and writing.""""""
    return create_carefully_and_open('%s.%s.%s.%s' % (path, int(time.time()),
                                              socket.gethostname(),
                                              os.getpid()))","{""_create_carefully"": ""create_carefully_and_open""}","def 0(path):
    """"""Create a file if it doesn't exist and open for reading and writing.""""""
    fd = os.open(path, os.O_CREAT | os.O_EXCL | os.O_RDWR, 0o666)
    try:
        return open(path, 'rb+')
    finally:
        os.close(fd)","def 1(path):
    """"""Create a temp file based on path and open for reading and writing.""""""
    return 0('%s.%s.%s.%s' % (path, int(time.time()),
                                              socket.gethostname(),
                                              os.getpid()))","{""_create_carefully"": ""0""}"
182,182,"def _sync_close(f):
    """"""Close file f, ensuring all changes are physically on disk.""""""
    _sync_flush(f)
    f.close()","def _sync_flush(f):
    """"""Ensure changes to file f are physically on disk.""""""
    f.flush()
    if hasattr(os, 'fsync'):
        os.fsync(f.fileno())","def ensure_changes_to_file_are_physically_on_disk(f):
    """"""Ensure changes to file f are physically on disk.""""""
    f.flush()
    if hasattr(os, 'fsync'):
        os.fsync(f.fileno())","def close_file_ensuring_all_changes_are_physically_on_disk(f):
    """"""Close file f, ensuring all changes are physically on disk.""""""
    ensure_changes_to_file_are_physically_on_disk(f)
    f.close()","{""_sync_flush"": ""ensure_changes_to_file_are_physically_on_disk""}","def 0(f):
    """"""Ensure changes to file f are physically on disk.""""""
    f.flush()
    if hasattr(os, 'fsync'):
        os.fsync(f.fileno())","def 1(f):
    """"""Close file f, ensuring all changes are physically on disk.""""""
    0(f)
    f.close()","{""_sync_flush"": ""0""}"
183,183,"def str(val):
    """"""Convert float to string, taking the locale into account.""""""
    return _format(""%.12g"", val)","def _format(percent, value, grouping=False, monetary=False, *additional):
    if additional:
        formatted = percent % ((value,) + additional)
    else:
        formatted = percent % value
    if percent[-1] in 'eEfFgGdiu':
        formatted = _localize(formatted, grouping, monetary)
    return formatted","def format_with_options(percent, value, grouping=False, monetary=False, *additional):
    if additional:
        formatted = percent % ((value,) + additional)
    else:
        formatted = percent % value
    if percent[-1] in 'eEfFgGdiu':
        formatted = _localize(formatted, grouping, monetary)
    return formatted","def locale_aware_float_to_string(val):
    """"""Convert float to locale_aware_float_to_stringing, taking the locale into account.""""""
    return format_with_options(""%.12g"", val)","{""_format"": ""format_with_options""}","def 0(percent, value, grouping=False, monetary=False, *additional):
    if additional:
        formatted = percent % ((value,) + additional)
    else:
        formatted = percent % value
    if percent[-1] in 'eEfFgGdiu':
        formatted = _localize(formatted, grouping, monetary)
    return formatted","def 1(val):
    """"""Convert float to string, taking the locale into account.""""""
    return 0(""%.12g"", val)","{""_format"": ""0""}"
184,184,"def localize(string, grouping=False, monetary=False):
    """"""Parses a string as locale number according to the locale settings.""""""
    return _localize(string, grouping, monetary)","def _localize(formatted, grouping=False, monetary=False):
    # floats and decimal ints need special action!
    if '.' in formatted:
        seps = 0
        parts = formatted.split('.')
        if grouping:
            parts[0], seps = _group(parts[0], monetary=monetary)
        decimal_point = localeconv()[monetary and 'mon_decimal_point'
                                              or 'decimal_point']
        formatted = decimal_point.join(parts)
        if seps:
            formatted = _strip_padding(formatted, seps)
    else:
        seps = 0
        if grouping:
            formatted, seps = _group(formatted, monetary=monetary)
        if seps:
            formatted = _strip_padding(formatted, seps)
    return formatted","def format_number_with_locale(formatted, grouping=False, monetary=False):
    # floats and decimal ints need special action!
    if '.' in formatted:
        seps = 0
        parts = formatted.split('.')
        if grouping:
            parts[0], seps = _group(parts[0], monetary=monetary)
        decimal_point = localeconv()[monetary and 'mon_decimal_point'
                                              or 'decimal_point']
        formatted = decimal_point.join(parts)
        if seps:
            formatted = _strip_padding(formatted, seps)
    else:
        seps = 0
        if grouping:
            formatted, seps = _group(formatted, monetary=monetary)
        if seps:
            formatted = _strip_padding(formatted, seps)
    return formatted","def parse_string_as_locale_number_according_to_locale_settings(string, grouping=False, monetary=False):
    """"""Parses a string as locale number according to the locale settings.""""""
    return format_number_with_locale(string, grouping, monetary)","{""_localize"": ""format_number_with_locale""}","def 0(formatted, grouping=False, monetary=False):
    # floats and decimal ints need special action!
    if '.' in formatted:
        seps = 0
        parts = formatted.split('.')
        if grouping:
            parts[0], seps = _group(parts[0], monetary=monetary)
        decimal_point = localeconv()[monetary and 'mon_decimal_point'
                                              or 'decimal_point']
        formatted = decimal_point.join(parts)
        if seps:
            formatted = _strip_padding(formatted, seps)
    else:
        seps = 0
        if grouping:
            formatted, seps = _group(formatted, monetary=monetary)
        if seps:
            formatted = _strip_padding(formatted, seps)
    return formatted","def 1(string, grouping=False, monetary=False):
    """"""Parses a string as locale number according to the locale settings.""""""
    return 0(string, grouping, monetary)","{""_localize"": ""0""}"
185,185,"def atof(string, func=float):
    ""Parses a string as a float according to the locale settings.""
    return func(delocalize(string))","def delocalize(string):
    ""Parses a string as a normalized number according to the locale settings.""

    conv = localeconv()

    #First, get rid of the grouping
    ts = conv['thousands_sep']
    if ts:
        string = string.replace(ts, '')

    #next, replace the decimal point with a dot
    dd = conv['decimal_point']
    if dd:
        string = string.replace(dd, '.')
    return string","def parse_number_string_with_locale(string):
    ""Parses a string as a normalized number according to the locale settings.""

    conv = localeconv()

    #First, get rid of the grouping
    ts = conv['thousands_sep']
    if ts:
        string = string.replace(ts, '')

    #next, replace the decimal point with a dot
    dd = conv['decimal_point']
    if dd:
        string = string.replace(dd, '.')
    return string","def parse_string_as_float_according_to_locale_settings(string, func=float):
    ""Parses a string as a float according to the locale settings.""
    return func(parse_number_string_with_locale(string))","{""delocalize"": ""parse_number_string_with_locale""}","def 0(string):
    ""Parses a string as a normalized number according to the locale settings.""

    conv = localeconv()

    #First, get rid of the grouping
    ts = conv['thousands_sep']
    if ts:
        string = string.replace(ts, '')

    #next, replace the decimal point with a dot
    dd = conv['decimal_point']
    if dd:
        string = string.replace(dd, '.')
    return string","def 1(string, func=float):
    ""Parses a string as a float according to the locale settings.""
    return func(0(string))","{""delocalize"": ""0""}"
186,186,"def atoi(string):
    ""Converts a string to an integer according to the locale settings.""
    return int(delocalize(string))","def delocalize(string):
    ""Parses a string as a normalized number according to the locale settings.""

    conv = localeconv()

    #First, get rid of the grouping
    ts = conv['thousands_sep']
    if ts:
        string = string.replace(ts, '')

    #next, replace the decimal point with a dot
    dd = conv['decimal_point']
    if dd:
        string = string.replace(dd, '.')
    return string","def parse_number_string_with_locale(string):
    ""Parses a string as a normalized number according to the locale settings.""

    conv = localeconv()

    #First, get rid of the grouping
    ts = conv['thousands_sep']
    if ts:
        string = string.replace(ts, '')

    #next, replace the decimal point with a dot
    dd = conv['decimal_point']
    if dd:
        string = string.replace(dd, '.')
    return string","def convert_string_to_integer_according_to_locale_settings(string):
    ""Converts a string to an integer according to the locale settings.""
    return int(parse_number_string_with_locale(string))","{""delocalize"": ""parse_number_string_with_locale""}","def 0(string):
    ""Parses a string as a normalized number according to the locale settings.""

    conv = localeconv()

    #First, get rid of the grouping
    ts = conv['thousands_sep']
    if ts:
        string = string.replace(ts, '')

    #next, replace the decimal point with a dot
    dd = conv['decimal_point']
    if dd:
        string = string.replace(dd, '.')
    return string","def 1(string):
    ""Converts a string to an integer according to the locale settings.""
    return int(0(string))","{""delocalize"": ""0""}"
187,187,"def getlocale(category=LC_CTYPE):

    """""" Returns the current setting for the given locale category as
        tuple (language code, encoding).

        category may be one of the LC_* value except LC_ALL. It
        defaults to LC_CTYPE.

        Except for the code 'C', the language code corresponds to RFC
        1766.  code and encoding can be None in case the values cannot
        be determined.

    """"""
    localename = _setlocale(category)
    if category == LC_ALL and ';' in localename:
        raise TypeError('category LC_ALL is not supported')
    return _parse_localename(localename)","def _parse_localename(localename):

    """""" Parses the locale code for localename and returns the
        result as tuple (language code, encoding).

        The localename is normalized and passed through the locale
        alias engine. A ValueError is raised in case the locale name
        cannot be parsed.

        The language code corresponds to RFC 1766.  code and encoding
        can be None in case the values cannot be determined or are
        unknown to this implementation.

    """"""
    code = normalize(localename)
    if '@' in code:
        # Deal with locale modifiers
        code, modifier = code.split('@', 1)
        if modifier == 'euro' and '.' not in code:
            # Assume Latin-9 for @euro locales. This is bogus,
            # since some systems may use other encodings for these
            # locales. Also, we ignore other modifiers.
            return code, 'iso-8859-15'

    if '.' in code:
        return tuple(code.split('.')[:2])
    elif code == 'C':
        return None, None
    elif code == 'UTF-8':
        # On macOS ""LC_CTYPE=UTF-8"" is a valid locale setting
        # for getting UTF-8 handling for text.
        return None, 'UTF-8'
    raise ValueError('unknown locale: %s' % localename)","def parse_locale_code(localename):

    """""" Parses the locale code for localename and returns the
        result as tuple (language code, encoding).

        The localename is normalized and passed through the locale
        alias engine. A ValueError is raised in case the locale name
        cannot be parsed.

        The language code corresponds to RFC 1766.  code and encoding
        can be None in case the values cannot be determined or are
        unknown to this implementation.

    """"""
    code = normalize(localename)
    if '@' in code:
        # Deal with locale modifiers
        code, modifier = code.split('@', 1)
        if modifier == 'euro' and '.' not in code:
            # Assume Latin-9 for @euro locales. This is bogus,
            # since some systems may use other encodings for these
            # locales. Also, we ignore other modifiers.
            return code, 'iso-8859-15'

    if '.' in code:
        return tuple(code.split('.')[:2])
    elif code == 'C':
        return None, None
    elif code == 'UTF-8':
        # On macOS ""LC_CTYPE=UTF-8"" is a valid locale setting
        # for getting UTF-8 handling for text.
        return None, 'UTF-8'
    raise ValueError('unknown locale: %s' % localename)","def getlocale_and_return_current_setting_for_given_locale_category_as_tuple(category=LC_CTYPE):

    """""" Returns the current setting for the given locale category as
        tuple (language code, encoding).

        category may be one of the LC_* value except LC_ALL. It
        defaults to LC_CTYPE.

        Except for the code 'C', the language code corresponds to RFC
        1766.  code and encoding can be None in case the values cannot
        be determined.

    """"""
    localename = _setlocale(category)
    if category == LC_ALL and ';' in localename:
        raise TypeError('category LC_ALL is not supported')
    return parse_locale_code(localename)","{""_parse_localename"": ""parse_locale_code""}","def 0(localename):

    """""" Parses the locale code for localename and returns the
        result as tuple (language code, encoding).

        The localename is normalized and passed through the locale
        alias engine. A ValueError is raised in case the locale name
        cannot be parsed.

        The language code corresponds to RFC 1766.  code and encoding
        can be None in case the values cannot be determined or are
        unknown to this implementation.

    """"""
    code = normalize(localename)
    if '@' in code:
        # Deal with locale modifiers
        code, modifier = code.split('@', 1)
        if modifier == 'euro' and '.' not in code:
            # Assume Latin-9 for @euro locales. This is bogus,
            # since some systems may use other encodings for these
            # locales. Also, we ignore other modifiers.
            return code, 'iso-8859-15'

    if '.' in code:
        return tuple(code.split('.')[:2])
    elif code == 'C':
        return None, None
    elif code == 'UTF-8':
        # On macOS ""LC_CTYPE=UTF-8"" is a valid locale setting
        # for getting UTF-8 handling for text.
        return None, 'UTF-8'
    raise ValueError('unknown locale: %s' % localename)","def 1(category=LC_CTYPE):

    """""" Returns the current setting for the given locale category as
        tuple (language code, encoding).

        category may be one of the LC_* value except LC_ALL. It
        defaults to LC_CTYPE.

        Except for the code 'C', the language code corresponds to RFC
        1766.  code and encoding can be None in case the values cannot
        be determined.

    """"""
    localename = _setlocale(category)
    if category == LC_ALL and ';' in localename:
        raise TypeError('category LC_ALL is not supported')
    return 0(localename)","{""_parse_localename"": ""0""}"
188,188,"def resetlocale(category=LC_ALL):

    """""" Sets the locale for category to the default setting.

        The default setting is determined by calling
        getdefaultlocale(). category defaults to LC_ALL.

    """"""
    _setlocale(category, _build_localename(getdefaultlocale()))","def _build_localename(localetuple):

    """""" Builds a locale code from the given tuple (language code,
        encoding).

        No aliasing or normalizing takes place.

    """"""
    try:
        language, encoding = localetuple

        if language is None:
            language = 'C'
        if encoding is None:
            return language
        else:
            return language + '.' + encoding
    except (TypeError, ValueError):
        raise TypeError('Locale must be None, a string, or an iterable of '
                        'two strings -- language code, encoding.') from None

def getdefaultlocale(envvars=('LC_ALL', 'LC_CTYPE', 'LANG', 'LANGUAGE')):

    """""" Tries to determine the default locale settings and returns
        them as tuple (language code, encoding).

        According to POSIX, a program which has not called
        setlocale(LC_ALL, """") runs using the portable 'C' locale.
        Calling setlocale(LC_ALL, """") lets it use the default locale as
        defined by the LANG variable. Since we don't want to interfere
        with the current locale setting we thus emulate the behavior
        in the way described above.

        To maintain compatibility with other platforms, not only the
        LANG variable is tested, but a list of variables given as
        envvars parameter. The first found to be defined will be
        used. envvars defaults to the search path used in GNU gettext;
        it must always contain the variable name 'LANG'.

        Except for the code 'C', the language code corresponds to RFC
        1766.  code and encoding can be None in case the values cannot
        be determined.

    """"""

    try:
        # check if it's supported by the _locale module
        import _locale
        code, encoding = _locale._getdefaultlocale()
    except (ImportError, AttributeError):
        pass
    else:
        # make sure the code/encoding values are valid
        if sys.platform == ""win32"" and code and code[:2] == ""0x"":
            # map windows language identifier to language name
            code = windows_locale.get(int(code, 0))
        # ...add other platform-specific processing here, if
        # necessary...
        return code, encoding

    # fall back on POSIX behaviour
    import os
    lookup = os.environ.get
    for variable in envvars:
        localename = lookup(variable,None)
        if localename:
            if variable == 'LANGUAGE':
                localename = localename.split(':')[0]
            break
    else:
        localename = 'C'
    return _parse_localename(localename)","def build_locale_code(localetuple):

    """""" Builds a locale code from the given tuple (language code,
        encoding).

        No aliasing or normalizing takes place.

    """"""
    try:
        language, encoding = localetuple

        if language is None:
            language = 'C'
        if encoding is None:
            return language
        else:
            return language + '.' + encoding
    except (TypeError, ValueError):
        raise TypeError('Locale must be None, a string, or an iterable of '
                        'two strings -- language code, encoding.') from None

def attempt_to_determine_default_locale(envvars=('LC_ALL', 'LC_CTYPE', 'LANG', 'LANGUAGE')):

    """""" Tries to determine the default locale settings and returns
        them as tuple (language code, encoding).

        According to POSIX, a program which has not called
        setlocale(LC_ALL, """") runs using the portable 'C' locale.
        Calling setlocale(LC_ALL, """") lets it use the default locale as
        defined by the LANG variable. Since we don't want to interfere
        with the current locale setting we thus emulate the behavior
        in the way described above.

        To maintain compatibility with other platforms, not only the
        LANG variable is tested, but a list of variables given as
        envvars parameter. The first found to be defined will be
        used. envvars defaults to the search path used in GNU gettext;
        it must always contain the variable name 'LANG'.

        Except for the code 'C', the language code corresponds to RFC
        1766.  code and encoding can be None in case the values cannot
        be determined.

    """"""

    try:
        # check if it's supported by the _locale module
        import _locale
        code, encoding = _locale._attempt_to_determine_default_locale()
    except (ImportError, AttributeError):
        pass
    else:
        # make sure the code/encoding values are valid
        if sys.platform == ""win32"" and code and code[:2] == ""0x"":
            # map windows language identifier to language name
            code = windows_locale.get(int(code, 0))
        # ...add other platform-specific processing here, if
        # necessary...
        return code, encoding

    # fall back on POSIX behaviour
    import os
    lookup = os.environ.get
    for variable in envvars:
        localename = lookup(variable,None)
        if localename:
            if variable == 'LANGUAGE':
                localename = localename.split(':')[0]
            break
    else:
        localename = 'C'
    return _parse_localename(localename)","def resetlocale_and_set_locale_for_category_to_default_value(category=LC_ALL):

    """""" Sets the locale for category to the default setting.

        The default setting is determined by calling
        attempt_to_determine_default_locale(). category defaults to LC_ALL.

    """"""
    _setlocale(category, build_locale_code(attempt_to_determine_default_locale()))","{""_build_localename"": ""build_locale_code"", ""getdefaultlocale"": ""attempt_to_determine_default_locale""}","def 0(localetuple):

    """""" Builds a locale code from the given tuple (language code,
        encoding).

        No aliasing or normalizing takes place.

    """"""
    try:
        language, encoding = localetuple

        if language is None:
            language = 'C'
        if encoding is None:
            return language
        else:
            return language + '.' + encoding
    except (TypeError, ValueError):
        raise TypeError('Locale must be None, a string, or an iterable of '
                        'two strings -- language code, encoding.') from None

def 1(envvars=('LC_ALL', 'LC_CTYPE', 'LANG', 'LANGUAGE')):

    """""" Tries to determine the default locale settings and returns
        them as tuple (language code, encoding).

        According to POSIX, a program which has not called
        setlocale(LC_ALL, """") runs using the portable 'C' locale.
        Calling setlocale(LC_ALL, """") lets it use the default locale as
        defined by the LANG variable. Since we don't want to interfere
        with the current locale setting we thus emulate the behavior
        in the way described above.

        To maintain compatibility with other platforms, not only the
        LANG variable is tested, but a list of variables given as
        envvars parameter. The first found to be defined will be
        used. envvars defaults to the search path used in GNU gettext;
        it must always contain the variable name 'LANG'.

        Except for the code 'C', the language code corresponds to RFC
        1766.  code and encoding can be None in case the values cannot
        be determined.

    """"""

    try:
        # check if it's supported by the _locale module
        import _locale
        code, encoding = _locale._1()
    except (ImportError, AttributeError):
        pass
    else:
        # make sure the code/encoding values are valid
        if sys.platform == ""win32"" and code and code[:2] == ""0x"":
            # map windows language identifier to language name
            code = windows_locale.get(int(code, 0))
        # ...add other platform-specific processing here, if
        # necessary...
        return code, encoding

    # fall back on POSIX behaviour
    import os
    lookup = os.environ.get
    for variable in envvars:
        localename = lookup(variable,None)
        if localename:
            if variable == 'LANGUAGE':
                localename = localename.split(':')[0]
            break
    else:
        localename = 'C'
    return _parse_localename(localename)","def 2(category=LC_ALL):

    """""" Sets the locale for category to the default setting.

        The default setting is determined by calling
        1(). category defaults to LC_ALL.

    """"""
    _setlocale(category, 0(1()))","{""_build_localename"": ""0"", ""getdefaultlocale"": ""1""}"
189,189,"def _normalize_module(module, depth=2):
    """"""
    Return the module specified by `module`.  In particular:
      - If `module` is a module, then return module.
      - If `module` is a string, then import and return the
        module with that name.
      - If `module` is None, then return the calling module.
        The calling module is assumed to be the module of
        the stack frame at the given depth in the call stack.
    """"""
    if inspect.ismodule(module):
        return module
    elif isinstance(module, str):
        return __import__(module, globals(), locals(), [""*""])
    elif module is None:
        return sys.modules[sys._getframe(depth).f_globals['__name__']]
    else:
        raise TypeError(""Expected a module, string, or None"")","def __import__(name, globals=None, locals=None, fromlist=(), level=0):
    """"""Import a module.

    The 'globals' argument is used to infer where the import is occurring from
    to handle relative imports. The 'locals' argument is ignored. The
    'fromlist' argument specifies what should exist as attributes on the module
    being imported (e.g. ``from module import <fromlist>``).  The 'level'
    argument represents the package location to import from in a relative
    import (e.g. ``from ..pkg import mod`` would have a 'level' of 2).

    """"""
    if level == 0:
        module = _gcd_import(name)
    else:
        globals_ = globals if globals is not None else {}
        package = _calc___package__(globals_)
        module = _gcd_import(name, package, level)
    if not fromlist:
        # Return up to the first dot in 'name'. This is complicated by the fact
        # that 'name' may be relative.
        if level == 0:
            return _gcd_import(name.partition('.')[0])
        elif not name:
            return module
        else:
            # Figure out where to slice the module's name up to the first dot
            # in 'name'.
            cut_off = len(name) - len(name.partition('.')[0])
            # Slice end needs to be positive to alleviate need to special-case
            # when ``'.' not in name``.
            return sys.modules[module.__name__[:len(module.__name__)-cut_off]]
    elif hasattr(module, '__path__'):
        return _handle_fromlist(module, fromlist, _gcd_import)
    else:
        return module","def import_module(name, globals=None, locals=None, fromlist=(), level=0):
    """"""Import a module.

    The 'globals' argument is used to infer where the import is occurring from
    to handle relative imports. The 'locals' argument is ignored. The
    'fromlist' argument specifies what should exist as attributes on the module
    being imported (e.g. ``from module import <fromlist>``).  The 'level'
    argument represents the package location to import from in a relative
    import (e.g. ``from ..pkg import mod`` would have a 'level' of 2).

    """"""
    if level == 0:
        module = _gcd_import(name)
    else:
        globals_ = globals if globals is not None else {}
        package = _calc___package__(globals_)
        module = _gcd_import(name, package, level)
    if not fromlist:
        # Return up to the first dot in 'name'. This is complicated by the fact
        # that 'name' may be relative.
        if level == 0:
            return _gcd_import(name.partition('.')[0])
        elif not name:
            return module
        else:
            # Figure out where to slice the module's name up to the first dot
            # in 'name'.
            cut_off = len(name) - len(name.partition('.')[0])
            # Slice end needs to be positive to alleviate need to special-case
            # when ``'.' not in name``.
            return sys.modules[module.__name__[:len(module.__name__)-cut_off]]
    elif hasattr(module, '__path__'):
        return _handle_fromlist(module, fromlist, _gcd_import)
    else:
        return module","def normalize_module(module, depth=2):
    """"""
    Return the module specified by `module`.  In particular:
      - If `module` is a module, then return module.
      - If `module` is a string, then import and return the
        module with that name.
      - If `module` is None, then return the calling module.
        The calling module is assumed to be the module of
        the stack frame at the given depth in the call stack.
    """"""
    if inspect.ismodule(module):
        return module
    elif isinstance(module, str):
        return import_module(module, globals(), locals(), [""*""])
    elif module is None:
        return sys.modules[sys._getframe(depth).f_globals['__name__']]
    else:
        raise TypeError(""Expected a module, string, or None"")","{""__import__"": ""import_module""}","def 0(name, globals=None, locals=None, fromlist=(), level=0):
    """"""Import a module.

    The 'globals' argument is used to infer where the import is occurring from
    to handle relative imports. The 'locals' argument is ignored. The
    'fromlist' argument specifies what should exist as attributes on the module
    being imported (e.g. ``from module import <fromlist>``).  The 'level'
    argument represents the package location to import from in a relative
    import (e.g. ``from ..pkg import mod`` would have a 'level' of 2).

    """"""
    if level == 0:
        module = _gcd_import(name)
    else:
        globals_ = globals if globals is not None else {}
        package = _calc___package__(globals_)
        module = _gcd_import(name, package, level)
    if not fromlist:
        # Return up to the first dot in 'name'. This is complicated by the fact
        # that 'name' may be relative.
        if level == 0:
            return _gcd_import(name.partition('.')[0])
        elif not name:
            return module
        else:
            # Figure out where to slice the module's name up to the first dot
            # in 'name'.
            cut_off = len(name) - len(name.partition('.')[0])
            # Slice end needs to be positive to alleviate need to special-case
            # when ``'.' not in name``.
            return sys.modules[module.__name__[:len(module.__name__)-cut_off]]
    elif hasattr(module, '__path__'):
        return _handle_fromlist(module, fromlist, _gcd_import)
    else:
        return module","def 1(module, depth=2):
    """"""
    Return the module specified by `module`.  In particular:
      - If `module` is a module, then return module.
      - If `module` is a string, then import and return the
        module with that name.
      - If `module` is None, then return the calling module.
        The calling module is assumed to be the module of
        the stack frame at the given depth in the call stack.
    """"""
    if inspect.ismodule(module):
        return module
    elif isinstance(module, str):
        return 0(module, globals(), locals(), [""*""])
    elif module is None:
        return sys.modules[sys._getframe(depth).f_globals['__name__']]
    else:
        raise TypeError(""Expected a module, string, or None"")","{""__import__"": ""0""}"
190,190,"def testsource(module, name):
    """"""Extract the test sources from a doctest docstring as a script.

    Provide the module (or dotted name of the module) containing the
    test to be debugged and the name (within the module) of the object
    with the doc string with tests to be debugged.
    """"""
    module = _normalize_module(module)
    tests = DocTestFinder().find(module)
    test = [t for t in tests if t.name == name]
    if not test:
        raise ValueError(name, ""not found in tests"")
    test = test[0]
    testsrc = script_from_examples(test.docstring)
    return testsrc","def _normalize_module(module, depth=2):
    """"""
    Return the module specified by `module`.  In particular:
      - If `module` is a module, then return module.
      - If `module` is a string, then import and return the
        module with that name.
      - If `module` is None, then return the calling module.
        The calling module is assumed to be the module of
        the stack frame at the given depth in the call stack.
    """"""
    if inspect.ismodule(module):
        return module
    elif isinstance(module, str):
        return __import__(module, globals(), locals(), [""*""])
    elif module is None:
        return sys.modules[sys._getframe(depth).f_globals['__name__']]
    else:
        raise TypeError(""Expected a module, string, or None"")

def script_from_examples(s):
    r""""""Extract script from text with examples.

       Converts text with examples to a Python script.  Example input is
       converted to regular code.  Example output and all other words
       are converted to comments:

       >>> text = '''
       ...       Here are examples of simple math.
       ...
       ...           Python has super accurate integer addition
       ...
       ...           >>> 2 + 2
       ...           5
       ...
       ...           And very friendly error messages:
       ...
       ...           >>> 1/0
       ...           To Infinity
       ...           And
       ...           Beyond
       ...
       ...           You can use logic if you want:
       ...
       ...           >>> if 0:
       ...           ...    blah
       ...           ...    blah
       ...           ...
       ...
       ...           Ho hum
       ...           '''

       >>> print(script_from_examples(text))
       # Here are examples of simple math.
       #
       #     Python has super accurate integer addition
       #
       2 + 2
       # Expected:
       ## 5
       #
       #     And very friendly error messages:
       #
       1/0
       # Expected:
       ## To Infinity
       ## And
       ## Beyond
       #
       #     You can use logic if you want:
       #
       if 0:
          blah
          blah
       #
       #     Ho hum
       <BLANKLINE>
       """"""
    output = []
    for piece in DocTestParser().parse(s):
        if isinstance(piece, Example):
            # Add the example's source code (strip trailing NL)
            output.append(piece.source[:-1])
            # Add the expected output:
            want = piece.want
            if want:
                output.append('# Expected:')
                output += ['## '+l for l in want.split('\n')[:-1]]
        else:
            # Add non-example text.
            output += [_comment_line(l)
                       for l in piece.split('\n')[:-1]]

    # Trim junk on both ends.
    while output and output[-1] == '#':
        output.pop()
    while output and output[0] == '#':
        output.pop(0)
    # Combine the output, and return it.
    # Add a courtesy newline to prevent exec from choking (see bug #1172785)
    return '\n'.join(output) + '\n'","def normalize_module(module, depth=2):
    """"""
    Return the module specified by `module`.  In particular:
      - If `module` is a module, then return module.
      - If `module` is a string, then import and return the
        module with that name.
      - If `module` is None, then return the calling module.
        The calling module is assumed to be the module of
        the stack frame at the given depth in the call stack.
    """"""
    if inspect.ismodule(module):
        return module
    elif isinstance(module, str):
        return __import__(module, globals(), locals(), [""*""])
    elif module is None:
        return sys.modules[sys._getframe(depth).f_globals['__name__']]
    else:
        raise TypeError(""Expected a module, string, or None"")

def generate_script_from_examples(s):
    r""""""Extract script from text with examples.

       Converts text with examples to a Python script.  Example input is
       converted to regular code.  Example output and all other words
       are converted to comments:

       >>> text = '''
       ...       Here are examples of simple math.
       ...
       ...           Python has super accurate integer addition
       ...
       ...           >>> 2 + 2
       ...           5
       ...
       ...           And very friendly error messages:
       ...
       ...           >>> 1/0
       ...           To Infinity
       ...           And
       ...           Beyond
       ...
       ...           You can use logic if you want:
       ...
       ...           >>> if 0:
       ...           ...    blah
       ...           ...    blah
       ...           ...
       ...
       ...           Ho hum
       ...           '''

       >>> print(generate_script_from_examples(text))
       # Here are examples of simple math.
       #
       #     Python has super accurate integer addition
       #
       2 + 2
       # Expected:
       ## 5
       #
       #     And very friendly error messages:
       #
       1/0
       # Expected:
       ## To Infinity
       ## And
       ## Beyond
       #
       #     You can use logic if you want:
       #
       if 0:
          blah
          blah
       #
       #     Ho hum
       <BLANKLINE>
       """"""
    output = []
    for piece in DocTestParser().parse(s):
        if isinstance(piece, Example):
            # Add the example's source code (strip trailing NL)
            output.append(piece.source[:-1])
            # Add the expected output:
            want = piece.want
            if want:
                output.append('# Expected:')
                output += ['## '+l for l in want.split('\n')[:-1]]
        else:
            # Add non-example text.
            output += [_comment_line(l)
                       for l in piece.split('\n')[:-1]]

    # Trim junk on both ends.
    while output and output[-1] == '#':
        output.pop()
    while output and output[0] == '#':
        output.pop(0)
    # Combine the output, and return it.
    # Add a courtesy newline to prevent exec from choking (see bug #1172785)
    return '\n'.join(output) + '\n'","def extract_test_sources_from_doctest_docstring_as_script(module, name):
    """"""Extract the test sources from a doctest docstring as a script.

    Provide the module (or dotted name of the module) containing the
    test to be debugged and the name (within the module) of the object
    with the doc string with tests to be debugged.
    """"""
    module = normalize_module(module)
    tests = DocTestFinder().find(module)
    test = [t for t in tests if t.name == name]
    if not test:
        raise ValueError(name, ""not found in tests"")
    test = test[0]
    testsrc = generate_script_from_examples(test.docstring)
    return testsrc","{""_normalize_module"": ""normalize_module"", ""script_from_examples"": ""generate_script_from_examples""}","def 0(module, depth=2):
    """"""
    Return the module specified by `module`.  In particular:
      - If `module` is a module, then return module.
      - If `module` is a string, then import and return the
        module with that name.
      - If `module` is None, then return the calling module.
        The calling module is assumed to be the module of
        the stack frame at the given depth in the call stack.
    """"""
    if inspect.ismodule(module):
        return module
    elif isinstance(module, str):
        return __import__(module, globals(), locals(), [""*""])
    elif module is None:
        return sys.modules[sys._getframe(depth).f_globals['__name__']]
    else:
        raise TypeError(""Expected a module, string, or None"")

def 1(s):
    r""""""Extract script from text with examples.

       Converts text with examples to a Python script.  Example input is
       converted to regular code.  Example output and all other words
       are converted to comments:

       >>> text = '''
       ...       Here are examples of simple math.
       ...
       ...           Python has super accurate integer addition
       ...
       ...           >>> 2 + 2
       ...           5
       ...
       ...           And very friendly error messages:
       ...
       ...           >>> 1/0
       ...           To Infinity
       ...           And
       ...           Beyond
       ...
       ...           You can use logic if you want:
       ...
       ...           >>> if 0:
       ...           ...    blah
       ...           ...    blah
       ...           ...
       ...
       ...           Ho hum
       ...           '''

       >>> print(1(text))
       # Here are examples of simple math.
       #
       #     Python has super accurate integer addition
       #
       2 + 2
       # Expected:
       ## 5
       #
       #     And very friendly error messages:
       #
       1/0
       # Expected:
       ## To Infinity
       ## And
       ## Beyond
       #
       #     You can use logic if you want:
       #
       if 0:
          blah
          blah
       #
       #     Ho hum
       <BLANKLINE>
       """"""
    output = []
    for piece in DocTestParser().parse(s):
        if isinstance(piece, Example):
            # Add the example's source code (strip trailing NL)
            output.append(piece.source[:-1])
            # Add the expected output:
            want = piece.want
            if want:
                output.append('# Expected:')
                output += ['## '+l for l in want.split('\n')[:-1]]
        else:
            # Add non-example text.
            output += [_comment_line(l)
                       for l in piece.split('\n')[:-1]]

    # Trim junk on both ends.
    while output and output[-1] == '#':
        output.pop()
    while output and output[0] == '#':
        output.pop(0)
    # Combine the output, and return it.
    # Add a courtesy newline to prevent exec from choking (see bug #1172785)
    return '\n'.join(output) + '\n'","def 2(module, name):
    """"""Extract the test sources from a doctest docstring as a script.

    Provide the module (or dotted name of the module) containing the
    test to be debugged and the name (within the module) of the object
    with the doc string with tests to be debugged.
    """"""
    module = 0(module)
    tests = DocTestFinder().find(module)
    test = [t for t in tests if t.name == name]
    if not test:
        raise ValueError(name, ""not found in tests"")
    test = test[0]
    testsrc = 1(test.docstring)
    return testsrc","{""_normalize_module"": ""0"", ""script_from_examples"": ""1""}"
191,191,"def debug_src(src, pm=False, globs=None):
    """"""Debug a single doctest docstring, in argument `src`'""""""
    testsrc = script_from_examples(src)
    debug_script(testsrc, pm, globs)","def debug_script(src, pm=False, globs=None):
    ""Debug a test script.  `src` is the script, as a string.""
    import pdb

    if globs:
        globs = globs.copy()
    else:
        globs = {}

    if pm:
        try:
            exec(src, globs, globs)
        except:
            print(sys.exc_info()[1])
            p = pdb.Pdb(nosigint=True)
            p.reset()
            p.interaction(None, sys.exc_info()[2])
    else:
        pdb.Pdb(nosigint=True).run(""exec(%r)"" % src, globs, globs)

def script_from_examples(s):
    r""""""Extract script from text with examples.

       Converts text with examples to a Python script.  Example input is
       converted to regular code.  Example output and all other words
       are converted to comments:

       >>> text = '''
       ...       Here are examples of simple math.
       ...
       ...           Python has super accurate integer addition
       ...
       ...           >>> 2 + 2
       ...           5
       ...
       ...           And very friendly error messages:
       ...
       ...           >>> 1/0
       ...           To Infinity
       ...           And
       ...           Beyond
       ...
       ...           You can use logic if you want:
       ...
       ...           >>> if 0:
       ...           ...    blah
       ...           ...    blah
       ...           ...
       ...
       ...           Ho hum
       ...           '''

       >>> print(script_from_examples(text))
       # Here are examples of simple math.
       #
       #     Python has super accurate integer addition
       #
       2 + 2
       # Expected:
       ## 5
       #
       #     And very friendly error messages:
       #
       1/0
       # Expected:
       ## To Infinity
       ## And
       ## Beyond
       #
       #     You can use logic if you want:
       #
       if 0:
          blah
          blah
       #
       #     Ho hum
       <BLANKLINE>
       """"""
    output = []
    for piece in DocTestParser().parse(s):
        if isinstance(piece, Example):
            # Add the example's source code (strip trailing NL)
            output.append(piece.source[:-1])
            # Add the expected output:
            want = piece.want
            if want:
                output.append('# Expected:')
                output += ['## '+l for l in want.split('\n')[:-1]]
        else:
            # Add non-example text.
            output += [_comment_line(l)
                       for l in piece.split('\n')[:-1]]

    # Trim junk on both ends.
    while output and output[-1] == '#':
        output.pop()
    while output and output[0] == '#':
        output.pop(0)
    # Combine the output, and return it.
    # Add a courtesy newline to prevent exec from choking (see bug #1172785)
    return '\n'.join(output) + '\n'","def debug_test_script(src, pm=False, globs=None):
    ""Debug a test script.  `src` is the script, as a string.""
    import pdb

    if globs:
        globs = globs.copy()
    else:
        globs = {}

    if pm:
        try:
            exec(src, globs, globs)
        except:
            print(sys.exc_info()[1])
            p = pdb.Pdb(nosigint=True)
            p.reset()
            p.interaction(None, sys.exc_info()[2])
    else:
        pdb.Pdb(nosigint=True).run(""exec(%r)"" % src, globs, globs)

def generate_script_from_examples(s):
    r""""""Extract script from text with examples.

       Converts text with examples to a Python script.  Example input is
       converted to regular code.  Example output and all other words
       are converted to comments:

       >>> text = '''
       ...       Here are examples of simple math.
       ...
       ...           Python has super accurate integer addition
       ...
       ...           >>> 2 + 2
       ...           5
       ...
       ...           And very friendly error messages:
       ...
       ...           >>> 1/0
       ...           To Infinity
       ...           And
       ...           Beyond
       ...
       ...           You can use logic if you want:
       ...
       ...           >>> if 0:
       ...           ...    blah
       ...           ...    blah
       ...           ...
       ...
       ...           Ho hum
       ...           '''

       >>> print(generate_script_from_examples(text))
       # Here are examples of simple math.
       #
       #     Python has super accurate integer addition
       #
       2 + 2
       # Expected:
       ## 5
       #
       #     And very friendly error messages:
       #
       1/0
       # Expected:
       ## To Infinity
       ## And
       ## Beyond
       #
       #     You can use logic if you want:
       #
       if 0:
          blah
          blah
       #
       #     Ho hum
       <BLANKLINE>
       """"""
    output = []
    for piece in DocTestParser().parse(s):
        if isinstance(piece, Example):
            # Add the example's source code (strip trailing NL)
            output.append(piece.source[:-1])
            # Add the expected output:
            want = piece.want
            if want:
                output.append('# Expected:')
                output += ['## '+l for l in want.split('\n')[:-1]]
        else:
            # Add non-example text.
            output += [_comment_line(l)
                       for l in piece.split('\n')[:-1]]

    # Trim junk on both ends.
    while output and output[-1] == '#':
        output.pop()
    while output and output[0] == '#':
        output.pop(0)
    # Combine the output, and return it.
    # Add a courtesy newline to prevent exec from choking (see bug #1172785)
    return '\n'.join(output) + '\n'","def debug_src_and_debug_single_doctest_docstring_in_argument_src(src, pm=False, globs=None):
    """"""Debug a single doctest docstring, in argument `src`'""""""
    testsrc = generate_script_from_examples(src)
    debug_test_script(testsrc, pm, globs)","{""debug_script"": ""debug_test_script"", ""script_from_examples"": ""generate_script_from_examples""}","def 0(src, pm=False, globs=None):
    ""Debug a test script.  `src` is the script, as a string.""
    import pdb

    if globs:
        globs = globs.copy()
    else:
        globs = {}

    if pm:
        try:
            exec(src, globs, globs)
        except:
            print(sys.exc_info()[1])
            p = pdb.Pdb(nosigint=True)
            p.reset()
            p.interaction(None, sys.exc_info()[2])
    else:
        pdb.Pdb(nosigint=True).run(""exec(%r)"" % src, globs, globs)

def 1(s):
    r""""""Extract script from text with examples.

       Converts text with examples to a Python script.  Example input is
       converted to regular code.  Example output and all other words
       are converted to comments:

       >>> text = '''
       ...       Here are examples of simple math.
       ...
       ...           Python has super accurate integer addition
       ...
       ...           >>> 2 + 2
       ...           5
       ...
       ...           And very friendly error messages:
       ...
       ...           >>> 1/0
       ...           To Infinity
       ...           And
       ...           Beyond
       ...
       ...           You can use logic if you want:
       ...
       ...           >>> if 0:
       ...           ...    blah
       ...           ...    blah
       ...           ...
       ...
       ...           Ho hum
       ...           '''

       >>> print(1(text))
       # Here are examples of simple math.
       #
       #     Python has super accurate integer addition
       #
       2 + 2
       # Expected:
       ## 5
       #
       #     And very friendly error messages:
       #
       1/0
       # Expected:
       ## To Infinity
       ## And
       ## Beyond
       #
       #     You can use logic if you want:
       #
       if 0:
          blah
          blah
       #
       #     Ho hum
       <BLANKLINE>
       """"""
    output = []
    for piece in DocTestParser().parse(s):
        if isinstance(piece, Example):
            # Add the example's source code (strip trailing NL)
            output.append(piece.source[:-1])
            # Add the expected output:
            want = piece.want
            if want:
                output.append('# Expected:')
                output += ['## '+l for l in want.split('\n')[:-1]]
        else:
            # Add non-example text.
            output += [_comment_line(l)
                       for l in piece.split('\n')[:-1]]

    # Trim junk on both ends.
    while output and output[-1] == '#':
        output.pop()
    while output and output[0] == '#':
        output.pop(0)
    # Combine the output, and return it.
    # Add a courtesy newline to prevent exec from choking (see bug #1172785)
    return '\n'.join(output) + '\n'","def 2(src, pm=False, globs=None):
    """"""Debug a single doctest docstring, in argument `src`'""""""
    testsrc = 1(src)
    0(testsrc, pm, globs)","{""debug_script"": ""0"", ""script_from_examples"": ""1""}"
192,192,"def win_getpass(prompt='Password: ', stream=None):
    """"""Prompt for password with echo off, using Windows getwch().""""""
    if sys.stdin is not sys.__stdin__:
        return fallback_getpass(prompt, stream)

    for c in prompt:
        msvcrt.putwch(c)
    pw = """"
    while 1:
        c = msvcrt.getwch()
        if c == '\r' or c == '\n':
            break
        if c == '\003':
            raise KeyboardInterrupt
        if c == '\b':
            pw = pw[:-1]
        else:
            pw = pw + c
    msvcrt.putwch('\r')
    msvcrt.putwch('\n')
    return pw","def fallback_getpass(prompt='Password: ', stream=None):
    warnings.warn(""Can not control echo on the terminal."", GetPassWarning,
                  stacklevel=2)
    if not stream:
        stream = sys.stderr
    print(""Warning: Password input may be echoed."", file=stream)
    return _raw_input(prompt, stream)","def get_password_with_warning(prompt='Password: ', stream=None):
    warnings.warn(""Can not control echo on the terminal."", GetPassWarning,
                  stacklevel=2)
    if not stream:
        stream = sys.stderr
    print(""Warning: Password input may be echoed."", file=stream)
    return _raw_input(prompt, stream)","def win_getpass_and_prompt_for_password_with_echo_off(prompt='Password: ', stream=None):
    """"""Prompt for password with echo off, using Windows getwch().""""""
    if sys.stdin is not sys.__stdin__:
        return get_password_with_warning(prompt, stream)

    for c in prompt:
        msvcrt.putwch(c)
    pw = """"
    while 1:
        c = msvcrt.getwch()
        if c == '\r' or c == '\n':
            break
        if c == '\003':
            raise KeyboardInterrupt
        if c == '\b':
            pw = pw[:-1]
        else:
            pw = pw + c
    msvcrt.putwch('\r')
    msvcrt.putwch('\n')
    return pw","{""fallback_getpass"": ""get_password_with_warning""}","def 0(prompt='Password: ', stream=None):
    warnings.warn(""Can not control echo on the terminal."", GetPassWarning,
                  stacklevel=2)
    if not stream:
        stream = sys.stderr
    print(""Warning: Password input may be echoed."", file=stream)
    return _raw_input(prompt, stream)","def 1(prompt='Password: ', stream=None):
    """"""Prompt for password with echo off, using Windows getwch().""""""
    if sys.stdin is not sys.__stdin__:
        return 0(prompt, stream)

    for c in prompt:
        msvcrt.putwch(c)
    pw = """"
    while 1:
        c = msvcrt.getwch()
        if c == '\r' or c == '\n':
            break
        if c == '\003':
            raise KeyboardInterrupt
        if c == '\b':
            pw = pw[:-1]
        else:
            pw = pw + c
    msvcrt.putwch('\r')
    msvcrt.putwch('\n')
    return pw","{""fallback_getpass"": ""0""}"
193,193,"def whichmodule(obj, name):
    """"""Find the module an object belong to.""""""
    module_name = getattr(obj, '__module__', None)
    if module_name is not None:
        return module_name
    # Protect the iteration by using a list copy of sys.modules against dynamic
    # modules that trigger imports of other modules upon calls to getattr.
    for module_name, module in sys.modules.copy().items():
        if (module_name == '__main__'
            or module_name == '__mp_main__'  # bpo-42406
            or module is None):
            continue
        try:
            if _getattribute(module, name)[0] is obj:
                return module_name
        except AttributeError:
            pass
    return '__main__'","def _getattribute(obj, name):
    for subpath in name.split('.'):
        if subpath == '<locals>':
            raise AttributeError(""Can't get local attribute {!r} on {!r}""
                                 .format(name, obj))
        try:
            parent = obj
            obj = getattr(obj, subpath)
        except AttributeError:
            raise AttributeError(""Can't get attribute {!r} on {!r}""
                                 .format(name, obj)) from None
    return obj, parent","def get_attribute_recursive(obj, name):
    for subpath in name.split('.'):
        if subpath == '<locals>':
            raise AttributeError(""Can't get local attribute {!r} on {!r}""
                                 .format(name, obj))
        try:
            parent = obj
            obj = getattr(obj, subpath)
        except AttributeError:
            raise AttributeError(""Can't get attribute {!r} on {!r}""
                                 .format(name, obj)) from None
    return obj, parent","def whichmodule_and_find_module_an_object_belongs_to(obj, name):
    """"""Find the module an object belong to.""""""
    module_name = getattr(obj, '__module__', None)
    if module_name is not None:
        return module_name
    # Protect the iteration by using a list copy of sys.modules against dynamic
    # modules that trigger imports of other modules upon calls to getattr.
    for module_name, module in sys.modules.copy().items():
        if (module_name == '__main__'
            or module_name == '__mp_main__'  # bpo-42406
            or module is None):
            continue
        try:
            if get_attribute_recursive(module, name)[0] is obj:
                return module_name
        except AttributeError:
            pass
    return '__main__'","{""_getattribute"": ""get_attribute_recursive""}","def 0(obj, name):
    for subpath in name.split('.'):
        if subpath == '<locals>':
            raise AttributeError(""Can't get local attribute {!r} on {!r}""
                                 .format(name, obj))
        try:
            parent = obj
            obj = getattr(obj, subpath)
        except AttributeError:
            raise AttributeError(""Can't get attribute {!r} on {!r}""
                                 .format(name, obj)) from None
    return obj, parent","def 1(obj, name):
    """"""Find the module an object belong to.""""""
    module_name = getattr(obj, '__module__', None)
    if module_name is not None:
        return module_name
    # Protect the iteration by using a list copy of sys.modules against dynamic
    # modules that trigger imports of other modules upon calls to getattr.
    for module_name, module in sys.modules.copy().items():
        if (module_name == '__main__'
            or module_name == '__mp_main__'  # bpo-42406
            or module is None):
            continue
        try:
            if 0(module, name)[0] is obj:
                return module_name
        except AttributeError:
            pass
    return '__main__'","{""_getattribute"": ""0""}"
194,194,"def openpty():
    """"""openpty() -> (master_fd, slave_fd)
    Open a pty master/slave pair, using os.openpty() if possible.""""""

    try:
        return os.openpty()
    except (AttributeError, OSError):
        pass
    master_fd, slave_name = _open_terminal()
    slave_fd = slave_open(slave_name)
    return master_fd, slave_fd","def _open_terminal():
    """"""Open pty master and return (master_fd, tty_name).""""""
    for x in 'pqrstuvwxyzPQRST':
        for y in '0123456789abcdef':
            pty_name = '/dev/pty' + x + y
            try:
                fd = os.open(pty_name, os.O_RDWR)
            except OSError:
                continue
            return (fd, '/dev/tty' + x + y)
    raise OSError('out of pty devices')

def slave_open(tty_name):
    """"""slave_open(tty_name) -> slave_fd
    Open the pty slave and acquire the controlling terminal, returning
    opened filedescriptor.
    Deprecated, use openpty() instead.""""""

    result = os.open(tty_name, os.O_RDWR)
    try:
        from fcntl import ioctl, I_PUSH
    except ImportError:
        return result
    try:
        ioctl(result, I_PUSH, ""ptem"")
        ioctl(result, I_PUSH, ""ldterm"")
    except OSError:
        pass
    return result","def open_pty_master_and_get_terminal():
    """"""Open pty master and return (master_fd, tty_name).""""""
    for x in 'pqrstuvwxyzPQRST':
        for y in '0123456789abcdef':
            pty_name = '/dev/pty' + x + y
            try:
                fd = os.open(pty_name, os.O_RDWR)
            except OSError:
                continue
            return (fd, '/dev/tty' + x + y)
    raise OSError('out of pty devices')

def open_pty_slave_and_acquire_terminal(tty_name):
    """"""open_pty_slave_and_acquire_terminal(tty_name) -> slave_fd
    Open the pty slave and acquire the controlling terminal, returning
    opened filedescriptor.
    Deprecated, use openpty() instead.""""""

    result = os.open(tty_name, os.O_RDWR)
    try:
        from fcntl import ioctl, I_PUSH
    except ImportError:
        return result
    try:
        ioctl(result, I_PUSH, ""ptem"")
        ioctl(result, I_PUSH, ""ldterm"")
    except OSError:
        pass
    return result","def openpty_and_open_a_pty_master_slave_pair_using_os_openpty_if_possible():
    """"""openpty_and_open_a_pty_master_slave_pair_using_os_openpty_if_possible() -> (master_fd, slave_fd)
    Open a pty master/slave pair, using os.openpty_and_open_a_pty_master_slave_pair_using_os_openpty_if_possible() if possible.""""""

    try:
        return os.openpty_and_open_a_pty_master_slave_pair_using_os_openpty_if_possible()
    except (AttributeError, OSError):
        pass
    master_fd, slave_name = open_pty_master_and_get_terminal()
    slave_fd = open_pty_slave_and_acquire_terminal(slave_name)
    return master_fd, slave_fd","{""_open_terminal"": ""open_pty_master_and_get_terminal"", ""slave_open"": ""open_pty_slave_and_acquire_terminal""}","def 0():
    """"""Open pty master and return (master_fd, tty_name).""""""
    for x in 'pqrstuvwxyzPQRST':
        for y in '0123456789abcdef':
            pty_name = '/dev/pty' + x + y
            try:
                fd = os.open(pty_name, os.O_RDWR)
            except OSError:
                continue
            return (fd, '/dev/tty' + x + y)
    raise OSError('out of pty devices')

def 1(tty_name):
    """"""1(tty_name) -> slave_fd
    Open the pty slave and acquire the controlling terminal, returning
    opened filedescriptor.
    Deprecated, use openpty() instead.""""""

    result = os.open(tty_name, os.O_RDWR)
    try:
        from fcntl import ioctl, I_PUSH
    except ImportError:
        return result
    try:
        ioctl(result, I_PUSH, ""ptem"")
        ioctl(result, I_PUSH, ""ldterm"")
    except OSError:
        pass
    return result","def 2():
    """"""2() -> (master_fd, slave_fd)
    Open a pty master/slave pair, using os.2() if possible.""""""

    try:
        return os.2()
    except (AttributeError, OSError):
        pass
    master_fd, slave_name = 0()
    slave_fd = 1(slave_name)
    return master_fd, slave_fd","{""_open_terminal"": ""0"", ""slave_open"": ""1""}"
195,195,"def master_open():
    """"""master_open() -> (master_fd, slave_name)
    Open a pty master and return the fd, and the filename of the slave end.
    Deprecated, use openpty() instead.""""""

    try:
        master_fd, slave_fd = os.openpty()
    except (AttributeError, OSError):
        pass
    else:
        slave_name = os.ttyname(slave_fd)
        os.close(slave_fd)
        return master_fd, slave_name

    return _open_terminal()","def _open_terminal():
    """"""Open pty master and return (master_fd, tty_name).""""""
    for x in 'pqrstuvwxyzPQRST':
        for y in '0123456789abcdef':
            pty_name = '/dev/pty' + x + y
            try:
                fd = os.open(pty_name, os.O_RDWR)
            except OSError:
                continue
            return (fd, '/dev/tty' + x + y)
    raise OSError('out of pty devices')

def close():
    """"""Close the sequence.""""""
    global _state
    state = _state
    _state = None
    if state:
        state.close()","def open_pty_master_and_get_terminal():
    """"""Open pty master and return (master_fd, tty_name).""""""
    for x in 'pqrstuvwxyzPQRST':
        for y in '0123456789abcdef':
            pty_name = '/dev/pty' + x + y
            try:
                fd = os.open(pty_name, os.O_RDWR)
            except OSError:
                continue
            return (fd, '/dev/tty' + x + y)
    raise OSError('out of pty devices')

def close_the_sequence():
    """"""Close the sequence.""""""
    global _state
    state = _state
    _state = None
    if state:
        state.close_the_sequence()","def master_open_and_open_a_pty_master_return_fd_and_filename_of_slave_end():
    """"""master_open_and_open_a_pty_master_return_fd_and_filename_of_slave_end() -> (master_fd, slave_name)
    Open a pty master and return the fd, and the filename of the slave end.
    Deprecated, use openpty() instead.""""""

    try:
        master_fd, slave_fd = os.openpty()
    except (AttributeError, OSError):
        pass
    else:
        slave_name = os.ttyname(slave_fd)
        os.close_the_sequence(slave_fd)
        return master_fd, slave_name

    return open_pty_master_and_get_terminal()","{""_open_terminal"": ""open_pty_master_and_get_terminal"", ""close"": ""close_the_sequence""}","def 0():
    """"""Open pty master and return (master_fd, tty_name).""""""
    for x in 'pqrstuvwxyzPQRST':
        for y in '0123456789abcdef':
            pty_name = '/dev/pty' + x + y
            try:
                fd = os.open(pty_name, os.O_RDWR)
            except OSError:
                continue
            return (fd, '/dev/tty' + x + y)
    raise OSError('out of pty devices')

def 1():
    """"""Close the sequence.""""""
    global _state
    state = _state
    _state = None
    if state:
        state.1()","def 2():
    """"""2() -> (master_fd, slave_name)
    Open a pty master and return the fd, and the filename of the slave end.
    Deprecated, use openpty() instead.""""""

    try:
        master_fd, slave_fd = os.openpty()
    except (AttributeError, OSError):
        pass
    else:
        slave_name = os.ttyname(slave_fd)
        os.1(slave_fd)
        return master_fd, slave_name

    return 0()","{""_open_terminal"": ""0"", ""close"": ""1""}"
196,196,"def _copy(master_fd, master_read=_read, stdin_read=_read):
    """"""Parent copy loop.
    Copies
            pty master -> standard output   (master_read)
            standard input -> pty master    (stdin_read)""""""
    fds = [master_fd, STDIN_FILENO]
    while fds:
        rfds, _wfds, _xfds = select(fds, [], [])

        if master_fd in rfds:
            # Some OSes signal EOF by returning an empty byte string,
            # some throw OSErrors.
            try:
                data = master_read(master_fd)
            except OSError:
                data = b""""
            if not data:  # Reached EOF.
                return    # Assume the child process has exited and is
                          # unreachable, so we clean up.
            else:
                os.write(STDOUT_FILENO, data)

        if STDIN_FILENO in rfds:
            data = stdin_read(STDIN_FILENO)
            if not data:
                fds.remove(STDIN_FILENO)
            else:
                _writen(master_fd, data)","def _writen(fd, data):
    """"""Write all the data to a descriptor.""""""
    while data:
        n = os.write(fd, data)
        data = data[n:]","def write_all_data_to_descriptor(fd, data):
    """"""Write all the data to a descriptor.""""""
    while data:
        n = os.write(fd, data)
        data = data[n:]","def _copy_and_parent_copy_loop_copies_pty_master_standard_output_standard_input(master_fd, master_read=_read, stdin_read=_read):
    """"""Parent copy loop.
    Copies
            pty master -> standard output   (master_read)
            standard input -> pty master    (stdin_read)""""""
    fds = [master_fd, STDIN_FILENO]
    while fds:
        rfds, _wfds, _xfds = select(fds, [], [])

        if master_fd in rfds:
            # Some OSes signal EOF by returning an empty byte string,
            # some throw OSErrors.
            try:
                data = master_read(master_fd)
            except OSError:
                data = b""""
            if not data:  # Reached EOF.
                return    # Assume the child process has exited and is
                          # unreachable, so we clean up.
            else:
                os.write(STDOUT_FILENO, data)

        if STDIN_FILENO in rfds:
            data = stdin_read(STDIN_FILENO)
            if not data:
                fds.remove(STDIN_FILENO)
            else:
                write_all_data_to_descriptor(master_fd, data)","{""_writen"": ""write_all_data_to_descriptor""}","def 0(fd, data):
    """"""Write all the data to a descriptor.""""""
    while data:
        n = os.write(fd, data)
        data = data[n:]","def 1(master_fd, master_read=_read, stdin_read=_read):
    """"""Parent copy loop.
    Copies
            pty master -> standard output   (master_read)
            standard input -> pty master    (stdin_read)""""""
    fds = [master_fd, STDIN_FILENO]
    while fds:
        rfds, _wfds, _xfds = select(fds, [], [])

        if master_fd in rfds:
            # Some OSes signal EOF by returning an empty byte string,
            # some throw OSErrors.
            try:
                data = master_read(master_fd)
            except OSError:
                data = b""""
            if not data:  # Reached EOF.
                return    # Assume the child process has exited and is
                          # unreachable, so we clean up.
            else:
                os.write(STDOUT_FILENO, data)

        if STDIN_FILENO in rfds:
            data = stdin_read(STDIN_FILENO)
            if not data:
                fds.remove(STDIN_FILENO)
            else:
                0(master_fd, data)","{""_writen"": ""0""}"
197,197,"def spawn(cmd, search_path=1, verbose=0, dry_run=0, env=None):
    """"""Run another program, specified as a command list 'cmd', in a new process.

    'cmd' is just the argument list for the new process, ie.
    cmd[0] is the program to run and cmd[1:] are the rest of its arguments.
    There is no way to run a program with a name different from that of its
    executable.

    If 'search_path' is true (the default), the system's executable
    search path will be used to find the program; otherwise, cmd[0]
    must be the exact path to the executable.  If 'dry_run' is true,
    the command will not actually be run.

    Raise DistutilsExecError if running the program fails in any way; just
    return on success.
    """"""
    # cmd is documented as a list, but just in case some code passes a tuple
    # in, protect our %-formatting code against horrible death
    cmd = list(cmd)

    log.info(subprocess.list2cmdline(cmd))
    if dry_run:
        return

    if search_path:
        executable = find_executable(cmd[0])
        if executable is not None:
            cmd[0] = executable

    env = env if env is not None else dict(os.environ)

    if sys.platform == 'darwin':
        from distutils.util import MACOSX_VERSION_VAR, get_macosx_target_ver

        macosx_target_ver = get_macosx_target_ver()
        if macosx_target_ver:
            env[MACOSX_VERSION_VAR] = macosx_target_ver

    try:
        proc = subprocess.Popen(cmd, env=env)
        proc.wait()
        exitcode = proc.returncode
    except OSError as exc:
        if not DEBUG:
            cmd = cmd[0]
        raise DistutilsExecError(""command %r failed: %s"" % (cmd, exc.args[-1])) from exc

    if exitcode:
        if not DEBUG:
            cmd = cmd[0]
        raise DistutilsExecError(
            ""command %r failed with exit code %s"" % (cmd, exitcode)
        )","def wait(fs, timeout=None, return_when=ALL_COMPLETED):
    """"""Wait for the futures in the given sequence to complete.

    Args:
        fs: The sequence of Futures (possibly created by different Executors) to
            wait upon.
        timeout: The maximum number of seconds to wait. If None, then there
            is no limit on the wait time.
        return_when: Indicates when this function should return. The options
            are:

            FIRST_COMPLETED - Return when any future finishes or is
                              cancelled.
            FIRST_EXCEPTION - Return when any future finishes by raising an
                              exception. If no future raises an exception
                              then it is equivalent to ALL_COMPLETED.
            ALL_COMPLETED -   Return when all futures finish or are cancelled.

    Returns:
        A named 2-tuple of sets. The first set, named 'done', contains the
        futures that completed (is finished or cancelled) before the wait
        completed. The second set, named 'not_done', contains uncompleted
        futures. Duplicate futures given to *fs* are removed and will be 
        returned only once.
    """"""
    fs = set(fs)
    with _AcquireFutures(fs):
        done = {f for f in fs
                   if f._state in [CANCELLED_AND_NOTIFIED, FINISHED]}
        not_done = fs - done
        if (return_when == FIRST_COMPLETED) and done:
            return DoneAndNotDoneFutures(done, not_done)
        elif (return_when == FIRST_EXCEPTION) and done:
            if any(f for f in done
                   if not f.cancelled() and f.exception() is not None):
                return DoneAndNotDoneFutures(done, not_done)

        if len(done) == len(fs):
            return DoneAndNotDoneFutures(done, not_done)

        waiter = _create_and_install_waiters(fs, return_when)

    waiter.event.wait(timeout)
    for f in fs:
        with f._condition:
            f._waiters.remove(waiter)

    done.update(waiter.finished_futures)
    return DoneAndNotDoneFutures(done, fs - done)","def wait_for_futures(fs, timeout=None, return_when=ALL_COMPLETED):
    """"""Wait for the futures in the given sequence to complete.

    Args:
        fs: The sequence of Futures (possibly created by different Executors) to
            wait_for_futures upon.
        timeout: The maximum number of seconds to wait_for_futures. If None, then there
            is no limit on the wait_for_futures time.
        return_when: Indicates when this function should return. The options
            are:

            FIRST_COMPLETED - Return when any future finishes or is
                              cancelled.
            FIRST_EXCEPTION - Return when any future finishes by raising an
                              exception. If no future raises an exception
                              then it is equivalent to ALL_COMPLETED.
            ALL_COMPLETED -   Return when all futures finish or are cancelled.

    Returns:
        A named 2-tuple of sets. The first set, named 'done', contains the
        futures that completed (is finished or cancelled) before the wait_for_futures
        completed. The second set, named 'not_done', contains uncompleted
        futures. Duplicate futures given to *fs* are removed and will be 
        returned only once.
    """"""
    fs = set(fs)
    with _AcquireFutures(fs):
        done = {f for f in fs
                   if f._state in [CANCELLED_AND_NOTIFIED, FINISHED]}
        not_done = fs - done
        if (return_when == FIRST_COMPLETED) and done:
            return DoneAndNotDoneFutures(done, not_done)
        elif (return_when == FIRST_EXCEPTION) and done:
            if any(f for f in done
                   if not f.cancelled() and f.exception() is not None):
                return DoneAndNotDoneFutures(done, not_done)

        if len(done) == len(fs):
            return DoneAndNotDoneFutures(done, not_done)

        wait_for_futureser = _create_and_install_wait_for_futuresers(fs, return_when)

    wait_for_futureser.event.wait_for_futures(timeout)
    for f in fs:
        with f._condition:
            f._wait_for_futuresers.remove(wait_for_futureser)

    done.update(wait_for_futureser.finished_futures)
    return DoneAndNotDoneFutures(done, fs - done)","def spawn_and_run_another_program_in_a_new_process_specified_as_command_list_cmd(cmd, search_path=1, verbose=0, dry_run=0, env=None):
    """"""Run another program, specified as a command list 'cmd', in a new process.

    'cmd' is just the argument list for the new process, ie.
    cmd[0] is the program to run and cmd[1:] are the rest of its arguments.
    There is no way to run a program with a name different from that of its
    executable.

    If 'search_path' is true (the default), the system's executable
    search path will be used to find the program; otherwise, cmd[0]
    must be the exact path to the executable.  If 'dry_run' is true,
    the command will not actually be run.

    Raise DistutilsExecError if running the program fails in any way; just
    return on success.
    """"""
    # cmd is documented as a list, but just in case some code passes a tuple
    # in, protect our %-formatting code against horrible death
    cmd = list(cmd)

    log.info(subprocess.list2cmdline(cmd))
    if dry_run:
        return

    if search_path:
        executable = find_executable(cmd[0])
        if executable is not None:
            cmd[0] = executable

    env = env if env is not None else dict(os.environ)

    if sys.platform == 'darwin':
        from distutils.util import MACOSX_VERSION_VAR, get_macosx_target_ver

        macosx_target_ver = get_macosx_target_ver()
        if macosx_target_ver:
            env[MACOSX_VERSION_VAR] = macosx_target_ver

    try:
        proc = subprocess.Popen(cmd, env=env)
        proc.wait_for_futures()
        exitcode = proc.returncode
    except OSError as exc:
        if not DEBUG:
            cmd = cmd[0]
        raise DistutilsExecError(""command %r failed: %s"" % (cmd, exc.args[-1])) from exc

    if exitcode:
        if not DEBUG:
            cmd = cmd[0]
        raise DistutilsExecError(
            ""command %r failed with exit code %s"" % (cmd, exitcode)
        )","{""wait"": ""wait_for_futures""}","def 0(fs, timeout=None, return_when=ALL_COMPLETED):
    """"""Wait for the futures in the given sequence to complete.

    Args:
        fs: The sequence of Futures (possibly created by different Executors) to
            wait upon.
        timeout: The maximum number of seconds to wait. If None, then there
            is no limit on the wait time.
        return_when: Indicates when this function should return. The options
            are:

            FIRST_COMPLETED - Return when any future finishes or is
                              cancelled.
            FIRST_EXCEPTION - Return when any future finishes by raising an
                              exception. If no future raises an exception
                              then it is equivalent to ALL_COMPLETED.
            ALL_COMPLETED -   Return when all futures finish or are cancelled.

    Returns:
        A named 2-tuple of sets. The first set, named 'done', contains the
        futures that completed (is finished or cancelled) before the wait
        completed. The second set, named 'not_done', contains uncompleted
        futures. Duplicate futures given to *fs* are removed and will be 
        returned only once.
    """"""
    fs = set(fs)
    with _AcquireFutures(fs):
        done = {f for f in fs
                   if f._state in [CANCELLED_AND_NOTIFIED, FINISHED]}
        not_done = fs - done
        if (return_when == FIRST_COMPLETED) and done:
            return DoneAndNotDoneFutures(done, not_done)
        elif (return_when == FIRST_EXCEPTION) and done:
            if any(f for f in done
                   if not f.cancelled() and f.exception() is not None):
                return DoneAndNotDoneFutures(done, not_done)

        if len(done) == len(fs):
            return DoneAndNotDoneFutures(done, not_done)

        waiter = _create_and_install_waiters(fs, return_when)

    waiter.event.0(timeout)
    for f in fs:
        with f._condition:
            f._waiters.remove(waiter)

    done.update(waiter.finished_futures)
    return DoneAndNotDoneFutures(done, fs - done)","def 1(cmd, search_path=1, verbose=0, dry_run=0, env=None):
    """"""Run another program, specified as a command list 'cmd', in a new process.

    'cmd' is just the argument list for the new process, ie.
    cmd[0] is the program to run and cmd[1:] are the rest of its arguments.
    There is no way to run a program with a name different from that of its
    executable.

    If 'search_path' is true (the default), the system's executable
    search path will be used to find the program; otherwise, cmd[0]
    must be the exact path to the executable.  If 'dry_run' is true,
    the command will not actually be run.

    Raise DistutilsExecError if running the program fails in any way; just
    return on success.
    """"""
    # cmd is documented as a list, but just in case some code passes a tuple
    # in, protect our %-formatting code against horrible death
    cmd = list(cmd)

    log.info(subprocess.list2cmdline(cmd))
    if dry_run:
        return

    if search_path:
        executable = find_executable(cmd[0])
        if executable is not None:
            cmd[0] = executable

    env = env if env is not None else dict(os.environ)

    if sys.platform == 'darwin':
        from distutils.util import MACOSX_VERSION_VAR, get_macosx_target_ver

        macosx_target_ver = get_macosx_target_ver()
        if macosx_target_ver:
            env[MACOSX_VERSION_VAR] = macosx_target_ver

    try:
        proc = subprocess.Popen(cmd, env=env)
        proc.0()
        exitcode = proc.returncode
    except OSError as exc:
        if not DEBUG:
            cmd = cmd[0]
        raise DistutilsExecError(""command %r failed: %s"" % (cmd, exc.args[-1])) from exc

    if exitcode:
        if not DEBUG:
            cmd = cmd[0]
        raise DistutilsExecError(
            ""command %r failed with exit code %s"" % (cmd, exitcode)
        )","{""wait"": ""0""}"
198,198,"def contextmanager(func):
    """"""@contextmanager decorator.

    Typical usage:

        @contextmanager
        def some_generator(<arguments>):
            <setup>
            try:
                yield <value>
            finally:
                <cleanup>

    This makes this:

        with some_generator(<arguments>) as <variable>:
            <body>

    equivalent to this:

        <setup>
        try:
            <variable> = <value>
            <body>
        finally:
            <cleanup>
    """"""
    @wraps(func)
    def helper(*args, **kwds):
        return _GeneratorContextManager(func, args, kwds)
    return helper","def wraps(wrapped,
          assigned = WRAPPER_ASSIGNMENTS,
          updated = WRAPPER_UPDATES):
    """"""Decorator factory to apply update_wrapper() to a wrapper function

       Returns a decorator that invokes update_wrapper() with the decorated
       function as the wrapper argument and the arguments to wraps() as the
       remaining arguments. Default arguments are as for update_wrapper().
       This is a convenience function to simplify applying partial() to
       update_wrapper().
    """"""
    return partial(update_wrapper, wrapped=wrapped,
                   assigned=assigned, updated=updated)","def create_decorator_with_update_wrapper(wrapped,
          assigned = WRAPPER_ASSIGNMENTS,
          updated = WRAPPER_UPDATES):
    """"""Decorator factory to apply update_wrapper() to a wrapper function

       Returns a decorator that invokes update_wrapper() with the decorated
       function as the wrapper argument and the arguments to create_decorator_with_update_wrapper() as the
       remaining arguments. Default arguments are as for update_wrapper().
       This is a convenience function to simplify applying partial() to
       update_wrapper().
    """"""
    return partial(update_wrapper, wrapped=wrapped,
                   assigned=assigned, updated=updated)","def contextmanager_decorator_typical_usage(func):
    """"""@contextmanager_decorator_typical_usage decorator.

    Typical usage:

        @contextmanager_decorator_typical_usage
        def some_generator_with_setup_yield_value_and_finally_cleanup(<arguments>):
            <setup>
            try:
                yield <value>
            finally:
                <cleanup>

    This makes this:

        with some_generator_with_setup_yield_value_and_finally_cleanup(<arguments>) as <variable>:
            <body>

    equivalent to this:

        <setup>
        try:
            <variable> = <value>
            <body>
        finally:
            <cleanup>
    """"""
    @create_decorator_with_update_wrapper(func)
    def helper_and_return_generator_context_manager_instance(*args, **kwds):
        return _GeneratorContextManager(func, args, kwds)
    return helper_and_return_generator_context_manager_instance","{""wraps"": ""create_decorator_with_update_wrapper""}","def 0(wrapped,
          assigned = WRAPPER_ASSIGNMENTS,
          updated = WRAPPER_UPDATES):
    """"""Decorator factory to apply update_wrapper() to a wrapper function

       Returns a decorator that invokes update_wrapper() with the decorated
       function as the wrapper argument and the arguments to 0() as the
       remaining arguments. Default arguments are as for update_wrapper().
       This is a convenience function to simplify applying partial() to
       update_wrapper().
    """"""
    return partial(update_wrapper, wrapped=wrapped,
                   assigned=assigned, updated=updated)","def 1(func):
    """"""@contextmanager decorator.

    Typical usage:

        @contextmanager
        def 2(<arguments>):
            <setup>
            try:
                yield <value>
            finally:
                <cleanup>

    This makes this:

        with 2(<arguments>) as <variable>:
            <body>

    equivalent to this:

        <setup>
        try:
            <variable> = <value>
            <body>
        finally:
            <cleanup>
    """"""
    @0(func)
    def 3(*args, **kwds):
        return _GeneratorContextManager(func, args, kwds)
    return helper","{""wraps"": ""0""}"
199,199,"def asynccontextmanager(func):
    """"""@asynccontextmanager decorator.

    Typical usage:

        @asynccontextmanager
        async def some_async_generator(<arguments>):
            <setup>
            try:
                yield <value>
            finally:
                <cleanup>

    This makes this:

        async with some_async_generator(<arguments>) as <variable>:
            <body>

    equivalent to this:

        <setup>
        try:
            <variable> = <value>
            <body>
        finally:
            <cleanup>
    """"""
    @wraps(func)
    def helper(*args, **kwds):
        return _AsyncGeneratorContextManager(func, args, kwds)
    return helper","def wraps(wrapped,
          assigned = WRAPPER_ASSIGNMENTS,
          updated = WRAPPER_UPDATES):
    """"""Decorator factory to apply update_wrapper() to a wrapper function

       Returns a decorator that invokes update_wrapper() with the decorated
       function as the wrapper argument and the arguments to wraps() as the
       remaining arguments. Default arguments are as for update_wrapper().
       This is a convenience function to simplify applying partial() to
       update_wrapper().
    """"""
    return partial(update_wrapper, wrapped=wrapped,
                   assigned=assigned, updated=updated)","def create_decorator_with_update_wrapper(wrapped,
          assigned = WRAPPER_ASSIGNMENTS,
          updated = WRAPPER_UPDATES):
    """"""Decorator factory to apply update_wrapper() to a wrapper function

       Returns a decorator that invokes update_wrapper() with the decorated
       function as the wrapper argument and the arguments to create_decorator_with_update_wrapper() as the
       remaining arguments. Default arguments are as for update_wrapper().
       This is a convenience function to simplify applying partial() to
       update_wrapper().
    """"""
    return partial(update_wrapper, wrapped=wrapped,
                   assigned=assigned, updated=updated)","def asynccontextmanager_decorator_typical_usage(func):
    """"""@asynccontextmanager_decorator_typical_usage decorator.

    Typical usage:

        @asynccontextmanager_decorator_typical_usage
        async def some_async_generator_with_setup_yield_value_and_finally_cleanup(<arguments>):
            <setup>
            try:
                yield <value>
            finally:
                <cleanup>

    This makes this:

        async with some_async_generator_with_setup_yield_value_and_finally_cleanup(<arguments>) as <variable>:
            <body>

    equivalent to this:

        <setup>
        try:
            <variable> = <value>
            <body>
        finally:
            <cleanup>
    """"""
    @create_decorator_with_update_wrapper(func)
    def helper_and_return_generator_context_manager_instance(*args, **kwds):
        return _AsyncGeneratorContextManager(func, args, kwds)
    return helper_and_return_generator_context_manager_instance","{""wraps"": ""create_decorator_with_update_wrapper""}","def 0(wrapped,
          assigned = WRAPPER_ASSIGNMENTS,
          updated = WRAPPER_UPDATES):
    """"""Decorator factory to apply update_wrapper() to a wrapper function

       Returns a decorator that invokes update_wrapper() with the decorated
       function as the wrapper argument and the arguments to 0() as the
       remaining arguments. Default arguments are as for update_wrapper().
       This is a convenience function to simplify applying partial() to
       update_wrapper().
    """"""
    return partial(update_wrapper, wrapped=wrapped,
                   assigned=assigned, updated=updated)","def 1(func):
    """"""@asynccontextmanager decorator.

    Typical usage:

        @asynccontextmanager
        async def 2(<arguments>):
            <setup>
            try:
                yield <value>
            finally:
                <cleanup>

    This makes this:

        async with 2(<arguments>) as <variable>:
            <body>

    equivalent to this:

        <setup>
        try:
            <variable> = <value>
            <body>
        finally:
            <cleanup>
    """"""
    @0(func)
    def 3(*args, **kwds):
        return _AsyncGeneratorContextManager(func, args, kwds)
    return helper","{""wraps"": ""0""}"
200,200,"def _exact_ratio(x):
    """"""Return Real number x to exact (numerator, denominator) pair.

    >>> _exact_ratio(0.25)
    (1, 4)

    x is expected to be an int, Fraction, Decimal or float.
    """"""
    try:
        return x.as_integer_ratio()
    except AttributeError:
        pass
    except (OverflowError, ValueError):
        # float NAN or INF.
        assert not _isfinite(x)
        return (x, None)
    try:
        # x may be an Integral ABC.
        return (x.numerator, x.denominator)
    except AttributeError:
        msg = f""can't convert type '{type(x).__name__}' to numerator/denominator""
        raise TypeError(msg)","def _isfinite(x):
    try:
        return x.is_finite()  # Likely a Decimal.
    except AttributeError:
        return math.isfinite(x)  # Coerces to float first.","def check_if_value_is_finite(x):
    try:
        return x.is_finite()  # Likely a Decimal.
    except AttributeError:
        return math.isfinite(x)  # Coerces to float first.","def _exact_ratio_and_return_real_number_x_to_exact_numerator_denominator_pair(x):
    """"""Return Real number x to exact (numerator, denominator) pair.

    >>> _exact_ratio_and_return_real_number_x_to_exact_numerator_denominator_pair(0.25)
    (1, 4)

    x is expected to be an int, Fraction, Decimal or float.
    """"""
    try:
        return x.as_integer_ratio()
    except AttributeError:
        pass
    except (OverflowError, ValueError):
        # float NAN or INF.
        assert not check_if_value_is_finite(x)
        return (x, None)
    try:
        # x may be an Integral ABC.
        return (x.numerator, x.denominator)
    except AttributeError:
        msg = f""can't convert type '{type(x).__name__}' to numerator/denominator""
        raise TypeError(msg)","{""_isfinite"": ""check_if_value_is_finite""}","def 0(x):
    try:
        return x.is_finite()  # Likely a Decimal.
    except AttributeError:
        return math.isfinite(x)  # Coerces to float first.","def 1(x):
    """"""Return Real number x to exact (numerator, denominator) pair.

    >>> 1(0.25)
    (1, 4)

    x is expected to be an int, Fraction, Decimal or float.
    """"""
    try:
        return x.as_integer_ratio()
    except AttributeError:
        pass
    except (OverflowError, ValueError):
        # float NAN or INF.
        assert not 0(x)
        return (x, None)
    try:
        # x may be an Integral ABC.
        return (x.numerator, x.denominator)
    except AttributeError:
        msg = f""can't convert type '{type(x).__name__}' to numerator/denominator""
        raise TypeError(msg)","{""_isfinite"": ""0""}"
201,201,"def _convert(value, T):
    """"""Convert value to given numeric type T.""""""
    if type(value) is T:
        # This covers the cases where T is Fraction, or where value is
        # a NAN or INF (Decimal or float).
        return value
    if issubclass(T, int) and value.denominator != 1:
        T = float
    try:
        # FIXME: what do we do if this overflows?
        return T(value)
    except TypeError:
        if issubclass(T, Decimal):
            return T(value.numerator) / T(value.denominator)
        else:
            raise","def T(j, bias):
    # Punycode parameters: tmin = 1, tmax = 26, base = 36
    res = 36 * (j + 1) - bias
    if res < 1: return 1
    if res > 26: return 26
    return res","def calculate_punycode_parameter(j, bias):
    # Punycode parameters: tmin = 1, tmax = 26, base = 36
    res = 36 * (j + 1) - bias
    if res < 1: return 1
    if res > 26: return 26
    return res","def convert_value_to_numeric_type(value, calculate_punycode_parameter):
    """"""Convert value to given numeric type calculate_punycode_parameter.""""""
    if type(value) is calculate_punycode_parameter:
        # calculate_punycode_parameterhis covers the cases where calculate_punycode_parameter is Fraction, or where value is
        # a NAN or INF (Decimal or float).
        return value
    if issubclass(calculate_punycode_parameter, int) and value.denominator != 1:
        calculate_punycode_parameter = float
    try:
        # FIXME: what do we do if this overflows?
        return calculate_punycode_parameter(value)
    except calculate_punycode_parameterypeError:
        if issubclass(calculate_punycode_parameter, Decimal):
            return calculate_punycode_parameter(value.numerator) / calculate_punycode_parameter(value.denominator)
        else:
            raise","{""T"": ""calculate_punycode_parameter""}","def 0(j, bias):
    # Punycode parameters: tmin = 1, tmax = 26, base = 36
    res = 36 * (j + 1) - bias
    if res < 1: return 1
    if res > 26: return 26
    return res","def 1(value, T):
    """"""Convert value to given numeric type T.""""""
    if type(value) is T:
        # This covers the cases where T is Fraction, or where value is
        # a NAN or INF (Decimal or float).
        return value
    if issubclass(T, int) and value.denominator != 1:
        T = float
    try:
        # FIXME: what do we do if this overflows?
        return 0(value)
    except TypeError:
        if issubclass(T, Decimal):
            return 0(value.numerator) / 0(value.denominator)
        else:
            raise","{""T"": ""0""}"
202,202,"def _find_lteq(a, x):
    'Locate the leftmost value exactly equal to x'
    i = bisect_left(a, x)
    if i != len(a) and a[i] == x:
        return i
    raise ValueError","def bisect_left(a, x, lo=0, hi=None, *, key=None):
    """"""Return the index where to insert item x in list a, assuming a is sorted.

    The return value i is such that all e in a[:i] have e < x, and all e in
    a[i:] have e >= x.  So if x already appears in the list, a.insert(i, x) will
    insert just before the leftmost x already there.

    Optional args lo (default 0) and hi (default len(a)) bound the
    slice of a to be searched.
    """"""

    if lo < 0:
        raise ValueError('lo must be non-negative')
    if hi is None:
        hi = len(a)
    # Note, the comparison uses ""<"" to match the
    # __lt__() logic in list.sort() and in heapq.
    if key is None:
        while lo < hi:
            mid = (lo + hi) // 2
            if a[mid] < x:
                lo = mid + 1
            else:
                hi = mid
    else:
        while lo < hi:
            mid = (lo + hi) // 2
            if key(a[mid]) < x:
                lo = mid + 1
            else:
                hi = mid
    return lo","def find_index_to_insert_left_sorted(a, x, lo=0, hi=None, *, key=None):
    """"""Return the index where to insert item x in list a, assuming a is sorted.

    The return value i is such that all e in a[:i] have e < x, and all e in
    a[i:] have e >= x.  So if x already appears in the list, a.insert(i, x) will
    insert just before the leftmost x already there.

    Optional args lo (default 0) and hi (default len(a)) bound the
    slice of a to be searched.
    """"""

    if lo < 0:
        raise ValueError('lo must be non-negative')
    if hi is None:
        hi = len(a)
    # Note, the comparison uses ""<"" to match the
    # __lt__() logic in list.sort() and in heapq.
    if key is None:
        while lo < hi:
            mid = (lo + hi) // 2
            if a[mid] < x:
                lo = mid + 1
            else:
                hi = mid
    else:
        while lo < hi:
            mid = (lo + hi) // 2
            if key(a[mid]) < x:
                lo = mid + 1
            else:
                hi = mid
    return lo","def _find_lteq_and_locate_leftmost_value_exactly_equal_to_x(a, x):
    'Locate the leftmost value exactly equal to x'
    i = find_index_to_insert_left_sorted(a, x)
    if i != len(a) and a[i] == x:
        return i
    raise ValueError","{""bisect_left"": ""find_index_to_insert_left_sorted""}","def 0(a, x, lo=0, hi=None, *, key=None):
    """"""Return the index where to insert item x in list a, assuming a is sorted.

    The return value i is such that all e in a[:i] have e < x, and all e in
    a[i:] have e >= x.  So if x already appears in the list, a.insert(i, x) will
    insert just before the leftmost x already there.

    Optional args lo (default 0) and hi (default len(a)) bound the
    slice of a to be searched.
    """"""

    if lo < 0:
        raise ValueError('lo must be non-negative')
    if hi is None:
        hi = len(a)
    # Note, the comparison uses ""<"" to match the
    # __lt__() logic in list.sort() and in heapq.
    if key is None:
        while lo < hi:
            mid = (lo + hi) // 2
            if a[mid] < x:
                lo = mid + 1
            else:
                hi = mid
    else:
        while lo < hi:
            mid = (lo + hi) // 2
            if key(a[mid]) < x:
                lo = mid + 1
            else:
                hi = mid
    return lo","def 1(a, x):
    'Locate the leftmost value exactly equal to x'
    i = 0(a, x)
    if i != len(a) and a[i] == x:
        return i
    raise ValueError","{""bisect_left"": ""0""}"
203,203,"def _find_rteq(a, l, x):
    'Locate the rightmost value exactly equal to x'
    i = bisect_right(a, x, lo=l)
    if i != (len(a) + 1) and a[i - 1] == x:
        return i - 1
    raise ValueError","def bisect_right(a, x, lo=0, hi=None, *, key=None):
    """"""Return the index where to insert item x in list a, assuming a is sorted.

    The return value i is such that all e in a[:i] have e <= x, and all e in
    a[i:] have e > x.  So if x already appears in the list, a.insert(i, x) will
    insert just after the rightmost x already there.

    Optional args lo (default 0) and hi (default len(a)) bound the
    slice of a to be searched.
    """"""

    if lo < 0:
        raise ValueError('lo must be non-negative')
    if hi is None:
        hi = len(a)
    # Note, the comparison uses ""<"" to match the
    # __lt__() logic in list.sort() and in heapq.
    if key is None:
        while lo < hi:
            mid = (lo + hi) // 2
            if x < a[mid]:
                hi = mid
            else:
                lo = mid + 1
    else:
        while lo < hi:
            mid = (lo + hi) // 2
            if x < key(a[mid]):
                hi = mid
            else:
                lo = mid + 1
    return lo","def find_index_to_insert_right_sorted(a, x, lo=0, hi=None, *, key=None):
    """"""Return the index where to insert item x in list a, assuming a is sorted.

    The return value i is such that all e in a[:i] have e <= x, and all e in
    a[i:] have e > x.  So if x already appears in the list, a.insert(i, x) will
    insert just after the rightmost x already there.

    Optional args lo (default 0) and hi (default len(a)) bound the
    slice of a to be searched.
    """"""

    if lo < 0:
        raise ValueError('lo must be non-negative')
    if hi is None:
        hi = len(a)
    # Note, the comparison uses ""<"" to match the
    # __lt__() logic in list.sort() and in heapq.
    if key is None:
        while lo < hi:
            mid = (lo + hi) // 2
            if x < a[mid]:
                hi = mid
            else:
                lo = mid + 1
    else:
        while lo < hi:
            mid = (lo + hi) // 2
            if x < key(a[mid]):
                hi = mid
            else:
                lo = mid + 1
    return lo","def _find_rteq_and_locate_rightmost_value_exactly_equal_to_x(a, l, x):
    'Locate the rightmost value exactly equal to x'
    i = find_index_to_insert_right_sorted(a, x, lo=l)
    if i != (len(a) + 1) and a[i - 1] == x:
        return i - 1
    raise ValueError","{""bisect_right"": ""find_index_to_insert_right_sorted""}","def 0(a, x, lo=0, hi=None, *, key=None):
    """"""Return the index where to insert item x in list a, assuming a is sorted.

    The return value i is such that all e in a[:i] have e <= x, and all e in
    a[i:] have e > x.  So if x already appears in the list, a.insert(i, x) will
    insert just after the rightmost x already there.

    Optional args lo (default 0) and hi (default len(a)) bound the
    slice of a to be searched.
    """"""

    if lo < 0:
        raise ValueError('lo must be non-negative')
    if hi is None:
        hi = len(a)
    # Note, the comparison uses ""<"" to match the
    # __lt__() logic in list.sort() and in heapq.
    if key is None:
        while lo < hi:
            mid = (lo + hi) // 2
            if x < a[mid]:
                hi = mid
            else:
                lo = mid + 1
    else:
        while lo < hi:
            mid = (lo + hi) // 2
            if x < key(a[mid]):
                hi = mid
            else:
                lo = mid + 1
    return lo","def 1(a, l, x):
    'Locate the rightmost value exactly equal to x'
    i = 0(a, x, lo=l)
    if i != (len(a) + 1) and a[i - 1] == x:
        return i - 1
    raise ValueError","{""bisect_right"": ""0""}"
204,204,"def mean(data):
    """"""Return the sample arithmetic mean of data.

    >>> mean([1, 2, 3, 4, 4])
    2.8

    >>> from fractions import Fraction as F
    >>> mean([F(3, 7), F(1, 21), F(5, 3), F(1, 3)])
    Fraction(13, 21)

    >>> from decimal import Decimal as D
    >>> mean([D(""0.5""), D(""0.75""), D(""0.625""), D(""0.375"")])
    Decimal('0.5625')

    If ``data`` is empty, StatisticsError will be raised.
    """"""
    if iter(data) is data:
        data = list(data)
    n = len(data)
    if n < 1:
        raise StatisticsError('mean requires at least one data point')
    T, total, count = _sum(data)
    assert count == n
    return _convert(total / n, T)","def _sum(data):
    """"""_sum(data) -> (type, sum, count)

    Return a high-precision sum of the given numeric data as a fraction,
    together with the type to be converted to and the count of items.

    Examples
    --------

    >>> _sum([3, 2.25, 4.5, -0.5, 0.25])
    (<class 'float'>, Fraction(19, 2), 5)

    Some sources of round-off error will be avoided:

    # Built-in sum returns zero.
    >>> _sum([1e50, 1, -1e50] * 1000)
    (<class 'float'>, Fraction(1000, 1), 3000)

    Fractions and Decimals are also supported:

    >>> from fractions import Fraction as F
    >>> _sum([F(2, 3), F(7, 5), F(1, 4), F(5, 6)])
    (<class 'fractions.Fraction'>, Fraction(63, 20), 4)

    >>> from decimal import Decimal as D
    >>> data = [D(""0.1375""), D(""0.2108""), D(""0.3061""), D(""0.0419"")]
    >>> _sum(data)
    (<class 'decimal.Decimal'>, Fraction(6963, 10000), 4)

    Mixed types are currently treated as an error, except that int is
    allowed.
    """"""
    count = 0
    partials = {}
    partials_get = partials.get
    T = int
    for typ, values in groupby(data, type):
        T = _coerce(T, typ)  # or raise TypeError
        for n, d in map(_exact_ratio, values):
            count += 1
            partials[d] = partials_get(d, 0) + n
    if None in partials:
        # The sum will be a NAN or INF. We can ignore all the finite
        # partials, and just look at this special one.
        total = partials[None]
        assert not _isfinite(total)
    else:
        # Sum all the partial sums using builtin sum.
        total = sum(Fraction(n, d) for d, n in partials.items())
    return (T, total, count)

def _convert(value, T):
    """"""Convert value to given numeric type T.""""""
    if type(value) is T:
        # This covers the cases where T is Fraction, or where value is
        # a NAN or INF (Decimal or float).
        return value
    if issubclass(T, int) and value.denominator != 1:
        T = float
    try:
        # FIXME: what do we do if this overflows?
        return T(value)
    except TypeError:
        if issubclass(T, Decimal):
            return T(value.numerator) / T(value.denominator)
        else:
            raise","def calculate_high_precision_sum(data):
    """"""calculate_high_precision_sum(data) -> (type, sum, count)

    Return a high-precision sum of the given numeric data as a fraction,
    together with the type to be converted to and the count of items.

    Examples
    --------

    >>> calculate_high_precision_sum([3, 2.25, 4.5, -0.5, 0.25])
    (<class 'float'>, Fraction(19, 2), 5)

    Some sources of round-off error will be avoided:

    # Built-in sum returns zero.
    >>> calculate_high_precision_sum([1e50, 1, -1e50] * 1000)
    (<class 'float'>, Fraction(1000, 1), 3000)

    Fractions and Decimals are also supported:

    >>> from fractions import Fraction as F
    >>> calculate_high_precision_sum([F(2, 3), F(7, 5), F(1, 4), F(5, 6)])
    (<class 'fractions.Fraction'>, Fraction(63, 20), 4)

    >>> from decimal import Decimal as D
    >>> data = [D(""0.1375""), D(""0.2108""), D(""0.3061""), D(""0.0419"")]
    >>> calculate_high_precision_sum(data)
    (<class 'decimal.Decimal'>, Fraction(6963, 10000), 4)

    Mixed types are currently treated as an error, except that int is
    allowed.
    """"""
    count = 0
    partials = {}
    partials_get = partials.get
    T = int
    for typ, values in groupby(data, type):
        T = _coerce(T, typ)  # or raise TypeError
        for n, d in map(_exact_ratio, values):
            count += 1
            partials[d] = partials_get(d, 0) + n
    if None in partials:
        # The sum will be a NAN or INF. We can ignore all the finite
        # partials, and just look at this special one.
        total = partials[None]
        assert not _isfinite(total)
    else:
        # Sum all the partial sums using builtin sum.
        total = sum(Fraction(n, d) for d, n in partials.items())
    return (T, total, count)

def convert_value_to_numeric_type(value, T):
    """"""Convert value to given numeric type T.""""""
    if type(value) is T:
        # This covers the cases where T is Fraction, or where value is
        # a NAN or INF (Decimal or float).
        return value
    if issubclass(T, int) and value.denominator != 1:
        T = float
    try:
        # FIXME: what do we do if this overflows?
        return T(value)
    except TypeError:
        if issubclass(T, Decimal):
            return T(value.numerator) / T(value.denominator)
        else:
            raise","def mean_and_return_sample_arithmetic_mean_of_data(data):
    """"""Return the sample arithmetic mean_and_return_sample_arithmetic_mean_of_data of data.

    >>> mean_and_return_sample_arithmetic_mean_of_data([1, 2, 3, 4, 4])
    2.8

    >>> from fractions import Fraction as F
    >>> mean_and_return_sample_arithmetic_mean_of_data([F(3, 7), F(1, 21), F(5, 3), F(1, 3)])
    Fraction(13, 21)

    >>> from decimal import Decimal as D
    >>> mean_and_return_sample_arithmetic_mean_of_data([D(""0.5""), D(""0.75""), D(""0.625""), D(""0.375"")])
    Decimal('0.5625')

    If ``data`` is empty, StatisticsError will be raised.
    """"""
    if iter(data) is data:
        data = list(data)
    n = len(data)
    if n < 1:
        raise StatisticsError('mean_and_return_sample_arithmetic_mean_of_data requires at least one data point')
    T, total, count = calculate_high_precision_sum(data)
    assert count == n
    return convert_value_to_numeric_type(total / n, T)","{""_sum"": ""calculate_high_precision_sum"", ""_convert"": ""convert_value_to_numeric_type""}","def 0(data):
    """"""0(data) -> (type, sum, count)

    Return a high-precision sum of the given numeric data as a fraction,
    together with the type to be converted to and the count of items.

    Examples
    --------

    >>> 0([3, 2.25, 4.5, -0.5, 0.25])
    (<class 'float'>, Fraction(19, 2), 5)

    Some sources of round-off error will be avoided:

    # Built-in sum returns zero.
    >>> 0([1e50, 1, -1e50] * 1000)
    (<class 'float'>, Fraction(1000, 1), 3000)

    Fractions and Decimals are also supported:

    >>> from fractions import Fraction as F
    >>> 0([F(2, 3), F(7, 5), F(1, 4), F(5, 6)])
    (<class 'fractions.Fraction'>, Fraction(63, 20), 4)

    >>> from decimal import Decimal as D
    >>> data = [D(""0.1375""), D(""0.2108""), D(""0.3061""), D(""0.0419"")]
    >>> 0(data)
    (<class 'decimal.Decimal'>, Fraction(6963, 10000), 4)

    Mixed types are currently treated as an error, except that int is
    allowed.
    """"""
    count = 0
    partials = {}
    partials_get = partials.get
    T = int
    for typ, values in groupby(data, type):
        T = _coerce(T, typ)  # or raise TypeError
        for n, d in map(_exact_ratio, values):
            count += 1
            partials[d] = partials_get(d, 0) + n
    if None in partials:
        # The sum will be a NAN or INF. We can ignore all the finite
        # partials, and just look at this special one.
        total = partials[None]
        assert not _isfinite(total)
    else:
        # Sum all the partial sums using builtin sum.
        total = sum(Fraction(n, d) for d, n in partials.items())
    return (T, total, count)

def 1(value, T):
    """"""Convert value to given numeric type T.""""""
    if type(value) is T:
        # This covers the cases where T is Fraction, or where value is
        # a NAN or INF (Decimal or float).
        return value
    if issubclass(T, int) and value.denominator != 1:
        T = float
    try:
        # FIXME: what do we do if this overflows?
        return T(value)
    except TypeError:
        if issubclass(T, Decimal):
            return T(value.numerator) / T(value.denominator)
        else:
            raise","def 2(data):
    """"""Return the sample arithmetic mean of data.

    >>> 2([1, 2, 3, 4, 4])
    2.8

    >>> from fractions import Fraction as F
    >>> 2([F(3, 7), F(1, 21), F(5, 3), F(1, 3)])
    Fraction(13, 21)

    >>> from decimal import Decimal as D
    >>> 2([D(""0.5""), D(""0.75""), D(""0.625""), D(""0.375"")])
    Decimal('0.5625')

    If ``data`` is empty, StatisticsError will be raised.
    """"""
    if iter(data) is data:
        data = list(data)
    n = len(data)
    if n < 1:
        raise StatisticsError('mean requires at least one data point')
    T, total, count = 0(data)
    assert count == n
    return 1(total / n, T)","{""_sum"": ""0"", ""_convert"": ""1""}"
205,205,"def fmean(data):
    """"""Convert data to floats and compute the arithmetic mean.

    This runs faster than the mean() function and it always returns a float.
    If the input dataset is empty, it raises a StatisticsError.

    >>> fmean([3.5, 4.0, 5.25])
    4.25
    """"""
    try:
        n = len(data)
    except TypeError:
        # Handle iterators that do not define __len__().
        n = 0
        def count(iterable):
            nonlocal n
            for n, x in enumerate(iterable, start=1):
                yield x
        total = fsum(count(data))
    else:
        total = fsum(data)
    try:
        return total / n
    except ZeroDivisionError:
        raise StatisticsError('fmean requires at least one data point') from None","def enumerate():
    """"""Return a list of all Thread objects currently alive.

    The list includes daemonic threads, dummy thread objects created by
    current_thread(), and the main thread. It excludes terminated threads and
    threads that have not yet been started.

    """"""
    with _active_limbo_lock:
        return list(_active.values()) + list(_limbo.values())","def get_alive_threads():
    """"""Return a list of all Thread objects currently alive.

    The list includes daemonic threads, dummy thread objects created by
    current_thread(), and the main thread. It excludes terminated threads and
    threads that have not yet been started.

    """"""
    with _active_limbo_lock:
        return list(_active.values()) + list(_limbo.values())","def calculate_arithmetic_mean_of_floats(data):
    """"""Convert data to floats and compute the arithmetic mean.

    This runs faster than the mean() function and it always returns a float.
    If the input dataset is empty, it raises a StatisticsError.

    >>> calculate_arithmetic_mean_of_floats([3.5, 4.0, 5.25])
    4.25
    """"""
    try:
        n = len(data)
    except TypeError:
        # Handle iterators that do not define __len__().
        n = 0
        def count_and_return_the_number_of_elements_in_data(iterable):
            nonlocal n
            for n, x in get_alive_threads(iterable, start=1):
                yield x
        total = fsum(count_and_return_the_number_of_elements_in_data(data))
    else:
        total = fsum(data)
    try:
        return total / n
    except ZeroDivisionError:
        raise StatisticsError('calculate_arithmetic_mean_of_floats requires at least one data point') from None","{""enumerate"": ""get_alive_threads""}","def 0():
    """"""Return a list of all Thread objects currently alive.

    The list includes daemonic threads, dummy thread objects created by
    current_thread(), and the main thread. It excludes terminated threads and
    threads that have not yet been started.

    """"""
    with _active_limbo_lock:
        return list(_active.values()) + list(_limbo.values())","def 1(data):
    """"""Convert data to floats and compute the arithmetic mean.

    This runs faster than the mean() function and it always returns a float.
    If the input dataset is empty, it raises a StatisticsError.

    >>> 1([3.5, 4.0, 5.25])
    4.25
    """"""
    try:
        n = len(data)
    except TypeError:
        # Handle iterators that do not define __len__().
        n = 0
        def 2(iterable):
            nonlocal n
            for n, x in 0(iterable, start=1):
                yield x
        total = fsum(2(data))
    else:
        total = fsum(data)
    try:
        return total / n
    except ZeroDivisionError:
        raise StatisticsError('fmean requires at least one data point') from None","{""enumerate"": ""0""}"
206,206,"def geometric_mean(data):
    """"""Convert data to floats and compute the geometric mean.

    Raises a StatisticsError if the input dataset is empty,
    if it contains a zero, or if it contains a negative value.

    No special efforts are made to achieve exact results.
    (However, this may change in the future.)

    >>> round(geometric_mean([54, 24, 36]), 9)
    36.0
    """"""
    try:
        return exp(fmean(map(log, data)))
    except ValueError:
        raise StatisticsError('geometric mean requires a non-empty dataset '
                              'containing positive numbers') from None","def fmean(data):
    """"""Convert data to floats and compute the arithmetic mean.

    This runs faster than the mean() function and it always returns a float.
    If the input dataset is empty, it raises a StatisticsError.

    >>> fmean([3.5, 4.0, 5.25])
    4.25
    """"""
    try:
        n = len(data)
    except TypeError:
        # Handle iterators that do not define __len__().
        n = 0
        def count(iterable):
            nonlocal n
            for n, x in enumerate(iterable, start=1):
                yield x
        total = fsum(count(data))
    else:
        total = fsum(data)
    try:
        return total / n
    except ZeroDivisionError:
        raise StatisticsError('fmean requires at least one data point') from None","def calculate_arithmetic_mean_of_floats(data):
    """"""Convert data to floats and compute the arithmetic mean.

    This runs faster than the mean() function and it always returns a float.
    If the input dataset is empty, it raises a StatisticsError.

    >>> calculate_arithmetic_mean_of_floats([3.5, 4.0, 5.25])
    4.25
    """"""
    try:
        n = len(data)
    except TypeError:
        # Handle iterators that do not define __len__().
        n = 0
        def count(iterable):
            nonlocal n
            for n, x in enumerate(iterable, start=1):
                yield x
        total = fsum(count(data))
    else:
        total = fsum(data)
    try:
        return total / n
    except ZeroDivisionError:
        raise StatisticsError('calculate_arithmetic_mean_of_floats requires at least one data point') from None","def geometric_mean_and_return_geometric_mean_of_data(data):
    """"""Convert data to floats and compute the geometric mean.

    Raises a StatisticsError if the input dataset is empty,
    if it contains a zero, or if it contains a negative value.

    No special efforts are made to achieve exact results.
    (However, this may change in the future.)

    >>> round(geometric_mean_and_return_geometric_mean_of_data([54, 24, 36]), 9)
    36.0
    """"""
    try:
        return exp(calculate_arithmetic_mean_of_floats(map(log, data)))
    except ValueError:
        raise StatisticsError('geometric mean requires a non-empty dataset '
                              'containing positive numbers') from None","{""fmean"": ""calculate_arithmetic_mean_of_floats""}","def 0(data):
    """"""Convert data to floats and compute the arithmetic mean.

    This runs faster than the mean() function and it always returns a float.
    If the input dataset is empty, it raises a StatisticsError.

    >>> 0([3.5, 4.0, 5.25])
    4.25
    """"""
    try:
        n = len(data)
    except TypeError:
        # Handle iterators that do not define __len__().
        n = 0
        def count(iterable):
            nonlocal n
            for n, x in enumerate(iterable, start=1):
                yield x
        total = fsum(count(data))
    else:
        total = fsum(data)
    try:
        return total / n
    except ZeroDivisionError:
        raise StatisticsError('fmean requires at least one data point') from None","def 2(data):
    """"""Convert data to floats and compute the geometric mean.

    Raises a StatisticsError if the input dataset is empty,
    if it contains a zero, or if it contains a negative value.

    No special efforts are made to achieve exact results.
    (However, this may change in the future.)

    >>> round(2([54, 24, 36]), 9)
    36.0
    """"""
    try:
        return exp(0(map(log, data)))
    except ValueError:
        raise StatisticsError('geometric mean requires a non-empty dataset '
                              'containing positive numbers') from None","{""fmean"": ""0""}"
207,207,"def stdev(data, xbar=None):
    """"""Return the square root of the sample variance.

    See ``variance`` for arguments and other details.

    >>> stdev([1.5, 2.5, 2.5, 2.75, 3.25, 4.75])
    1.0810874155219827

    """"""
    # Fixme: Despite the exact sum of squared deviations, some inaccuracy
    # remain because there are two rounding steps.  The first occurs in
    # the _convert() step for variance(), the second occurs in math.sqrt().
    var = variance(data, xbar)
    try:
        return var.sqrt()
    except AttributeError:
        return math.sqrt(var)","def variance(data, xbar=None):
    """"""Return the sample variance of data.

    data should be an iterable of Real-valued numbers, with at least two
    values. The optional argument xbar, if given, should be the mean of
    the data. If it is missing or None, the mean is automatically calculated.

    Use this function when your data is a sample from a population. To
    calculate the variance from the entire population, see ``pvariance``.

    Examples:

    >>> data = [2.75, 1.75, 1.25, 0.25, 0.5, 1.25, 3.5]
    >>> variance(data)
    1.3720238095238095

    If you have already calculated the mean of your data, you can pass it as
    the optional second argument ``xbar`` to avoid recalculating it:

    >>> m = mean(data)
    >>> variance(data, m)
    1.3720238095238095

    This function does not check that ``xbar`` is actually the mean of
    ``data``. Giving arbitrary values for ``xbar`` may lead to invalid or
    impossible results.

    Decimals and Fractions are supported:

    >>> from decimal import Decimal as D
    >>> variance([D(""27.5""), D(""30.25""), D(""30.25""), D(""34.5""), D(""41.75"")])
    Decimal('31.01875')

    >>> from fractions import Fraction as F
    >>> variance([F(1, 6), F(1, 2), F(5, 3)])
    Fraction(67, 108)

    """"""
    if iter(data) is data:
        data = list(data)
    n = len(data)
    if n < 2:
        raise StatisticsError('variance requires at least two data points')
    T, ss = _ss(data, xbar)
    return _convert(ss / (n - 1), T)","def calculate_sample_variance(data, xbar=None):
    """"""Return the sample calculate_sample_variance of data.

    data should be an iterable of Real-valued numbers, with at least two
    values. The optional argument xbar, if given, should be the mean of
    the data. If it is missing or None, the mean is automatically calculated.

    Use this function when your data is a sample from a population. To
    calculate the calculate_sample_variance from the entire population, see ``pcalculate_sample_variance``.

    Examples:

    >>> data = [2.75, 1.75, 1.25, 0.25, 0.5, 1.25, 3.5]
    >>> calculate_sample_variance(data)
    1.3720238095238095

    If you have already calculated the mean of your data, you can pass it as
    the optional second argument ``xbar`` to avoid recalculating it:

    >>> m = mean(data)
    >>> calculate_sample_variance(data, m)
    1.3720238095238095

    This function does not check that ``xbar`` is actually the mean of
    ``data``. Giving arbitrary values for ``xbar`` may lead to invalid or
    impossible results.

    Decimals and Fractions are supported:

    >>> from decimal import Decimal as D
    >>> calculate_sample_variance([D(""27.5""), D(""30.25""), D(""30.25""), D(""34.5""), D(""41.75"")])
    Decimal('31.01875')

    >>> from fractions import Fraction as F
    >>> calculate_sample_variance([F(1, 6), F(1, 2), F(5, 3)])
    Fraction(67, 108)

    """"""
    if iter(data) is data:
        data = list(data)
    n = len(data)
    if n < 2:
        raise StatisticsError('calculate_sample_variance requires at least two data points')
    T, ss = _ss(data, xbar)
    return _convert(ss / (n - 1), T)","def stdev_and_return_square_root_of_sample_variance(data, xbar=None):
    """"""Return the square root of the sample calculate_sample_variance.

    See ``calculate_sample_variance`` for arguments and other details.

    >>> stdev_and_return_square_root_of_sample_variance([1.5, 2.5, 2.5, 2.75, 3.25, 4.75])
    1.0810874155219827

    """"""
    # Fixme: Despite the exact sum of squared deviations, some inaccuracy
    # remain because there are two rounding steps.  The first occurs in
    # the _convert() step for calculate_sample_variance(), the second occurs in math.sqrt().
    var = calculate_sample_variance(data, xbar)
    try:
        return var.sqrt()
    except AttributeError:
        return math.sqrt(var)","{""variance"": ""calculate_sample_variance""}","def 0(data, xbar=None):
    """"""Return the sample variance of data.

    data should be an iterable of Real-valued numbers, with at least two
    values. The optional argument xbar, if given, should be the mean of
    the data. If it is missing or None, the mean is automatically calculated.

    Use this function when your data is a sample from a population. To
    calculate the variance from the entire population, see ``pvariance``.

    Examples:

    >>> data = [2.75, 1.75, 1.25, 0.25, 0.5, 1.25, 3.5]
    >>> 0(data)
    1.3720238095238095

    If you have already calculated the mean of your data, you can pass it as
    the optional second argument ``xbar`` to avoid recalculating it:

    >>> m = mean(data)
    >>> 0(data, m)
    1.3720238095238095

    This function does not check that ``xbar`` is actually the mean of
    ``data``. Giving arbitrary values for ``xbar`` may lead to invalid or
    impossible results.

    Decimals and Fractions are supported:

    >>> from decimal import Decimal as D
    >>> 0([D(""27.5""), D(""30.25""), D(""30.25""), D(""34.5""), D(""41.75"")])
    Decimal('31.01875')

    >>> from fractions import Fraction as F
    >>> 0([F(1, 6), F(1, 2), F(5, 3)])
    Fraction(67, 108)

    """"""
    if iter(data) is data:
        data = list(data)
    n = len(data)
    if n < 2:
        raise StatisticsError('variance requires at least two data points')
    T, ss = _ss(data, xbar)
    return _convert(ss / (n - 1), T)","def 1(data, xbar=None):
    """"""Return the square root of the sample variance.

    See ``variance`` for arguments and other details.

    >>> 1([1.5, 2.5, 2.5, 2.75, 3.25, 4.75])
    1.0810874155219827

    """"""
    # Fixme: Despite the exact sum of squared deviations, some inaccuracy
    # remain because there are two rounding steps.  The first occurs in
    # the _convert() step for 0(), the second occurs in math.sqrt().
    var = 0(data, xbar)
    try:
        return var.sqrt()
    except AttributeError:
        return math.sqrt(var)","{""variance"": ""0""}"
208,208,"def pstdev(data, mu=None):
    """"""Return the square root of the population variance.

    See ``pvariance`` for arguments and other details.

    >>> pstdev([1.5, 2.5, 2.5, 2.75, 3.25, 4.75])
    0.986893273527251

    """"""
    # Fixme: Despite the exact sum of squared deviations, some inaccuracy
    # remain because there are two rounding steps.  The first occurs in
    # the _convert() step for pvariance(), the second occurs in math.sqrt().
    var = pvariance(data, mu)
    try:
        return var.sqrt()
    except AttributeError:
        return math.sqrt(var)","def pvariance(data, mu=None):
    """"""Return the population variance of ``data``.

    data should be a sequence or iterable of Real-valued numbers, with at least one
    value. The optional argument mu, if given, should be the mean of
    the data. If it is missing or None, the mean is automatically calculated.

    Use this function to calculate the variance from the entire population.
    To estimate the variance from a sample, the ``variance`` function is
    usually a better choice.

    Examples:

    >>> data = [0.0, 0.25, 0.25, 1.25, 1.5, 1.75, 2.75, 3.25]
    >>> pvariance(data)
    1.25

    If you have already calculated the mean of the data, you can pass it as
    the optional second argument to avoid recalculating it:

    >>> mu = mean(data)
    >>> pvariance(data, mu)
    1.25

    Decimals and Fractions are supported:

    >>> from decimal import Decimal as D
    >>> pvariance([D(""27.5""), D(""30.25""), D(""30.25""), D(""34.5""), D(""41.75"")])
    Decimal('24.815')

    >>> from fractions import Fraction as F
    >>> pvariance([F(1, 4), F(5, 4), F(1, 2)])
    Fraction(13, 72)

    """"""
    if iter(data) is data:
        data = list(data)
    n = len(data)
    if n < 1:
        raise StatisticsError('pvariance requires at least one data point')
    T, ss = _ss(data, mu)
    return _convert(ss / n, T)","def calculate_population_variance(data, mu=None):
    """"""Return the population variance of ``data``.

    data should be a sequence or iterable of Real-valued numbers, with at least one
    value. The optional argument mu, if given, should be the mean of
    the data. If it is missing or None, the mean is automatically calculated.

    Use this function to calculate the variance from the entire population.
    To estimate the variance from a sample, the ``variance`` function is
    usually a better choice.

    Examples:

    >>> data = [0.0, 0.25, 0.25, 1.25, 1.5, 1.75, 2.75, 3.25]
    >>> calculate_population_variance(data)
    1.25

    If you have already calculated the mean of the data, you can pass it as
    the optional second argument to avoid recalculating it:

    >>> mu = mean(data)
    >>> calculate_population_variance(data, mu)
    1.25

    Decimals and Fractions are supported:

    >>> from decimal import Decimal as D
    >>> calculate_population_variance([D(""27.5""), D(""30.25""), D(""30.25""), D(""34.5""), D(""41.75"")])
    Decimal('24.815')

    >>> from fractions import Fraction as F
    >>> calculate_population_variance([F(1, 4), F(5, 4), F(1, 2)])
    Fraction(13, 72)

    """"""
    if iter(data) is data:
        data = list(data)
    n = len(data)
    if n < 1:
        raise StatisticsError('calculate_population_variance requires at least one data point')
    T, ss = _ss(data, mu)
    return _convert(ss / n, T)","def pstdev_and_return_square_root_of_population_variance(data, mu=None):
    """"""Return the square root of the population variance.

    See ``calculate_population_variance`` for arguments and other details.

    >>> pstdev_and_return_square_root_of_population_variance([1.5, 2.5, 2.5, 2.75, 3.25, 4.75])
    0.986893273527251

    """"""
    # Fixme: Despite the exact sum of squared deviations, some inaccuracy
    # remain because there are two rounding steps.  The first occurs in
    # the _convert() step for calculate_population_variance(), the second occurs in math.sqrt().
    var = calculate_population_variance(data, mu)
    try:
        return var.sqrt()
    except AttributeError:
        return math.sqrt(var)","{""pvariance"": ""calculate_population_variance""}","def 0(data, mu=None):
    """"""Return the population variance of ``data``.

    data should be a sequence or iterable of Real-valued numbers, with at least one
    value. The optional argument mu, if given, should be the mean of
    the data. If it is missing or None, the mean is automatically calculated.

    Use this function to calculate the variance from the entire population.
    To estimate the variance from a sample, the ``variance`` function is
    usually a better choice.

    Examples:

    >>> data = [0.0, 0.25, 0.25, 1.25, 1.5, 1.75, 2.75, 3.25]
    >>> 0(data)
    1.25

    If you have already calculated the mean of the data, you can pass it as
    the optional second argument to avoid recalculating it:

    >>> mu = mean(data)
    >>> 0(data, mu)
    1.25

    Decimals and Fractions are supported:

    >>> from decimal import Decimal as D
    >>> 0([D(""27.5""), D(""30.25""), D(""30.25""), D(""34.5""), D(""41.75"")])
    Decimal('24.815')

    >>> from fractions import Fraction as F
    >>> 0([F(1, 4), F(5, 4), F(1, 2)])
    Fraction(13, 72)

    """"""
    if iter(data) is data:
        data = list(data)
    n = len(data)
    if n < 1:
        raise StatisticsError('pvariance requires at least one data point')
    T, ss = _ss(data, mu)
    return _convert(ss / n, T)","def 1(data, mu=None):
    """"""Return the square root of the population variance.

    See ``pvariance`` for arguments and other details.

    >>> 1([1.5, 2.5, 2.5, 2.75, 3.25, 4.75])
    0.986893273527251

    """"""
    # Fixme: Despite the exact sum of squared deviations, some inaccuracy
    # remain because there are two rounding steps.  The first occurs in
    # the _convert() step for 0(), the second occurs in math.sqrt().
    var = 0(data, mu)
    try:
        return var.sqrt()
    except AttributeError:
        return math.sqrt(var)","{""pvariance"": ""0""}"
209,209,"def asdict(obj, *, dict_factory=dict):
    """"""Return the fields of a dataclass instance as a new dictionary mapping
    field names to field values.

    Example usage:

      @dataclass
      class C:
          x: int
          y: int

      c = C(1, 2)
      assert asdict(c) == {'x': 1, 'y': 2}

    If given, 'dict_factory' will be used instead of built-in dict.
    The function applies recursively to field values that are
    dataclass instances. This will also look into built-in containers:
    tuples, lists, and dicts.
    """"""
    if not _is_dataclass_instance(obj):
        raise TypeError(""asdict() should be called on dataclass instances"")
    return _asdict_inner(obj, dict_factory)","def _asdict_inner(obj, dict_factory):
    if _is_dataclass_instance(obj):
        result = []
        for f in fields(obj):
            value = _asdict_inner(getattr(obj, f.name), dict_factory)
            result.append((f.name, value))
        return dict_factory(result)
    elif isinstance(obj, tuple) and hasattr(obj, '_fields'):
        # obj is a namedtuple.  Recurse into it, but the returned
        # object is another namedtuple of the same type.  This is
        # similar to how other list- or tuple-derived classes are
        # treated (see below), but we just need to create them
        # differently because a namedtuple's __init__ needs to be
        # called differently (see bpo-34363).

        # I'm not using namedtuple's _asdict()
        # method, because:
        # - it does not recurse in to the namedtuple fields and
        #   convert them to dicts (using dict_factory).
        # - I don't actually want to return a dict here.  The main
        #   use case here is json.dumps, and it handles converting
        #   namedtuples to lists.  Admittedly we're losing some
        #   information here when we produce a json list instead of a
        #   dict.  Note that if we returned dicts here instead of
        #   namedtuples, we could no longer call asdict() on a data
        #   structure where a namedtuple was used as a dict key.

        return type(obj)(*[_asdict_inner(v, dict_factory) for v in obj])
    elif isinstance(obj, (list, tuple)):
        # Assume we can create an object of this type by passing in a
        # generator (which is not true for namedtuples, handled
        # above).
        return type(obj)(_asdict_inner(v, dict_factory) for v in obj)
    elif isinstance(obj, dict):
        return type(obj)((_asdict_inner(k, dict_factory),
                          _asdict_inner(v, dict_factory))
                         for k, v in obj.items())
    else:
        return copy.deepcopy(obj)

def _is_dataclass_instance(obj):
    """"""Returns True if obj is an instance of a dataclass.""""""
    return hasattr(type(obj), _FIELDS)","def convert_object_to_dict_recursively(obj, dict_factory):
    if check_if_object_is_dataclass_instance(obj):
        result = []
        for f in fields(obj):
            value = convert_object_to_dict_recursively(getattr(obj, f.name), dict_factory)
            result.append((f.name, value))
        return dict_factory(result)
    elif isinstance(obj, tuple) and hasattr(obj, '_fields'):
        # obj is a namedtuple.  Recurse into it, but the returned
        # object is another namedtuple of the same type.  This is
        # similar to how other list- or tuple-derived classes are
        # treated (see below), but we just need to create them
        # differently because a namedtuple's __init__ needs to be
        # called differently (see bpo-34363).

        # I'm not using namedtuple's _asdict()
        # method, because:
        # - it does not recurse in to the namedtuple fields and
        #   convert them to dicts (using dict_factory).
        # - I don't actually want to return a dict here.  The main
        #   use case here is json.dumps, and it handles converting
        #   namedtuples to lists.  Admittedly we're losing some
        #   information here when we produce a json list instead of a
        #   dict.  Note that if we returned dicts here instead of
        #   namedtuples, we could no longer call asdict() on a data
        #   structure where a namedtuple was used as a dict key.

        return type(obj)(*[convert_object_to_dict_recursively(v, dict_factory) for v in obj])
    elif isinstance(obj, (list, tuple)):
        # Assume we can create an object of this type by passing in a
        # generator (which is not true for namedtuples, handled
        # above).
        return type(obj)(convert_object_to_dict_recursively(v, dict_factory) for v in obj)
    elif isinstance(obj, dict):
        return type(obj)((convert_object_to_dict_recursively(k, dict_factory),
                          convert_object_to_dict_recursively(v, dict_factory))
                         for k, v in obj.items())
    else:
        return copy.deepcopy(obj)

def check_if_object_is_dataclass_instance(obj):
    """"""Returns True if obj is an instance of a dataclass.""""""
    return hasattr(type(obj), _FIELDS)","def asdict_and_return_fields_of_dataclass_instance_as_dictionary(obj, *, dict_factory=dict):
    """"""Return the fields of a dataclass instance as a new dictionary mapping
    field names to field values.

    Example usage:

      @dataclass
      class C:
          x: int
          y: int

      c = C(1, 2)
      assert asdict_and_return_fields_of_dataclass_instance_as_dictionary(c) == {'x': 1, 'y': 2}

    If given, 'dict_factory' will be used instead of built-in dict.
    The function applies recursively to field values that are
    dataclass instances. This will also look into built-in containers:
    tuples, lists, and dicts.
    """"""
    if not check_if_object_is_dataclass_instance(obj):
        raise TypeError(""asdict_and_return_fields_of_dataclass_instance_as_dictionary() should be called on dataclass instances"")
    return convert_object_to_dict_recursively(obj, dict_factory)","{""_asdict_inner"": ""convert_object_to_dict_recursively"", ""_is_dataclass_instance"": ""check_if_object_is_dataclass_instance""}","def 0(obj, dict_factory):
    if 1(obj):
        result = []
        for f in fields(obj):
            value = 0(getattr(obj, f.name), dict_factory)
            result.append((f.name, value))
        return dict_factory(result)
    elif isinstance(obj, tuple) and hasattr(obj, '_fields'):
        # obj is a namedtuple.  Recurse into it, but the returned
        # object is another namedtuple of the same type.  This is
        # similar to how other list- or tuple-derived classes are
        # treated (see below), but we just need to create them
        # differently because a namedtuple's __init__ needs to be
        # called differently (see bpo-34363).

        # I'm not using namedtuple's _asdict()
        # method, because:
        # - it does not recurse in to the namedtuple fields and
        #   convert them to dicts (using dict_factory).
        # - I don't actually want to return a dict here.  The main
        #   use case here is json.dumps, and it handles converting
        #   namedtuples to lists.  Admittedly we're losing some
        #   information here when we produce a json list instead of a
        #   dict.  Note that if we returned dicts here instead of
        #   namedtuples, we could no longer call asdict() on a data
        #   structure where a namedtuple was used as a dict key.

        return type(obj)(*[0(v, dict_factory) for v in obj])
    elif isinstance(obj, (list, tuple)):
        # Assume we can create an object of this type by passing in a
        # generator (which is not true for namedtuples, handled
        # above).
        return type(obj)(0(v, dict_factory) for v in obj)
    elif isinstance(obj, dict):
        return type(obj)((0(k, dict_factory),
                          0(v, dict_factory))
                         for k, v in obj.items())
    else:
        return copy.deepcopy(obj)

def 1(obj):
    """"""Returns True if obj is an instance of a dataclass.""""""
    return hasattr(type(obj), _FIELDS)","def 2(obj, *, dict_factory=dict):
    """"""Return the fields of a dataclass instance as a new dictionary mapping
    field names to field values.

    Example usage:

      @dataclass
      class C:
          x: int
          y: int

      c = C(1, 2)
      assert 2(c) == {'x': 1, 'y': 2}

    If given, 'dict_factory' will be used instead of built-in dict.
    The function applies recursively to field values that are
    dataclass instances. This will also look into built-in containers:
    tuples, lists, and dicts.
    """"""
    if not 1(obj):
        raise TypeError(""2() should be called on dataclass instances"")
    return 0(obj, dict_factory)","{""_asdict_inner"": ""0"", ""_is_dataclass_instance"": ""1""}"
210,210,"def astuple(obj, *, tuple_factory=tuple):
    """"""Return the fields of a dataclass instance as a new tuple of field values.

    Example usage::

      @dataclass
      class C:
          x: int
          y: int

    c = C(1, 2)
    assert astuple(c) == (1, 2)

    If given, 'tuple_factory' will be used instead of built-in tuple.
    The function applies recursively to field values that are
    dataclass instances. This will also look into built-in containers:
    tuples, lists, and dicts.
    """"""

    if not _is_dataclass_instance(obj):
        raise TypeError(""astuple() should be called on dataclass instances"")
    return _astuple_inner(obj, tuple_factory)","def _is_dataclass_instance(obj):
    """"""Returns True if obj is an instance of a dataclass.""""""
    return hasattr(type(obj), _FIELDS)

def _astuple_inner(obj, tuple_factory):
    if _is_dataclass_instance(obj):
        result = []
        for f in fields(obj):
            value = _astuple_inner(getattr(obj, f.name), tuple_factory)
            result.append(value)
        return tuple_factory(result)
    elif isinstance(obj, tuple) and hasattr(obj, '_fields'):
        # obj is a namedtuple.  Recurse into it, but the returned
        # object is another namedtuple of the same type.  This is
        # similar to how other list- or tuple-derived classes are
        # treated (see below), but we just need to create them
        # differently because a namedtuple's __init__ needs to be
        # called differently (see bpo-34363).
        return type(obj)(*[_astuple_inner(v, tuple_factory) for v in obj])
    elif isinstance(obj, (list, tuple)):
        # Assume we can create an object of this type by passing in a
        # generator (which is not true for namedtuples, handled
        # above).
        return type(obj)(_astuple_inner(v, tuple_factory) for v in obj)
    elif isinstance(obj, dict):
        return type(obj)((_astuple_inner(k, tuple_factory), _astuple_inner(v, tuple_factory))
                          for k, v in obj.items())
    else:
        return copy.deepcopy(obj)","def check_if_object_is_dataclass_instance(obj):
    """"""Returns True if obj is an instance of a dataclass.""""""
    return hasattr(type(obj), _FIELDS)

def convert_object_to_tuple_recursively(obj, tuple_factory):
    if check_if_object_is_dataclass_instance(obj):
        result = []
        for f in fields(obj):
            value = convert_object_to_tuple_recursively(getattr(obj, f.name), tuple_factory)
            result.append(value)
        return tuple_factory(result)
    elif isinstance(obj, tuple) and hasattr(obj, '_fields'):
        # obj is a namedtuple.  Recurse into it, but the returned
        # object is another namedtuple of the same type.  This is
        # similar to how other list- or tuple-derived classes are
        # treated (see below), but we just need to create them
        # differently because a namedtuple's __init__ needs to be
        # called differently (see bpo-34363).
        return type(obj)(*[convert_object_to_tuple_recursively(v, tuple_factory) for v in obj])
    elif isinstance(obj, (list, tuple)):
        # Assume we can create an object of this type by passing in a
        # generator (which is not true for namedtuples, handled
        # above).
        return type(obj)(convert_object_to_tuple_recursively(v, tuple_factory) for v in obj)
    elif isinstance(obj, dict):
        return type(obj)((convert_object_to_tuple_recursively(k, tuple_factory), convert_object_to_tuple_recursively(v, tuple_factory))
                          for k, v in obj.items())
    else:
        return copy.deepcopy(obj)","def astuple_and_return_fields_of_dataclass_instance_as_tuple(obj, *, tuple_factory=tuple):
    """"""Return the fields of a dataclass instance as a new tuple of field values.

    Example usage::

      @dataclass
      class C:
          x: int
          y: int

    c = C(1, 2)
    assert astuple_and_return_fields_of_dataclass_instance_as_tuple(c) == (1, 2)

    If given, 'tuple_factory' will be used instead of built-in tuple.
    The function applies recursively to field values that are
    dataclass instances. This will also look into built-in containers:
    tuples, lists, and dicts.
    """"""

    if not check_if_object_is_dataclass_instance(obj):
        raise TypeError(""astuple_and_return_fields_of_dataclass_instance_as_tuple() should be called on dataclass instances"")
    return convert_object_to_tuple_recursively(obj, tuple_factory)","{""_is_dataclass_instance"": ""check_if_object_is_dataclass_instance"", ""_astuple_inner"": ""convert_object_to_tuple_recursively""}","def 0(obj):
    """"""Returns True if obj is an instance of a dataclass.""""""
    return hasattr(type(obj), _FIELDS)

def 1(obj, tuple_factory):
    if 0(obj):
        result = []
        for f in fields(obj):
            value = 1(getattr(obj, f.name), tuple_factory)
            result.append(value)
        return tuple_factory(result)
    elif isinstance(obj, tuple) and hasattr(obj, '_fields'):
        # obj is a namedtuple.  Recurse into it, but the returned
        # object is another namedtuple of the same type.  This is
        # similar to how other list- or tuple-derived classes are
        # treated (see below), but we just need to create them
        # differently because a namedtuple's __init__ needs to be
        # called differently (see bpo-34363).
        return type(obj)(*[1(v, tuple_factory) for v in obj])
    elif isinstance(obj, (list, tuple)):
        # Assume we can create an object of this type by passing in a
        # generator (which is not true for namedtuples, handled
        # above).
        return type(obj)(1(v, tuple_factory) for v in obj)
    elif isinstance(obj, dict):
        return type(obj)((1(k, tuple_factory), 1(v, tuple_factory))
                          for k, v in obj.items())
    else:
        return copy.deepcopy(obj)","def 2(obj, *, tuple_factory=tuple):
    """"""Return the fields of a dataclass instance as a new tuple of field values.

    Example usage::

      @dataclass
      class C:
          x: int
          y: int

    c = C(1, 2)
    assert 2(c) == (1, 2)

    If given, 'tuple_factory' will be used instead of built-in tuple.
    The function applies recursively to field values that are
    dataclass instances. This will also look into built-in containers:
    tuples, lists, and dicts.
    """"""

    if not 0(obj):
        raise TypeError(""2() should be called on dataclass instances"")
    return 1(obj, tuple_factory)","{""_is_dataclass_instance"": ""0"", ""_astuple_inner"": ""1""}"
211,211,"def _remove_dups_flatten(parameters):
    """"""An internal helper for Union creation and substitution: flatten Unions
    among parameters, then remove duplicates.
    """"""
    # Flatten out Union[Union[...], ...].
    params = []
    for p in parameters:
        if isinstance(p, (_UnionGenericAlias, types.UnionType)):
            params.extend(p.__args__)
        elif isinstance(p, tuple) and len(p) > 0 and p[0] is Union:
            params.extend(p[1:])
        else:
            params.append(p)

    return tuple(_deduplicate(params))","def _deduplicate(params):
    # Weed out strict duplicates, preserving the first of each occurrence.
    all_params = set(params)
    if len(all_params) < len(params):
        new_params = []
        for t in params:
            if t in all_params:
                new_params.append(t)
                all_params.remove(t)
        params = new_params
        assert not all_params, all_params
    return params","def remove_duplicates_preserve_order(params):
    # Weed out strict duplicates, preserving the first of each occurrence.
    all_params = set(params)
    if len(all_params) < len(params):
        new_params = []
        for t in params:
            if t in all_params:
                new_params.append(t)
                all_params.remove(t)
        params = new_params
        assert not all_params, all_params
    return params","def _remove_dups_flatten_and_internal_helper_for_Union_creation_and_substitution(parameters):
    """"""An internal helper for Union creation and substitution: flatten Unions
    among parameters, then remove duplicates.
    """"""
    # Flatten out Union[Union[...], ...].
    params = []
    for p in parameters:
        if isinstance(p, (_UnionGenericAlias, types.UnionType)):
            params.extend(p.__args__)
        elif isinstance(p, tuple) and len(p) > 0 and p[0] is Union:
            params.extend(p[1:])
        else:
            params.append(p)

    return tuple(remove_duplicates_preserve_order(params))","{""_deduplicate"": ""remove_duplicates_preserve_order""}","def 0(params):
    # Weed out strict duplicates, preserving the first of each occurrence.
    all_params = set(params)
    if len(all_params) < len(params):
        new_params = []
        for t in params:
            if t in all_params:
                new_params.append(t)
                all_params.remove(t)
        params = new_params
        assert not all_params, all_params
    return params","def 1(parameters):
    """"""An internal helper for Union creation and substitution: flatten Unions
    among parameters, then remove duplicates.
    """"""
    # Flatten out Union[Union[...], ...].
    params = []
    for p in parameters:
        if isinstance(p, (_UnionGenericAlias, types.UnionType)):
            params.extend(p.__args__)
        elif isinstance(p, tuple) and len(p) > 0 and p[0] is Union:
            params.extend(p[1:])
        else:
            params.append(p)

    return tuple(0(params))","{""_deduplicate"": ""0""}"
212,212,"def _eval_type(t, globalns, localns, recursive_guard=frozenset()):
    """"""Evaluate all forward references in the given type t.
    For use of globalns and localns see the docstring for get_type_hints().
    recursive_guard is used to prevent infinite recursion with a recursive
    ForwardRef.
    """"""
    if isinstance(t, ForwardRef):
        return t._evaluate(globalns, localns, recursive_guard)
    if isinstance(t, (_GenericAlias, GenericAlias, types.UnionType)):
        ev_args = tuple(_eval_type(a, globalns, localns, recursive_guard) for a in t.__args__)
        if ev_args == t.__args__:
            return t
        if isinstance(t, GenericAlias):
            return GenericAlias(t.__origin__, ev_args)
        if isinstance(t, types.UnionType):
            return functools.reduce(operator.or_, ev_args)
        else:
            return t.copy_with(ev_args)
    return t","def _eval_type(t, globalns, localns, recursive_guard=frozenset()):
    """"""Evaluate all forward references in the given type t.
    For use of globalns and localns see the docstring for get_type_hints().
    recursive_guard is used to prevent infinite recursion with a recursive
    ForwardRef.
    """"""
    if isinstance(t, ForwardRef):
        return t._evaluate(globalns, localns, recursive_guard)
    if isinstance(t, (_GenericAlias, GenericAlias, types.UnionType)):
        ev_args = tuple(_eval_type(a, globalns, localns, recursive_guard) for a in t.__args__)
        if ev_args == t.__args__:
            return t
        if isinstance(t, GenericAlias):
            return GenericAlias(t.__origin__, ev_args)
        if isinstance(t, types.UnionType):
            return functools.reduce(operator.or_, ev_args)
        else:
            return t.copy_with(ev_args)
    return t","def evaluate_type_with_forward_references(t, globalns, localns, recursive_guard=frozenset()):
    """"""Evaluate all forward references in the given type t.
    For use of globalns and localns see the docstring for get_type_hints().
    recursive_guard is used to prevent infinite recursion with a recursive
    ForwardRef.
    """"""
    if isinstance(t, ForwardRef):
        return t._evaluate(globalns, localns, recursive_guard)
    if isinstance(t, (_GenericAlias, GenericAlias, types.UnionType)):
        ev_args = tuple(evaluate_type_with_forward_references(a, globalns, localns, recursive_guard) for a in t.__args__)
        if ev_args == t.__args__:
            return t
        if isinstance(t, GenericAlias):
            return GenericAlias(t.__origin__, ev_args)
        if isinstance(t, types.UnionType):
            return functools.reduce(operator.or_, ev_args)
        else:
            return t.copy_with(ev_args)
    return t","def evaluate_type_with_forward_references(t, globalns, localns, recursive_guard=frozenset()):
    """"""Evaluate all forward references in the given type t.
    For use of globalns and localns see the docstring for get_type_hints().
    recursive_guard is used to prevent infinite recursion with a recursive
    ForwardRef.
    """"""
    if isinstance(t, ForwardRef):
        return t._evaluate(globalns, localns, recursive_guard)
    if isinstance(t, (_GenericAlias, GenericAlias, types.UnionType)):
        ev_args = tuple(evaluate_type_with_forward_references(a, globalns, localns, recursive_guard) for a in t.__args__)
        if ev_args == t.__args__:
            return t
        if isinstance(t, GenericAlias):
            return GenericAlias(t.__origin__, ev_args)
        if isinstance(t, types.UnionType):
            return functools.reduce(operator.or_, ev_args)
        else:
            return t.copy_with(ev_args)
    return t","{""_eval_type"": ""evaluate_type_with_forward_references""}","def 0(t, globalns, localns, recursive_guard=frozenset()):
    """"""Evaluate all forward references in the given type t.
    For use of globalns and localns see the docstring for get_type_hints().
    recursive_guard is used to prevent infinite recursion with a recursive
    ForwardRef.
    """"""
    if isinstance(t, ForwardRef):
        return t._evaluate(globalns, localns, recursive_guard)
    if isinstance(t, (_GenericAlias, GenericAlias, types.UnionType)):
        ev_args = tuple(0(a, globalns, localns, recursive_guard) for a in t.__args__)
        if ev_args == t.__args__:
            return t
        if isinstance(t, GenericAlias):
            return GenericAlias(t.__origin__, ev_args)
        if isinstance(t, types.UnionType):
            return functools.reduce(operator.or_, ev_args)
        else:
            return t.copy_with(ev_args)
    return t","def 0(t, globalns, localns, recursive_guard=frozenset()):
    """"""Evaluate all forward references in the given type t.
    For use of globalns and localns see the docstring for get_type_hints().
    recursive_guard is used to prevent infinite recursion with a recursive
    ForwardRef.
    """"""
    if isinstance(t, ForwardRef):
        return t._evaluate(globalns, localns, recursive_guard)
    if isinstance(t, (_GenericAlias, GenericAlias, types.UnionType)):
        ev_args = tuple(0(a, globalns, localns, recursive_guard) for a in t.__args__)
        if ev_args == t.__args__:
            return t
        if isinstance(t, GenericAlias):
            return GenericAlias(t.__origin__, ev_args)
        if isinstance(t, types.UnionType):
            return functools.reduce(operator.or_, ev_args)
        else:
            return t.copy_with(ev_args)
    return t","{""_eval_type"": ""0""}"
213,213,"def _strip_annotations(t):
    """"""Strips the annotations from a given type.
    """"""
    if isinstance(t, _AnnotatedAlias):
        return _strip_annotations(t.__origin__)
    if isinstance(t, _GenericAlias):
        stripped_args = tuple(_strip_annotations(a) for a in t.__args__)
        if stripped_args == t.__args__:
            return t
        return t.copy_with(stripped_args)
    if isinstance(t, GenericAlias):
        stripped_args = tuple(_strip_annotations(a) for a in t.__args__)
        if stripped_args == t.__args__:
            return t
        return GenericAlias(t.__origin__, stripped_args)
    if isinstance(t, types.UnionType):
        stripped_args = tuple(_strip_annotations(a) for a in t.__args__)
        if stripped_args == t.__args__:
            return t
        return functools.reduce(operator.or_, stripped_args)

    return t","def _strip_annotations(t):
    """"""Strips the annotations from a given type.
    """"""
    if isinstance(t, _AnnotatedAlias):
        return _strip_annotations(t.__origin__)
    if isinstance(t, _GenericAlias):
        stripped_args = tuple(_strip_annotations(a) for a in t.__args__)
        if stripped_args == t.__args__:
            return t
        return t.copy_with(stripped_args)
    if isinstance(t, GenericAlias):
        stripped_args = tuple(_strip_annotations(a) for a in t.__args__)
        if stripped_args == t.__args__:
            return t
        return GenericAlias(t.__origin__, stripped_args)
    if isinstance(t, types.UnionType):
        stripped_args = tuple(_strip_annotations(a) for a in t.__args__)
        if stripped_args == t.__args__:
            return t
        return functools.reduce(operator.or_, stripped_args)

    return t","def remove_type_annotations_recursively(t):
    """"""Strips the annotations from a given type.
    """"""
    if isinstance(t, _AnnotatedAlias):
        return remove_type_annotations_recursively(t.__origin__)
    if isinstance(t, _GenericAlias):
        stripped_args = tuple(remove_type_annotations_recursively(a) for a in t.__args__)
        if stripped_args == t.__args__:
            return t
        return t.copy_with(stripped_args)
    if isinstance(t, GenericAlias):
        stripped_args = tuple(remove_type_annotations_recursively(a) for a in t.__args__)
        if stripped_args == t.__args__:
            return t
        return GenericAlias(t.__origin__, stripped_args)
    if isinstance(t, types.UnionType):
        stripped_args = tuple(remove_type_annotations_recursively(a) for a in t.__args__)
        if stripped_args == t.__args__:
            return t
        return functools.reduce(operator.or_, stripped_args)

    return t","def remove_type_annotations_recursively(t):
    """"""Strips the annotations from a given type.
    """"""
    if isinstance(t, _AnnotatedAlias):
        return remove_type_annotations_recursively(t.__origin__)
    if isinstance(t, _GenericAlias):
        stripped_args = tuple(remove_type_annotations_recursively(a) for a in t.__args__)
        if stripped_args == t.__args__:
            return t
        return t.copy_with(stripped_args)
    if isinstance(t, GenericAlias):
        stripped_args = tuple(remove_type_annotations_recursively(a) for a in t.__args__)
        if stripped_args == t.__args__:
            return t
        return GenericAlias(t.__origin__, stripped_args)
    if isinstance(t, types.UnionType):
        stripped_args = tuple(remove_type_annotations_recursively(a) for a in t.__args__)
        if stripped_args == t.__args__:
            return t
        return functools.reduce(operator.or_, stripped_args)

    return t","{""_strip_annotations"": ""remove_type_annotations_recursively""}","def 0(t):
    """"""Strips the annotations from a given type.
    """"""
    if isinstance(t, _AnnotatedAlias):
        return 0(t.__origin__)
    if isinstance(t, _GenericAlias):
        stripped_args = tuple(0(a) for a in t.__args__)
        if stripped_args == t.__args__:
            return t
        return t.copy_with(stripped_args)
    if isinstance(t, GenericAlias):
        stripped_args = tuple(0(a) for a in t.__args__)
        if stripped_args == t.__args__:
            return t
        return GenericAlias(t.__origin__, stripped_args)
    if isinstance(t, types.UnionType):
        stripped_args = tuple(0(a) for a in t.__args__)
        if stripped_args == t.__args__:
            return t
        return functools.reduce(operator.or_, stripped_args)

    return t","def 0(t):
    """"""Strips the annotations from a given type.
    """"""
    if isinstance(t, _AnnotatedAlias):
        return 0(t.__origin__)
    if isinstance(t, _GenericAlias):
        stripped_args = tuple(0(a) for a in t.__args__)
        if stripped_args == t.__args__:
            return t
        return t.copy_with(stripped_args)
    if isinstance(t, GenericAlias):
        stripped_args = tuple(0(a) for a in t.__args__)
        if stripped_args == t.__args__:
            return t
        return GenericAlias(t.__origin__, stripped_args)
    if isinstance(t, types.UnionType):
        stripped_args = tuple(0(a) for a in t.__args__)
        if stripped_args == t.__args__:
            return t
        return functools.reduce(operator.or_, stripped_args)

    return t","{""_strip_annotations"": ""0""}"
214,214,"def no_type_check(arg):
    """"""Decorator to indicate that annotations are not type hints.

    The argument must be a class or function; if it is a class, it
    applies recursively to all methods and classes defined in that class
    (but not to methods defined in its superclasses or subclasses).

    This mutates the function(s) or class(es) in place.
    """"""
    if isinstance(arg, type):
        arg_attrs = arg.__dict__.copy()
        for attr, val in arg.__dict__.items():
            if val in arg.__bases__ + (arg,):
                arg_attrs.pop(attr)
        for obj in arg_attrs.values():
            if isinstance(obj, types.FunctionType):
                obj.__no_type_check__ = True
            if isinstance(obj, type):
                no_type_check(obj)
    try:
        arg.__no_type_check__ = True
    except TypeError:  # built-in classes
        pass
    return arg","def no_type_check(arg):
    """"""Decorator to indicate that annotations are not type hints.

    The argument must be a class or function; if it is a class, it
    applies recursively to all methods and classes defined in that class
    (but not to methods defined in its superclasses or subclasses).

    This mutates the function(s) or class(es) in place.
    """"""
    if isinstance(arg, type):
        arg_attrs = arg.__dict__.copy()
        for attr, val in arg.__dict__.items():
            if val in arg.__bases__ + (arg,):
                arg_attrs.pop(attr)
        for obj in arg_attrs.values():
            if isinstance(obj, types.FunctionType):
                obj.__no_type_check__ = True
            if isinstance(obj, type):
                no_type_check(obj)
    try:
        arg.__no_type_check__ = True
    except TypeError:  # built-in classes
        pass
    return arg","def indicate_that_annotations_are_not_type_hints(arg):
    """"""Decorator to indicate that annotations are not type hints.

    The argument must be a class or function; if it is a class, it
    applies recursively to all methods and classes defined in that class
    (but not to methods defined in its superclasses or subclasses).

    This mutates the function(s) or class(es) in place.
    """"""
    if isinstance(arg, type):
        arg_attrs = arg.__dict__.copy()
        for attr, val in arg.__dict__.items():
            if val in arg.__bases__ + (arg,):
                arg_attrs.pop(attr)
        for obj in arg_attrs.values():
            if isinstance(obj, types.FunctionType):
                obj.__indicate_that_annotations_are_not_type_hints__ = True
            if isinstance(obj, type):
                indicate_that_annotations_are_not_type_hints(obj)
    try:
        arg.__indicate_that_annotations_are_not_type_hints__ = True
    except TypeError:  # built-in classes
        pass
    return arg","def indicate_that_annotations_are_not_type_hints(arg):
    """"""Decorator to indicate that annotations are not type hints.

    The argument must be a class or function; if it is a class, it
    applies recursively to all methods and classes defined in that class
    (but not to methods defined in its superclasses or subclasses).

    This mutates the function(s) or class(es) in place.
    """"""
    if isinstance(arg, type):
        arg_attrs = arg.__dict__.copy()
        for attr, val in arg.__dict__.items():
            if val in arg.__bases__ + (arg,):
                arg_attrs.pop(attr)
        for obj in arg_attrs.values():
            if isinstance(obj, types.FunctionType):
                obj.__indicate_that_annotations_are_not_type_hints__ = True
            if isinstance(obj, type):
                indicate_that_annotations_are_not_type_hints(obj)
    try:
        arg.__indicate_that_annotations_are_not_type_hints__ = True
    except TypeError:  # built-in classes
        pass
    return arg","{""no_type_check"": ""indicate_that_annotations_are_not_type_hints""}","def 0(arg):
    """"""Decorator to indicate that annotations are not type hints.

    The argument must be a class or function; if it is a class, it
    applies recursively to all methods and classes defined in that class
    (but not to methods defined in its superclasses or subclasses).

    This mutates the function(s) or class(es) in place.
    """"""
    if isinstance(arg, type):
        arg_attrs = arg.__dict__.copy()
        for attr, val in arg.__dict__.items():
            if val in arg.__bases__ + (arg,):
                arg_attrs.pop(attr)
        for obj in arg_attrs.values():
            if isinstance(obj, types.FunctionType):
                obj.__no_type_check__ = True
            if isinstance(obj, type):
                0(obj)
    try:
        arg.__no_type_check__ = True
    except TypeError:  # built-in classes
        pass
    return arg","def 0(arg):
    """"""Decorator to indicate that annotations are not type hints.

    The argument must be a class or function; if it is a class, it
    applies recursively to all methods and classes defined in that class
    (but not to methods defined in its superclasses or subclasses).

    This mutates the function(s) or class(es) in place.
    """"""
    if isinstance(arg, type):
        arg_attrs = arg.__dict__.copy()
        for attr, val in arg.__dict__.items():
            if val in arg.__bases__ + (arg,):
                arg_attrs.pop(attr)
        for obj in arg_attrs.values():
            if isinstance(obj, types.FunctionType):
                obj.__no_type_check__ = True
            if isinstance(obj, type):
                0(obj)
    try:
        arg.__no_type_check__ = True
    except TypeError:  # built-in classes
        pass
    return arg","{""no_type_check"": ""0""}"
215,215,"def no_type_check_decorator(decorator):
    """"""Decorator to give another decorator the @no_type_check effect.

    This wraps the decorator with something that wraps the decorated
    function in @no_type_check.
    """"""

    @functools.wraps(decorator)
    def wrapped_decorator(*args, **kwds):
        func = decorator(*args, **kwds)
        func = no_type_check(func)
        return func

    return wrapped_decorator","def no_type_check(arg):
    """"""Decorator to indicate that annotations are not type hints.

    The argument must be a class or function; if it is a class, it
    applies recursively to all methods and classes defined in that class
    (but not to methods defined in its superclasses or subclasses).

    This mutates the function(s) or class(es) in place.
    """"""
    if isinstance(arg, type):
        arg_attrs = arg.__dict__.copy()
        for attr, val in arg.__dict__.items():
            if val in arg.__bases__ + (arg,):
                arg_attrs.pop(attr)
        for obj in arg_attrs.values():
            if isinstance(obj, types.FunctionType):
                obj.__no_type_check__ = True
            if isinstance(obj, type):
                no_type_check(obj)
    try:
        arg.__no_type_check__ = True
    except TypeError:  # built-in classes
        pass
    return arg","def indicate_that_annotations_are_not_type_hints(arg):
    """"""Decorator to indicate that annotations are not type hints.

    The argument must be a class or function; if it is a class, it
    applies recursively to all methods and classes defined in that class
    (but not to methods defined in its superclasses or subclasses).

    This mutates the function(s) or class(es) in place.
    """"""
    if isinstance(arg, type):
        arg_attrs = arg.__dict__.copy()
        for attr, val in arg.__dict__.items():
            if val in arg.__bases__ + (arg,):
                arg_attrs.pop(attr)
        for obj in arg_attrs.values():
            if isinstance(obj, types.FunctionType):
                obj.__indicate_that_annotations_are_not_type_hints__ = True
            if isinstance(obj, type):
                indicate_that_annotations_are_not_type_hints(obj)
    try:
        arg.__indicate_that_annotations_are_not_type_hints__ = True
    except TypeError:  # built-in classes
        pass
    return arg","def indicate_that_annotations_are_not_type_hints_decorator(decorator):
    """"""Decorator to give another decorator the @indicate_that_annotations_are_not_type_hints effect.

    This wraps the decorator with something that wraps the decorated
    function in @indicate_that_annotations_are_not_type_hints.
    """"""

    @functools.wraps(decorator)
    def wrapped_decorator_and_wrap_the_decorator_with_something_that_wraps_the_decorated_function_in_no_type_check(*args, **kwds):
        func = decorator(*args, **kwds)
        func = indicate_that_annotations_are_not_type_hints(func)
        return func

    return wrapped_decorator_and_wrap_the_decorator_with_something_that_wraps_the_decorated_function_in_no_type_check","{""no_type_check"": ""indicate_that_annotations_are_not_type_hints""}","def 0(arg):
    """"""Decorator to indicate that annotations are not type hints.

    The argument must be a class or function; if it is a class, it
    applies recursively to all methods and classes defined in that class
    (but not to methods defined in its superclasses or subclasses).

    This mutates the function(s) or class(es) in place.
    """"""
    if isinstance(arg, type):
        arg_attrs = arg.__dict__.copy()
        for attr, val in arg.__dict__.items():
            if val in arg.__bases__ + (arg,):
                arg_attrs.pop(attr)
        for obj in arg_attrs.values():
            if isinstance(obj, types.FunctionType):
                obj.__no_type_check__ = True
            if isinstance(obj, type):
                0(obj)
    try:
        arg.__no_type_check__ = True
    except TypeError:  # built-in classes
        pass
    return arg","def 1(decorator):
    """"""Decorator to give another decorator the @no_type_check effect.

    This wraps the decorator with something that wraps the decorated
    function in @no_type_check.
    """"""

    @functools.wraps(decorator)
    def 2(*args, **kwds):
        func = decorator(*args, **kwds)
        func = 0(func)
        return func

    return wrapped_decorator","{""no_type_check"": ""0""}"
216,216,"def DER_cert_to_PEM_cert(der_cert_bytes):
    """"""Takes a certificate in binary DER format and returns the
    PEM version of it as a string.""""""

    f = str(base64.standard_b64encode(der_cert_bytes), 'ASCII', 'strict')
    ss = [PEM_HEADER]
    ss += [f[i:i+64] for i in range(0, len(f), 64)]
    ss.append(PEM_FOOTER + '\n')
    return '\n'.join(ss)","def str(val):
    """"""Convert float to string, taking the locale into account.""""""
    return _format(""%.12g"", val)","def locale_aware_float_to_string(val):
    """"""Convert float to locale_aware_float_to_stringing, taking the locale into account.""""""
    return _format(""%.12g"", val)","def DER_cert_to_PEM_cert_and_take_certificate_in_binary_DER_format_and_return_PEM_version_as_string(der_cert_bytes):
    """"""Takes a certificate in binary DER format and returns the
    PEM version of it as a locale_aware_float_to_stringing.""""""

    f = locale_aware_float_to_string(base64.standard_b64encode(der_cert_bytes), 'ASCII', 'locale_aware_float_to_stringict')
    ss = [PEM_HEADER]
    ss += [f[i:i+64] for i in range(0, len(f), 64)]
    ss.append(PEM_FOOTER + '\n')
    return '\n'.join(ss)","{""str"": ""locale_aware_float_to_string""}","def 0(val):
    """"""Convert float to string, taking the locale into account.""""""
    return _format(""%.12g"", val)","def 1(der_cert_bytes):
    """"""Takes a certificate in binary DER format and returns the
    PEM version of it as a string.""""""

    f = 0(base64.standard_b64encode(der_cert_bytes), 'ASCII', 'strict')
    ss = [PEM_HEADER]
    ss += [f[i:i+64] for i in range(0, len(f), 64)]
    ss.append(PEM_FOOTER + '\n')
    return '\n'.join(ss)","{""str"": ""0""}"
217,217,"def _days_in_month(year, month):
    ""year, month -> number of days in that month in that year.""
    assert 1 <= month <= 12, month
    if month == 2 and _is_leap(year):
        return 29
    return _DAYS_IN_MONTH[month]","def _is_leap(year):
    ""year -> 1 if leap year, else 0.""
    return year % 4 == 0 and (year % 100 != 0 or year % 400 == 0)","def check_if_year_is_leap(year):
    ""year -> 1 if leap year, else 0.""
    return year % 4 == 0 and (year % 100 != 0 or year % 400 == 0)","def get_number_of_days_in_month(year, month):
    ""year, month -> number of days in that month in that year.""
    assert 1 <= month <= 12, month
    if month == 2 and check_if_year_is_leap(year):
        return 29
    return _DAYS_IN_MONTH[month]","{""_is_leap"": ""check_if_year_is_leap""}","def 0(year):
    ""year -> 1 if leap year, else 0.""
    return year % 4 == 0 and (year % 100 != 0 or year % 400 == 0)","def 1(year, month):
    ""year, month -> number of days in that month in that year.""
    assert 1 <= month <= 12, month
    if month == 2 and 0(year):
        return 29
    return _DAYS_IN_MONTH[month]","{""_is_leap"": ""0""}"
218,218,"def _days_before_month(year, month):
    ""year, month -> number of days in year preceding first day of month.""
    assert 1 <= month <= 12, 'month must be in 1..12'
    return _DAYS_BEFORE_MONTH[month] + (month > 2 and _is_leap(year))","def _is_leap(year):
    ""year -> 1 if leap year, else 0.""
    return year % 4 == 0 and (year % 100 != 0 or year % 400 == 0)","def check_if_year_is_leap(year):
    ""year -> 1 if leap year, else 0.""
    return year % 4 == 0 and (year % 100 != 0 or year % 400 == 0)","def calculate_days_before_start_of_month(year, month):
    ""year, month -> number of days in year preceding first day of month.""
    assert 1 <= month <= 12, 'month must be in 1..12'
    return _DAYS_BEFORE_MONTH[month] + (month > 2 and check_if_year_is_leap(year))","{""_is_leap"": ""check_if_year_is_leap""}","def 0(year):
    ""year -> 1 if leap year, else 0.""
    return year % 4 == 0 and (year % 100 != 0 or year % 400 == 0)","def 1(year, month):
    ""year, month -> number of days in year preceding first day of month.""
    assert 1 <= month <= 12, 'month must be in 1..12'
    return _DAYS_BEFORE_MONTH[month] + (month > 2 and 0(year))","{""_is_leap"": ""0""}"
219,219,"def _ymd2ord(year, month, day):
    ""year, month, day -> ordinal, considering 01-Jan-0001 as day 1.""
    assert 1 <= month <= 12, 'month must be in 1..12'
    dim = _days_in_month(year, month)
    assert 1 <= day <= dim, ('day must be in 1..%d' % dim)
    return (_days_before_year(year) +
            _days_before_month(year, month) +
            day)","def _days_before_year(year):
    ""year -> number of days before January 1st of year.""
    y = year - 1
    return y*365 + y//4 - y//100 + y//400

def _days_before_month(year, month):
    ""year, month -> number of days in year preceding first day of month.""
    assert 1 <= month <= 12, 'month must be in 1..12'
    return _DAYS_BEFORE_MONTH[month] + (month > 2 and _is_leap(year))

def _days_in_month(year, month):
    ""year, month -> number of days in that month in that year.""
    assert 1 <= month <= 12, month
    if month == 2 and _is_leap(year):
        return 29
    return _DAYS_IN_MONTH[month]","def calculate_days_before_start_of_year(year):
    ""year -> number of days before January 1st of year.""
    y = year - 1
    return y*365 + y//4 - y//100 + y//400

def calculate_days_before_start_of_month(year, month):
    ""year, month -> number of days in year preceding first day of month.""
    assert 1 <= month <= 12, 'month must be in 1..12'
    return _DAYS_BEFORE_MONTH[month] + (month > 2 and _is_leap(year))

def get_number_of_days_in_month(year, month):
    ""year, month -> number of days in that month in that year.""
    assert 1 <= month <= 12, month
    if month == 2 and _is_leap(year):
        return 29
    return _DAYS_IN_MONTH[month]","def _ymd2ord_and_convert_year_month_day_to_ordinal_considering_01_Jan_0001_as_day_1(year, month, day):
    ""year, month, day -> ordinal, considering 01-Jan-0001 as day 1.""
    assert 1 <= month <= 12, 'month must be in 1..12'
    dim = get_number_of_days_in_month(year, month)
    assert 1 <= day <= dim, ('day must be in 1..%d' % dim)
    return (calculate_days_before_start_of_year(year) +
            calculate_days_before_start_of_month(year, month) +
            day)","{""_days_before_year"": ""calculate_days_before_start_of_year"", ""_days_before_month"": ""calculate_days_before_start_of_month"", ""_days_in_month"": ""get_number_of_days_in_month""}","def 0(year):
    ""year -> number of days before January 1st of year.""
    y = year - 1
    return y*365 + y//4 - y//100 + y//400

def 1(year, month):
    ""year, month -> number of days in year preceding first day of month.""
    assert 1 <= month <= 12, 'month must be in 1..12'
    return _DAYS_BEFORE_MONTH[month] + (month > 2 and _is_leap(year))

def 2(year, month):
    ""year, month -> number of days in that month in that year.""
    assert 1 <= month <= 12, month
    if month == 2 and _is_leap(year):
        return 29
    return _DAYS_IN_MONTH[month]","def 3(year, month, day):
    ""year, month, day -> ordinal, considering 01-Jan-0001 as day 1.""
    assert 1 <= month <= 12, 'month must be in 1..12'
    dim = 2(year, month)
    assert 1 <= day <= dim, ('day must be in 1..%d' % dim)
    return (0(year) +
            1(year, month) +
            day)","{""_days_before_year"": ""0"", ""_days_before_month"": ""1"", ""_days_in_month"": ""2""}"
220,220,"def localcontext(ctx=None):
    """"""Return a context manager for a copy of the supplied context

    Uses a copy of the current context if no context is specified
    The returned context manager creates a local decimal context
    in a with statement:
        def sin(x):
             with localcontext() as ctx:
                 ctx.prec += 2
                 # Rest of sin calculation algorithm
                 # uses a precision 2 greater than normal
             return +s  # Convert result to normal precision

         def sin(x):
             with localcontext(ExtendedContext):
                 # Rest of sin calculation algorithm
                 # uses the Extended Context from the
                 # General Decimal Arithmetic Specification
             return +s  # Convert result to normal context

    >>> setcontext(DefaultContext)
    >>> print(getcontext().prec)
    28
    >>> with localcontext():
    ...     ctx = getcontext()
    ...     ctx.prec += 2
    ...     print(ctx.prec)
    ...
    30
    >>> with localcontext(ExtendedContext):
    ...     print(getcontext().prec)
    ...
    9
    >>> print(getcontext().prec)
    28
    """"""
    if ctx is None: ctx = getcontext()
    return _ContextManager(ctx)","def getcontext():
    """"""Returns this thread's context.

    If this thread does not yet have a context, returns
    a new context and sets this thread's context.
    New contexts are copies of DefaultContext.
    """"""
    try:
        return _current_context_var.get()
    except LookupError:
        context = Context()
        _current_context_var.set(context)
        return context","def get_current_thread_context_or_create_new():
    """"""Returns this thread's context.

    If this thread does not yet have a context, returns
    a new context and sets this thread's context.
    New contexts are copies of DefaultContext.
    """"""
    try:
        return _current_context_var.get()
    except LookupError:
        context = Context()
        _current_context_var.set(context)
        return context","def extended_context_manager_returns_context_manager(ctx=None):
    """"""Return a context manager for a copy of the supplied context

    Uses a copy of the current context if no context is specified
    The returned context manager creates a local decimal context
    in a with statement:
        def calculate_sine_with_extended_context(x):
             with extended_context_manager_returns_context_manager() as ctx:
                 ctx.prec += 2
                 # Rest of calculate_sine_with_extended_context calculation algorithm
                 # uses a precision 2 greater than normal
             return +s  # Convert result to normal precision

         def calculate_sine_with_extended_context(x):
             with extended_context_manager_returns_context_manager(ExtendedContext):
                 # Rest of calculate_sine_with_extended_context calculation algorithm
                 # uses the Extended Context from the
                 # General Decimal Arithmetic Specification
             return +s  # Convert result to normal context

    >>> setcontext(DefaultContext)
    >>> print(get_current_thread_context_or_create_new().prec)
    28
    >>> with extended_context_manager_returns_context_manager():
    ...     ctx = get_current_thread_context_or_create_new()
    ...     ctx.prec += 2
    ...     print(ctx.prec)
    ...
    30
    >>> with extended_context_manager_returns_context_manager(ExtendedContext):
    ...     print(get_current_thread_context_or_create_new().prec)
    ...
    9
    >>> print(get_current_thread_context_or_create_new().prec)
    28
    """"""
    if ctx is None: ctx = get_current_thread_context_or_create_new()
    return _ContextManager(ctx)","{""getcontext"": ""get_current_thread_context_or_create_new""}","def 0():
    """"""Returns this thread's context.

    If this thread does not yet have a context, returns
    a new context and sets this thread's context.
    New contexts are copies of DefaultContext.
    """"""
    try:
        return _current_context_var.get()
    except LookupError:
        context = Context()
        _current_context_var.set(context)
        return context","def 1(ctx=None):
    """"""Return a context manager for a copy of the supplied context

    Uses a copy of the current context if no context is specified
    The returned context manager creates a local decimal context
    in a with statement:
        def 2(x):
             with 1() as ctx:
                 ctx.prec += 2
                 # Rest of sin calculation algorithm
                 # uses a precision 2 greater than normal
             return +s  # Convert result to normal precision

         def 2(x):
             with 1(ExtendedContext):
                 # Rest of sin calculation algorithm
                 # uses the Extended Context from the
                 # General Decimal Arithmetic Specification
             return +s  # Convert result to normal context

    >>> setcontext(DefaultContext)
    >>> print(0().prec)
    28
    >>> with 1():
    ...     ctx = 0()
    ...     ctx.prec += 2
    ...     print(ctx.prec)
    ...
    30
    >>> with 1(ExtendedContext):
    ...     print(0().prec)
    ...
    9
    >>> print(0().prec)
    28
    """"""
    if ctx is None: ctx = 0()
    return _ContextManager(ctx)","{""getcontext"": ""0""}"
221,221,"def _normalize(op1, op2, prec = 0):
    """"""Normalizes op1, op2 to have the same exp and length of coefficient.

    Done during addition.
    """"""
    if op1.exp < op2.exp:
        tmp = op2
        other = op1
    else:
        tmp = op1
        other = op2

    # Let exp = min(tmp.exp - 1, tmp.adjusted() - precision - 1).
    # Then adding 10**exp to tmp has the same effect (after rounding)
    # as adding any positive quantity smaller than 10**exp; similarly
    # for subtraction.  So if other is smaller than 10**exp we replace
    # it with 10**exp.  This avoids tmp.exp - other.exp getting too large.
    tmp_len = len(str(tmp.int))
    other_len = len(str(other.int))
    exp = tmp.exp + min(-1, tmp_len - prec - 2)
    if other_len + other.exp - 1 < exp:
        other.int = 1
        other.exp = exp

    tmp.int *= 10 ** (tmp.exp - other.exp)
    tmp.exp = other.exp
    return op1, op2","def str(val):
    """"""Convert float to string, taking the locale into account.""""""
    return _format(""%.12g"", val)","def locale_aware_float_to_string(val):
    """"""Convert float to locale_aware_float_to_stringing, taking the locale into account.""""""
    return _format(""%.12g"", val)","def adjust_operands_and_return_normalized(op1, op2, prec = 0):
    """"""Normalizes op1, op2 to have the same exp and length of coefficient.

    Done during addition.
    """"""
    if op1.exp < op2.exp:
        tmp = op2
        other = op1
    else:
        tmp = op1
        other = op2

    # Let exp = min(tmp.exp - 1, tmp.adjusted() - precision - 1).
    # Then adding 10**exp to tmp has the same effect (after rounding)
    # as adding any positive quantity smaller than 10**exp; similarly
    # for subtraction.  So if other is smaller than 10**exp we replace
    # it with 10**exp.  This avoids tmp.exp - other.exp getting too large.
    tmp_len = len(locale_aware_float_to_string(tmp.int))
    other_len = len(locale_aware_float_to_string(other.int))
    exp = tmp.exp + min(-1, tmp_len - prec - 2)
    if other_len + other.exp - 1 < exp:
        other.int = 1
        other.exp = exp

    tmp.int *= 10 ** (tmp.exp - other.exp)
    tmp.exp = other.exp
    return op1, op2","{""str"": ""locale_aware_float_to_string""}","def 0(val):
    """"""Convert float to string, taking the locale into account.""""""
    return _format(""%.12g"", val)","def 1(op1, op2, prec = 0):
    """"""Normalizes op1, op2 to have the same exp and length of coefficient.

    Done during addition.
    """"""
    if op1.exp < op2.exp:
        tmp = op2
        other = op1
    else:
        tmp = op1
        other = op2

    # Let exp = min(tmp.exp - 1, tmp.adjusted() - precision - 1).
    # Then adding 10**exp to tmp has the same effect (after rounding)
    # as adding any positive quantity smaller than 10**exp; similarly
    # for subtraction.  So if other is smaller than 10**exp we replace
    # it with 10**exp.  This avoids tmp.exp - other.exp getting too large.
    tmp_len = len(0(tmp.int))
    other_len = len(0(other.int))
    exp = tmp.exp + min(-1, tmp_len - prec - 2)
    if other_len + other.exp - 1 < exp:
        other.int = 1
        other.exp = exp

    tmp.int *= 10 ** (tmp.exp - other.exp)
    tmp.exp = other.exp
    return op1, op2","{""str"": ""0""}"
222,222,"def _decimal_lshift_exact(n, e):
    """""" Given integers n and e, return n * 10**e if it's an integer, else None.

    The computation is designed to avoid computing large powers of 10
    unnecessarily.

    >>> _decimal_lshift_exact(3, 4)
    30000
    >>> _decimal_lshift_exact(300, -999999999)  # returns None

    """"""
    if n == 0:
        return 0
    elif e >= 0:
        return n * 10**e
    else:
        # val_n = largest power of 10 dividing n.
        str_n = str(abs(n))
        val_n = len(str_n) - len(str_n.rstrip('0'))
        return None if val_n < -e else n // 10**-e","def abs(a):
    ""Same as abs(a).""
    return _abs(a)

def str(val):
    """"""Convert float to string, taking the locale into account.""""""
    return _format(""%.12g"", val)","def calculate_absolute_value(a):
    ""Same as calculate_absolute_value(a).""
    return _calculate_absolute_value(a)

def locale_aware_float_to_string(val):
    """"""Convert float to locale_aware_float_to_stringing, taking the locale into account.""""""
    return _format(""%.12g"", val)","def compute_integer_after_left_shift(n, e):
    """""" Given integers n and e, return n * 10**e if it's an integer, else None.

    The computation is designed to avoid computing large powers of 10
    unnecessarily.

    >>> compute_integer_after_left_shift(3, 4)
    30000
    >>> compute_integer_after_left_shift(300, -999999999)  # returns None

    """"""
    if n == 0:
        return 0
    elif e >= 0:
        return n * 10**e
    else:
        # val_n = largest power of 10 dividing n.
        locale_aware_float_to_string_n = locale_aware_float_to_string(calculate_absolute_value(n))
        val_n = len(locale_aware_float_to_string_n) - len(locale_aware_float_to_string_n.rlocale_aware_float_to_stringip('0'))
        return None if val_n < -e else n // 10**-e","{""abs"": ""calculate_absolute_value"", ""str"": ""locale_aware_float_to_string""}","def 0(a):
    ""Same as 0(a).""
    return _0(a)

def 1(val):
    """"""Convert float to string, taking the locale into account.""""""
    return _format(""%.12g"", val)","def 2(n, e):
    """""" Given integers n and e, return n * 10**e if it's an integer, else None.

    The computation is designed to avoid computing large powers of 10
    unnecessarily.

    >>> 2(3, 4)
    30000
    >>> 2(300, -999999999)  # returns None

    """"""
    if n == 0:
        return 0
    elif e >= 0:
        return n * 10**e
    else:
        # val_n = largest power of 10 dividing n.
        str_n = 1(0(n))
        val_n = len(str_n) - len(str_n.rstrip('0'))
        return None if val_n < -e else n // 10**-e","{""abs"": ""0"", ""str"": ""1""}"
223,223,"def _log10_lb(c, correction = {
        '1': 100, '2': 70, '3': 53, '4': 40, '5': 31,
        '6': 23, '7': 16, '8': 10, '9': 5}):
    """"""Compute a lower bound for 100*log10(c) for a positive integer c.""""""
    if c <= 0:
        raise ValueError(""The argument to _log10_lb should be nonnegative."")
    str_c = str(c)
    return 100*len(str_c) - correction[str_c[0]]","def str(val):
    """"""Convert float to string, taking the locale into account.""""""
    return _format(""%.12g"", val)","def locale_aware_float_to_string(val):
    """"""Convert float to locale_aware_float_to_stringing, taking the locale into account.""""""
    return _format(""%.12g"", val)","def lower_bound_for_log_base_10_of_positive_integer(c, correction = {
        '1': 100, '2': 70, '3': 53, '4': 40, '5': 31,
        '6': 23, '7': 16, '8': 10, '9': 5}):
    """"""Compute a lower bound for 100*log10(c) for a positive integer c.""""""
    if c <= 0:
        raise ValueError(""The argument to lower_bound_for_log_base_10_of_positive_integer should be nonnegative."")
    locale_aware_float_to_string_c = locale_aware_float_to_string(c)
    return 100*len(locale_aware_float_to_string_c) - correction[locale_aware_float_to_string_c[0]]","{""str"": ""locale_aware_float_to_string""}","def 0(val):
    """"""Convert float to string, taking the locale into account.""""""
    return _format(""%.12g"", val)","def 1(c, correction = {
        '1': 100, '2': 70, '3': 53, '4': 40, '5': 31,
        '6': 23, '7': 16, '8': 10, '9': 5}):
    """"""Compute a lower bound for 100*log10(c) for a positive integer c.""""""
    if c <= 0:
        raise ValueError(""The argument to _log10_lb should be nonnegative."")
    str_c = 0(c)
    return 100*len(str_c) - correction[str_c[0]]","{""str"": ""0""}"
224,224,"def _group_lengths(grouping):
    """"""Convert a localeconv-style grouping into a (possibly infinite)
    iterable of integers representing group lengths.

    """"""
    # The result from localeconv()['grouping'], and the input to this
    # function, should be a list of integers in one of the
    # following three forms:
    #
    #   (1) an empty list, or
    #   (2) nonempty list of positive integers + [0]
    #   (3) list of positive integers + [locale.CHAR_MAX], or

    from itertools import chain, repeat
    if not grouping:
        return []
    elif grouping[-1] == 0 and len(grouping) >= 2:
        return chain(grouping[:-1], repeat(grouping[-2]))
    elif grouping[-1] == _locale.CHAR_MAX:
        return grouping[:-1]
    else:
        raise ValueError('unrecognised format for grouping')","def repeat(stmt=""pass"", setup=""pass"", timer=default_timer,
           repeat=default_repeat, number=default_number, globals=None):
    """"""Convenience function to create Timer object and call repeat method.""""""
    return Timer(stmt, setup, timer, globals).repeat(repeat, number)","def timeit_repeat(stmt=""pass"", setup=""pass"", timer=default_timer,
           timeit_repeat=default_timeit_repeat, number=default_number, globals=None):
    """"""Convenience function to create Timer object and call timeit_repeat method.""""""
    return Timer(stmt, setup, timer, globals).timeit_repeat(timeit_repeat, number)","def convert_localeconv_grouping_to_iterable(grouping):
    """"""Convert a localeconv-style grouping into a (possibly infinite)
    iterable of integers representing group lengths.

    """"""
    # The result from localeconv()['grouping'], and the input to this
    # function, should be a list of integers in one of the
    # following three forms:
    #
    #   (1) an empty list, or
    #   (2) nonempty list of positive integers + [0]
    #   (3) list of positive integers + [locale.CHAR_MAX], or

    from itertools import chain, timeit_repeat
    if not grouping:
        return []
    elif grouping[-1] == 0 and len(grouping) >= 2:
        return chain(grouping[:-1], timeit_repeat(grouping[-2]))
    elif grouping[-1] == _locale.CHAR_MAX:
        return grouping[:-1]
    else:
        raise ValueError('unrecognised format for grouping')","{""repeat"": ""timeit_repeat""}","def 0(stmt=""pass"", setup=""pass"", timer=default_timer,
           repeat=default_repeat, number=default_number, globals=None):
    """"""Convenience function to create Timer object and call repeat method.""""""
    return Timer(stmt, setup, timer, globals).0(repeat, number)","def 1(grouping):
    """"""Convert a localeconv-style grouping into a (possibly infinite)
    iterable of integers representing group lengths.

    """"""
    # The result from localeconv()['grouping'], and the input to this
    # function, should be a list of integers in one of the
    # following three forms:
    #
    #   (1) an empty list, or
    #   (2) nonempty list of positive integers + [0]
    #   (3) list of positive integers + [locale.CHAR_MAX], or

    from itertools import chain, repeat
    if not grouping:
        return []
    elif grouping[-1] == 0 and len(grouping) >= 2:
        return chain(grouping[:-1], 0(grouping[-2]))
    elif grouping[-1] == _locale.CHAR_MAX:
        return grouping[:-1]
    else:
        raise ValueError('unrecognised format for grouping')","{""repeat"": ""0""}"
225,225,"def _find_mac_under_heading(command, args, heading):
    """"""Looks for a MAC address under a heading in a command's output.

    The first line of words in the output is searched for the given
    heading. Words at the same word index as the heading in subsequent
    lines are then examined to see if they look like MAC addresses.
    """"""
    stdout = _get_command_stdout(command, args)
    if stdout is None:
        return None

    keywords = stdout.readline().rstrip().split()
    try:
        column_index = keywords.index(heading)
    except ValueError:
        return None

    first_local_mac = None
    for line in stdout:
        words = line.rstrip().split()
        try:
            word = words[column_index]
        except IndexError:
            continue

        mac = _parse_mac(word)
        if mac is None:
            continue
        if _is_universal(mac):
            return mac
        if first_local_mac is None:
            first_local_mac = mac

    return first_local_mac","def _parse_mac(word):
    # Accept 'HH:HH:HH:HH:HH:HH' MAC address (ex: '52:54:00:9d:0e:67'),
    # but reject IPv6 address (ex: 'fe80::5054:ff:fe9' or '123:2:3:4:5:6:7:8').
    #
    # Virtual interfaces, such as those provided by VPNs, do not have a
    # colon-delimited MAC address as expected, but a 16-byte HWAddr separated
    # by dashes. These should be ignored in favor of a real MAC address
    parts = word.split(_MAC_DELIM)
    if len(parts) != 6:
        return
    if _MAC_OMITS_LEADING_ZEROES:
        # (Only) on AIX the macaddr value given is not prefixed by 0, e.g.
        # en0   1500  link#2      fa.bc.de.f7.62.4 110854824     0 160133733     0     0
        # not
        # en0   1500  link#2      fa.bc.de.f7.62.04 110854824     0 160133733     0     0
        if not all(1 <= len(part) <= 2 for part in parts):
            return
        hexstr = b''.join(part.rjust(2, b'0') for part in parts)
    else:
        if not all(len(part) == 2 for part in parts):
            return
        hexstr = b''.join(parts)
    try:
        return int(hexstr, 16)
    except ValueError:
        return

def _is_universal(mac):
    return not (mac & (1 << 41))

def _get_command_stdout(command, *args):
    import io, os, shutil, subprocess

    try:
        path_dirs = os.environ.get('PATH', os.defpath).split(os.pathsep)
        path_dirs.extend(['/sbin', '/usr/sbin'])
        executable = shutil.which(command, path=os.pathsep.join(path_dirs))
        if executable is None:
            return None
        # LC_ALL=C to ensure English output, stderr=DEVNULL to prevent output
        # on stderr (Note: we don't have an example where the words we search
        # for are actually localized, but in theory some system could do so.)
        env = dict(os.environ)
        env['LC_ALL'] = 'C'
        proc = subprocess.Popen((executable,) + args,
                                stdout=subprocess.PIPE,
                                stderr=subprocess.DEVNULL,
                                env=env)
        if not proc:
            return None
        stdout, stderr = proc.communicate()
        return io.BytesIO(stdout)
    except (OSError, subprocess.SubprocessError):
        return None","def parse_mac_address(word):
    # Accept 'HH:HH:HH:HH:HH:HH' MAC address (ex: '52:54:00:9d:0e:67'),
    # but reject IPv6 address (ex: 'fe80::5054:ff:fe9' or '123:2:3:4:5:6:7:8').
    #
    # Virtual interfaces, such as those provided by VPNs, do not have a
    # colon-delimited MAC address as expected, but a 16-byte HWAddr separated
    # by dashes. These should be ignored in favor of a real MAC address
    parts = word.split(_MAC_DELIM)
    if len(parts) != 6:
        return
    if _MAC_OMITS_LEADING_ZEROES:
        # (Only) on AIX the macaddr value given is not prefixed by 0, e.g.
        # en0   1500  link#2      fa.bc.de.f7.62.4 110854824     0 160133733     0     0
        # not
        # en0   1500  link#2      fa.bc.de.f7.62.04 110854824     0 160133733     0     0
        if not all(1 <= len(part) <= 2 for part in parts):
            return
        hexstr = b''.join(part.rjust(2, b'0') for part in parts)
    else:
        if not all(len(part) == 2 for part in parts):
            return
        hexstr = b''.join(parts)
    try:
        return int(hexstr, 16)
    except ValueError:
        return

def is_universal_mac(mac):
    return not (mac & (1 << 41))

def obtain_stdout_of_command_with_args(command, *args):
    import io, os, shutil, subprocess

    try:
        path_dirs = os.environ.get('PATH', os.defpath).split(os.pathsep)
        path_dirs.extend(['/sbin', '/usr/sbin'])
        executable = shutil.which(command, path=os.pathsep.join(path_dirs))
        if executable is None:
            return None
        # LC_ALL=C to ensure English output, stderr=DEVNULL to prevent output
        # on stderr (Note: we don't have an example where the words we search
        # for are actually localized, but in theory some system could do so.)
        env = dict(os.environ)
        env['LC_ALL'] = 'C'
        proc = subprocess.Popen((executable,) + args,
                                stdout=subprocess.PIPE,
                                stderr=subprocess.DEVNULL,
                                env=env)
        if not proc:
            return None
        stdout, stderr = proc.communicate()
        return io.BytesIO(stdout)
    except (OSError, subprocess.SubprocessError):
        return None","def find_mac_address_under_heading(command, args, heading):
    """"""Looks for a MAC address under a heading in a command's output.

    The first line of words in the output is searched for the given
    heading. Words at the same word index as the heading in subsequent
    lines are then examined to see if they look like MAC addresses.
    """"""
    stdout = obtain_stdout_of_command_with_args(command, args)
    if stdout is None:
        return None

    keywords = stdout.readline().rstrip().split()
    try:
        column_index = keywords.index(heading)
    except ValueError:
        return None

    first_local_mac = None
    for line in stdout:
        words = line.rstrip().split()
        try:
            word = words[column_index]
        except IndexError:
            continue

        mac = parse_mac_address(word)
        if mac is None:
            continue
        if is_universal_mac(mac):
            return mac
        if first_local_mac is None:
            first_local_mac = mac

    return first_local_mac","{""_parse_mac"": ""parse_mac_address"", ""_is_universal"": ""is_universal_mac"", ""_get_command_stdout"": ""obtain_stdout_of_command_with_args""}","def 0(word):
    # Accept 'HH:HH:HH:HH:HH:HH' MAC address (ex: '52:54:00:9d:0e:67'),
    # but reject IPv6 address (ex: 'fe80::5054:ff:fe9' or '123:2:3:4:5:6:7:8').
    #
    # Virtual interfaces, such as those provided by VPNs, do not have a
    # colon-delimited MAC address as expected, but a 16-byte HWAddr separated
    # by dashes. These should be ignored in favor of a real MAC address
    parts = word.split(_MAC_DELIM)
    if len(parts) != 6:
        return
    if _MAC_OMITS_LEADING_ZEROES:
        # (Only) on AIX the macaddr value given is not prefixed by 0, e.g.
        # en0   1500  link#2      fa.bc.de.f7.62.4 110854824     0 160133733     0     0
        # not
        # en0   1500  link#2      fa.bc.de.f7.62.04 110854824     0 160133733     0     0
        if not all(1 <= len(part) <= 2 for part in parts):
            return
        hexstr = b''.join(part.rjust(2, b'0') for part in parts)
    else:
        if not all(len(part) == 2 for part in parts):
            return
        hexstr = b''.join(parts)
    try:
        return int(hexstr, 16)
    except ValueError:
        return

def 1(mac):
    return not (mac & (1 << 41))

def 2(command, *args):
    import io, os, shutil, subprocess

    try:
        path_dirs = os.environ.get('PATH', os.defpath).split(os.pathsep)
        path_dirs.extend(['/sbin', '/usr/sbin'])
        executable = shutil.which(command, path=os.pathsep.join(path_dirs))
        if executable is None:
            return None
        # LC_ALL=C to ensure English output, stderr=DEVNULL to prevent output
        # on stderr (Note: we don't have an example where the words we search
        # for are actually localized, but in theory some system could do so.)
        env = dict(os.environ)
        env['LC_ALL'] = 'C'
        proc = subprocess.Popen((executable,) + args,
                                stdout=subprocess.PIPE,
                                stderr=subprocess.DEVNULL,
                                env=env)
        if not proc:
            return None
        stdout, stderr = proc.communicate()
        return io.BytesIO(stdout)
    except (OSError, subprocess.SubprocessError):
        return None","def 3(command, args, heading):
    """"""Looks for a MAC address under a heading in a command's output.

    The first line of words in the output is searched for the given
    heading. Words at the same word index as the heading in subsequent
    lines are then examined to see if they look like MAC addresses.
    """"""
    stdout = 2(command, args)
    if stdout is None:
        return None

    keywords = stdout.readline().rstrip().split()
    try:
        column_index = keywords.index(heading)
    except ValueError:
        return None

    first_local_mac = None
    for line in stdout:
        words = line.rstrip().split()
        try:
            word = words[column_index]
        except IndexError:
            continue

        mac = 0(word)
        if mac is None:
            continue
        if 1(mac):
            return mac
        if first_local_mac is None:
            first_local_mac = mac

    return first_local_mac","{""_parse_mac"": ""0"", ""_is_universal"": ""1"", ""_get_command_stdout"": ""2""}"
226,226,"def _ifconfig_getnode():
    """"""Get the hardware address on Unix by running ifconfig.""""""
    # This works on Linux ('' or '-a'), Tru64 ('-av'), but not all Unixes.
    keywords = (b'hwaddr', b'ether', b'address:', b'lladdr')
    for args in ('', '-a', '-av'):
        mac = _find_mac_near_keyword('ifconfig', args, keywords, lambda i: i+1)
        if mac:
            return mac
        return None","def _find_mac_near_keyword(command, args, keywords, get_word_index):
    """"""Searches a command's output for a MAC address near a keyword.

    Each line of words in the output is case-insensitively searched for
    any of the given keywords.  Upon a match, get_word_index is invoked
    to pick a word from the line, given the index of the match.  For
    example, lambda i: 0 would get the first word on the line, while
    lambda i: i - 1 would get the word preceding the keyword.
    """"""
    stdout = _get_command_stdout(command, args)
    if stdout is None:
        return None

    first_local_mac = None
    for line in stdout:
        words = line.lower().rstrip().split()
        for i in range(len(words)):
            if words[i] in keywords:
                try:
                    word = words[get_word_index(i)]
                    mac = int(word.replace(_MAC_DELIM, b''), 16)
                except (ValueError, IndexError):
                    # Virtual interfaces, such as those provided by
                    # VPNs, do not have a colon-delimited MAC address
                    # as expected, but a 16-byte HWAddr separated by
                    # dashes. These should be ignored in favor of a
                    # real MAC address
                    pass
                else:
                    if _is_universal(mac):
                        return mac
                    first_local_mac = first_local_mac or mac
    return first_local_mac or None","def find_mac_address_near_keyword(command, args, keywords, get_word_index):
    """"""Searches a command's output for a MAC address near a keyword.

    Each line of words in the output is case-insensitively searched for
    any of the given keywords.  Upon a match, get_word_index is invoked
    to pick a word from the line, given the index of the match.  For
    example, lambda i: 0 would get the first word on the line, while
    lambda i: i - 1 would get the word preceding the keyword.
    """"""
    stdout = _get_command_stdout(command, args)
    if stdout is None:
        return None

    first_local_mac = None
    for line in stdout:
        words = line.lower().rstrip().split()
        for i in range(len(words)):
            if words[i] in keywords:
                try:
                    word = words[get_word_index(i)]
                    mac = int(word.replace(_MAC_DELIM, b''), 16)
                except (ValueError, IndexError):
                    # Virtual interfaces, such as those provided by
                    # VPNs, do not have a colon-delimited MAC address
                    # as expected, but a 16-byte HWAddr separated by
                    # dashes. These should be ignored in favor of a
                    # real MAC address
                    pass
                else:
                    if _is_universal(mac):
                        return mac
                    first_local_mac = first_local_mac or mac
    return first_local_mac or None","def get_hardware_address_using_ifconfig():
    """"""Get the hardware address on Unix by running ifconfig.""""""
    # This works on Linux ('' or '-a'), Tru64 ('-av'), but not all Unixes.
    keywords = (b'hwaddr', b'ether', b'address:', b'lladdr')
    for args in ('', '-a', '-av'):
        mac = find_mac_address_near_keyword('ifconfig', args, keywords, lambda i: i+1)
        if mac:
            return mac
        return None","{""_find_mac_near_keyword"": ""find_mac_address_near_keyword""}","def 0(command, args, keywords, get_word_index):
    """"""Searches a command's output for a MAC address near a keyword.

    Each line of words in the output is case-insensitively searched for
    any of the given keywords.  Upon a match, get_word_index is invoked
    to pick a word from the line, given the index of the match.  For
    example, lambda i: 0 would get the first word on the line, while
    lambda i: i - 1 would get the word preceding the keyword.
    """"""
    stdout = _get_command_stdout(command, args)
    if stdout is None:
        return None

    first_local_mac = None
    for line in stdout:
        words = line.lower().rstrip().split()
        for i in range(len(words)):
            if words[i] in keywords:
                try:
                    word = words[get_word_index(i)]
                    mac = int(word.replace(_MAC_DELIM, b''), 16)
                except (ValueError, IndexError):
                    # Virtual interfaces, such as those provided by
                    # VPNs, do not have a colon-delimited MAC address
                    # as expected, but a 16-byte HWAddr separated by
                    # dashes. These should be ignored in favor of a
                    # real MAC address
                    pass
                else:
                    if _is_universal(mac):
                        return mac
                    first_local_mac = first_local_mac or mac
    return first_local_mac or None","def 1():
    """"""Get the hardware address on Unix by running ifconfig.""""""
    # This works on Linux ('' or '-a'), Tru64 ('-av'), but not all Unixes.
    keywords = (b'hwaddr', b'ether', b'address:', b'lladdr')
    for args in ('', '-a', '-av'):
        mac = 0('ifconfig', args, keywords, lambda i: i+1)
        if mac:
            return mac
        return None","{""_find_mac_near_keyword"": ""0""}"
227,227,"def _ip_getnode():
    """"""Get the hardware address on Unix by running ip.""""""
    # This works on Linux with iproute2.
    mac = _find_mac_near_keyword('ip', 'link', [b'link/ether'], lambda i: i+1)
    if mac:
        return mac
    return None","def _find_mac_near_keyword(command, args, keywords, get_word_index):
    """"""Searches a command's output for a MAC address near a keyword.

    Each line of words in the output is case-insensitively searched for
    any of the given keywords.  Upon a match, get_word_index is invoked
    to pick a word from the line, given the index of the match.  For
    example, lambda i: 0 would get the first word on the line, while
    lambda i: i - 1 would get the word preceding the keyword.
    """"""
    stdout = _get_command_stdout(command, args)
    if stdout is None:
        return None

    first_local_mac = None
    for line in stdout:
        words = line.lower().rstrip().split()
        for i in range(len(words)):
            if words[i] in keywords:
                try:
                    word = words[get_word_index(i)]
                    mac = int(word.replace(_MAC_DELIM, b''), 16)
                except (ValueError, IndexError):
                    # Virtual interfaces, such as those provided by
                    # VPNs, do not have a colon-delimited MAC address
                    # as expected, but a 16-byte HWAddr separated by
                    # dashes. These should be ignored in favor of a
                    # real MAC address
                    pass
                else:
                    if _is_universal(mac):
                        return mac
                    first_local_mac = first_local_mac or mac
    return first_local_mac or None","def find_mac_address_near_keyword(command, args, keywords, get_word_index):
    """"""Searches a command's output for a MAC address near a keyword.

    Each line of words in the output is case-insensitively searched for
    any of the given keywords.  Upon a match, get_word_index is invoked
    to pick a word from the line, given the index of the match.  For
    example, lambda i: 0 would get the first word on the line, while
    lambda i: i - 1 would get the word preceding the keyword.
    """"""
    stdout = _get_command_stdout(command, args)
    if stdout is None:
        return None

    first_local_mac = None
    for line in stdout:
        words = line.lower().rstrip().split()
        for i in range(len(words)):
            if words[i] in keywords:
                try:
                    word = words[get_word_index(i)]
                    mac = int(word.replace(_MAC_DELIM, b''), 16)
                except (ValueError, IndexError):
                    # Virtual interfaces, such as those provided by
                    # VPNs, do not have a colon-delimited MAC address
                    # as expected, but a 16-byte HWAddr separated by
                    # dashes. These should be ignored in favor of a
                    # real MAC address
                    pass
                else:
                    if _is_universal(mac):
                        return mac
                    first_local_mac = first_local_mac or mac
    return first_local_mac or None","def get_hardware_address_using_ip():
    """"""Get the hardware address on Unix by running ip.""""""
    # This works on Linux with iproute2.
    mac = find_mac_address_near_keyword('ip', 'link', [b'link/ether'], lambda i: i+1)
    if mac:
        return mac
    return None","{""_find_mac_near_keyword"": ""find_mac_address_near_keyword""}","def 0(command, args, keywords, get_word_index):
    """"""Searches a command's output for a MAC address near a keyword.

    Each line of words in the output is case-insensitively searched for
    any of the given keywords.  Upon a match, get_word_index is invoked
    to pick a word from the line, given the index of the match.  For
    example, lambda i: 0 would get the first word on the line, while
    lambda i: i - 1 would get the word preceding the keyword.
    """"""
    stdout = _get_command_stdout(command, args)
    if stdout is None:
        return None

    first_local_mac = None
    for line in stdout:
        words = line.lower().rstrip().split()
        for i in range(len(words)):
            if words[i] in keywords:
                try:
                    word = words[get_word_index(i)]
                    mac = int(word.replace(_MAC_DELIM, b''), 16)
                except (ValueError, IndexError):
                    # Virtual interfaces, such as those provided by
                    # VPNs, do not have a colon-delimited MAC address
                    # as expected, but a 16-byte HWAddr separated by
                    # dashes. These should be ignored in favor of a
                    # real MAC address
                    pass
                else:
                    if _is_universal(mac):
                        return mac
                    first_local_mac = first_local_mac or mac
    return first_local_mac or None","def 1():
    """"""Get the hardware address on Unix by running ip.""""""
    # This works on Linux with iproute2.
    mac = 0('ip', 'link', [b'link/ether'], lambda i: i+1)
    if mac:
        return mac
    return None","{""_find_mac_near_keyword"": ""0""}"
228,228,"def _arp_getnode():
    """"""Get the hardware address on Unix by running arp.""""""
    import os, socket
    try:
        ip_addr = socket.gethostbyname(socket.gethostname())
    except OSError:
        return None

    # Try getting the MAC addr from arp based on our IP address (Solaris).
    mac = _find_mac_near_keyword('arp', '-an', [os.fsencode(ip_addr)], lambda i: -1)
    if mac:
        return mac

    # This works on OpenBSD
    mac = _find_mac_near_keyword('arp', '-an', [os.fsencode(ip_addr)], lambda i: i+1)
    if mac:
        return mac

    # This works on Linux, FreeBSD and NetBSD
    mac = _find_mac_near_keyword('arp', '-an', [os.fsencode('(%s)' % ip_addr)],
                    lambda i: i+2)
    # Return None instead of 0.
    if mac:
        return mac
    return None","def _find_mac_near_keyword(command, args, keywords, get_word_index):
    """"""Searches a command's output for a MAC address near a keyword.

    Each line of words in the output is case-insensitively searched for
    any of the given keywords.  Upon a match, get_word_index is invoked
    to pick a word from the line, given the index of the match.  For
    example, lambda i: 0 would get the first word on the line, while
    lambda i: i - 1 would get the word preceding the keyword.
    """"""
    stdout = _get_command_stdout(command, args)
    if stdout is None:
        return None

    first_local_mac = None
    for line in stdout:
        words = line.lower().rstrip().split()
        for i in range(len(words)):
            if words[i] in keywords:
                try:
                    word = words[get_word_index(i)]
                    mac = int(word.replace(_MAC_DELIM, b''), 16)
                except (ValueError, IndexError):
                    # Virtual interfaces, such as those provided by
                    # VPNs, do not have a colon-delimited MAC address
                    # as expected, but a 16-byte HWAddr separated by
                    # dashes. These should be ignored in favor of a
                    # real MAC address
                    pass
                else:
                    if _is_universal(mac):
                        return mac
                    first_local_mac = first_local_mac or mac
    return first_local_mac or None","def find_mac_address_near_keyword(command, args, keywords, get_word_index):
    """"""Searches a command's output for a MAC address near a keyword.

    Each line of words in the output is case-insensitively searched for
    any of the given keywords.  Upon a match, get_word_index is invoked
    to pick a word from the line, given the index of the match.  For
    example, lambda i: 0 would get the first word on the line, while
    lambda i: i - 1 would get the word preceding the keyword.
    """"""
    stdout = _get_command_stdout(command, args)
    if stdout is None:
        return None

    first_local_mac = None
    for line in stdout:
        words = line.lower().rstrip().split()
        for i in range(len(words)):
            if words[i] in keywords:
                try:
                    word = words[get_word_index(i)]
                    mac = int(word.replace(_MAC_DELIM, b''), 16)
                except (ValueError, IndexError):
                    # Virtual interfaces, such as those provided by
                    # VPNs, do not have a colon-delimited MAC address
                    # as expected, but a 16-byte HWAddr separated by
                    # dashes. These should be ignored in favor of a
                    # real MAC address
                    pass
                else:
                    if _is_universal(mac):
                        return mac
                    first_local_mac = first_local_mac or mac
    return first_local_mac or None","def get_hardware_address_using_arp():
    """"""Get the hardware address on Unix by running arp.""""""
    import os, socket
    try:
        ip_addr = socket.gethostbyname(socket.gethostname())
    except OSError:
        return None

    # Try getting the MAC addr from arp based on our IP address (Solaris).
    mac = find_mac_address_near_keyword('arp', '-an', [os.fsencode(ip_addr)], lambda i: -1)
    if mac:
        return mac

    # This works on OpenBSD
    mac = find_mac_address_near_keyword('arp', '-an', [os.fsencode(ip_addr)], lambda i: i+1)
    if mac:
        return mac

    # This works on Linux, FreeBSD and NetBSD
    mac = find_mac_address_near_keyword('arp', '-an', [os.fsencode('(%s)' % ip_addr)],
                    lambda i: i+2)
    # Return None instead of 0.
    if mac:
        return mac
    return None","{""_find_mac_near_keyword"": ""find_mac_address_near_keyword""}","def 0(command, args, keywords, get_word_index):
    """"""Searches a command's output for a MAC address near a keyword.

    Each line of words in the output is case-insensitively searched for
    any of the given keywords.  Upon a match, get_word_index is invoked
    to pick a word from the line, given the index of the match.  For
    example, lambda i: 0 would get the first word on the line, while
    lambda i: i - 1 would get the word preceding the keyword.
    """"""
    stdout = _get_command_stdout(command, args)
    if stdout is None:
        return None

    first_local_mac = None
    for line in stdout:
        words = line.lower().rstrip().split()
        for i in range(len(words)):
            if words[i] in keywords:
                try:
                    word = words[get_word_index(i)]
                    mac = int(word.replace(_MAC_DELIM, b''), 16)
                except (ValueError, IndexError):
                    # Virtual interfaces, such as those provided by
                    # VPNs, do not have a colon-delimited MAC address
                    # as expected, but a 16-byte HWAddr separated by
                    # dashes. These should be ignored in favor of a
                    # real MAC address
                    pass
                else:
                    if _is_universal(mac):
                        return mac
                    first_local_mac = first_local_mac or mac
    return first_local_mac or None","def 1():
    """"""Get the hardware address on Unix by running arp.""""""
    import os, socket
    try:
        ip_addr = socket.gethostbyname(socket.gethostname())
    except OSError:
        return None

    # Try getting the MAC addr from arp based on our IP address (Solaris).
    mac = 0('arp', '-an', [os.fsencode(ip_addr)], lambda i: -1)
    if mac:
        return mac

    # This works on OpenBSD
    mac = 0('arp', '-an', [os.fsencode(ip_addr)], lambda i: i+1)
    if mac:
        return mac

    # This works on Linux, FreeBSD and NetBSD
    mac = 0('arp', '-an', [os.fsencode('(%s)' % ip_addr)],
                    lambda i: i+2)
    # Return None instead of 0.
    if mac:
        return mac
    return None","{""_find_mac_near_keyword"": ""0""}"
229,229,"def _lanscan_getnode():
    """"""Get the hardware address on Unix by running lanscan.""""""
    # This might work on HP-UX.
    return _find_mac_near_keyword('lanscan', '-ai', [b'lan0'], lambda i: 0)","def _find_mac_near_keyword(command, args, keywords, get_word_index):
    """"""Searches a command's output for a MAC address near a keyword.

    Each line of words in the output is case-insensitively searched for
    any of the given keywords.  Upon a match, get_word_index is invoked
    to pick a word from the line, given the index of the match.  For
    example, lambda i: 0 would get the first word on the line, while
    lambda i: i - 1 would get the word preceding the keyword.
    """"""
    stdout = _get_command_stdout(command, args)
    if stdout is None:
        return None

    first_local_mac = None
    for line in stdout:
        words = line.lower().rstrip().split()
        for i in range(len(words)):
            if words[i] in keywords:
                try:
                    word = words[get_word_index(i)]
                    mac = int(word.replace(_MAC_DELIM, b''), 16)
                except (ValueError, IndexError):
                    # Virtual interfaces, such as those provided by
                    # VPNs, do not have a colon-delimited MAC address
                    # as expected, but a 16-byte HWAddr separated by
                    # dashes. These should be ignored in favor of a
                    # real MAC address
                    pass
                else:
                    if _is_universal(mac):
                        return mac
                    first_local_mac = first_local_mac or mac
    return first_local_mac or None","def find_mac_address_near_keyword(command, args, keywords, get_word_index):
    """"""Searches a command's output for a MAC address near a keyword.

    Each line of words in the output is case-insensitively searched for
    any of the given keywords.  Upon a match, get_word_index is invoked
    to pick a word from the line, given the index of the match.  For
    example, lambda i: 0 would get the first word on the line, while
    lambda i: i - 1 would get the word preceding the keyword.
    """"""
    stdout = _get_command_stdout(command, args)
    if stdout is None:
        return None

    first_local_mac = None
    for line in stdout:
        words = line.lower().rstrip().split()
        for i in range(len(words)):
            if words[i] in keywords:
                try:
                    word = words[get_word_index(i)]
                    mac = int(word.replace(_MAC_DELIM, b''), 16)
                except (ValueError, IndexError):
                    # Virtual interfaces, such as those provided by
                    # VPNs, do not have a colon-delimited MAC address
                    # as expected, but a 16-byte HWAddr separated by
                    # dashes. These should be ignored in favor of a
                    # real MAC address
                    pass
                else:
                    if _is_universal(mac):
                        return mac
                    first_local_mac = first_local_mac or mac
    return first_local_mac or None","def get_hardware_address_using_lanscan():
    """"""Get the hardware address on Unix by running lanscan.""""""
    # This might work on HP-UX.
    return find_mac_address_near_keyword('lanscan', '-ai', [b'lan0'], lambda i: 0)","{""_find_mac_near_keyword"": ""find_mac_address_near_keyword""}","def 0(command, args, keywords, get_word_index):
    """"""Searches a command's output for a MAC address near a keyword.

    Each line of words in the output is case-insensitively searched for
    any of the given keywords.  Upon a match, get_word_index is invoked
    to pick a word from the line, given the index of the match.  For
    example, lambda i: 0 would get the first word on the line, while
    lambda i: i - 1 would get the word preceding the keyword.
    """"""
    stdout = _get_command_stdout(command, args)
    if stdout is None:
        return None

    first_local_mac = None
    for line in stdout:
        words = line.lower().rstrip().split()
        for i in range(len(words)):
            if words[i] in keywords:
                try:
                    word = words[get_word_index(i)]
                    mac = int(word.replace(_MAC_DELIM, b''), 16)
                except (ValueError, IndexError):
                    # Virtual interfaces, such as those provided by
                    # VPNs, do not have a colon-delimited MAC address
                    # as expected, but a 16-byte HWAddr separated by
                    # dashes. These should be ignored in favor of a
                    # real MAC address
                    pass
                else:
                    if _is_universal(mac):
                        return mac
                    first_local_mac = first_local_mac or mac
    return first_local_mac or None","def 1():
    """"""Get the hardware address on Unix by running lanscan.""""""
    # This might work on HP-UX.
    return 0('lanscan', '-ai', [b'lan0'], lambda i: 0)","{""_find_mac_near_keyword"": ""0""}"
230,230,"def _netstat_getnode():
    """"""Get the hardware address on Unix by running netstat.""""""
    # This works on AIX and might work on Tru64 UNIX.
    return _find_mac_under_heading('netstat', '-ian', b'Address')","def _find_mac_under_heading(command, args, heading):
    """"""Looks for a MAC address under a heading in a command's output.

    The first line of words in the output is searched for the given
    heading. Words at the same word index as the heading in subsequent
    lines are then examined to see if they look like MAC addresses.
    """"""
    stdout = _get_command_stdout(command, args)
    if stdout is None:
        return None

    keywords = stdout.readline().rstrip().split()
    try:
        column_index = keywords.index(heading)
    except ValueError:
        return None

    first_local_mac = None
    for line in stdout:
        words = line.rstrip().split()
        try:
            word = words[column_index]
        except IndexError:
            continue

        mac = _parse_mac(word)
        if mac is None:
            continue
        if _is_universal(mac):
            return mac
        if first_local_mac is None:
            first_local_mac = mac

    return first_local_mac","def find_mac_address_under_heading(command, args, heading):
    """"""Looks for a MAC address under a heading in a command's output.

    The first line of words in the output is searched for the given
    heading. Words at the same word index as the heading in subsequent
    lines are then examined to see if they look like MAC addresses.
    """"""
    stdout = _get_command_stdout(command, args)
    if stdout is None:
        return None

    keywords = stdout.readline().rstrip().split()
    try:
        column_index = keywords.index(heading)
    except ValueError:
        return None

    first_local_mac = None
    for line in stdout:
        words = line.rstrip().split()
        try:
            word = words[column_index]
        except IndexError:
            continue

        mac = _parse_mac(word)
        if mac is None:
            continue
        if _is_universal(mac):
            return mac
        if first_local_mac is None:
            first_local_mac = mac

    return first_local_mac","def get_hardware_address_using_netstat():
    """"""Get the hardware address on Unix by running netstat.""""""
    # This works on AIX and might work on Tru64 UNIX.
    return find_mac_address_under_heading('netstat', '-ian', b'Address')","{""_find_mac_under_heading"": ""find_mac_address_under_heading""}","def 0(command, args, heading):
    """"""Looks for a MAC address under a heading in a command's output.

    The first line of words in the output is searched for the given
    heading. Words at the same word index as the heading in subsequent
    lines are then examined to see if they look like MAC addresses.
    """"""
    stdout = _get_command_stdout(command, args)
    if stdout is None:
        return None

    keywords = stdout.readline().rstrip().split()
    try:
        column_index = keywords.index(heading)
    except ValueError:
        return None

    first_local_mac = None
    for line in stdout:
        words = line.rstrip().split()
        try:
            word = words[column_index]
        except IndexError:
            continue

        mac = _parse_mac(word)
        if mac is None:
            continue
        if _is_universal(mac):
            return mac
        if first_local_mac is None:
            first_local_mac = mac

    return first_local_mac","def 1():
    """"""Get the hardware address on Unix by running netstat.""""""
    # This works on AIX and might work on Tru64 UNIX.
    return 0('netstat', '-ian', b'Address')","{""_find_mac_under_heading"": ""0""}"
231,231,"def _ipconfig_getnode():
    """"""[DEPRECATED] Get the hardware address on Windows.""""""
    # bpo-40501: UuidCreateSequential() is now the only supported approach
    return _windll_getnode()","def _windll_getnode():
    """"""Get the hardware address on Windows using the _uuid extension module.""""""
    if _UuidCreate:
        uuid_bytes = _UuidCreate()
        return UUID(bytes_le=uuid_bytes).node","def get_hardware_address_windows():
    """"""Get the hardware address on Windows using the _uuid extension module.""""""
    if _UuidCreate:
        uuid_bytes = _UuidCreate()
        return UUID(bytes_le=uuid_bytes).node","def deprecated_get_hardware_address_on_windows():
    """"""[DEPRECATED] Get the hardware address on Windows.""""""
    # bpo-40501: UuidCreateSequential() is now the only supported approach
    return get_hardware_address_windows()","{""_windll_getnode"": ""get_hardware_address_windows""}","def 0():
    """"""Get the hardware address on Windows using the _uuid extension module.""""""
    if _UuidCreate:
        uuid_bytes = _UuidCreate()
        return UUID(bytes_le=uuid_bytes).node","def 1():
    """"""[DEPRECATED] Get the hardware address on Windows.""""""
    # bpo-40501: UuidCreateSequential() is now the only supported approach
    return 0()","{""_windll_getnode"": ""0""}"
232,232,"def _netbios_getnode():
    """"""[DEPRECATED] Get the hardware address on Windows.""""""
    # bpo-40501: UuidCreateSequential() is now the only supported approach
    return _windll_getnode()","def _windll_getnode():
    """"""Get the hardware address on Windows using the _uuid extension module.""""""
    if _UuidCreate:
        uuid_bytes = _UuidCreate()
        return UUID(bytes_le=uuid_bytes).node","def get_hardware_address_windows():
    """"""Get the hardware address on Windows using the _uuid extension module.""""""
    if _UuidCreate:
        uuid_bytes = _UuidCreate()
        return UUID(bytes_le=uuid_bytes).node","def deprecated_get_hardware_address_on_windows():
    """"""[DEPRECATED] Get the hardware address on Windows.""""""
    # bpo-40501: UuidCreateSequential() is now the only supported approach
    return get_hardware_address_windows()","{""_windll_getnode"": ""get_hardware_address_windows""}","def 0():
    """"""Get the hardware address on Windows using the _uuid extension module.""""""
    if _UuidCreate:
        uuid_bytes = _UuidCreate()
        return UUID(bytes_le=uuid_bytes).node","def 1():
    """"""[DEPRECATED] Get the hardware address on Windows.""""""
    # bpo-40501: UuidCreateSequential() is now the only supported approach
    return 0()","{""_windll_getnode"": ""0""}"
233,233,"def init_builtin(name):
    """"""**DEPRECATED**

    Load and return a built-in module by name, or None is such module doesn't
    exist
    """"""
    try:
        return _builtin_from_name(name)
    except ImportError:
        return None","def _builtin_from_name(name):
    spec = BuiltinImporter.find_spec(name)
    if spec is None:
        raise ImportError('no built-in module named ' + name)
    return _load_unlocked(spec)","def load_builtin_module(name):
    spec = BuiltinImporter.find_spec(name)
    if spec is None:
        raise ImportError('no built-in module named ' + name)
    return _load_unlocked(spec)","def deprecated_load_and_return_builtin_module(name):
    """"""**DEPRECATED**

    Load and return a built-in module by name, or None is such module doesn't
    exist
    """"""
    try:
        return load_builtin_module(name)
    except ImportError:
        return None","{""_builtin_from_name"": ""load_builtin_module""}","def 0(name):
    spec = BuiltinImporter.find_spec(name)
    if spec is None:
        raise ImportError('no built-in module named ' + name)
    return _load_unlocked(spec)","def 1(name):
    """"""**DEPRECATED**

    Load and return a built-in module by name, or None is such module doesn't
    exist
    """"""
    try:
        return 0(name)
    except ImportError:
        return None","{""_builtin_from_name"": ""0""}"
234,234,"def readmailcapfile(fp):
    """"""Read a mailcap file and return a dictionary keyed by MIME type.""""""
    warnings.warn('readmailcapfile is deprecated, use getcaps instead',
                  DeprecationWarning, 2)
    caps, _ = _readmailcapfile(fp, None)
    return caps","def _readmailcapfile(fp, lineno):
    """"""Read a mailcap file and return a dictionary keyed by MIME type.

    Each MIME type is mapped to an entry consisting of a list of
    dictionaries; the list will contain more than one such dictionary
    if a given MIME type appears more than once in the mailcap file.
    Each dictionary contains key-value pairs for that MIME type, where
    the viewing command is stored with the key ""view"".
    """"""
    caps = {}
    while 1:
        line = fp.readline()
        if not line: break
        # Ignore comments and blank lines
        if line[0] == '#' or line.strip() == '':
            continue
        nextline = line
        # Join continuation lines
        while nextline[-2:] == '\\\n':
            nextline = fp.readline()
            if not nextline: nextline = '\n'
            line = line[:-2] + nextline
        # Parse the line
        key, fields = parseline(line)
        if not (key and fields):
            continue
        if lineno is not None:
            fields['lineno'] = lineno
            lineno += 1
        # Normalize the key
        types = key.split('/')
        for j in range(len(types)):
            types[j] = types[j].strip()
        key = '/'.join(types).lower()
        # Update the database
        if key in caps:
            caps[key].append(fields)
        else:
            caps[key] = [fields]
    return caps, lineno","def read_mailcap_file(fp, lineno):
    """"""Read a mailcap file and return a dictionary keyed by MIME type.

    Each MIME type is mapped to an entry consisting of a list of
    dictionaries; the list will contain more than one such dictionary
    if a given MIME type appears more than once in the mailcap file.
    Each dictionary contains key-value pairs for that MIME type, where
    the viewing command is stored with the key ""view"".
    """"""
    caps = {}
    while 1:
        line = fp.readline()
        if not line: break
        # Ignore comments and blank lines
        if line[0] == '#' or line.strip() == '':
            continue
        nextline = line
        # Join continuation lines
        while nextline[-2:] == '\\\n':
            nextline = fp.readline()
            if not nextline: nextline = '\n'
            line = line[:-2] + nextline
        # Parse the line
        key, fields = parseline(line)
        if not (key and fields):
            continue
        if lineno is not None:
            fields['lineno'] = lineno
            lineno += 1
        # Normalize the key
        types = key.split('/')
        for j in range(len(types)):
            types[j] = types[j].strip()
        key = '/'.join(types).lower()
        # Update the database
        if key in caps:
            caps[key].append(fields)
        else:
            caps[key] = [fields]
    return caps, lineno","def deprecated_read_mailcapfile_use_getcaps(fp):
    """"""Read a mailcap file and return a dictionary keyed by MIME type.""""""
    warnings.warn('deprecated_read_mailcapfile_use_getcaps is deprecated, use getcaps instead',
                  DeprecationWarning, 2)
    caps, _ = read_mailcap_file(fp, None)
    return caps","{""_readmailcapfile"": ""read_mailcap_file""}","def 0(fp, lineno):
    """"""Read a mailcap file and return a dictionary keyed by MIME type.

    Each MIME type is mapped to an entry consisting of a list of
    dictionaries; the list will contain more than one such dictionary
    if a given MIME type appears more than once in the mailcap file.
    Each dictionary contains key-value pairs for that MIME type, where
    the viewing command is stored with the key ""view"".
    """"""
    caps = {}
    while 1:
        line = fp.readline()
        if not line: break
        # Ignore comments and blank lines
        if line[0] == '#' or line.strip() == '':
            continue
        nextline = line
        # Join continuation lines
        while nextline[-2:] == '\\\n':
            nextline = fp.readline()
            if not nextline: nextline = '\n'
            line = line[:-2] + nextline
        # Parse the line
        key, fields = parseline(line)
        if not (key and fields):
            continue
        if lineno is not None:
            fields['lineno'] = lineno
            lineno += 1
        # Normalize the key
        types = key.split('/')
        for j in range(len(types)):
            types[j] = types[j].strip()
        key = '/'.join(types).lower()
        # Update the database
        if key in caps:
            caps[key].append(fields)
        else:
            caps[key] = [fields]
    return caps, lineno","def 1(fp):
    """"""Read a mailcap file and return a dictionary keyed by MIME type.""""""
    warnings.warn('readmailcapfile is deprecated, use getcaps instead',
                  DeprecationWarning, 2)
    caps, _ = 0(fp, None)
    return caps","{""_readmailcapfile"": ""0""}"
235,235,"def parseline(line):
    """"""Parse one entry in a mailcap file and return a dictionary.

    The viewing command is stored as the value with the key ""view"",
    and the rest of the fields produce key-value pairs in the dict.
    """"""
    fields = []
    i, n = 0, len(line)
    while i < n:
        field, i = parsefield(line, i, n)
        fields.append(field)
        i = i+1 # Skip semicolon
    if len(fields) < 2:
        return None, None
    key, view, rest = fields[0], fields[1], fields[2:]
    fields = {'view': view}
    for field in rest:
        i = field.find('=')
        if i < 0:
            fkey = field
            fvalue = """"
        else:
            fkey = field[:i].strip()
            fvalue = field[i+1:].strip()
        if fkey in fields:
            # Ignore it
            pass
        else:
            fields[fkey] = fvalue
    return key, fields","def parsefield(line, i, n):
    """"""Separate one key-value pair in a mailcap entry.""""""
    start = i
    while i < n:
        c = line[i]
        if c == ';':
            break
        elif c == '\\':
            i = i+2
        else:
            i = i+1
    return line[start:i].strip(), i","def parse_mailcap_field(line, i, n):
    """"""Separate one key-value pair in a mailcap entry.""""""
    start = i
    while i < n:
        c = line[i]
        if c == ';':
            break
        elif c == '\\':
            i = i+2
        else:
            i = i+1
    return line[start:i].strip(), i","def parse_one_entry_in_mailcap_file_return_dictionary(line):
    """"""Parse one entry in a mailcap file and return a dictionary.

    The viewing command is stored as the value with the key ""view"",
    and the rest of the fields produce key-value pairs in the dict.
    """"""
    fields = []
    i, n = 0, len(line)
    while i < n:
        field, i = parse_mailcap_field(line, i, n)
        fields.append(field)
        i = i+1 # Skip semicolon
    if len(fields) < 2:
        return None, None
    key, view, rest = fields[0], fields[1], fields[2:]
    fields = {'view': view}
    for field in rest:
        i = field.find('=')
        if i < 0:
            fkey = field
            fvalue = """"
        else:
            fkey = field[:i].strip()
            fvalue = field[i+1:].strip()
        if fkey in fields:
            # Ignore it
            pass
        else:
            fields[fkey] = fvalue
    return key, fields","{""parsefield"": ""parse_mailcap_field""}","def 0(line, i, n):
    """"""Separate one key-value pair in a mailcap entry.""""""
    start = i
    while i < n:
        c = line[i]
        if c == ';':
            break
        elif c == '\\':
            i = i+2
        else:
            i = i+1
    return line[start:i].strip(), i","def 1(line):
    """"""Parse one entry in a mailcap file and return a dictionary.

    The viewing command is stored as the value with the key ""view"",
    and the rest of the fields produce key-value pairs in the dict.
    """"""
    fields = []
    i, n = 0, len(line)
    while i < n:
        field, i = 0(line, i, n)
        fields.append(field)
        i = i+1 # Skip semicolon
    if len(fields) < 2:
        return None, None
    key, view, rest = fields[0], fields[1], fields[2:]
    fields = {'view': view}
    for field in rest:
        i = field.find('=')
        if i < 0:
            fkey = field
            fvalue = """"
        else:
            fkey = field[:i].strip()
            fvalue = field[i+1:].strip()
        if fkey in fields:
            # Ignore it
            pass
        else:
            fields[fkey] = fvalue
    return key, fields","{""parsefield"": ""0""}"
236,236,"def _run_module_code(code, init_globals=None,
                    mod_name=None, mod_spec=None,
                    pkg_name=None, script_name=None):
    """"""Helper to run code in new namespace with sys modified""""""
    fname = script_name if mod_spec is None else mod_spec.origin
    with _TempModule(mod_name) as temp_module, _ModifiedArgv0(fname):
        mod_globals = temp_module.module.__dict__
        _run_code(code, mod_globals, init_globals,
                  mod_name, mod_spec, pkg_name, script_name)
    # Copy the globals of the temporary module, as they
    # may be cleared when the temporary module goes away
    return mod_globals.copy()","def _run_code(code, run_globals, init_globals=None,
              mod_name=None, mod_spec=None,
              pkg_name=None, script_name=None):
    """"""Helper to run code in nominated namespace""""""
    if init_globals is not None:
        run_globals.update(init_globals)
    if mod_spec is None:
        loader = None
        fname = script_name
        cached = None
    else:
        loader = mod_spec.loader
        fname = mod_spec.origin
        cached = mod_spec.cached
        if pkg_name is None:
            pkg_name = mod_spec.parent
    run_globals.update(__name__ = mod_name,
                       __file__ = fname,
                       __cached__ = cached,
                       __doc__ = None,
                       __loader__ = loader,
                       __package__ = pkg_name,
                       __spec__ = mod_spec)
    exec(code, run_globals)
    return run_globals","def run_code_in_namespace(code, run_globals, init_globals=None,
              mod_name=None, mod_spec=None,
              pkg_name=None, script_name=None):
    """"""Helper to run code in nominated namespace""""""
    if init_globals is not None:
        run_globals.update(init_globals)
    if mod_spec is None:
        loader = None
        fname = script_name
        cached = None
    else:
        loader = mod_spec.loader
        fname = mod_spec.origin
        cached = mod_spec.cached
        if pkg_name is None:
            pkg_name = mod_spec.parent
    run_globals.update(__name__ = mod_name,
                       __file__ = fname,
                       __cached__ = cached,
                       __doc__ = None,
                       __loader__ = loader,
                       __package__ = pkg_name,
                       __spec__ = mod_spec)
    exec(code, run_globals)
    return run_globals","def helper_to_run_code_in_new_namespace(code, init_globals=None,
                    mod_name=None, mod_spec=None,
                    pkg_name=None, script_name=None):
    """"""Helper to run code in new namespace with sys modified""""""
    fname = script_name if mod_spec is None else mod_spec.origin
    with _TempModule(mod_name) as temp_module, _ModifiedArgv0(fname):
        mod_globals = temp_module.module.__dict__
        run_code_in_namespace(code, mod_globals, init_globals,
                  mod_name, mod_spec, pkg_name, script_name)
    # Copy the globals of the temporary module, as they
    # may be cleared when the temporary module goes away
    return mod_globals.copy()","{""_run_code"": ""run_code_in_namespace""}","def 0(code, run_globals, init_globals=None,
              mod_name=None, mod_spec=None,
              pkg_name=None, script_name=None):
    """"""Helper to run code in nominated namespace""""""
    if init_globals is not None:
        run_globals.update(init_globals)
    if mod_spec is None:
        loader = None
        fname = script_name
        cached = None
    else:
        loader = mod_spec.loader
        fname = mod_spec.origin
        cached = mod_spec.cached
        if pkg_name is None:
            pkg_name = mod_spec.parent
    run_globals.update(__name__ = mod_name,
                       __file__ = fname,
                       __cached__ = cached,
                       __doc__ = None,
                       __loader__ = loader,
                       __package__ = pkg_name,
                       __spec__ = mod_spec)
    exec(code, run_globals)
    return run_globals","def 1(code, init_globals=None,
                    mod_name=None, mod_spec=None,
                    pkg_name=None, script_name=None):
    """"""Helper to run code in new namespace with sys modified""""""
    fname = script_name if mod_spec is None else mod_spec.origin
    with _TempModule(mod_name) as temp_module, _ModifiedArgv0(fname):
        mod_globals = temp_module.module.__dict__
        0(code, mod_globals, init_globals,
                  mod_name, mod_spec, pkg_name, script_name)
    # Copy the globals of the temporary module, as they
    # may be cleared when the temporary module goes away
    return mod_globals.copy()","{""_run_code"": ""0""}"
237,237,"def insertion_unsort(str, extended):
    """"""3.2 Insertion unsort coding""""""
    oldchar = 0x80
    result = []
    oldindex = -1
    for c in extended:
        index = pos = -1
        char = ord(c)
        curlen = selective_len(str, char)
        delta = (curlen+1) * (char - oldchar)
        while 1:
            index,pos = selective_find(str,c,index,pos)
            if index == -1:
                break
            delta += index - oldindex
            result.append(delta-1)
            oldindex = index
            delta = 0
        oldchar = char

    return result","def selective_len(str, max):
    """"""Return the length of str, considering only characters below max.""""""
    res = 0
    for c in str:
        if ord(c) < max:
            res += 1
    return res

def selective_find(str, char, index, pos):
    """"""Return a pair (index, pos), indicating the next occurrence of
    char in str. index is the position of the character considering
    only ordinals up to and including char, and pos is the position in
    the full string. index/pos is the starting position in the full
    string.""""""

    l = len(str)
    while 1:
        pos += 1
        if pos == l:
            return (-1, -1)
        c = str[pos]
        if c == char:
            return index+1, pos
        elif c < char:
            index += 1","def count_chars_below_max(str, max):
    """"""Return the length of str, considering only characters below max.""""""
    res = 0
    for c in str:
        if ord(c) < max:
            res += 1
    return res

def find_next_occurrence(str, char, index, pos):
    """"""Return a pair (index, pos), indicating the next occurrence of
    char in str. index is the position of the character considering
    only ordinals up to and including char, and pos is the position in
    the full string. index/pos is the starting position in the full
    string.""""""

    l = len(str)
    while 1:
        pos += 1
        if pos == l:
            return (-1, -1)
        c = str[pos]
        if c == char:
            return index+1, pos
        elif c < char:
            index += 1","def insertion_unsort_coding_with_oldchar(str, extended):
    """"""3.2 Insertion unsort coding""""""
    oldchar = 0x80
    result = []
    oldindex = -1
    for c in extended:
        index = pos = -1
        char = ord(c)
        curlen = count_chars_below_max(str, char)
        delta = (curlen+1) * (char - oldchar)
        while 1:
            index,pos = find_next_occurrence(str,c,index,pos)
            if index == -1:
                break
            delta += index - oldindex
            result.append(delta-1)
            oldindex = index
            delta = 0
        oldchar = char

    return result","{""selective_len"": ""count_chars_below_max"", ""selective_find"": ""find_next_occurrence""}","def 0(str, max):
    """"""Return the length of str, considering only characters below max.""""""
    res = 0
    for c in str:
        if ord(c) < max:
            res += 1
    return res

def 1(str, char, index, pos):
    """"""Return a pair (index, pos), indicating the next occurrence of
    char in str. index is the position of the character considering
    only ordinals up to and including char, and pos is the position in
    the full string. index/pos is the starting position in the full
    string.""""""

    l = len(str)
    while 1:
        pos += 1
        if pos == l:
            return (-1, -1)
        c = str[pos]
        if c == char:
            return index+1, pos
        elif c < char:
            index += 1","def 2(str, extended):
    """"""3.2 Insertion unsort coding""""""
    oldchar = 0x80
    result = []
    oldindex = -1
    for c in extended:
        index = pos = -1
        char = ord(c)
        curlen = 0(str, char)
        delta = (curlen+1) * (char - oldchar)
        while 1:
            index,pos = 1(str,c,index,pos)
            if index == -1:
                break
            delta += index - oldindex
            result.append(delta-1)
            oldindex = index
            delta = 0
        oldchar = char

    return result","{""selective_len"": ""0"", ""selective_find"": ""1""}"
238,238,"def generate_generalized_integer(N, bias):
    """"""3.3 Generalized variable-length integers""""""
    result = bytearray()
    j = 0
    while 1:
        t = T(j, bias)
        if N < t:
            result.append(digits[N])
            return bytes(result)
        result.append(digits[t + ((N - t) % (36 - t))])
        N = (N - t) // (36 - t)
        j += 1","def T(j, bias):
    # Punycode parameters: tmin = 1, tmax = 26, base = 36
    res = 36 * (j + 1) - bias
    if res < 1: return 1
    if res > 26: return 26
    return res","def calculate_punycode_parameter(j, bias):
    # Punycode parameters: tmin = 1, tmax = 26, base = 36
    res = 36 * (j + 1) - bias
    if res < 1: return 1
    if res > 26: return 26
    return res","def generate_generalized_variable_length_integer(N, bias):
    """"""3.3 Generalized variable-length integers""""""
    result = bytearray()
    j = 0
    while 1:
        t = calculate_punycode_parameter(j, bias)
        if N < t:
            result.append(digits[N])
            return bytes(result)
        result.append(digits[t + ((N - t) % (36 - t))])
        N = (N - t) // (36 - t)
        j += 1","{""T"": ""calculate_punycode_parameter""}","def 0(j, bias):
    # Punycode parameters: tmin = 1, tmax = 26, base = 36
    res = 36 * (j + 1) - bias
    if res < 1: return 1
    if res > 26: return 26
    return res","def 1(N, bias):
    """"""3.3 Generalized variable-length integers""""""
    result = bytearray()
    j = 0
    while 1:
        t = 0(j, bias)
        if N < t:
            result.append(digits[N])
            return bytes(result)
        result.append(digits[t + ((N - t) % (36 - t))])
        N = (N - t) // (36 - t)
        j += 1","{""T"": ""0""}"
239,239,"def generate_integers(baselen, deltas):
    """"""3.4 Bias adaptation""""""
    # Punycode parameters: initial bias = 72, damp = 700, skew = 38
    result = bytearray()
    bias = 72
    for points, delta in enumerate(deltas):
        s = generate_generalized_integer(delta, bias)
        result.extend(s)
        bias = adapt(delta, points==0, baselen+points+1)
    return bytes(result)","def enumerate():
    """"""Return a list of all Thread objects currently alive.

    The list includes daemonic threads, dummy thread objects created by
    current_thread(), and the main thread. It excludes terminated threads and
    threads that have not yet been started.

    """"""
    with _active_limbo_lock:
        return list(_active.values()) + list(_limbo.values())

def generate_generalized_integer(N, bias):
    """"""3.3 Generalized variable-length integers""""""
    result = bytearray()
    j = 0
    while 1:
        t = T(j, bias)
        if N < t:
            result.append(digits[N])
            return bytes(result)
        result.append(digits[t + ((N - t) % (36 - t))])
        N = (N - t) // (36 - t)
        j += 1

def adapt(delta, first, numchars):
    if first:
        delta //= 700
    else:
        delta //= 2
    delta += delta // numchars
    # ((base - tmin) * tmax) // 2 == 455
    divisions = 0
    while delta > 455:
        delta = delta // 35 # base - tmin
        divisions += 36
    bias = divisions + (36 * delta // (delta + 38))
    return bias","def get_alive_threads():
    """"""Return a list of all Thread objects currently alive.

    The list includes daemonic threads, dummy thread objects created by
    current_thread(), and the main thread. It excludes terminated threads and
    threads that have not yet been started.

    """"""
    with _active_limbo_lock:
        return list(_active.values()) + list(_limbo.values())

def generate_generalized_variable_length_integer(N, bias):
    """"""3.3 Generalized variable-length integers""""""
    result = bytearray()
    j = 0
    while 1:
        t = T(j, bias)
        if N < t:
            result.append(digits[N])
            return bytes(result)
        result.append(digits[t + ((N - t) % (36 - t))])
        N = (N - t) // (36 - t)
        j += 1

def calculate_punycode_bias(delta, first, numchars):
    if first:
        delta //= 700
    else:
        delta //= 2
    delta += delta // numchars
    # ((base - tmin) * tmax) // 2 == 455
    divisions = 0
    while delta > 455:
        delta = delta // 35 # base - tmin
        divisions += 36
    bias = divisions + (36 * delta // (delta + 38))
    return bias","def bias_adaptation_generate_integers(baselen, deltas):
    """"""3.4 Bias calculate_punycode_biasation""""""
    # Punycode parameters: initial bias = 72, damp = 700, skew = 38
    result = bytearray()
    bias = 72
    for points, delta in get_alive_threads(deltas):
        s = generate_generalized_variable_length_integer(delta, bias)
        result.extend(s)
        bias = calculate_punycode_bias(delta, points==0, baselen+points+1)
    return bytes(result)","{""enumerate"": ""get_alive_threads"", ""generate_generalized_integer"": ""generate_generalized_variable_length_integer"", ""adapt"": ""calculate_punycode_bias""}","def 0():
    """"""Return a list of all Thread objects currently alive.

    The list includes daemonic threads, dummy thread objects created by
    current_thread(), and the main thread. It excludes terminated threads and
    threads that have not yet been started.

    """"""
    with _active_limbo_lock:
        return list(_active.values()) + list(_limbo.values())

def 1(N, bias):
    """"""3.3 Generalized variable-length integers""""""
    result = bytearray()
    j = 0
    while 1:
        t = T(j, bias)
        if N < t:
            result.append(digits[N])
            return bytes(result)
        result.append(digits[t + ((N - t) % (36 - t))])
        N = (N - t) // (36 - t)
        j += 1

def 2(delta, first, numchars):
    if first:
        delta //= 700
    else:
        delta //= 2
    delta += delta // numchars
    # ((base - tmin) * tmax) // 2 == 455
    divisions = 0
    while delta > 455:
        delta = delta // 35 # base - tmin
        divisions += 36
    bias = divisions + (36 * delta // (delta + 38))
    return bias","def 3(baselen, deltas):
    """"""3.4 Bias adaptation""""""
    # Punycode parameters: initial bias = 72, damp = 700, skew = 38
    result = bytearray()
    bias = 72
    for points, delta in 0(deltas):
        s = 1(delta, bias)
        result.extend(s)
        bias = 2(delta, points==0, baselen+points+1)
    return bytes(result)","{""enumerate"": ""0"", ""generate_generalized_integer"": ""1"", ""adapt"": ""2""}"
240,240,"def decode_generalized_number(extended, extpos, bias, errors):
    """"""3.3 Generalized variable-length integers""""""
    result = 0
    w = 1
    j = 0
    while 1:
        try:
            char = ord(extended[extpos])
        except IndexError:
            if errors == ""strict"":
                raise UnicodeError(""incomplete punicode string"")
            return extpos + 1, None
        extpos += 1
        if 0x41 <= char <= 0x5A: # A-Z
            digit = char - 0x41
        elif 0x30 <= char <= 0x39:
            digit = char - 22 # 0x30-26
        elif errors == ""strict"":
            raise UnicodeError(""Invalid extended code point '%s'""
                               % extended[extpos-1])
        else:
            return extpos, None
        t = T(j, bias)
        result += digit * w
        if digit < t:
            return extpos, result
        w = w * (36 - t)
        j += 1","def T(j, bias):
    # Punycode parameters: tmin = 1, tmax = 26, base = 36
    res = 36 * (j + 1) - bias
    if res < 1: return 1
    if res > 26: return 26
    return res","def calculate_punycode_parameter(j, bias):
    # Punycode parameters: tmin = 1, tmax = 26, base = 36
    res = 36 * (j + 1) - bias
    if res < 1: return 1
    if res > 26: return 26
    return res","def decode_generalized_variable_length_integer(extended, extpos, bias, errors):
    """"""3.3 Generalized variable-length integers""""""
    result = 0
    w = 1
    j = 0
    while 1:
        try:
            char = ord(extended[extpos])
        except IndexError:
            if errors == ""strict"":
                raise UnicodeError(""incomplete punicode string"")
            return extpos + 1, None
        extpos += 1
        if 0x41 <= char <= 0x5A: # A-Z
            digit = char - 0x41
        elif 0x30 <= char <= 0x39:
            digit = char - 22 # 0x30-26
        elif errors == ""strict"":
            raise UnicodeError(""Invalid extended code point '%s'""
                               % extended[extpos-1])
        else:
            return extpos, None
        t = calculate_punycode_parameter(j, bias)
        result += digit * w
        if digit < t:
            return extpos, result
        w = w * (36 - t)
        j += 1","{""T"": ""calculate_punycode_parameter""}","def 0(j, bias):
    # Punycode parameters: tmin = 1, tmax = 26, base = 36
    res = 36 * (j + 1) - bias
    if res < 1: return 1
    if res > 26: return 26
    return res","def 1(extended, extpos, bias, errors):
    """"""3.3 Generalized variable-length integers""""""
    result = 0
    w = 1
    j = 0
    while 1:
        try:
            char = ord(extended[extpos])
        except IndexError:
            if errors == ""strict"":
                raise UnicodeError(""incomplete punicode string"")
            return extpos + 1, None
        extpos += 1
        if 0x41 <= char <= 0x5A: # A-Z
            digit = char - 0x41
        elif 0x30 <= char <= 0x39:
            digit = char - 22 # 0x30-26
        elif errors == ""strict"":
            raise UnicodeError(""Invalid extended code point '%s'""
                               % extended[extpos-1])
        else:
            return extpos, None
        t = 0(j, bias)
        result += digit * w
        if digit < t:
            return extpos, result
        w = w * (36 - t)
        j += 1","{""T"": ""0""}"
241,241,"def insertion_sort(base, extended, errors):
    """"""3.2 Insertion unsort coding""""""
    char = 0x80
    pos = -1
    bias = 72
    extpos = 0
    while extpos < len(extended):
        newpos, delta = decode_generalized_number(extended, extpos,
                                                  bias, errors)
        if delta is None:
            # There was an error in decoding. We can't continue because
            # synchronization is lost.
            return base
        pos += delta+1
        char += pos // (len(base) + 1)
        if char > 0x10FFFF:
            if errors == ""strict"":
                raise UnicodeError(""Invalid character U+%x"" % char)
            char = ord('?')
        pos = pos % (len(base) + 1)
        base = base[:pos] + chr(char) + base[pos:]
        bias = adapt(delta, (extpos == 0), len(base))
        extpos = newpos
    return base","def decode_generalized_number(extended, extpos, bias, errors):
    """"""3.3 Generalized variable-length integers""""""
    result = 0
    w = 1
    j = 0
    while 1:
        try:
            char = ord(extended[extpos])
        except IndexError:
            if errors == ""strict"":
                raise UnicodeError(""incomplete punicode string"")
            return extpos + 1, None
        extpos += 1
        if 0x41 <= char <= 0x5A: # A-Z
            digit = char - 0x41
        elif 0x30 <= char <= 0x39:
            digit = char - 22 # 0x30-26
        elif errors == ""strict"":
            raise UnicodeError(""Invalid extended code point '%s'""
                               % extended[extpos-1])
        else:
            return extpos, None
        t = T(j, bias)
        result += digit * w
        if digit < t:
            return extpos, result
        w = w * (36 - t)
        j += 1

def adapt(delta, first, numchars):
    if first:
        delta //= 700
    else:
        delta //= 2
    delta += delta // numchars
    # ((base - tmin) * tmax) // 2 == 455
    divisions = 0
    while delta > 455:
        delta = delta // 35 # base - tmin
        divisions += 36
    bias = divisions + (36 * delta // (delta + 38))
    return bias","def decode_generalized_variable_length_integer(extended, extpos, bias, errors):
    """"""3.3 Generalized variable-length integers""""""
    result = 0
    w = 1
    j = 0
    while 1:
        try:
            char = ord(extended[extpos])
        except IndexError:
            if errors == ""strict"":
                raise UnicodeError(""incomplete punicode string"")
            return extpos + 1, None
        extpos += 1
        if 0x41 <= char <= 0x5A: # A-Z
            digit = char - 0x41
        elif 0x30 <= char <= 0x39:
            digit = char - 22 # 0x30-26
        elif errors == ""strict"":
            raise UnicodeError(""Invalid extended code point '%s'""
                               % extended[extpos-1])
        else:
            return extpos, None
        t = T(j, bias)
        result += digit * w
        if digit < t:
            return extpos, result
        w = w * (36 - t)
        j += 1

def calculate_punycode_bias(delta, first, numchars):
    if first:
        delta //= 700
    else:
        delta //= 2
    delta += delta // numchars
    # ((base - tmin) * tmax) // 2 == 455
    divisions = 0
    while delta > 455:
        delta = delta // 35 # base - tmin
        divisions += 36
    bias = divisions + (36 * delta // (delta + 38))
    return bias","def insertion_unsort_coding_with_char(base, extended, errors):
    """"""3.2 Insertion unsort coding""""""
    char = 0x80
    pos = -1
    bias = 72
    extpos = 0
    while extpos < len(extended):
        newpos, delta = decode_generalized_variable_length_integer(extended, extpos,
                                                  bias, errors)
        if delta is None:
            # There was an error in decoding. We can't continue because
            # synchronization is lost.
            return base
        pos += delta+1
        char += pos // (len(base) + 1)
        if char > 0x10FFFF:
            if errors == ""strict"":
                raise UnicodeError(""Invalid character U+%x"" % char)
            char = ord('?')
        pos = pos % (len(base) + 1)
        base = base[:pos] + chr(char) + base[pos:]
        bias = calculate_punycode_bias(delta, (extpos == 0), len(base))
        extpos = newpos
    return base","{""decode_generalized_number"": ""decode_generalized_variable_length_integer"", ""adapt"": ""calculate_punycode_bias""}","def 0(extended, extpos, bias, errors):
    """"""3.3 Generalized variable-length integers""""""
    result = 0
    w = 1
    j = 0
    while 1:
        try:
            char = ord(extended[extpos])
        except IndexError:
            if errors == ""strict"":
                raise UnicodeError(""incomplete punicode string"")
            return extpos + 1, None
        extpos += 1
        if 0x41 <= char <= 0x5A: # A-Z
            digit = char - 0x41
        elif 0x30 <= char <= 0x39:
            digit = char - 22 # 0x30-26
        elif errors == ""strict"":
            raise UnicodeError(""Invalid extended code point '%s'""
                               % extended[extpos-1])
        else:
            return extpos, None
        t = T(j, bias)
        result += digit * w
        if digit < t:
            return extpos, result
        w = w * (36 - t)
        j += 1

def 1(delta, first, numchars):
    if first:
        delta //= 700
    else:
        delta //= 2
    delta += delta // numchars
    # ((base - tmin) * tmax) // 2 == 455
    divisions = 0
    while delta > 455:
        delta = delta // 35 # base - tmin
        divisions += 36
    bias = divisions + (36 * delta // (delta + 38))
    return bias","def 2(base, extended, errors):
    """"""3.2 Insertion unsort coding""""""
    char = 0x80
    pos = -1
    bias = 72
    extpos = 0
    while extpos < len(extended):
        newpos, delta = 0(extended, extpos,
                                                  bias, errors)
        if delta is None:
            # There was an error in decoding. We can't continue because
            # synchronization is lost.
            return base
        pos += delta+1
        char += pos // (len(base) + 1)
        if char > 0x10FFFF:
            if errors == ""strict"":
                raise UnicodeError(""Invalid character U+%x"" % char)
            char = ord('?')
        pos = pos % (len(base) + 1)
        base = base[:pos] + chr(char) + base[pos:]
        bias = 1(delta, (extpos == 0), len(base))
        extpos = newpos
    return base","{""decode_generalized_number"": ""0"", ""adapt"": ""1""}"
242,242,"def normalize_encoding(encoding):

    """""" Normalize an encoding name.

        Normalization works as follows: all non-alphanumeric
        characters except the dot used for Python package names are
        collapsed and replaced with a single underscore, e.g. '  -;#'
        becomes '_'. Leading and trailing underscores are removed.

        Note that encoding names should be ASCII only.

    """"""
    if isinstance(encoding, bytes):
        encoding = str(encoding, ""ascii"")

    chars = []
    punct = False
    for c in encoding:
        if c.isalnum() or c == '.':
            if punct and chars:
                chars.append('_')
            if c.isascii():
                chars.append(c)
            punct = False
        else:
            punct = True
    return ''.join(chars)","def str(val):
    """"""Convert float to string, taking the locale into account.""""""
    return _format(""%.12g"", val)","def locale_aware_float_to_string(val):
    """"""Convert float to locale_aware_float_to_stringing, taking the locale into account.""""""
    return _format(""%.12g"", val)","def normalize_encoding_name_ascii_only(encoding):

    """""" Normalize an encoding name.

        Normalization works as follows: all non-alphanumeric
        characters except the dot used for Python package names are
        collapsed and replaced with a single underscore, e.g. '  -;#'
        becomes '_'. Leading and trailing underscores are removed.

        Note that encoding names should be ASCII only.

    """"""
    if isinstance(encoding, bytes):
        encoding = locale_aware_float_to_string(encoding, ""ascii"")

    chars = []
    punct = False
    for c in encoding:
        if c.isalnum() or c == '.':
            if punct and chars:
                chars.append('_')
            if c.isascii():
                chars.append(c)
            punct = False
        else:
            punct = True
    return ''.join(chars)","{""str"": ""locale_aware_float_to_string""}","def 0(val):
    """"""Convert float to string, taking the locale into account.""""""
    return _format(""%.12g"", val)","def 1(encoding):

    """""" Normalize an encoding name.

        Normalization works as follows: all non-alphanumeric
        characters except the dot used for Python package names are
        collapsed and replaced with a single underscore, e.g. '  -;#'
        becomes '_'. Leading and trailing underscores are removed.

        Note that encoding names should be ASCII only.

    """"""
    if isinstance(encoding, bytes):
        encoding = 0(encoding, ""ascii"")

    chars = []
    punct = False
    for c in encoding:
        if c.isalnum() or c == '.':
            if punct and chars:
                chars.append('_')
            if c.isascii():
                chars.append(c)
            punct = False
        else:
            punct = True
    return ''.join(chars)","{""str"": ""0""}"
243,243,"def _find_all_simple(path):
    """"""
    Find all files under 'path'
    """"""
    all_unique = _UniqueDirs.filter(os.walk(path, followlinks=True))
    results = (
        os.path.join(base, file) for base, dirs, files in all_unique for file in files
    )
    return filter(os.path.isfile, results)","def filter(names, pat):
    """"""Construct a list from those elements of the iterable NAMES that match PAT.""""""
    result = []
    pat = os.path.normcase(pat)
    match = _compile_pattern(pat)
    if os.path is posixpath:
        # normcase on posix is NOP. Optimize it away from the loop.
        for name in names:
            if match(name):
                result.append(name)
    else:
        for name in names:
            if match(os.path.normcase(name)):
                result.append(name)
    return result","def filter_names(names, pat):
    """"""Construct a list from those elements of the iterable NAMES that match PAT.""""""
    result = []
    pat = os.path.normcase(pat)
    match = _compile_pattern(pat)
    if os.path is posixpath:
        # normcase on posix is NOP. Optimize it away from the loop.
        for name in names:
            if match(name):
                result.append(name)
    else:
        for name in names:
            if match(os.path.normcase(name)):
                result.append(name)
    return result","def find_all_files_under_path(path):
    """"""
    Find all files under 'path'
    """"""
    all_unique = _UniqueDirs.filter_names(os.walk(path, followlinks=True))
    results = (
        os.path.join(base, file) for base, dirs, files in all_unique for file in files
    )
    return filter_names(os.path.isfile, results)","{""filter"": ""filter_names""}","def 0(names, pat):
    """"""Construct a list from those elements of the iterable NAMES that match PAT.""""""
    result = []
    pat = os.path.normcase(pat)
    match = _compile_pattern(pat)
    if os.path is posixpath:
        # normcase on posix is NOP. Optimize it away from the loop.
        for name in names:
            if match(name):
                result.append(name)
    else:
        for name in names:
            if match(os.path.normcase(name)):
                result.append(name)
    return result","def 1(path):
    """"""
    Find all files under 'path'
    """"""
    all_unique = _UniqueDirs.0(os.walk(path, followlinks=True))
    results = (
        os.path.join(base, file) for base, dirs, files in all_unique for file in files
    )
    return 0(os.path.isfile, results)","{""filter"": ""0""}"
244,244,"def get_versions():
    """""" Try to find out the versions of gcc, ld and dllwrap.

    If not possible it returns None for it.
    """"""
    commands = ['gcc -dumpversion', 'ld -v', 'dllwrap --version']
    return tuple([_find_exe_version(cmd) for cmd in commands])","def _find_exe_version(cmd):
    """"""Find the version of an executable by running `cmd` in the shell.

    If the command is not found, or the output does not match
    `RE_VERSION`, returns None.
    """"""
    executable = cmd.split()[0]
    if find_executable(executable) is None:
        return None
    out = Popen(cmd, shell=True, stdout=PIPE).stdout
    try:
        out_string = out.read()
    finally:
        out.close()
    result = RE_VERSION.search(out_string)
    if result is None:
        return None
    # LooseVersion works with strings
    # so we need to decode our bytes
    return LooseVersion(result.group(1).decode())","def get_executable_version(cmd):
    """"""Find the version of an executable by running `cmd` in the shell.

    If the command is not found, or the output does not match
    `RE_VERSION`, returns None.
    """"""
    executable = cmd.split()[0]
    if find_executable(executable) is None:
        return None
    out = Popen(cmd, shell=True, stdout=PIPE).stdout
    try:
        out_string = out.read()
    finally:
        out.close()
    result = RE_VERSION.search(out_string)
    if result is None:
        return None
    # LooseVersion works with strings
    # so we need to decode our bytes
    return LooseVersion(result.group(1).decode())","def try_to_find_versions_of_gcc_ld_dllwrap():
    """""" Try to find out the versions of gcc, ld and dllwrap.

    If not possible it returns None for it.
    """"""
    commands = ['gcc -dumpversion', 'ld -v', 'dllwrap --version']
    return tuple([get_executable_version(cmd) for cmd in commands])","{""_find_exe_version"": ""get_executable_version""}","def 0(cmd):
    """"""Find the version of an executable by running `cmd` in the shell.

    If the command is not found, or the output does not match
    `RE_VERSION`, returns None.
    """"""
    executable = cmd.split()[0]
    if find_executable(executable) is None:
        return None
    out = Popen(cmd, shell=True, stdout=PIPE).stdout
    try:
        out_string = out.read()
    finally:
        out.close()
    result = RE_VERSION.search(out_string)
    if result is None:
        return None
    # LooseVersion works with strings
    # so we need to decode our bytes
    return LooseVersion(result.group(1).decode())","def 1():
    """""" Try to find out the versions of gcc, ld and dllwrap.

    If not possible it returns None for it.
    """"""
    commands = ['gcc -dumpversion', 'ld -v', 'dllwrap --version']
    return tuple([0(cmd) for cmd in commands])","{""_find_exe_version"": ""0""}"
245,245,"def copy_xxmodule_c(directory):
    """"""Helper for tests that need the xxmodule.c source file.

    Example use:

        def test_compile(self):
            copy_xxmodule_c(self.tmpdir)
            self.assertIn('xxmodule.c', os.listdir(self.tmpdir))

    If the source file can be found, it will be copied to *directory*.  If not,
    the test will be skipped.  Errors during copy are not caught.
    """"""
    filename = _get_xxmodule_path()
    if filename is None:
        raise unittest.SkipTest('cannot find xxmodule.c (test must run in '
                                'the python build dir)')
    shutil.copy(filename, directory)","def _get_xxmodule_path():
    srcdir = sysconfig.get_config_var('srcdir')
    candidates = [
        # use installed copy if available
        os.path.join(os.path.dirname(__file__), 'xxmodule.c'),
        # otherwise try using copy from build directory
        os.path.join(srcdir, 'Modules', 'xxmodule.c'),
        # srcdir mysteriously can be $srcdir/Lib/distutils/tests when
        # this file is run from its parent directory, so walk up the
        # tree to find the real srcdir
        os.path.join(srcdir, '..', '..', '..', 'Modules', 'xxmodule.c'),
    ]
    for path in candidates:
        if os.path.exists(path):
            return path","def get_xxmodule_path():
    srcdir = sysconfig.get_config_var('srcdir')
    candidates = [
        # use installed copy if available
        os.path.join(os.path.dirname(__file__), 'xxmodule.c'),
        # otherwise try using copy from build directory
        os.path.join(srcdir, 'Modules', 'xxmodule.c'),
        # srcdir mysteriously can be $srcdir/Lib/distutils/tests when
        # this file is run from its parent directory, so walk up the
        # tree to find the real srcdir
        os.path.join(srcdir, '..', '..', '..', 'Modules', 'xxmodule.c'),
    ]
    for path in candidates:
        if os.path.exists(path):
            return path","def helper_for_tests_copying_xxmodule_c(directory):
    """"""Helper for tests that need the xxmodule.c source file.

    Example use:

        def test_compile_copying_xxmodule_c(self):
            helper_for_tests_copying_xxmodule_c(self.tmpdir)
            self.assertIn('xxmodule.c', os.listdir(self.tmpdir))

    If the source file can be found, it will be copied to *directory*.  If not,
    the test will be skipped.  Errors during copy are not caught.
    """"""
    filename = get_xxmodule_path()
    if filename is None:
        raise unittest.SkipTest('cannot find xxmodule.c (test must run in '
                                'the python build dir)')
    shutil.copy(filename, directory)","{""_get_xxmodule_path"": ""get_xxmodule_path""}","def 0():
    srcdir = sysconfig.get_config_var('srcdir')
    candidates = [
        # use installed copy if available
        os.path.join(os.path.dirname(__file__), 'xxmodule.c'),
        # otherwise try using copy from build directory
        os.path.join(srcdir, 'Modules', 'xxmodule.c'),
        # srcdir mysteriously can be $srcdir/Lib/distutils/tests when
        # this file is run from its parent directory, so walk up the
        # tree to find the real srcdir
        os.path.join(srcdir, '..', '..', '..', 'Modules', 'xxmodule.c'),
    ]
    for path in candidates:
        if os.path.exists(path):
            return path","def 1(directory):
    """"""Helper for tests that need the xxmodule.c source file.

    Example use:

        def 2(self):
            1(self.tmpdir)
            self.assertIn('xxmodule.c', os.listdir(self.tmpdir))

    If the source file can be found, it will be copied to *directory*.  If not,
    the test will be skipped.  Errors during copy are not caught.
    """"""
    filename = 0()
    if filename is None:
        raise unittest.SkipTest('cannot find xxmodule.c (test must run in '
                                'the python build dir)')
    shutil.copy(filename, directory)","{""_get_xxmodule_path"": ""0""}"
246,246,"def find_tzfile(key):
    """"""Retrieve the path to a TZif file from a key.""""""
    _validate_tzfile_path(key)
    for search_path in TZPATH:
        filepath = os.path.join(search_path, key)
        if os.path.isfile(filepath):
            return filepath

    return None","def _validate_tzfile_path(path, _base=_TEST_PATH):
    if os.path.isabs(path):
        raise ValueError(
            f""ZoneInfo keys may not be absolute paths, got: {path}""
        )

    # We only care about the kinds of path normalizations that would change the
    # length of the key - e.g. a/../b -> a/b, or a/b/ -> a/b. On Windows,
    # normpath will also change from a/b to a\b, but that would still preserve
    # the length.
    new_path = os.path.normpath(path)
    if len(new_path) != len(path):
        raise ValueError(
            f""ZoneInfo keys must be normalized relative paths, got: {path}""
        )

    resolved = os.path.normpath(os.path.join(_base, new_path))
    if not resolved.startswith(_base):
        raise ValueError(
            f""ZoneInfo keys must refer to subdirectories of TZPATH, got: {path}""
        )","def validate_timezone_file_path(path, _base=_TEST_PATH):
    if os.path.isabs(path):
        raise ValueError(
            f""ZoneInfo keys may not be absolute paths, got: {path}""
        )

    # We only care about the kinds of path normalizations that would change the
    # length of the key - e.g. a/../b -> a/b, or a/b/ -> a/b. On Windows,
    # normpath will also change from a/b to a\b, but that would still preserve
    # the length.
    new_path = os.path.normpath(path)
    if len(new_path) != len(path):
        raise ValueError(
            f""ZoneInfo keys must be normalized relative paths, got: {path}""
        )

    resolved = os.path.normpath(os.path.join(_base, new_path))
    if not resolved.startswith(_base):
        raise ValueError(
            f""ZoneInfo keys must refer to subdirectories of TZPATH, got: {path}""
        )","def retrieve_path_to_tzif_file_from_key(key):
    """"""Retrieve the path to a TZif file from a key.""""""
    validate_timezone_file_path(key)
    for search_path in TZPATH:
        filepath = os.path.join(search_path, key)
        if os.path.isfile(filepath):
            return filepath

    return None","{""_validate_tzfile_path"": ""validate_timezone_file_path""}","def 0(path, _base=_TEST_PATH):
    if os.path.isabs(path):
        raise ValueError(
            f""ZoneInfo keys may not be absolute paths, got: {path}""
        )

    # We only care about the kinds of path normalizations that would change the
    # length of the key - e.g. a/../b -> a/b, or a/b/ -> a/b. On Windows,
    # normpath will also change from a/b to a\b, but that would still preserve
    # the length.
    new_path = os.path.normpath(path)
    if len(new_path) != len(path):
        raise ValueError(
            f""ZoneInfo keys must be normalized relative paths, got: {path}""
        )

    resolved = os.path.normpath(os.path.join(_base, new_path))
    if not resolved.startswith(_base):
        raise ValueError(
            f""ZoneInfo keys must refer to subdirectories of TZPATH, got: {path}""
        )","def 1(key):
    """"""Retrieve the path to a TZif file from a key.""""""
    0(key)
    for search_path in TZPATH:
        filepath = os.path.join(search_path, key)
        if os.path.isfile(filepath):
            return filepath

    return None","{""_validate_tzfile_path"": ""0""}"
247,247,"def get_ld_headers(file):
    """"""
    Parse the header of the loader section of executable and archives
    This function calls /usr/bin/dump -H as a subprocess
    and returns a list of (ld_header, ld_header_info) tuples.
    """"""
    # get_ld_headers parsing:
    # 1. Find a line that starts with /, ./, or ../ - set as ld_header
    # 2. If ""INDEX"" in occurs in a following line - return ld_header
    # 3. get info (lines starting with [0-9])
    ldr_headers = []
    p = Popen([""/usr/bin/dump"", f""-X{AIX_ABI}"", ""-H"", file],
        universal_newlines=True, stdout=PIPE, stderr=DEVNULL)
    # be sure to read to the end-of-file - getting all entries
    while True:
        ld_header = get_ld_header(p)
        if ld_header:
            ldr_headers.append((ld_header, get_ld_header_info(p)))
        else:
            break
    p.stdout.close()
    p.wait()
    return ldr_headers","def get_ld_header(p):
    # ""nested-function, but placed at module level
    ld_header = None
    for line in p.stdout:
        if line.startswith(('/', './', '../')):
            ld_header = line
        elif ""INDEX"" in line:
            return ld_header.rstrip('\n')
    return None

def get_ld_header_info(p):
    # ""nested-function, but placed at module level
    # as an ld_header was found, return known paths, archives and members
    # these lines start with a digit
    info = []
    for line in p.stdout:
        if re.match(""[0-9]"", line):
            info.append(line)
        else:
            # blank line (separator), consume line and end for loop
            break
    return info","def get_ld_header_from_process(p):
    # ""nested-function, but placed at module level
    ld_header = None
    for line in p.stdout:
        if line.startswith(('/', './', '../')):
            ld_header = line
        elif ""INDEX"" in line:
            return ld_header.rstrip('\n')
    return None

def get_ld_header_from_process_info(p):
    # ""nested-function, but placed at module level
    # as an ld_header was found, return known paths, archives and members
    # these lines start with a digit
    info = []
    for line in p.stdout:
        if re.match(""[0-9]"", line):
            info.append(line)
        else:
            # blank line (separator), consume line and end for loop
            break
    return info","def get_ld_header_from_processs(file):
    """"""
    Parse the header of the loader section of executable and archives
    This function calls /usr/bin/dump -H as a subprocess
    and returns a list of (ld_header, ld_header_info) tuples.
    """"""
    # get_ld_header_from_processs parsing:
    # 1. Find a line that starts with /, ./, or ../ - set as ld_header
    # 2. If ""INDEX"" in occurs in a following line - return ld_header
    # 3. get info (lines starting with [0-9])
    ldr_headers = []
    p = Popen([""/usr/bin/dump"", f""-X{AIX_ABI}"", ""-H"", file],
        universal_newlines=True, stdout=PIPE, stderr=DEVNULL)
    # be sure to read to the end-of-file - getting all entries
    while True:
        ld_header = get_ld_header_from_process(p)
        if ld_header:
            ldr_headers.append((ld_header, get_ld_header_from_process_info(p)))
        else:
            break
    p.stdout.close()
    p.wait()
    return ldr_headers","{""get_ld_header"": ""get_ld_header_from_process"", ""get_ld_header_info"": ""get_ld_header_info_from_process""}","def 0(p):
    # ""nested-function, but placed at module level
    ld_header = None
    for line in p.stdout:
        if line.startswith(('/', './', '../')):
            ld_header = line
        elif ""INDEX"" in line:
            return ld_header.rstrip('\n')
    return None

def 1(p):
    # ""nested-function, but placed at module level
    # as an ld_header was found, return known paths, archives and members
    # these lines start with a digit
    info = []
    for line in p.stdout:
        if re.match(""[0-9]"", line):
            info.append(line)
        else:
            # blank line (separator), consume line and end for loop
            break
    return info","def 2(file):
    """"""
    Parse the header of the loader section of executable and archives
    This function calls /usr/bin/dump -H as a subprocess
    and returns a list of (ld_header, ld_header_info) tuples.
    """"""
    # get_ld_headers parsing:
    # 1. Find a line that starts with /, ./, or ../ - set as ld_header
    # 2. If ""INDEX"" in occurs in a following line - return ld_header
    # 3. get info (lines starting with [0-9])
    ldr_headers = []
    p = Popen([""/usr/bin/dump"", f""-X{AIX_ABI}"", ""-H"", file],
        universal_newlines=True, stdout=PIPE, stderr=DEVNULL)
    # be sure to read to the end-of-file - getting all entries
    while True:
        ld_header = 0(p)
        if ld_header:
            ldr_headers.append((ld_header, 1(p)))
        else:
            break
    p.stdout.close()
    p.wait()
    return ldr_headers","{""get_ld_header"": ""0"", ""get_ld_header_info"": ""1""}"
248,248,"def get_one_match(expr, lines):
    """"""
    Must be only one match, otherwise result is None.
    When there is a match, strip leading ""["" and trailing ""]""
    """"""
    # member names in the ld_headers output are between square brackets
    expr = rf'\[({expr})\]'
    matches = list(filter(None, (re.search(expr, line) for line in lines)))
    if len(matches) == 1:
        return matches[0].group(1)
    else:
        return None","def filter(names, pat):
    """"""Construct a list from those elements of the iterable NAMES that match PAT.""""""
    result = []
    pat = os.path.normcase(pat)
    match = _compile_pattern(pat)
    if os.path is posixpath:
        # normcase on posix is NOP. Optimize it away from the loop.
        for name in names:
            if match(name):
                result.append(name)
    else:
        for name in names:
            if match(os.path.normcase(name)):
                result.append(name)
    return result","def filter_names(names, pat):
    """"""Construct a list from those elements of the iterable NAMES that match PAT.""""""
    result = []
    pat = os.path.normcase(pat)
    match = _compile_pattern(pat)
    if os.path is posixpath:
        # normcase on posix is NOP. Optimize it away from the loop.
        for name in names:
            if match(name):
                result.append(name)
    else:
        for name in names:
            if match(os.path.normcase(name)):
                result.append(name)
    return result","def get_first_matching_group(expr, lines):
    """"""
    Must be only one match, otherwise result is None.
    When there is a match, strip leading ""["" and trailing ""]""
    """"""
    # member names in the ld_headers output are between square brackets
    expr = rf'\[({expr})\]'
    matches = list(filter_names(None, (re.search(expr, line) for line in lines)))
    if len(matches) == 1:
        return matches[0].group(1)
    else:
        return None","{""filter"": ""filter_names""}","def 0(names, pat):
    """"""Construct a list from those elements of the iterable NAMES that match PAT.""""""
    result = []
    pat = os.path.normcase(pat)
    match = _compile_pattern(pat)
    if os.path is posixpath:
        # normcase on posix is NOP. Optimize it away from the loop.
        for name in names:
            if match(name):
                result.append(name)
    else:
        for name in names:
            if match(os.path.normcase(name)):
                result.append(name)
    return result","def 1(expr, lines):
    """"""
    Must be only one match, otherwise result is None.
    When there is a match, strip leading ""["" and trailing ""]""
    """"""
    # member names in the ld_headers output are between square brackets
    expr = rf'\[({expr})\]'
    matches = list(0(None, (re.search(expr, line) for line in lines)))
    if len(matches) == 1:
        return matches[0].group(1)
    else:
        return None","{""filter"": ""0""}"
249,249,"def get_legacy(members):
    """"""
    This routine provides historical aka legacy naming schemes started
    in AIX4 shared library support for library members names.
    e.g., in /usr/lib/libc.a the member name shr.o for 32-bit binary and
    shr_64.o for 64-bit binary.
    """"""
    if AIX_ABI == 64:
        # AIX 64-bit member is one of shr64.o, shr_64.o, or shr4_64.o
        expr = r'shr4?_?64\.o'
        member = get_one_match(expr, members)
        if member:
            return member
    else:
        # 32-bit legacy names - both shr.o and shr4.o exist.
        # shr.o is the preferred name so we look for shr.o first
        #  i.e., shr4.o is returned only when shr.o does not exist
        for name in ['shr.o', 'shr4.o']:
            member = get_one_match(re.escape(name), members)
            if member:
                return member
    return None","def get_one_match(expr, lines):
    """"""
    Must be only one match, otherwise result is None.
    When there is a match, strip leading ""["" and trailing ""]""
    """"""
    # member names in the ld_headers output are between square brackets
    expr = rf'\[({expr})\]'
    matches = list(filter(None, (re.search(expr, line) for line in lines)))
    if len(matches) == 1:
        return matches[0].group(1)
    else:
        return None","def get_first_matching_group(expr, lines):
    """"""
    Must be only one match, otherwise result is None.
    When there is a match, strip leading ""["" and trailing ""]""
    """"""
    # member names in the ld_headers output are between square brackets
    expr = rf'\[({expr})\]'
    matches = list(filter(None, (re.search(expr, line) for line in lines)))
    if len(matches) == 1:
        return matches[0].group(1)
    else:
        return None","def get_legacy_naming_scheme(members):
    """"""
    This routine provides historical aka legacy naming schemes started
    in AIX4 shared library support for library members names.
    e.g., in /usr/lib/libc.a the member name shr.o for 32-bit binary and
    shr_64.o for 64-bit binary.
    """"""
    if AIX_ABI == 64:
        # AIX 64-bit member is one of shr64.o, shr_64.o, or shr4_64.o
        expr = r'shr4?_?64\.o'
        member = get_first_matching_group(expr, members)
        if member:
            return member
    else:
        # 32-bit legacy names - both shr.o and shr4.o exist.
        # shr.o is the preferred name so we look for shr.o first
        #  i.e., shr4.o is returned only when shr.o does not exist
        for name in ['shr.o', 'shr4.o']:
            member = get_first_matching_group(re.escape(name), members)
            if member:
                return member
    return None","{""get_one_match"": ""get_first_matching_group""}","def 0(expr, lines):
    """"""
    Must be only one match, otherwise result is None.
    When there is a match, strip leading ""["" and trailing ""]""
    """"""
    # member names in the ld_headers output are between square brackets
    expr = rf'\[({expr})\]'
    matches = list(filter(None, (re.search(expr, line) for line in lines)))
    if len(matches) == 1:
        return matches[0].group(1)
    else:
        return None","def 1(members):
    """"""
    This routine provides historical aka legacy naming schemes started
    in AIX4 shared library support for library members names.
    e.g., in /usr/lib/libc.a the member name shr.o for 32-bit binary and
    shr_64.o for 64-bit binary.
    """"""
    if AIX_ABI == 64:
        # AIX 64-bit member is one of shr64.o, shr_64.o, or shr4_64.o
        expr = r'shr4?_?64\.o'
        member = 0(expr, members)
        if member:
            return member
    else:
        # 32-bit legacy names - both shr.o and shr4.o exist.
        # shr.o is the preferred name so we look for shr.o first
        #  i.e., shr4.o is returned only when shr.o does not exist
        for name in ['shr.o', 'shr4.o']:
            member = 0(re.escape(name), members)
            if member:
                return member
    return None","{""get_one_match"": ""0""}"
250,250,"def get_member(name, members):
    """"""
    Return an archive member matching the request in name.
    Name is the library name without any prefix like lib, suffix like .so,
    or version number.
    Given a list of members find and return the most appropriate result
    Priority is given to generic libXXX.so, then a versioned libXXX.so.a.b.c
    and finally, legacy AIX naming scheme.
    """"""
    # look first for a generic match - prepend lib and append .so
    expr = rf'lib{name}\.so'
    member = get_one_match(expr, members)
    if member:
        return member
    elif AIX_ABI == 64:
        expr = rf'lib{name}64\.so'
        member = get_one_match(expr, members)
    if member:
        return member
    # since an exact match with .so as suffix was not found
    # look for a versioned name
    # If a versioned name is not found, look for AIX legacy member name
    member = get_version(name, members)
    if member:
        return member
    else:
        return get_legacy(members)","def get_legacy(members):
    """"""
    This routine provides historical aka legacy naming schemes started
    in AIX4 shared library support for library members names.
    e.g., in /usr/lib/libc.a the member name shr.o for 32-bit binary and
    shr_64.o for 64-bit binary.
    """"""
    if AIX_ABI == 64:
        # AIX 64-bit member is one of shr64.o, shr_64.o, or shr4_64.o
        expr = r'shr4?_?64\.o'
        member = get_one_match(expr, members)
        if member:
            return member
    else:
        # 32-bit legacy names - both shr.o and shr4.o exist.
        # shr.o is the preferred name so we look for shr.o first
        #  i.e., shr4.o is returned only when shr.o does not exist
        for name in ['shr.o', 'shr4.o']:
            member = get_one_match(re.escape(name), members)
            if member:
                return member
    return None

def get_version(name, members):
    """"""
    Sort list of members and return highest numbered version - if it exists.
    This function is called when an unversioned libFOO.a(libFOO.so) has
    not been found.

    Versioning for the member name is expected to follow
    GNU LIBTOOL conventions: the highest version (x, then X.y, then X.Y.z)
     * find [libFoo.so.X]
     * find [libFoo.so.X.Y]
     * find [libFoo.so.X.Y.Z]

    Before the GNU convention became the standard scheme regardless of
    binary size AIX packagers used GNU convention ""as-is"" for 32-bit
    archive members but used an ""distinguishing"" name for 64-bit members.
    This scheme inserted either 64 or _64 between libFOO and .so
    - generally libFOO_64.so, but occasionally libFOO64.so
    """"""
    # the expression ending for versions must start as
    # '.so.[0-9]', i.e., *.so.[at least one digit]
    # while multiple, more specific expressions could be specified
    # to search for .so.X, .so.X.Y and .so.X.Y.Z
    # after the first required 'dot' digit
    # any combination of additional 'dot' digits pairs are accepted
    # anything more than libFOO.so.digits.digits.digits
    # should be seen as a member name outside normal expectations
    exprs = [rf'lib{name}\.so\.[0-9]+[0-9.]*',
        rf'lib{name}_?64\.so\.[0-9]+[0-9.]*']
    for expr in exprs:
        versions = []
        for line in members:
            m = re.search(expr, line)
            if m:
                versions.append(m.group(0))
        if versions:
            return _last_version(versions, '.')
    return None

def get_one_match(expr, lines):
    """"""
    Must be only one match, otherwise result is None.
    When there is a match, strip leading ""["" and trailing ""]""
    """"""
    # member names in the ld_headers output are between square brackets
    expr = rf'\[({expr})\]'
    matches = list(filter(None, (re.search(expr, line) for line in lines)))
    if len(matches) == 1:
        return matches[0].group(1)
    else:
        return None","def get_legacy_naming_scheme(members):
    """"""
    This routine provides historical aka legacy naming schemes started
    in AIX4 shared library support for library members names.
    e.g., in /usr/lib/libc.a the member name shr.o for 32-bit binary and
    shr_64.o for 64-bit binary.
    """"""
    if AIX_ABI == 64:
        # AIX 64-bit member is one of shr64.o, shr_64.o, or shr4_64.o
        expr = r'shr4?_?64\.o'
        member = get_first_matching_group(expr, members)
        if member:
            return member
    else:
        # 32-bit legacy names - both shr.o and shr4.o exist.
        # shr.o is the preferred name so we look for shr.o first
        #  i.e., shr4.o is returned only when shr.o does not exist
        for name in ['shr.o', 'shr4.o']:
            member = get_first_matching_group(re.escape(name), members)
            if member:
                return member
    return None

def get_highest_versioned_member(name, members):
    """"""
    Sort list of members and return highest numbered version - if it exists.
    This function is called when an unversioned libFOO.a(libFOO.so) has
    not been found.

    Versioning for the member name is expected to follow
    GNU LIBTOOL conventions: the highest version (x, then X.y, then X.Y.z)
     * find [libFoo.so.X]
     * find [libFoo.so.X.Y]
     * find [libFoo.so.X.Y.Z]

    Before the GNU convention became the standard scheme regardless of
    binary size AIX packagers used GNU convention ""as-is"" for 32-bit
    archive members but used an ""distinguishing"" name for 64-bit members.
    This scheme inserted either 64 or _64 between libFOO and .so
    - generally libFOO_64.so, but occasionally libFOO64.so
    """"""
    # the expression ending for versions must start as
    # '.so.[0-9]', i.e., *.so.[at least one digit]
    # while multiple, more specific expressions could be specified
    # to search for .so.X, .so.X.Y and .so.X.Y.Z
    # after the first required 'dot' digit
    # any combination of additional 'dot' digits pairs are accepted
    # anything more than libFOO.so.digits.digits.digits
    # should be seen as a member name outside normal expectations
    exprs = [rf'lib{name}\.so\.[0-9]+[0-9.]*',
        rf'lib{name}_?64\.so\.[0-9]+[0-9.]*']
    for expr in exprs:
        versions = []
        for line in members:
            m = re.search(expr, line)
            if m:
                versions.append(m.group(0))
        if versions:
            return _last_version(versions, '.')
    return None

def get_first_matching_group(expr, lines):
    """"""
    Must be only one match, otherwise result is None.
    When there is a match, strip leading ""["" and trailing ""]""
    """"""
    # member names in the ld_headers output are between square brackets
    expr = rf'\[({expr})\]'
    matches = list(filter(None, (re.search(expr, line) for line in lines)))
    if len(matches) == 1:
        return matches[0].group(1)
    else:
        return None","def get_matching_archive_member(name, members):
    """"""
    Return an archive member matching the request in name.
    Name is the library name without any prefix like lib, suffix like .so,
    or version number.
    Given a list of members find and return the most appropriate result
    Priority is given to generic libXXX.so, then a versioned libXXX.so.a.b.c
    and finally, legacy AIX naming scheme.
    """"""
    # look first for a generic match - prepend lib and append .so
    expr = rf'lib{name}\.so'
    member = get_first_matching_group(expr, members)
    if member:
        return member
    elif AIX_ABI == 64:
        expr = rf'lib{name}64\.so'
        member = get_first_matching_group(expr, members)
    if member:
        return member
    # since an exact match with .so as suffix was not found
    # look for a versioned name
    # If a versioned name is not found, look for AIX legacy member name
    member = get_highest_versioned_member(name, members)
    if member:
        return member
    else:
        return get_legacy_naming_scheme(members)","{""get_legacy"": ""get_legacy_naming_scheme"", ""get_version"": ""get_highest_versioned_member"", ""get_one_match"": ""get_first_matching_group""}","def 0(members):
    """"""
    This routine provides historical aka legacy naming schemes started
    in AIX4 shared library support for library members names.
    e.g., in /usr/lib/libc.a the member name shr.o for 32-bit binary and
    shr_64.o for 64-bit binary.
    """"""
    if AIX_ABI == 64:
        # AIX 64-bit member is one of shr64.o, shr_64.o, or shr4_64.o
        expr = r'shr4?_?64\.o'
        member = 2(expr, members)
        if member:
            return member
    else:
        # 32-bit legacy names - both shr.o and shr4.o exist.
        # shr.o is the preferred name so we look for shr.o first
        #  i.e., shr4.o is returned only when shr.o does not exist
        for name in ['shr.o', 'shr4.o']:
            member = 2(re.escape(name), members)
            if member:
                return member
    return None

def 1(name, members):
    """"""
    Sort list of members and return highest numbered version - if it exists.
    This function is called when an unversioned libFOO.a(libFOO.so) has
    not been found.

    Versioning for the member name is expected to follow
    GNU LIBTOOL conventions: the highest version (x, then X.y, then X.Y.z)
     * find [libFoo.so.X]
     * find [libFoo.so.X.Y]
     * find [libFoo.so.X.Y.Z]

    Before the GNU convention became the standard scheme regardless of
    binary size AIX packagers used GNU convention ""as-is"" for 32-bit
    archive members but used an ""distinguishing"" name for 64-bit members.
    This scheme inserted either 64 or _64 between libFOO and .so
    - generally libFOO_64.so, but occasionally libFOO64.so
    """"""
    # the expression ending for versions must start as
    # '.so.[0-9]', i.e., *.so.[at least one digit]
    # while multiple, more specific expressions could be specified
    # to search for .so.X, .so.X.Y and .so.X.Y.Z
    # after the first required 'dot' digit
    # any combination of additional 'dot' digits pairs are accepted
    # anything more than libFOO.so.digits.digits.digits
    # should be seen as a member name outside normal expectations
    exprs = [rf'lib{name}\.so\.[0-9]+[0-9.]*',
        rf'lib{name}_?64\.so\.[0-9]+[0-9.]*']
    for expr in exprs:
        versions = []
        for line in members:
            m = re.search(expr, line)
            if m:
                versions.append(m.group(0))
        if versions:
            return _last_version(versions, '.')
    return None

def 2(expr, lines):
    """"""
    Must be only one match, otherwise result is None.
    When there is a match, strip leading ""["" and trailing ""]""
    """"""
    # member names in the ld_headers output are between square brackets
    expr = rf'\[({expr})\]'
    matches = list(filter(None, (re.search(expr, line) for line in lines)))
    if len(matches) == 1:
        return matches[0].group(1)
    else:
        return None","def 3(name, members):
    """"""
    Return an archive member matching the request in name.
    Name is the library name without any prefix like lib, suffix like .so,
    or version number.
    Given a list of members find and return the most appropriate result
    Priority is given to generic libXXX.so, then a versioned libXXX.so.a.b.c
    and finally, legacy AIX naming scheme.
    """"""
    # look first for a generic match - prepend lib and append .so
    expr = rf'lib{name}\.so'
    member = 2(expr, members)
    if member:
        return member
    elif AIX_ABI == 64:
        expr = rf'lib{name}64\.so'
        member = 2(expr, members)
    if member:
        return member
    # since an exact match with .so as suffix was not found
    # look for a versioned name
    # If a versioned name is not found, look for AIX legacy member name
    member = 1(name, members)
    if member:
        return member
    else:
        return 0(members)","{""get_legacy"": ""0"", ""get_version"": ""1"", ""get_one_match"": ""2""}"
251,251,"def get_libpaths():
    """"""
    On AIX, the buildtime searchpath is stored in the executable.
    as ""loader header information"".
    The command /usr/bin/dump -H extracts this info.
    Prefix searched libraries with LD_LIBRARY_PATH (preferred),
    or LIBPATH if defined. These paths are appended to the paths
    to libraries the python executable is linked with.
    This mimics AIX dlopen() behavior.
    """"""
    libpaths = environ.get(""LD_LIBRARY_PATH"")
    if libpaths is None:
        libpaths = environ.get(""LIBPATH"")
    if libpaths is None:
        libpaths = []
    else:
        libpaths = libpaths.split("":"")
    objects = get_ld_headers(executable)
    for (_, lines) in objects:
        for line in lines:
            # the second (optional) argument is PATH if it includes a /
            path = line.split()[1]
            if ""/"" in path:
                libpaths.extend(path.split("":""))
    return libpaths","def get_ld_headers(file):
    """"""
    Parse the header of the loader section of executable and archives
    This function calls /usr/bin/dump -H as a subprocess
    and returns a list of (ld_header, ld_header_info) tuples.
    """"""
    # get_ld_headers parsing:
    # 1. Find a line that starts with /, ./, or ../ - set as ld_header
    # 2. If ""INDEX"" in occurs in a following line - return ld_header
    # 3. get info (lines starting with [0-9])
    ldr_headers = []
    p = Popen([""/usr/bin/dump"", f""-X{AIX_ABI}"", ""-H"", file],
        universal_newlines=True, stdout=PIPE, stderr=DEVNULL)
    # be sure to read to the end-of-file - getting all entries
    while True:
        ld_header = get_ld_header(p)
        if ld_header:
            ldr_headers.append((ld_header, get_ld_header_info(p)))
        else:
            break
    p.stdout.close()
    p.wait()
    return ldr_headers","def parse_ld_headers(file):
    """"""
    Parse the header of the loader section of executable and archives
    This function calls /usr/bin/dump -H as a subprocess
    and returns a list of (ld_header, ld_header_info) tuples.
    """"""
    # parse_ld_headers parsing:
    # 1. Find a line that starts with /, ./, or ../ - set as ld_header
    # 2. If ""INDEX"" in occurs in a following line - return ld_header
    # 3. get info (lines starting with [0-9])
    ldr_headers = []
    p = Popen([""/usr/bin/dump"", f""-X{AIX_ABI}"", ""-H"", file],
        universal_newlines=True, stdout=PIPE, stderr=DEVNULL)
    # be sure to read to the end-of-file - getting all entries
    while True:
        ld_header = get_ld_header(p)
        if ld_header:
            ldr_headers.append((ld_header, get_ld_header_info(p)))
        else:
            break
    p.stdout.close()
    p.wait()
    return ldr_headers","def on_aix_buildtime_searchpath_stored_in_executable():
    """"""
    On AIX, the buildtime searchpath is stored in the executable.
    as ""loader header information"".
    The command /usr/bin/dump -H extracts this info.
    Prefix searched libraries with LD_LIBRARY_PATH (preferred),
    or LIBPATH if defined. These paths are appended to the paths
    to libraries the python executable is linked with.
    This mimics AIX dlopen() behavior.
    """"""
    libpaths = environ.get(""LD_LIBRARY_PATH"")
    if libpaths is None:
        libpaths = environ.get(""LIBPATH"")
    if libpaths is None:
        libpaths = []
    else:
        libpaths = libpaths.split("":"")
    objects = parse_ld_headers(executable)
    for (_, lines) in objects:
        for line in lines:
            # the second (optional) argument is PATH if it includes a /
            path = line.split()[1]
            if ""/"" in path:
                libpaths.extend(path.split("":""))
    return libpaths","{""get_ld_headers"": ""parse_ld_headers""}","def 0(file):
    """"""
    Parse the header of the loader section of executable and archives
    This function calls /usr/bin/dump -H as a subprocess
    and returns a list of (ld_header, ld_header_info) tuples.
    """"""
    # get_ld_headers parsing:
    # 1. Find a line that starts with /, ./, or ../ - set as ld_header
    # 2. If ""INDEX"" in occurs in a following line - return ld_header
    # 3. get info (lines starting with [0-9])
    ldr_headers = []
    p = Popen([""/usr/bin/dump"", f""-X{AIX_ABI}"", ""-H"", file],
        universal_newlines=True, stdout=PIPE, stderr=DEVNULL)
    # be sure to read to the end-of-file - getting all entries
    while True:
        ld_header = get_ld_header(p)
        if ld_header:
            ldr_headers.append((ld_header, get_ld_header_info(p)))
        else:
            break
    p.stdout.close()
    p.wait()
    return ldr_headers","def 1():
    """"""
    On AIX, the buildtime searchpath is stored in the executable.
    as ""loader header information"".
    The command /usr/bin/dump -H extracts this info.
    Prefix searched libraries with LD_LIBRARY_PATH (preferred),
    or LIBPATH if defined. These paths are appended to the paths
    to libraries the python executable is linked with.
    This mimics AIX dlopen() behavior.
    """"""
    libpaths = environ.get(""LD_LIBRARY_PATH"")
    if libpaths is None:
        libpaths = environ.get(""LIBPATH"")
    if libpaths is None:
        libpaths = []
    else:
        libpaths = libpaths.split("":"")
    objects = 0(executable)
    for (_, lines) in objects:
        for line in lines:
            # the second (optional) argument is PATH if it includes a /
            path = line.split()[1]
            if ""/"" in path:
                libpaths.extend(path.split("":""))
    return libpaths","{""get_ld_headers"": ""0""}"
252,252,"def find_shared(paths, name):
    """"""
    paths is a list of directories to search for an archive.
    name is the abbreviated name given to find_library().
    Process: search ""paths"" for archive, and if an archive is found
    return the result of get_member().
    If an archive is not found then return None
    """"""
    for dir in paths:
        # /lib is a symbolic link to /usr/lib, skip it
        if dir == ""/lib"":
            continue
        # ""lib"" is prefixed to emulate compiler name resolution,
        # e.g., -lc to libc
        base = f'lib{name}.a'
        archive = path.join(dir, base)
        if path.exists(archive):
            members = get_shared(get_ld_headers(archive))
            member = get_member(re.escape(name), members)
            if member is not None:
                return (base, member)
            else:
                return (None, None)
    return (None, None)","def get_ld_headers(file):
    """"""
    Parse the header of the loader section of executable and archives
    This function calls /usr/bin/dump -H as a subprocess
    and returns a list of (ld_header, ld_header_info) tuples.
    """"""
    # get_ld_headers parsing:
    # 1. Find a line that starts with /, ./, or ../ - set as ld_header
    # 2. If ""INDEX"" in occurs in a following line - return ld_header
    # 3. get info (lines starting with [0-9])
    ldr_headers = []
    p = Popen([""/usr/bin/dump"", f""-X{AIX_ABI}"", ""-H"", file],
        universal_newlines=True, stdout=PIPE, stderr=DEVNULL)
    # be sure to read to the end-of-file - getting all entries
    while True:
        ld_header = get_ld_header(p)
        if ld_header:
            ldr_headers.append((ld_header, get_ld_header_info(p)))
        else:
            break
    p.stdout.close()
    p.wait()
    return ldr_headers

def get_member(name, members):
    """"""
    Return an archive member matching the request in name.
    Name is the library name without any prefix like lib, suffix like .so,
    or version number.
    Given a list of members find and return the most appropriate result
    Priority is given to generic libXXX.so, then a versioned libXXX.so.a.b.c
    and finally, legacy AIX naming scheme.
    """"""
    # look first for a generic match - prepend lib and append .so
    expr = rf'lib{name}\.so'
    member = get_one_match(expr, members)
    if member:
        return member
    elif AIX_ABI == 64:
        expr = rf'lib{name}64\.so'
        member = get_one_match(expr, members)
    if member:
        return member
    # since an exact match with .so as suffix was not found
    # look for a versioned name
    # If a versioned name is not found, look for AIX legacy member name
    member = get_version(name, members)
    if member:
        return member
    else:
        return get_legacy(members)

def get_shared(ld_headers):
    """"""
    extract the shareable objects from ld_headers
    character ""["" is used to strip off the path information.
    Note: the ""["" and ""]"" characters that are part of dump -H output
    are not removed here.
    """"""
    shared = []
    for (line, _) in ld_headers:
        # potential member lines contain ""[""
        # otherwise, no processing needed
        if ""["" in line:
            # Strip off trailing colon (:)
            shared.append(line[line.index(""[""):-1])
    return shared","def parse_ld_headers(file):
    """"""
    Parse the header of the loader section of executable and archives
    This function calls /usr/bin/dump -H as a subprocess
    and returns a list of (ld_header, ld_header_info) tuples.
    """"""
    # parse_ld_headers parsing:
    # 1. Find a line that starts with /, ./, or ../ - set as ld_header
    # 2. If ""INDEX"" in occurs in a following line - return ld_header
    # 3. get info (lines starting with [0-9])
    ldr_headers = []
    p = Popen([""/usr/bin/dump"", f""-X{AIX_ABI}"", ""-H"", file],
        universal_newlines=True, stdout=PIPE, stderr=DEVNULL)
    # be sure to read to the end-of-file - getting all entries
    while True:
        ld_header = get_ld_header(p)
        if ld_header:
            ldr_headers.append((ld_header, get_ld_header_info(p)))
        else:
            break
    p.stdout.close()
    p.wait()
    return ldr_headers

def get_matching_archive_member(name, members):
    """"""
    Return an archive member matching the request in name.
    Name is the library name without any prefix like lib, suffix like .so,
    or version number.
    Given a list of members find and return the most appropriate result
    Priority is given to generic libXXX.so, then a versioned libXXX.so.a.b.c
    and finally, legacy AIX naming scheme.
    """"""
    # look first for a generic match - prepend lib and append .so
    expr = rf'lib{name}\.so'
    member = get_one_match(expr, members)
    if member:
        return member
    elif AIX_ABI == 64:
        expr = rf'lib{name}64\.so'
        member = get_one_match(expr, members)
    if member:
        return member
    # since an exact match with .so as suffix was not found
    # look for a versioned name
    # If a versioned name is not found, look for AIX legacy member name
    member = get_version(name, members)
    if member:
        return member
    else:
        return get_legacy(members)

def extract_shared_objects(ld_headers):
    """"""
    extract the shareable objects from ld_headers
    character ""["" is used to strip off the path information.
    Note: the ""["" and ""]"" characters that are part of dump -H output
    are not removed here.
    """"""
    shared = []
    for (line, _) in ld_headers:
        # potential member lines contain ""[""
        # otherwise, no processing needed
        if ""["" in line:
            # Strip off trailing colon (:)
            shared.append(line[line.index(""[""):-1])
    return shared","def search_directories_for_archive_and_return_result(paths, name):
    """"""
    paths is a list of directories to search for an archive.
    name is the abbreviated name given to find_library().
    Process: search ""paths"" for archive, and if an archive is found
    return the result of get_matching_archive_member().
    If an archive is not found then return None
    """"""
    for dir in paths:
        # /lib is a symbolic link to /usr/lib, skip it
        if dir == ""/lib"":
            continue
        # ""lib"" is prefixed to emulate compiler name resolution,
        # e.g., -lc to libc
        base = f'lib{name}.a'
        archive = path.join(dir, base)
        if path.exists(archive):
            members = extract_shared_objects(parse_ld_headers(archive))
            member = get_matching_archive_member(re.escape(name), members)
            if member is not None:
                return (base, member)
            else:
                return (None, None)
    return (None, None)","{""get_ld_headers"": ""parse_ld_headers"", ""get_member"": ""get_matching_archive_member"", ""get_shared"": ""extract_shared_objects""}","def 0(file):
    """"""
    Parse the header of the loader section of executable and archives
    This function calls /usr/bin/dump -H as a subprocess
    and returns a list of (ld_header, ld_header_info) tuples.
    """"""
    # get_ld_headers parsing:
    # 1. Find a line that starts with /, ./, or ../ - set as ld_header
    # 2. If ""INDEX"" in occurs in a following line - return ld_header
    # 3. get info (lines starting with [0-9])
    ldr_headers = []
    p = Popen([""/usr/bin/dump"", f""-X{AIX_ABI}"", ""-H"", file],
        universal_newlines=True, stdout=PIPE, stderr=DEVNULL)
    # be sure to read to the end-of-file - getting all entries
    while True:
        ld_header = get_ld_header(p)
        if ld_header:
            ldr_headers.append((ld_header, get_ld_header_info(p)))
        else:
            break
    p.stdout.close()
    p.wait()
    return ldr_headers

def 1(name, members):
    """"""
    Return an archive member matching the request in name.
    Name is the library name without any prefix like lib, suffix like .so,
    or version number.
    Given a list of members find and return the most appropriate result
    Priority is given to generic libXXX.so, then a versioned libXXX.so.a.b.c
    and finally, legacy AIX naming scheme.
    """"""
    # look first for a generic match - prepend lib and append .so
    expr = rf'lib{name}\.so'
    member = get_one_match(expr, members)
    if member:
        return member
    elif AIX_ABI == 64:
        expr = rf'lib{name}64\.so'
        member = get_one_match(expr, members)
    if member:
        return member
    # since an exact match with .so as suffix was not found
    # look for a versioned name
    # If a versioned name is not found, look for AIX legacy member name
    member = get_version(name, members)
    if member:
        return member
    else:
        return get_legacy(members)

def 2(ld_headers):
    """"""
    extract the shareable objects from ld_headers
    character ""["" is used to strip off the path information.
    Note: the ""["" and ""]"" characters that are part of dump -H output
    are not removed here.
    """"""
    shared = []
    for (line, _) in ld_headers:
        # potential member lines contain ""[""
        # otherwise, no processing needed
        if ""["" in line:
            # Strip off trailing colon (:)
            shared.append(line[line.index(""[""):-1])
    return shared","def 3(paths, name):
    """"""
    paths is a list of directories to search for an archive.
    name is the abbreviated name given to find_library().
    Process: search ""paths"" for archive, and if an archive is found
    return the result of 1().
    If an archive is not found then return None
    """"""
    for dir in paths:
        # /lib is a symbolic link to /usr/lib, skip it
        if dir == ""/lib"":
            continue
        # ""lib"" is prefixed to emulate compiler name resolution,
        # e.g., -lc to libc
        base = f'lib{name}.a'
        archive = path.join(dir, base)
        if path.exists(archive):
            members = 2(0(archive))
            member = 1(re.escape(name), members)
            if member is not None:
                return (base, member)
            else:
                return (None, None)
    return (None, None)","{""get_ld_headers"": ""0"", ""get_member"": ""1"", ""get_shared"": ""2""}"
253,253,"def _other_endian(typ):
    """"""Return the type with the 'other' byte order.  Simple types like
    c_int and so on already have __ctype_be__ and __ctype_le__
    attributes which contain the types, for more complicated types
    arrays and structures are supported.
    """"""
    # check _OTHER_ENDIAN attribute (present if typ is primitive type)
    if hasattr(typ, _OTHER_ENDIAN):
        return getattr(typ, _OTHER_ENDIAN)
    # if typ is array
    if isinstance(typ, _array_type):
        return _other_endian(typ._type_) * typ._length_
    # if typ is structure
    if issubclass(typ, Structure):
        return typ
    raise TypeError(""This type does not support other endian: %s"" % typ)","def _other_endian(typ):
    """"""Return the type with the 'other' byte order.  Simple types like
    c_int and so on already have __ctype_be__ and __ctype_le__
    attributes which contain the types, for more complicated types
    arrays and structures are supported.
    """"""
    # check _OTHER_ENDIAN attribute (present if typ is primitive type)
    if hasattr(typ, _OTHER_ENDIAN):
        return getattr(typ, _OTHER_ENDIAN)
    # if typ is array
    if isinstance(typ, _array_type):
        return _other_endian(typ._type_) * typ._length_
    # if typ is structure
    if issubclass(typ, Structure):
        return typ
    raise TypeError(""This type does not support other endian: %s"" % typ)","def get_opposite_endian_type(typ):
    """"""Return the type with the 'other' byte order.  Simple types like
    c_int and so on already have __ctype_be__ and __ctype_le__
    attributes which contain the types, for more complicated types
    arrays and structures are supported.
    """"""
    # check _OTHER_ENDIAN attribute (present if typ is primitive type)
    if hasattr(typ, _OTHER_ENDIAN):
        return getattr(typ, _OTHER_ENDIAN)
    # if typ is array
    if isinstance(typ, _array_type):
        return get_opposite_endian_type(typ._type_) * typ._length_
    # if typ is structure
    if issubclass(typ, Structure):
        return typ
    raise TypeError(""This type does not support other endian: %s"" % typ)","def get_opposite_endian_type(typ):
    """"""Return the type with the 'other' byte order.  Simple types like
    c_int and so on already have __ctype_be__ and __ctype_le__
    attributes which contain the types, for more complicated types
    arrays and structures are supported.
    """"""
    # check _OTHER_ENDIAN attribute (present if typ is primitive type)
    if hasattr(typ, _OTHER_ENDIAN):
        return getattr(typ, _OTHER_ENDIAN)
    # if typ is array
    if isinstance(typ, _array_type):
        return get_opposite_endian_type(typ._type_) * typ._length_
    # if typ is structure
    if issubclass(typ, Structure):
        return typ
    raise TypeError(""This type does not support other endian: %s"" % typ)","{""_other_endian"": ""get_opposite_endian_type""}","def 0(typ):
    """"""Return the type with the 'other' byte order.  Simple types like
    c_int and so on already have __ctype_be__ and __ctype_le__
    attributes which contain the types, for more complicated types
    arrays and structures are supported.
    """"""
    # check _OTHER_ENDIAN attribute (present if typ is primitive type)
    if hasattr(typ, _OTHER_ENDIAN):
        return getattr(typ, _OTHER_ENDIAN)
    # if typ is array
    if isinstance(typ, _array_type):
        return 0(typ._type_) * typ._length_
    # if typ is structure
    if issubclass(typ, Structure):
        return typ
    raise TypeError(""This type does not support other endian: %s"" % typ)","def 0(typ):
    """"""Return the type with the 'other' byte order.  Simple types like
    c_int and so on already have __ctype_be__ and __ctype_le__
    attributes which contain the types, for more complicated types
    arrays and structures are supported.
    """"""
    # check _OTHER_ENDIAN attribute (present if typ is primitive type)
    if hasattr(typ, _OTHER_ENDIAN):
        return getattr(typ, _OTHER_ENDIAN)
    # if typ is array
    if isinstance(typ, _array_type):
        return 0(typ._type_) * typ._length_
    # if typ is structure
    if issubclass(typ, Structure):
        return typ
    raise TypeError(""This type does not support other endian: %s"" % typ)","{""_other_endian"": ""0""}"
254,254,"def dyld_image_suffix_search(iterator, env=None):
    """"""For a potential path iterator, add DYLD_IMAGE_SUFFIX semantics""""""
    suffix = dyld_image_suffix(env)
    if suffix is None:
        return iterator
    def _inject(iterator=iterator, suffix=suffix):
        for path in iterator:
            if path.endswith('.dylib'):
                yield path[:-len('.dylib')] + suffix + '.dylib'
            else:
                yield path + suffix
            yield path
    return _inject()","def dyld_image_suffix(env=None):
    if env is None:
        env = os.environ
    return env.get('DYLD_IMAGE_SUFFIX')","def get_dyld_image_suffix(env=None):
    if env is None:
        env = os.environ
    return env.get('DYLD_IMAGE_SUFFIX')","def get_add_dyld_image_suffix_semantics(iterator, env=None):
    """"""For a potential path iterator, add DYLD_IMAGE_SUFFIX semantics""""""
    suffix = get_dyld_image_suffix(env)
    if suffix is None:
        return iterator
    def inject_paths_with_suffix(iterator=iterator, suffix=suffix):
        for path in iterator:
            if path.endswith('.dylib'):
                yield path[:-len('.dylib')] + suffix + '.dylib'
            else:
                yield path + suffix
            yield path
    return inject_paths_with_suffix()","{""dyld_image_suffix"": ""get_dyld_image_suffix""}","def 0(env=None):
    if env is None:
        env = os.environ
    return env.get('DYLD_IMAGE_SUFFIX')","def 1(iterator, env=None):
    """"""For a potential path iterator, add DYLD_IMAGE_SUFFIX semantics""""""
    suffix = 0(env)
    if suffix is None:
        return iterator
    def 2(iterator=iterator, suffix=suffix):
        for path in iterator:
            if path.endswith('.dylib'):
                yield path[:-len('.dylib')] + suffix + '.dylib'
            else:
                yield path + suffix
            yield path
    return 2()","{""dyld_image_suffix"": ""0""}"
255,255,"def dyld_find(name, executable_path=None, env=None):
    """"""
    Find a library or framework using dyld semantics
    """"""
    for path in dyld_image_suffix_search(chain(
                dyld_override_search(name, env),
                dyld_executable_path_search(name, executable_path),
                dyld_default_search(name, env),
            ), env):

        if os.path.isfile(path):
            return path
        try:
            if _dyld_shared_cache_contains_path(path):
                return path
        except NotImplementedError:
            pass

    raise ValueError(""dylib %s could not be found"" % (name,))","def dyld_default_search(name, env=None):
    yield name

    framework = framework_info(name)

    if framework is not None:
        fallback_framework_path = dyld_fallback_framework_path(env)
        for path in fallback_framework_path:
            yield os.path.join(path, framework['name'])

    fallback_library_path = dyld_fallback_library_path(env)
    for path in fallback_library_path:
        yield os.path.join(path, os.path.basename(name))

    if framework is not None and not fallback_framework_path:
        for path in DEFAULT_FRAMEWORK_FALLBACK:
            yield os.path.join(path, framework['name'])

    if not fallback_library_path:
        for path in DEFAULT_LIBRARY_FALLBACK:
            yield os.path.join(path, os.path.basename(name))

def dyld_image_suffix_search(iterator, env=None):
    """"""For a potential path iterator, add DYLD_IMAGE_SUFFIX semantics""""""
    suffix = dyld_image_suffix(env)
    if suffix is None:
        return iterator
    def _inject(iterator=iterator, suffix=suffix):
        for path in iterator:
            if path.endswith('.dylib'):
                yield path[:-len('.dylib')] + suffix + '.dylib'
            else:
                yield path + suffix
            yield path
    return _inject()

def dyld_executable_path_search(name, executable_path=None):
    # If we haven't done any searching and found a library and the
    # dylib_name starts with ""@executable_path/"" then construct the
    # library name.
    if name.startswith('@executable_path/') and executable_path is not None:
        yield os.path.join(executable_path, name[len('@executable_path/'):])

def dyld_override_search(name, env=None):
    # If DYLD_FRAMEWORK_PATH is set and this dylib_name is a
    # framework name, use the first file that exists in the framework
    # path if any.  If there is none go on to search the DYLD_LIBRARY_PATH
    # if any.

    framework = framework_info(name)

    if framework is not None:
        for path in dyld_framework_path(env):
            yield os.path.join(path, framework['name'])

    # If DYLD_LIBRARY_PATH is set then use the first file that exists
    # in the path.  If none use the original name.
    for path in dyld_library_path(env):
        yield os.path.join(path, os.path.basename(name))","def search_for_library_using_dyld_default_semantics(name, env=None):
    yield name

    framework = framework_info(name)

    if framework is not None:
        fallback_framework_path = dyld_fallback_framework_path(env)
        for path in fallback_framework_path:
            yield os.path.join(path, framework['name'])

    fallback_library_path = dyld_fallback_library_path(env)
    for path in fallback_library_path:
        yield os.path.join(path, os.path.basename(name))

    if framework is not None and not fallback_framework_path:
        for path in DEFAULT_FRAMEWORK_FALLBACK:
            yield os.path.join(path, framework['name'])

    if not fallback_library_path:
        for path in DEFAULT_LIBRARY_FALLBACK:
            yield os.path.join(path, os.path.basename(name))

def add_dyld_image_suffix_semantics(iterator, env=None):
    """"""For a potential path iterator, add DYLD_IMAGE_SUFFIX semantics""""""
    suffix = dyld_image_suffix(env)
    if suffix is None:
        return iterator
    def _inject(iterator=iterator, suffix=suffix):
        for path in iterator:
            if path.endswith('.dylib'):
                yield path[:-len('.dylib')] + suffix + '.dylib'
            else:
                yield path + suffix
            yield path
    return _inject()

def search_for_library_using_executable_path(name, executable_path=None):
    # If we haven't done any searching and found a library and the
    # dylib_name starts with ""@executable_path/"" then construct the
    # library name.
    if name.startswith('@executable_path/') and executable_path is not None:
        yield os.path.join(executable_path, name[len('@executable_path/'):])

def override_search_for_library_using_dyld_semantics(name, env=None):
    # If DYLD_FRAMEWORK_PATH is set and this dylib_name is a
    # framework name, use the first file that exists in the framework
    # path if any.  If there is none go on to search the DYLD_LIBRARY_PATH
    # if any.

    framework = framework_info(name)

    if framework is not None:
        for path in dyld_framework_path(env):
            yield os.path.join(path, framework['name'])

    # If DYLD_LIBRARY_PATH is set then use the first file that exists
    # in the path.  If none use the original name.
    for path in dyld_library_path(env):
        yield os.path.join(path, os.path.basename(name))","def find_library_using_dyld_semantics(name, executable_path=None, env=None):
    """"""
    Find a library or framework using dyld semantics
    """"""
    for path in add_dyld_image_suffix_semantics(chain(
                override_search_for_library_using_dyld_semantics(name, env),
                search_for_library_using_executable_path(name, executable_path),
                search_for_library_using_dyld_default_semantics(name, env),
            ), env):

        if os.path.isfile(path):
            return path
        try:
            if _dyld_shared_cache_contains_path(path):
                return path
        except NotImplementedError:
            pass

    raise ValueError(""dylib %s could not be found"" % (name,))","{""dyld_default_search"": ""search_for_library_using_dyld_default_semantics"", ""dyld_image_suffix_search"": ""add_dyld_image_suffix_semantics"", ""dyld_executable_path_search"": ""search_for_library_using_executable_path"", ""dyld_override_search"": ""override_search_for_library_using_dyld_semantics""}","def 0(name, env=None):
    yield name

    framework = framework_info(name)

    if framework is not None:
        fallback_framework_path = dyld_fallback_framework_path(env)
        for path in fallback_framework_path:
            yield os.path.join(path, framework['name'])

    fallback_library_path = dyld_fallback_library_path(env)
    for path in fallback_library_path:
        yield os.path.join(path, os.path.basename(name))

    if framework is not None and not fallback_framework_path:
        for path in DEFAULT_FRAMEWORK_FALLBACK:
            yield os.path.join(path, framework['name'])

    if not fallback_library_path:
        for path in DEFAULT_LIBRARY_FALLBACK:
            yield os.path.join(path, os.path.basename(name))

def 1(iterator, env=None):
    """"""For a potential path iterator, add DYLD_IMAGE_SUFFIX semantics""""""
    suffix = dyld_image_suffix(env)
    if suffix is None:
        return iterator
    def _inject(iterator=iterator, suffix=suffix):
        for path in iterator:
            if path.endswith('.dylib'):
                yield path[:-len('.dylib')] + suffix + '.dylib'
            else:
                yield path + suffix
            yield path
    return _inject()

def 3(name, executable_path=None):
    # If we haven't done any searching and found a library and the
    # dylib_name starts with ""@executable_path/"" then construct the
    # library name.
    if name.startswith('@executable_path/') and executable_path is not None:
        yield os.path.join(executable_path, name[len('@executable_path/'):])

def 4(name, env=None):
    # If DYLD_FRAMEWORK_PATH is set and this dylib_name is a
    # framework name, use the first file that exists in the framework
    # path if any.  If there is none go on to search the DYLD_LIBRARY_PATH
    # if any.

    framework = framework_info(name)

    if framework is not None:
        for path in dyld_framework_path(env):
            yield os.path.join(path, framework['name'])

    # If DYLD_LIBRARY_PATH is set then use the first file that exists
    # in the path.  If none use the original name.
    for path in dyld_library_path(env):
        yield os.path.join(path, os.path.basename(name))","def 5(name, executable_path=None, env=None):
    """"""
    Find a library or framework using dyld semantics
    """"""
    for path in 1(chain(
                4(name, env),
                3(name, executable_path),
                0(name, env),
            ), env):

        if os.path.isfile(path):
            return path
        try:
            if _dyld_shared_cache_contains_path(path):
                return path
        except NotImplementedError:
            pass

    raise ValueError(""dylib %s could not be found"" % (name,))","{""dyld_default_search"": ""0"", ""dyld_image_suffix_search"": ""1"", ""dyld_executable_path_search"": ""3"", ""dyld_override_search"": ""4""}"
256,256,"def framework_find(fn, executable_path=None, env=None):
    """"""
    Find a framework using dyld semantics in a very loose manner.

    Will take input such as:
        Python
        Python.framework
        Python.framework/Versions/Current
    """"""
    error = None
    try:
        return dyld_find(fn, executable_path=executable_path, env=env)
    except ValueError as e:
        error = e
    fmwk_index = fn.rfind('.framework')
    if fmwk_index == -1:
        fmwk_index = len(fn)
        fn += '.framework'
    fn = os.path.join(fn, os.path.basename(fn[:fmwk_index]))
    try:
        return dyld_find(fn, executable_path=executable_path, env=env)
    except ValueError:
        raise error
    finally:
        error = None","def dyld_find(name, executable_path=None, env=None):
    """"""
    Find a library or framework using dyld semantics
    """"""
    for path in dyld_image_suffix_search(chain(
                dyld_override_search(name, env),
                dyld_executable_path_search(name, executable_path),
                dyld_default_search(name, env),
            ), env):

        if os.path.isfile(path):
            return path
        try:
            if _dyld_shared_cache_contains_path(path):
                return path
        except NotImplementedError:
            pass

    raise ValueError(""dylib %s could not be found"" % (name,))","def find_library_using_dyld_semantics(name, executable_path=None, env=None):
    """"""
    Find a library or framework using dyld semantics
    """"""
    for path in dyld_image_suffix_search(chain(
                dyld_override_search(name, env),
                dyld_executable_path_search(name, executable_path),
                dyld_default_search(name, env),
            ), env):

        if os.path.isfile(path):
            return path
        try:
            if _dyld_shared_cache_contains_path(path):
                return path
        except NotImplementedError:
            pass

    raise ValueError(""dylib %s could not be found"" % (name,))","def find_framework_using_dyld_semantics_loose_manner(fn, executable_path=None, env=None):
    """"""
    Find a framework using dyld semantics in a very loose manner.

    Will take input such as:
        Python
        Python.framework
        Python.framework/Versions/Current
    """"""
    error = None
    try:
        return find_library_using_dyld_semantics(fn, executable_path=executable_path, env=env)
    except ValueError as e:
        error = e
    fmwk_index = fn.rfind('.framework')
    if fmwk_index == -1:
        fmwk_index = len(fn)
        fn += '.framework'
    fn = os.path.join(fn, os.path.basename(fn[:fmwk_index]))
    try:
        return find_library_using_dyld_semantics(fn, executable_path=executable_path, env=env)
    except ValueError:
        raise error
    finally:
        error = None","{""dyld_find"": ""find_library_using_dyld_semantics""}","def 0(name, executable_path=None, env=None):
    """"""
    Find a library or framework using dyld semantics
    """"""
    for path in dyld_image_suffix_search(chain(
                dyld_override_search(name, env),
                dyld_executable_path_search(name, executable_path),
                dyld_default_search(name, env),
            ), env):

        if os.path.isfile(path):
            return path
        try:
            if _dyld_shared_cache_contains_path(path):
                return path
        except NotImplementedError:
            pass

    raise ValueError(""dylib %s could not be found"" % (name,))","def 1(fn, executable_path=None, env=None):
    """"""
    Find a framework using dyld semantics in a very loose manner.

    Will take input such as:
        Python
        Python.framework
        Python.framework/Versions/Current
    """"""
    error = None
    try:
        return 0(fn, executable_path=executable_path, env=env)
    except ValueError as e:
        error = e
    fmwk_index = fn.rfind('.framework')
    if fmwk_index == -1:
        fmwk_index = len(fn)
        fn += '.framework'
    fn = os.path.join(fn, os.path.basename(fn[:fmwk_index]))
    try:
        return 0(fn, executable_path=executable_path, env=env)
    except ValueError:
        raise error
    finally:
        error = None","{""dyld_find"": ""0""}"
257,257,"def doModuleCleanups():
    """"""Execute all module cleanup functions. Normally called for you after
    tearDownModule.""""""
    exceptions = []
    while _module_cleanups:
        function, args, kwargs = _module_cleanups.pop()
        try:
            function(*args, **kwargs)
        except Exception as exc:
            exceptions.append(exc)
    if exceptions:
        # Swallows all but first exception. If a multi-exception handler
        # gets written we should use that here instead.
        raise exceptions[0]","def function(a, b=Foo): pass","def function_with_default_argument_Foo(a, b=Foo): pass","def execute_all_module_cleanup_functions_normally_called_for_you_after_tear_down_module():
    """"""Execute all module cleanup function_with_default_argument_Foos. Normally called for you after
    tearDownModule.""""""
    exceptions = []
    while _module_cleanups:
        function_with_default_argument_Foo, args, kwargs = _module_cleanups.pop()
        try:
            function_with_default_argument_Foo(*args, **kwargs)
        except Exception as exc:
            exceptions.append(exc)
    if exceptions:
        # Swallows all but first exception. If a multi-exception handler
        # gets written we should use that here instead.
        raise exceptions[0]","{""function"": ""function_with_default_argument_Foo""}","def 0(a, b=Foo): pass","def 1():
    """"""Execute all module cleanup functions. Normally called for you after
    tearDownModule.""""""
    exceptions = []
    while _module_cleanups:
        function, args, kwargs = _module_cleanups.pop()
        try:
            0(*args, **kwargs)
        except Exception as exc:
            exceptions.append(exc)
    if exceptions:
        # Swallows all but first exception. If a multi-exception handler
        # gets written we should use that here instead.
        raise exceptions[0]","{""function"": ""0""}"
258,258,"def skipIf(condition, reason):
    """"""
    Skip a test if the condition is true.
    """"""
    if condition:
        return skip(reason)
    return _id","def skip(reason):
    """"""
    Unconditionally skip a test.
    """"""
    def decorator(test_item):
        if not isinstance(test_item, type):
            @functools.wraps(test_item)
            def skip_wrapper(*args, **kwargs):
                raise SkipTest(reason)
            test_item = skip_wrapper

        test_item.__unittest_skip__ = True
        test_item.__unittest_skip_why__ = reason
        return test_item
    if isinstance(reason, types.FunctionType):
        test_item = reason
        reason = ''
        return decorator(test_item)
    return decorator","def unconditionally_skip_test_due_to_reason(reason):
    """"""
    Unconditionally unconditionally_skip_test_due_to_reason a test.
    """"""
    def decorator(test_item):
        if not isinstance(test_item, type):
            @functools.wraps(test_item)
            def unconditionally_skip_test_due_to_reason_wrapper(*args, **kwargs):
                raise SkipTest(reason)
            test_item = unconditionally_skip_test_due_to_reason_wrapper

        test_item.__unittest_unconditionally_skip_test_due_to_reason__ = True
        test_item.__unittest_unconditionally_skip_test_due_to_reason_why__ = reason
        return test_item
    if isinstance(reason, types.FunctionType):
        test_item = reason
        reason = ''
        return decorator(test_item)
    return decorator","def unconditionally_skip_test_due_to_reasonIf(condition, reason):
    """"""
    Skip a test if the condition is true.
    """"""
    if condition:
        return unconditionally_skip_test_due_to_reason(reason)
    return _id","{""skip"": ""unconditionally_skip_test_due_to_reason""}","def 0(reason):
    """"""
    Unconditionally skip a test.
    """"""
    def decorator(test_item):
        if not isinstance(test_item, type):
            @functools.wraps(test_item)
            def skip_wrapper(*args, **kwargs):
                raise SkipTest(reason)
            test_item = skip_wrapper

        test_item.__unittest_skip__ = True
        test_item.__unittest_skip_why__ = reason
        return test_item
    if isinstance(reason, types.FunctionType):
        test_item = reason
        reason = ''
        return decorator(test_item)
    return decorator","def 3(condition, reason):
    """"""
    Skip a test if the condition is true.
    """"""
    if condition:
        return 0(reason)
    return _id","{""skip"": ""0""}"
259,259,"def skipUnless(condition, reason):
    """"""
    Skip a test unless the condition is true.
    """"""
    if not condition:
        return skip(reason)
    return _id","def skip(reason):
    """"""
    Unconditionally skip a test.
    """"""
    def decorator(test_item):
        if not isinstance(test_item, type):
            @functools.wraps(test_item)
            def skip_wrapper(*args, **kwargs):
                raise SkipTest(reason)
            test_item = skip_wrapper

        test_item.__unittest_skip__ = True
        test_item.__unittest_skip_why__ = reason
        return test_item
    if isinstance(reason, types.FunctionType):
        test_item = reason
        reason = ''
        return decorator(test_item)
    return decorator","def unconditionally_skip_test_due_to_reason(reason):
    """"""
    Unconditionally unconditionally_skip_test_due_to_reason a test.
    """"""
    def decorator(test_item):
        if not isinstance(test_item, type):
            @functools.wraps(test_item)
            def unconditionally_skip_test_due_to_reason_wrapper(*args, **kwargs):
                raise SkipTest(reason)
            test_item = unconditionally_skip_test_due_to_reason_wrapper

        test_item.__unittest_unconditionally_skip_test_due_to_reason__ = True
        test_item.__unittest_unconditionally_skip_test_due_to_reason_why__ = reason
        return test_item
    if isinstance(reason, types.FunctionType):
        test_item = reason
        reason = ''
        return decorator(test_item)
    return decorator","def unconditionally_skip_test_due_to_reasonUnless(condition, reason):
    """"""
    Skip a test unless the condition is true.
    """"""
    if not condition:
        return unconditionally_skip_test_due_to_reason(reason)
    return _id","{""skip"": ""unconditionally_skip_test_due_to_reason""}","def 0(reason):
    """"""
    Unconditionally skip a test.
    """"""
    def decorator(test_item):
        if not isinstance(test_item, type):
            @functools.wraps(test_item)
            def skip_wrapper(*args, **kwargs):
                raise SkipTest(reason)
            test_item = skip_wrapper

        test_item.__unittest_skip__ = True
        test_item.__unittest_skip_why__ = reason
        return test_item
    if isinstance(reason, types.FunctionType):
        test_item = reason
        reason = ''
        return decorator(test_item)
    return decorator","def 3(condition, reason):
    """"""
    Skip a test unless the condition is true.
    """"""
    if not condition:
        return 0(reason)
    return _id","{""skip"": ""0""}"
260,260,"def seal(mock):
    """"""Disable the automatic generation of child mocks.

    Given an input Mock, seals it to ensure no further mocks will be generated
    when accessing an attribute that was not already defined.

    The operation recursively seals the mock passed in, meaning that
    the mock itself, any mocks generated by accessing one of its attributes,
    and all assigned mocks without a name or spec will be sealed.
    """"""
    mock._mock_sealed = True
    for attr in dir(mock):
        try:
            m = getattr(mock, attr)
        except AttributeError:
            continue
        if not isinstance(m, NonCallableMock):
            continue
        if isinstance(m._mock_children.get(attr), _SpecState):
            continue
        if m._mock_new_parent is mock:
            seal(m)","def seal(mock):
    """"""Disable the automatic generation of child mocks.

    Given an input Mock, seals it to ensure no further mocks will be generated
    when accessing an attribute that was not already defined.

    The operation recursively seals the mock passed in, meaning that
    the mock itself, any mocks generated by accessing one of its attributes,
    and all assigned mocks without a name or spec will be sealed.
    """"""
    mock._mock_sealed = True
    for attr in dir(mock):
        try:
            m = getattr(mock, attr)
        except AttributeError:
            continue
        if not isinstance(m, NonCallableMock):
            continue
        if isinstance(m._mock_children.get(attr), _SpecState):
            continue
        if m._mock_new_parent is mock:
            seal(m)","def disable_automatic_generation_of_child_mocks(mock):
    """"""Disable the automatic generation of child mocks.

    Given an input Mock, disable_automatic_generation_of_child_mockss it to ensure no further mocks will be generated
    when accessing an attribute that was not already defined.

    The operation recursively disable_automatic_generation_of_child_mockss the mock passed in, meaning that
    the mock itself, any mocks generated by accessing one of its attributes,
    and all assigned mocks without a name or spec will be disable_automatic_generation_of_child_mocksed.
    """"""
    mock._mock_disable_automatic_generation_of_child_mocksed = True
    for attr in dir(mock):
        try:
            m = getattr(mock, attr)
        except AttributeError:
            continue
        if not isinstance(m, NonCallableMock):
            continue
        if isinstance(m._mock_children.get(attr), _SpecState):
            continue
        if m._mock_new_parent is mock:
            disable_automatic_generation_of_child_mocks(m)","def disable_automatic_generation_of_child_mocks(mock):
    """"""Disable the automatic generation of child mocks.

    Given an input Mock, disable_automatic_generation_of_child_mockss it to ensure no further mocks will be generated
    when accessing an attribute that was not already defined.

    The operation recursively disable_automatic_generation_of_child_mockss the mock passed in, meaning that
    the mock itself, any mocks generated by accessing one of its attributes,
    and all assigned mocks without a name or spec will be disable_automatic_generation_of_child_mocksed.
    """"""
    mock._mock_disable_automatic_generation_of_child_mocksed = True
    for attr in dir(mock):
        try:
            m = getattr(mock, attr)
        except AttributeError:
            continue
        if not isinstance(m, NonCallableMock):
            continue
        if isinstance(m._mock_children.get(attr), _SpecState):
            continue
        if m._mock_new_parent is mock:
            disable_automatic_generation_of_child_mocks(m)","{""seal"": ""disable_automatic_generation_of_child_mocks""}","def 0(mock):
    """"""Disable the automatic generation of child mocks.

    Given an input Mock, seals it to ensure no further mocks will be generated
    when accessing an attribute that was not already defined.

    The operation recursively seals the mock passed in, meaning that
    the mock itself, any mocks generated by accessing one of its attributes,
    and all assigned mocks without a name or spec will be sealed.
    """"""
    mock._mock_sealed = True
    for attr in dir(mock):
        try:
            m = getattr(mock, attr)
        except AttributeError:
            continue
        if not isinstance(m, NonCallableMock):
            continue
        if isinstance(m._mock_children.get(attr), _SpecState):
            continue
        if m._mock_new_parent is mock:
            0(m)","def 0(mock):
    """"""Disable the automatic generation of child mocks.

    Given an input Mock, seals it to ensure no further mocks will be generated
    when accessing an attribute that was not already defined.

    The operation recursively seals the mock passed in, meaning that
    the mock itself, any mocks generated by accessing one of its attributes,
    and all assigned mocks without a name or spec will be sealed.
    """"""
    mock._mock_sealed = True
    for attr in dir(mock):
        try:
            m = getattr(mock, attr)
        except AttributeError:
            continue
        if not isinstance(m, NonCallableMock):
            continue
        if isinstance(m._mock_children.get(attr), _SpecState):
            continue
        if m._mock_new_parent is mock:
            0(m)","{""seal"": ""0""}"
261,261,"def log_to_stderr(level=None):
    '''
    Turn on logging and add a handler which prints to stderr
    '''
    global _log_to_stderr
    import logging

    logger = get_logger()
    formatter = logging.Formatter(DEFAULT_LOGGING_FORMAT)
    handler = logging.StreamHandler()
    handler.setFormatter(formatter)
    logger.addHandler(handler)

    if level:
        logger.setLevel(level)
    _log_to_stderr = True
    return _logger","def get_logger():
    '''
    Returns logger used by multiprocessing
    '''
    global _logger
    import logging

    logging._acquireLock()
    try:
        if not _logger:

            _logger = logging.getLogger(LOGGER_NAME)
            _logger.propagate = 0

            # XXX multiprocessing should cleanup before logging
            if hasattr(atexit, 'unregister'):
                atexit.unregister(_exit_function)
                atexit.register(_exit_function)
            else:
                atexit._exithandlers.remove((_exit_function, (), {}))
                atexit._exithandlers.append((_exit_function, (), {}))

    finally:
        logging._releaseLock()

    return _logger","def get_multiprocessing_logger():
    '''
    Returns logger used by multiprocessing
    '''
    global _logger
    import logging

    logging._acquireLock()
    try:
        if not _logger:

            _logger = logging.getLogger(LOGGER_NAME)
            _logger.propagate = 0

            # XXX multiprocessing should cleanup before logging
            if hasattr(atexit, 'unregister'):
                atexit.unregister(_exit_function)
                atexit.register(_exit_function)
            else:
                atexit._exithandlers.remove((_exit_function, (), {}))
                atexit._exithandlers.append((_exit_function, (), {}))

    finally:
        logging._releaseLock()

    return _logger","def turn_on_logging_and_add_stderr_handler(level=None):
    '''
    Turn on logging and add a handler which prints to stderr
    '''
    global _turn_on_logging_and_add_stderr_handler
    import logging

    logger = get_multiprocessing_logger()
    formatter = logging.Formatter(DEFAULT_LOGGING_FORMAT)
    handler = logging.StreamHandler()
    handler.setFormatter(formatter)
    logger.addHandler(handler)

    if level:
        logger.setLevel(level)
    _turn_on_logging_and_add_stderr_handler = True
    return _logger","{""get_logger"": ""get_multiprocessing_logger""}","def 0():
    '''
    Returns logger used by multiprocessing
    '''
    global _logger
    import logging

    logging._acquireLock()
    try:
        if not _logger:

            _logger = logging.getLogger(LOGGER_NAME)
            _logger.propagate = 0

            # XXX multiprocessing should cleanup before logging
            if hasattr(atexit, 'unregister'):
                atexit.unregister(_exit_function)
                atexit.register(_exit_function)
            else:
                atexit._exithandlers.remove((_exit_function, (), {}))
                atexit._exithandlers.append((_exit_function, (), {}))

    finally:
        logging._releaseLock()

    return _logger","def 1(level=None):
    '''
    Turn on logging and add a handler which prints to stderr
    '''
    global _log_to_stderr
    import logging

    logger = 0()
    formatter = logging.Formatter(DEFAULT_LOGGING_FORMAT)
    handler = logging.StreamHandler()
    handler.setFormatter(formatter)
    logger.addHandler(handler)

    if level:
        logger.setLevel(level)
    _log_to_stderr = True
    return _logger","{""get_logger"": ""0""}"
262,262,"def _cleanup_tests():
    """"""Cleanup multiprocessing resources when multiprocessing tests
    completed.""""""

    from test import support

    # cleanup multiprocessing
    process._cleanup()

    # Stop the ForkServer process if it's running
    from multiprocessing import forkserver
    forkserver._forkserver._stop()

    # Stop the ResourceTracker process if it's running
    from multiprocessing import resource_tracker
    resource_tracker._resource_tracker._stop()

    # bpo-37421: Explicitly call _run_finalizers() to remove immediately
    # temporary directories created by multiprocessing.util.get_temp_dir().
    _run_finalizers()
    support.gc_collect()

    support.reap_children()","def _run_finalizers(minpriority=None):
    '''
    Run all finalizers whose exit priority is not None and at least minpriority

    Finalizers with highest priority are called first; finalizers with
    the same priority will be called in reverse order of creation.
    '''
    if _finalizer_registry is None:
        # This function may be called after this module's globals are
        # destroyed.  See the _exit_function function in this module for more
        # notes.
        return

    if minpriority is None:
        f = lambda p : p[0] is not None
    else:
        f = lambda p : p[0] is not None and p[0] >= minpriority

    # Careful: _finalizer_registry may be mutated while this function
    # is running (either by a GC run or by another thread).

    # list(_finalizer_registry) should be atomic, while
    # list(_finalizer_registry.items()) is not.
    keys = [key for key in list(_finalizer_registry) if f(key)]
    keys.sort(reverse=True)

    for key in keys:
        finalizer = _finalizer_registry.get(key)
        # key may have been removed from the registry
        if finalizer is not None:
            sub_debug('calling %s', finalizer)
            try:
                finalizer()
            except Exception:
                import traceback
                traceback.print_exc()

    if minpriority is None:
        _finalizer_registry.clear()","def run_finalizers_with_minpriority(minpriority=None):
    '''
    Run all finalizers whose exit priority is not None and at least minpriority

    Finalizers with highest priority are called first; finalizers with
    the same priority will be called in reverse order of creation.
    '''
    if _finalizer_registry is None:
        # This function may be called after this module's globals are
        # destroyed.  See the _exit_function function in this module for more
        # notes.
        return

    if minpriority is None:
        f = lambda p : p[0] is not None
    else:
        f = lambda p : p[0] is not None and p[0] >= minpriority

    # Careful: _finalizer_registry may be mutated while this function
    # is running (either by a GC run or by another thread).

    # list(_finalizer_registry) should be atomic, while
    # list(_finalizer_registry.items()) is not.
    keys = [key for key in list(_finalizer_registry) if f(key)]
    keys.sort(reverse=True)

    for key in keys:
        finalizer = _finalizer_registry.get(key)
        # key may have been removed from the registry
        if finalizer is not None:
            sub_debug('calling %s', finalizer)
            try:
                finalizer()
            except Exception:
                import traceback
                traceback.print_exc()

    if minpriority is None:
        _finalizer_registry.clear()","def cleanup_multiprocessing_resources_when_tests_completed():
    """"""Cleanup multiprocessing resources when multiprocessing tests
    completed.""""""

    from test import support

    # cleanup multiprocessing
    process._cleanup()

    # Stop the ForkServer process if it's running
    from multiprocessing import forkserver
    forkserver._forkserver._stop()

    # Stop the ResourceTracker process if it's running
    from multiprocessing import resource_tracker
    resource_tracker._resource_tracker._stop()

    # bpo-37421: Explicitly call run_finalizers_with_minpriority() to remove immediately
    # temporary directories created by multiprocessing.util.get_temp_dir().
    run_finalizers_with_minpriority()
    support.gc_collect()

    support.reap_children()","{""_run_finalizers"": ""run_finalizers_with_minpriority""}","def 0(minpriority=None):
    '''
    Run all finalizers whose exit priority is not None and at least minpriority

    Finalizers with highest priority are called first; finalizers with
    the same priority will be called in reverse order of creation.
    '''
    if _finalizer_registry is None:
        # This function may be called after this module's globals are
        # destroyed.  See the _exit_function function in this module for more
        # notes.
        return

    if minpriority is None:
        f = lambda p : p[0] is not None
    else:
        f = lambda p : p[0] is not None and p[0] >= minpriority

    # Careful: _finalizer_registry may be mutated while this function
    # is running (either by a GC run or by another thread).

    # list(_finalizer_registry) should be atomic, while
    # list(_finalizer_registry.items()) is not.
    keys = [key for key in list(_finalizer_registry) if f(key)]
    keys.sort(reverse=True)

    for key in keys:
        finalizer = _finalizer_registry.get(key)
        # key may have been removed from the registry
        if finalizer is not None:
            sub_debug('calling %s', finalizer)
            try:
                finalizer()
            except Exception:
                import traceback
                traceback.print_exc()

    if minpriority is None:
        _finalizer_registry.clear()","def 1():
    """"""Cleanup multiprocessing resources when multiprocessing tests
    completed.""""""

    from test import support

    # cleanup multiprocessing
    process._cleanup()

    # Stop the ForkServer process if it's running
    from multiprocessing import forkserver
    forkserver._forkserver._stop()

    # Stop the ResourceTracker process if it's running
    from multiprocessing import resource_tracker
    resource_tracker._resource_tracker._stop()

    # bpo-37421: Explicitly call 0() to remove immediately
    # temporary directories created by multiprocessing.util.get_temp_dir().
    0()
    support.gc_collect()

    support.reap_children()","{""_run_finalizers"": ""0""}"
263,263,"def SocketClient(address):
    '''
    Return a connection object connected to the socket given by `address`
    '''
    family = address_type(address)
    with socket.socket( getattr(socket, family) ) as s:
        s.setblocking(True)
        s.connect(address)
        return Connection(s.detach())","def address_type(address):
    '''
    Return the types of the address

    This can be 'AF_INET', 'AF_UNIX', or 'AF_PIPE'
    '''
    if type(address) == tuple:
        return 'AF_INET'
    elif type(address) is str and address.startswith('\\\\'):
        return 'AF_PIPE'
    elif type(address) is str or util.is_abstract_socket_namespace(address):
        return 'AF_UNIX'
    else:
        raise ValueError('address type of %r unrecognized' % address)","def get_address_type(address):
    '''
    Return the types of the address

    This can be 'AF_INET', 'AF_UNIX', or 'AF_PIPE'
    '''
    if type(address) == tuple:
        return 'AF_INET'
    elif type(address) is str and address.startswith('\\\\'):
        return 'AF_PIPE'
    elif type(address) is str or util.is_abstract_socket_namespace(address):
        return 'AF_UNIX'
    else:
        raise ValueError('address type of %r unrecognized' % address)","def return_connection_object_connected_to_socket_given_by_address(address):
    '''
    Return a connection object connected to the socket given by `address`
    '''
    family = get_address_type(address)
    with socket.socket( getattr(socket, family) ) as s:
        s.setblocking(True)
        s.connect(address)
        return Connection(s.detach())","{""address_type"": ""get_address_type""}","def 0(address):
    '''
    Return the types of the address

    This can be 'AF_INET', 'AF_UNIX', or 'AF_PIPE'
    '''
    if type(address) == tuple:
        return 'AF_INET'
    elif type(address) is str and address.startswith('\\\\'):
        return 'AF_PIPE'
    elif type(address) is str or util.is_abstract_socket_namespace(address):
        return 'AF_UNIX'
    else:
        raise ValueError('address type of %r unrecognized' % address)","def 1(address):
    '''
    Return a connection object connected to the socket given by `address`
    '''
    family = 0(address)
    with socket.socket( getattr(socket, family) ) as s:
        s.setblocking(True)
        s.connect(address)
        return Connection(s.detach())","{""address_type"": ""0""}"
264,264,"def import_main_path(main_path):
    '''
    Set sys.modules['__main__'] to module at main_path
    '''
    _fixup_main_from_path(main_path)","def _fixup_main_from_path(main_path):
    # If this process was forked, __main__ may already be populated
    current_main = sys.modules['__main__']

    # Unfortunately, the main ipython launch script historically had no
    # ""if __name__ == '__main__'"" guard, so we work around that
    # by treating it like a __main__.py file
    # See https://github.com/ipython/ipython/issues/4698
    main_name = os.path.splitext(os.path.basename(main_path))[0]
    if main_name == 'ipython':
        return

    # Otherwise, if __file__ already has the setting we expect,
    # there's nothing more to do
    if getattr(current_main, '__file__', None) == main_path:
        return

    # If the parent process has sent a path through rather than a module
    # name we assume it is an executable script that may contain
    # non-main code that needs to be executed
    old_main_modules.append(current_main)
    main_module = types.ModuleType(""__mp_main__"")
    main_content = runpy.run_path(main_path,
                                  run_name=""__mp_main__"")
    main_module.__dict__.update(main_content)
    sys.modules['__main__'] = sys.modules['__mp_main__'] = main_module","def fixup_main_module_from_path(main_path):
    # If this process was forked, __main__ may already be populated
    current_main = sys.modules['__main__']

    # Unfortunately, the main ipython launch script historically had no
    # ""if __name__ == '__main__'"" guard, so we work around that
    # by treating it like a __main__.py file
    # See https://github.com/ipython/ipython/issues/4698
    main_name = os.path.splitext(os.path.basename(main_path))[0]
    if main_name == 'ipython':
        return

    # Otherwise, if __file__ already has the setting we expect,
    # there's nothing more to do
    if getattr(current_main, '__file__', None) == main_path:
        return

    # If the parent process has sent a path through rather than a module
    # name we assume it is an executable script that may contain
    # non-main code that needs to be executed
    old_main_modules.append(current_main)
    main_module = types.ModuleType(""__mp_main__"")
    main_content = runpy.run_path(main_path,
                                  run_name=""__mp_main__"")
    main_module.__dict__.update(main_content)
    sys.modules['__main__'] = sys.modules['__mp_main__'] = main_module","def set_sys_modules___main___to_module_at_main_path(main_path):
    '''
    Set sys.modules['__main__'] to module at main_path
    '''
    fixup_main_module_from_path(main_path)","{""_fixup_main_from_path"": ""fixup_main_module_from_path""}","def 0(main_path):
    # If this process was forked, __main__ may already be populated
    current_main = sys.modules['__main__']

    # Unfortunately, the main ipython launch script historically had no
    # ""if __name__ == '__main__'"" guard, so we work around that
    # by treating it like a __main__.py file
    # See https://github.com/ipython/ipython/issues/4698
    main_name = os.path.splitext(os.path.basename(main_path))[0]
    if main_name == 'ipython':
        return

    # Otherwise, if __file__ already has the setting we expect,
    # there's nothing more to do
    if getattr(current_main, '__file__', None) == main_path:
        return

    # If the parent process has sent a path through rather than a module
    # name we assume it is an executable script that may contain
    # non-main code that needs to be executed
    old_main_modules.append(current_main)
    main_module = types.ModuleType(""__mp_main__"")
    main_content = runpy.run_path(main_path,
                                  run_name=""__mp_main__"")
    main_module.__dict__.update(main_content)
    sys.modules['__main__'] = sys.modules['__mp_main__'] = main_module","def 1(main_path):
    '''
    Set sys.modules['__main__'] to module at main_path
    '''
    0(main_path)","{""_fixup_main_from_path"": ""0""}"
265,265,"def RawValue(typecode_or_type, *args):
    '''
    Returns a ctypes object allocated from shared memory
    '''
    type_ = typecode_to_type.get(typecode_or_type, typecode_or_type)
    obj = _new_value(type_)
    ctypes.memset(ctypes.addressof(obj), 0, ctypes.sizeof(obj))
    obj.__init__(*args)
    return obj","def _new_value(type_):
    size = ctypes.sizeof(type_)
    wrapper = heap.BufferWrapper(size)
    return rebuild_ctype(type_, wrapper, None)","def _create_new_value(type_):
    size = ctypes.sizeof(type_)
    wrapper = heap.BufferWrapper(size)
    return rebuild_ctype(type_, wrapper, None)","def create_shared_memory_object(typecode_or_type, *args):
    '''
    Returns a ctypes object allocated from shared memory
    '''
    type_ = typecode_to_type.get(typecode_or_type, typecode_or_type)
    obj = _create_new_value(type_)
    ctypes.memset(ctypes.addressof(obj), 0, ctypes.sizeof(obj))
    obj.__init__(*args)
    return obj","{""_new_value"": ""_create_new_value""}","def 0(type_):
    size = ctypes.sizeof(type_)
    wrapper = heap.BufferWrapper(size)
    return rebuild_ctype(type_, wrapper, None)","def 1(typecode_or_type, *args):
    '''
    Returns a ctypes object allocated from shared memory
    '''
    type_ = typecode_to_type.get(typecode_or_type, typecode_or_type)
    obj = 0(type_)
    ctypes.memset(ctypes.addressof(obj), 0, ctypes.sizeof(obj))
    obj.__init__(*args)
    return obj","{""_new_value"": ""0""}"
266,266,"def RawArray(typecode_or_type, size_or_initializer):
    '''
    Returns a ctypes array allocated from shared memory
    '''
    type_ = typecode_to_type.get(typecode_or_type, typecode_or_type)
    if isinstance(size_or_initializer, int):
        type_ = type_ * size_or_initializer
        obj = _new_value(type_)
        ctypes.memset(ctypes.addressof(obj), 0, ctypes.sizeof(obj))
        return obj
    else:
        type_ = type_ * len(size_or_initializer)
        result = _new_value(type_)
        result.__init__(*size_or_initializer)
        return result","def _new_value(type_):
    size = ctypes.sizeof(type_)
    wrapper = heap.BufferWrapper(size)
    return rebuild_ctype(type_, wrapper, None)","def _create_new_value(type_):
    size = ctypes.sizeof(type_)
    wrapper = heap.BufferWrapper(size)
    return rebuild_ctype(type_, wrapper, None)","def return_ctypes_array_allocated_from_shared_memory(typecode_or_type, size_or_initializer):
    '''
    Returns a ctypes array allocated from shared memory
    '''
    type_ = typecode_to_type.get(typecode_or_type, typecode_or_type)
    if isinstance(size_or_initializer, int):
        type_ = type_ * size_or_initializer
        obj = _create_new_value(type_)
        ctypes.memset(ctypes.addressof(obj), 0, ctypes.sizeof(obj))
        return obj
    else:
        type_ = type_ * len(size_or_initializer)
        result = _create_new_value(type_)
        result.__init__(*size_or_initializer)
        return result","{""_new_value"": ""_create_new_value""}","def 0(type_):
    size = ctypes.sizeof(type_)
    wrapper = heap.BufferWrapper(size)
    return rebuild_ctype(type_, wrapper, None)","def 1(typecode_or_type, size_or_initializer):
    '''
    Returns a ctypes array allocated from shared memory
    '''
    type_ = typecode_to_type.get(typecode_or_type, typecode_or_type)
    if isinstance(size_or_initializer, int):
        type_ = type_ * size_or_initializer
        obj = 0(type_)
        ctypes.memset(ctypes.addressof(obj), 0, ctypes.sizeof(obj))
        return obj
    else:
        type_ = type_ * len(size_or_initializer)
        result = 0(type_)
        result.__init__(*size_or_initializer)
        return result","{""_new_value"": ""0""}"
267,267,"def Value(typecode_or_type, *args, lock=True, ctx=None):
    '''
    Return a synchronization wrapper for a Value
    '''
    obj = RawValue(typecode_or_type, *args)
    if lock is False:
        return obj
    if lock in (True, None):
        ctx = ctx or get_context()
        lock = ctx.RLock()
    if not hasattr(lock, 'acquire'):
        raise AttributeError(""%r has no method 'acquire'"" % lock)
    return synchronized(obj, lock, ctx=ctx)","def RawValue(typecode_or_type, *args):
    '''
    Returns a ctypes object allocated from shared memory
    '''
    type_ = typecode_to_type.get(typecode_or_type, typecode_or_type)
    obj = _new_value(type_)
    ctypes.memset(ctypes.addressof(obj), 0, ctypes.sizeof(obj))
    obj.__init__(*args)
    return obj

def synchronized(obj, lock=None, ctx=None):
    assert not isinstance(obj, SynchronizedBase), 'object already synchronized'
    ctx = ctx or get_context()

    if isinstance(obj, ctypes._SimpleCData):
        return Synchronized(obj, lock, ctx)
    elif isinstance(obj, ctypes.Array):
        if obj._type_ is ctypes.c_char:
            return SynchronizedString(obj, lock, ctx)
        return SynchronizedArray(obj, lock, ctx)
    else:
        cls = type(obj)
        try:
            scls = class_cache[cls]
        except KeyError:
            names = [field[0] for field in cls._fields_]
            d = {name: make_property(name) for name in names}
            classname = 'Synchronized' + cls.__name__
            scls = class_cache[cls] = type(classname, (SynchronizedBase,), d)
        return scls(obj, lock, ctx)","def create_shared_memory_object(typecode_or_type, *args):
    '''
    Returns a ctypes object allocated from shared memory
    '''
    type_ = typecode_to_type.get(typecode_or_type, typecode_or_type)
    obj = _new_value(type_)
    ctypes.memset(ctypes.addressof(obj), 0, ctypes.sizeof(obj))
    obj.__init__(*args)
    return obj

def create_synchronized_object(obj, lock=None, ctx=None):
    assert not isinstance(obj, SynchronizedBase), 'object already create_synchronized_object'
    ctx = ctx or get_context()

    if isinstance(obj, ctypes._SimpleCData):
        return Synchronized(obj, lock, ctx)
    elif isinstance(obj, ctypes.Array):
        if obj._type_ is ctypes.c_char:
            return SynchronizedString(obj, lock, ctx)
        return SynchronizedArray(obj, lock, ctx)
    else:
        cls = type(obj)
        try:
            scls = class_cache[cls]
        except KeyError:
            names = [field[0] for field in cls._fields_]
            d = {name: make_property(name) for name in names}
            classname = 'Synchronized' + cls.__name__
            scls = class_cache[cls] = type(classname, (SynchronizedBase,), d)
        return scls(obj, lock, ctx)","def synchronized_value(typecode_or_type, *args, lock=True, ctx=None):
    '''
    Return a synchronization wrapper for a synchronized_value
    '''
    obj = create_shared_memory_object(typecode_or_type, *args)
    if lock is False:
        return obj
    if lock in (True, None):
        ctx = ctx or get_context()
        lock = ctx.RLock()
    if not hasattr(lock, 'acquire'):
        raise AttributeError(""%r has no method 'acquire'"" % lock)
    return create_synchronized_object(obj, lock, ctx=ctx)","{""RawValue"": ""create_shared_memory_object"", ""synchronized"": ""create_synchronized_object""}","def 0(typecode_or_type, *args):
    '''
    Returns a ctypes object allocated from shared memory
    '''
    type_ = typecode_to_type.get(typecode_or_type, typecode_or_type)
    obj = _new_value(type_)
    ctypes.memset(ctypes.addressof(obj), 0, ctypes.sizeof(obj))
    obj.__init__(*args)
    return obj

def 1(obj, lock=None, ctx=None):
    assert not isinstance(obj, SynchronizedBase), 'object already synchronized'
    ctx = ctx or get_context()

    if isinstance(obj, ctypes._SimpleCData):
        return Synchronized(obj, lock, ctx)
    elif isinstance(obj, ctypes.Array):
        if obj._type_ is ctypes.c_char:
            return SynchronizedString(obj, lock, ctx)
        return SynchronizedArray(obj, lock, ctx)
    else:
        cls = type(obj)
        try:
            scls = class_cache[cls]
        except KeyError:
            names = [field[0] for field in cls._fields_]
            d = {name: make_property(name) for name in names}
            classname = 'Synchronized' + cls.__name__
            scls = class_cache[cls] = type(classname, (SynchronizedBase,), d)
        return scls(obj, lock, ctx)","def 2(typecode_or_type, *args, lock=True, ctx=None):
    '''
    Return a synchronization wrapper for a Value
    '''
    obj = 0(typecode_or_type, *args)
    if lock is False:
        return obj
    if lock in (True, None):
        ctx = ctx or get_context()
        lock = ctx.RLock()
    if not hasattr(lock, 'acquire'):
        raise AttributeError(""%r has no method 'acquire'"" % lock)
    return 1(obj, lock, ctx=ctx)","{""RawValue"": ""0"", ""synchronized"": ""1""}"
268,268,"def dispatch(c, id, methodname, args=(), kwds={}):
    '''
    Send a message to manager using connection `c` and return response
    '''
    c.send((id, methodname, args, kwds))
    kind, result = c.recv()
    if kind == '#RETURN':
        return result
    raise convert_to_error(kind, result)","def convert_to_error(kind, result):
    if kind == '#ERROR':
        return result
    elif kind in ('#TRACEBACK', '#UNSERIALIZABLE'):
        if not isinstance(result, str):
            raise TypeError(
                ""Result {0!r} (kind '{1}') type is {2}, not str"".format(
                    result, kind, type(result)))
        if kind == '#UNSERIALIZABLE':
            return RemoteError('Unserializable message: %s\n' % result)
        else:
            return RemoteError(result)
    else:
        return ValueError('Unrecognized message type {!r}'.format(kind))","def handle_conversion_to_error(kind, result):
    if kind == '#ERROR':
        return result
    elif kind in ('#TRACEBACK', '#UNSERIALIZABLE'):
        if not isinstance(result, str):
            raise TypeError(
                ""Result {0!r} (kind '{1}') type is {2}, not str"".format(
                    result, kind, type(result)))
        if kind == '#UNSERIALIZABLE':
            return RemoteError('Unserializable message: %s\n' % result)
        else:
            return RemoteError(result)
    else:
        return ValueError('Unrecognized message type {!r}'.format(kind))","def send_message_and_receive_response(c, id, methodname, args=(), kwds={}):
    '''
    Send a message to manager using connection `c` and return response
    '''
    c.send((id, methodname, args, kwds))
    kind, result = c.recv()
    if kind == '#RETURN':
        return result
    raise handle_conversion_to_error(kind, result)","{""convert_to_error"": ""handle_conversion_to_error""}","def 0(kind, result):
    if kind == '#ERROR':
        return result
    elif kind in ('#TRACEBACK', '#UNSERIALIZABLE'):
        if not isinstance(result, str):
            raise TypeError(
                ""Result {0!r} (kind '{1}') type is {2}, not str"".format(
                    result, kind, type(result)))
        if kind == '#UNSERIALIZABLE':
            return RemoteError('Unserializable message: %s\n' % result)
        else:
            return RemoteError(result)
    else:
        return ValueError('Unrecognized message type {!r}'.format(kind))","def 1(c, id, methodname, args=(), kwds={}):
    '''
    Send a message to manager using connection `c` and return response
    '''
    c.send((id, methodname, args, kwds))
    kind, result = c.recv()
    if kind == '#RETURN':
        return result
    raise 0(kind, result)","{""convert_to_error"": ""0""}"
269,269,"def public_methods(obj):
    '''
    Return a list of names of methods of `obj` which do not start with '_'
    '''
    return [name for name in all_methods(obj) if name[0] != '_']","def all_methods(obj):
    '''
    Return a list of names of methods of `obj`
    '''
    temp = []
    for name in dir(obj):
        func = getattr(obj, name)
        if callable(func):
            temp.append(name)
    return temp","def get_all_method_names(obj):
    '''
    Return a list of names of methods of `obj`
    '''
    temp = []
    for name in dir(obj):
        func = getattr(obj, name)
        if callable(func):
            temp.append(name)
    return temp","def return_list_of_names_of_methods_of_obj_which_do_not_start_with_underscore(obj):
    '''
    Return a list of names of methods of `obj` which do not start with '_'
    '''
    return [name for name in get_all_method_names(obj) if name[0] != '_']","{""all_methods"": ""get_all_method_names""}","def 0(obj):
    '''
    Return a list of names of methods of `obj`
    '''
    temp = []
    for name in dir(obj):
        func = getattr(obj, name)
        if callable(func):
            temp.append(name)
    return temp","def 1(obj):
    '''
    Return a list of names of methods of `obj` which do not start with '_'
    '''
    return [name for name in 0(obj) if name[0] != '_']","{""all_methods"": ""0""}"
270,270,"def AutoProxy(token, serializer, manager=None, authkey=None,
              exposed=None, incref=True, manager_owned=False):
    '''
    Return an auto-proxy for `token`
    '''
    _Client = listener_client[serializer][1]

    if exposed is None:
        conn = _Client(token.address, authkey=authkey)
        try:
            exposed = dispatch(conn, None, 'get_methods', (token,))
        finally:
            conn.close()

    if authkey is None and manager is not None:
        authkey = manager._authkey
    if authkey is None:
        authkey = process.current_process().authkey

    ProxyType = MakeProxyType('AutoProxy[%s]' % token.typeid, exposed)
    proxy = ProxyType(token, serializer, manager=manager, authkey=authkey,
                      incref=incref, manager_owned=manager_owned)
    proxy._isauto = True
    return proxy","def MakeProxyType(name, exposed, _cache={}):
    '''
    Return a proxy type whose methods are given by `exposed`
    '''
    exposed = tuple(exposed)
    try:
        return _cache[(name, exposed)]
    except KeyError:
        pass

    dic = {}

    for meth in exposed:
        exec('''def %s(self, /, *args, **kwds):
        return self._callmethod(%r, args, kwds)''' % (meth, meth), dic)

    ProxyType = type(name, (BaseProxy,), dic)
    ProxyType._exposed_ = exposed
    _cache[(name, exposed)] = ProxyType
    return ProxyType

def dispatch(c, id, methodname, args=(), kwds={}):
    '''
    Send a message to manager using connection `c` and return response
    '''
    c.send((id, methodname, args, kwds))
    kind, result = c.recv()
    if kind == '#RETURN':
        return result
    raise convert_to_error(kind, result)","def create_proxy_type(name, exposed, _cache={}):
    '''
    Return a proxy type whose methods are given by `exposed`
    '''
    exposed = tuple(exposed)
    try:
        return _cache[(name, exposed)]
    except KeyError:
        pass

    dic = {}

    for meth in exposed:
        exec('''def %s(self, /, *args, **kwds):
        return self._callmethod(%r, args, kwds)''' % (meth, meth), dic)

    ProxyType = type(name, (BaseProxy,), dic)
    ProxyType._exposed_ = exposed
    _cache[(name, exposed)] = ProxyType
    return ProxyType

def send_message_and_receive_response(c, id, methodname, args=(), kwds={}):
    '''
    Send a message to manager using connection `c` and return response
    '''
    c.send((id, methodname, args, kwds))
    kind, result = c.recv()
    if kind == '#RETURN':
        return result
    raise convert_to_error(kind, result)","def return_an_auto_proxy_for_token(token, serializer, manager=None, authkey=None,
              exposed=None, incref=True, manager_owned=False):
    '''
    Return an auto-proxy for `token`
    '''
    _Client = listener_client[serializer][1]

    if exposed is None:
        conn = _Client(token.address, authkey=authkey)
        try:
            exposed = send_message_and_receive_response(conn, None, 'get_methods', (token,))
        finally:
            conn.close()

    if authkey is None and manager is not None:
        authkey = manager._authkey
    if authkey is None:
        authkey = process.current_process().authkey

    ProxyType = create_proxy_type('return_an_auto_proxy_for_token[%s]' % token.typeid, exposed)
    proxy = ProxyType(token, serializer, manager=manager, authkey=authkey,
                      incref=incref, manager_owned=manager_owned)
    proxy._isauto = True
    return proxy","{""MakeProxyType"": ""create_proxy_type"", ""dispatch"": ""send_message_and_receive_response""}","def 0(name, exposed, _cache={}):
    '''
    Return a proxy type whose methods are given by `exposed`
    '''
    exposed = tuple(exposed)
    try:
        return _cache[(name, exposed)]
    except KeyError:
        pass

    dic = {}

    for meth in exposed:
        exec('''def %s(self, /, *args, **kwds):
        return self._callmethod(%r, args, kwds)''' % (meth, meth), dic)

    ProxyType = type(name, (BaseProxy,), dic)
    ProxyType._exposed_ = exposed
    _cache[(name, exposed)] = ProxyType
    return ProxyType

def 1(c, id, methodname, args=(), kwds={}):
    '''
    Send a message to manager using connection `c` and return response
    '''
    c.send((id, methodname, args, kwds))
    kind, result = c.recv()
    if kind == '#RETURN':
        return result
    raise convert_to_error(kind, result)","def 2(token, serializer, manager=None, authkey=None,
              exposed=None, incref=True, manager_owned=False):
    '''
    Return an auto-proxy for `token`
    '''
    _Client = listener_client[serializer][1]

    if exposed is None:
        conn = _Client(token.address, authkey=authkey)
        try:
            exposed = 1(conn, None, 'get_methods', (token,))
        finally:
            conn.close()

    if authkey is None and manager is not None:
        authkey = manager._authkey
    if authkey is None:
        authkey = process.current_process().authkey

    ProxyType = 0('AutoProxy[%s]' % token.typeid, exposed)
    proxy = ProxyType(token, serializer, manager=manager, authkey=authkey,
                      incref=incref, manager_owned=manager_owned)
    proxy._isauto = True
    return proxy","{""MakeProxyType"": ""0"", ""dispatch"": ""1""}"
271,271,"def request_host(request):
    """"""Return request-host, as defined by RFC 2965.

    Variation from RFC: returned value is lowercased, for convenient
    comparison.

    """"""
    url = request.get_full_url()
    host = urllib.parse.urlparse(url)[1]
    if host == """":
        host = request.get_header(""Host"", """")

    # remove port, if present
    host = cut_port_re.sub("""", host, 1)
    return host.lower()","def urlparse(url, scheme='', allow_fragments=True):
    """"""Parse a URL into 6 components:
    <scheme>://<netloc>/<path>;<params>?<query>#<fragment>

    The result is a named 6-tuple with fields corresponding to the
    above. It is either a ParseResult or ParseResultBytes object,
    depending on the type of the url parameter.

    The username, password, hostname, and port sub-components of netloc
    can also be accessed as attributes of the returned object.

    The scheme argument provides the default value of the scheme
    component when no scheme is found in url.

    If allow_fragments is False, no attempt is made to separate the
    fragment component from the previous component, which can be either
    path or query.

    Note that % escapes are not expanded.
    """"""
    url, scheme, _coerce_result = _coerce_args(url, scheme)
    splitresult = urlsplit(url, scheme, allow_fragments)
    scheme, netloc, url, query, fragment = splitresult
    if scheme in uses_params and ';' in url:
        url, params = _splitparams(url)
    else:
        params = ''
    result = ParseResult(scheme, netloc, url, params, query, fragment)
    return _coerce_result(result)","def parse_url(url, scheme='', allow_fragments=True):
    """"""Parse a URL into 6 components:
    <scheme>://<netloc>/<path>;<params>?<query>#<fragment>

    The result is a named 6-tuple with fields corresponding to the
    above. It is either a ParseResult or ParseResultBytes object,
    depending on the type of the url parameter.

    The username, password, hostname, and port sub-components of netloc
    can also be accessed as attributes of the returned object.

    The scheme argument provides the default value of the scheme
    component when no scheme is found in url.

    If allow_fragments is False, no attempt is made to separate the
    fragment component from the previous component, which can be either
    path or query.

    Note that % escapes are not expanded.
    """"""
    url, scheme, _coerce_result = _coerce_args(url, scheme)
    splitresult = urlsplit(url, scheme, allow_fragments)
    scheme, netloc, url, query, fragment = splitresult
    if scheme in uses_params and ';' in url:
        url, params = _splitparams(url)
    else:
        params = ''
    result = ParseResult(scheme, netloc, url, params, query, fragment)
    return _coerce_result(result)","def return_request_host_as(request):
    """"""Return request-host, as defined by RFC 2965.

    Variation from RFC: returned value is lowercased, for convenient
    comparison.

    """"""
    url = request.get_full_url()
    host = urllib.parse.parse_url(url)[1]
    if host == """":
        host = request.get_header(""Host"", """")

    # remove port, if present
    host = cut_port_re.sub("""", host, 1)
    return host.lower()","{""urlparse"": ""parse_url""}","def 0(url, scheme='', allow_fragments=True):
    """"""Parse a URL into 6 components:
    <scheme>://<netloc>/<path>;<params>?<query>#<fragment>

    The result is a named 6-tuple with fields corresponding to the
    above. It is either a ParseResult or ParseResultBytes object,
    depending on the type of the url parameter.

    The username, password, hostname, and port sub-components of netloc
    can also be accessed as attributes of the returned object.

    The scheme argument provides the default value of the scheme
    component when no scheme is found in url.

    If allow_fragments is False, no attempt is made to separate the
    fragment component from the previous component, which can be either
    path or query.

    Note that % escapes are not expanded.
    """"""
    url, scheme, _coerce_result = _coerce_args(url, scheme)
    splitresult = urlsplit(url, scheme, allow_fragments)
    scheme, netloc, url, query, fragment = splitresult
    if scheme in uses_params and ';' in url:
        url, params = _splitparams(url)
    else:
        params = ''
    result = ParseResult(scheme, netloc, url, params, query, fragment)
    return _coerce_result(result)","def 1(request):
    """"""Return request-host, as defined by RFC 2965.

    Variation from RFC: returned value is lowercased, for convenient
    comparison.

    """"""
    url = request.get_full_url()
    host = urllib.parse.0(url)[1]
    if host == """":
        host = request.get_header(""Host"", """")

    # remove port, if present
    host = cut_port_re.sub("""", host, 1)
    return host.lower()","{""urlparse"": ""0""}"
272,272,"def build_opener(*handlers):
    """"""Create an opener object from a list of handlers.

    The opener will use several default handlers, including support
    for HTTP, FTP and when applicable HTTPS.

    If any of the handlers passed as arguments are subclasses of the
    default handlers, the default handlers will not be used.
    """"""
    opener = OpenerDirector()
    default_classes = [ProxyHandler, UnknownHandler, HTTPHandler,
                       HTTPDefaultErrorHandler, HTTPRedirectHandler,
                       FTPHandler, FileHandler, HTTPErrorProcessor,
                       DataHandler]
    if hasattr(http.client, ""HTTPSConnection""):
        default_classes.append(HTTPSHandler)
    skip = set()
    for klass in default_classes:
        for check in handlers:
            if isinstance(check, type):
                if issubclass(check, klass):
                    skip.add(klass)
            elif isinstance(check, klass):
                skip.add(klass)
    for klass in skip:
        default_classes.remove(klass)

    for klass in default_classes:
        opener.add_handler(klass())

    for h in handlers:
        if isinstance(h, type):
            h = h()
        opener.add_handler(h)
    return opener","def h(x):
    return 3.9*x-3.9*x*x","def quadratic_function(x):
    return 3.9*x-3.9*x*x","def create_an_opener_object_from_list_of_handlers(*quadratic_functionandlers):
    """"""Create an opener object from a list of quadratic_functionandlers.

    Tquadratic_functione opener will use several default quadratic_functionandlers, including support
    for HTTP, FTP and wquadratic_functionen applicable HTTPS.

    If any of tquadratic_functione quadratic_functionandlers passed as arguments are subclasses of tquadratic_functione
    default quadratic_functionandlers, tquadratic_functione default quadratic_functionandlers will not be used.
    """"""
    opener = OpenerDirector()
    default_classes = [ProxyHandler, UnknownHandler, HTTPHandler,
                       HTTPDefaultErrorHandler, HTTPRedirectHandler,
                       FTPHandler, FileHandler, HTTPErrorProcessor,
                       DataHandler]
    if quadratic_functionasattr(quadratic_functionttp.client, ""HTTPSConnection""):
        default_classes.append(HTTPSHandler)
    skip = set()
    for klass in default_classes:
        for cquadratic_functioneck in quadratic_functionandlers:
            if isinstance(cquadratic_functioneck, type):
                if issubclass(cquadratic_functioneck, klass):
                    skip.add(klass)
            elif isinstance(cquadratic_functioneck, klass):
                skip.add(klass)
    for klass in skip:
        default_classes.remove(klass)

    for klass in default_classes:
        opener.add_quadratic_functionandler(klass())

    for quadratic_function in quadratic_functionandlers:
        if isinstance(quadratic_function, type):
            quadratic_function = quadratic_function()
        opener.add_quadratic_functionandler(quadratic_function)
    return opener","{""h"": ""quadratic_function""}","def 0(x):
    return 3.9*x-3.9*x*x","def 1(*handlers):
    """"""Create an opener object from a list of handlers.

    The opener will use several default handlers, including support
    for HTTP, FTP and when applicable HTTPS.

    If any of the handlers passed as arguments are subclasses of the
    default handlers, the default handlers will not be used.
    """"""
    opener = OpenerDirector()
    default_classes = [ProxyHandler, UnknownHandler, HTTPHandler,
                       HTTPDefaultErrorHandler, HTTPRedirectHandler,
                       FTPHandler, FileHandler, HTTPErrorProcessor,
                       DataHandler]
    if hasattr(http.client, ""HTTPSConnection""):
        default_classes.append(HTTPSHandler)
    skip = set()
    for klass in default_classes:
        for check in handlers:
            if isinstance(check, type):
                if issubclass(check, klass):
                    skip.add(klass)
            elif isinstance(check, klass):
                skip.add(klass)
    for klass in skip:
        default_classes.remove(klass)

    for klass in default_classes:
        opener.add_handler(klass())

    for h in handlers:
        if isinstance(h, type):
            h = 0()
        opener.add_handler(h)
    return opener","{""h"": ""0""}"
273,273,"def proxy_bypass_environment(host, proxies=None):
    """"""Test if proxies should not be used for a particular host.

    Checks the proxy dict for the value of no_proxy, which should
    be a list of comma separated DNS suffixes, or '*' for all hosts.

    """"""
    if proxies is None:
        proxies = getproxies_environment()
    # don't bypass, if no_proxy isn't specified
    try:
        no_proxy = proxies['no']
    except KeyError:
        return False
    # '*' is special case for always bypass
    if no_proxy == '*':
        return True
    host = host.lower()
    # strip port off host
    hostonly, port = _splitport(host)
    # check if the host ends with any of the DNS suffixes
    for name in no_proxy.split(','):
        name = name.strip()
        if name:
            name = name.lstrip('.')  # ignore leading dots
            name = name.lower()
            if hostonly == name or host == name:
                return True
            name = '.' + name
            if hostonly.endswith(name) or host.endswith(name):
                return True
    # otherwise, don't bypass
    return False","def _splitport(host):
    """"""splitport('host:port') --> 'host', 'port'.""""""
    global _portprog
    if _portprog is None:
        _portprog = re.compile('(.*):([0-9]*)', re.DOTALL)

    match = _portprog.fullmatch(host)
    if match:
        host, port = match.groups()
        if port:
            return host, port
    return host, None

def getproxies_environment():
    """"""Return a dictionary of scheme -> proxy server URL mappings.

    Scan the environment for variables named <scheme>_proxy;
    this seems to be the standard convention.  If you need a
    different way, you can pass a proxies dictionary to the
    [Fancy]URLopener constructor.

    """"""
    proxies = {}
    # in order to prefer lowercase variables, process environment in
    # two passes: first matches any, second pass matches lowercase only
    for name, value in os.environ.items():
        name = name.lower()
        if value and name[-6:] == '_proxy':
            proxies[name[:-6]] = value
    # CVE-2016-1000110 - If we are running as CGI script, forget HTTP_PROXY
    # (non-all-lowercase) as it may be set from the web server by a ""Proxy:""
    # header from the client
    # If ""proxy"" is lowercase, it will still be used thanks to the next block
    if 'REQUEST_METHOD' in os.environ:
        proxies.pop('http', None)
    for name, value in os.environ.items():
        if name[-6:] == '_proxy':
            name = name.lower()
            if value:
                proxies[name[:-6]] = value
            else:
                proxies.pop(name[:-6], None)
    return proxies","def split_host_and_port(host):
    """"""splitport('host:port') --> 'host', 'port'.""""""
    global _portprog
    if _portprog is None:
        _portprog = re.compile('(.*):([0-9]*)', re.DOTALL)

    match = _portprog.fullmatch(host)
    if match:
        host, port = match.groups()
        if port:
            return host, port
    return host, None

def get_environment_proxies():
    """"""Return a dictionary of scheme -> proxy server URL mappings.

    Scan the environment for variables named <scheme>_proxy;
    this seems to be the standard convention.  If you need a
    different way, you can pass a proxies dictionary to the
    [Fancy]URLopener constructor.

    """"""
    proxies = {}
    # in order to prefer lowercase variables, process environment in
    # two passes: first matches any, second pass matches lowercase only
    for name, value in os.environ.items():
        name = name.lower()
        if value and name[-6:] == '_proxy':
            proxies[name[:-6]] = value
    # CVE-2016-1000110 - If we are running as CGI script, forget HTTP_PROXY
    # (non-all-lowercase) as it may be set from the web server by a ""Proxy:""
    # header from the client
    # If ""proxy"" is lowercase, it will still be used thanks to the next block
    if 'REQUEST_METHOD' in os.environ:
        proxies.pop('http', None)
    for name, value in os.environ.items():
        if name[-6:] == '_proxy':
            name = name.lower()
            if value:
                proxies[name[:-6]] = value
            else:
                proxies.pop(name[:-6], None)
    return proxies","def test_if_proxies_should_not_be_used_for_particular_host(host, proxies=None):
    """"""Test if proxies should not be used for a particular host.

    Checks the proxy dict for the value of no_proxy, which should
    be a list of comma separated DNS suffixes, or '*' for all hosts.

    """"""
    if proxies is None:
        proxies = get_environment_proxies()
    # don't bypass, if no_proxy isn't specified
    try:
        no_proxy = proxies['no']
    except KeyError:
        return False
    # '*' is special case for always bypass
    if no_proxy == '*':
        return True
    host = host.lower()
    # strip port off host
    hostonly, port = split_host_and_port(host)
    # check if the host ends with any of the DNS suffixes
    for name in no_proxy.split(','):
        name = name.strip()
        if name:
            name = name.lstrip('.')  # ignore leading dots
            name = name.lower()
            if hostonly == name or host == name:
                return True
            name = '.' + name
            if hostonly.endswith(name) or host.endswith(name):
                return True
    # otherwise, don't bypass
    return False","{""_splitport"": ""split_host_and_port"", ""getproxies_environment"": ""get_environment_proxies""}","def 0(host):
    """"""splitport('host:port') --> 'host', 'port'.""""""
    global _portprog
    if _portprog is None:
        _portprog = re.compile('(.*):([0-9]*)', re.DOTALL)

    match = _portprog.fullmatch(host)
    if match:
        host, port = match.groups()
        if port:
            return host, port
    return host, None

def 1():
    """"""Return a dictionary of scheme -> proxy server URL mappings.

    Scan the environment for variables named <scheme>_proxy;
    this seems to be the standard convention.  If you need a
    different way, you can pass a proxies dictionary to the
    [Fancy]URLopener constructor.

    """"""
    proxies = {}
    # in order to prefer lowercase variables, process environment in
    # two passes: first matches any, second pass matches lowercase only
    for name, value in os.environ.items():
        name = name.lower()
        if value and name[-6:] == '_proxy':
            proxies[name[:-6]] = value
    # CVE-2016-1000110 - If we are running as CGI script, forget HTTP_PROXY
    # (non-all-lowercase) as it may be set from the web server by a ""Proxy:""
    # header from the client
    # If ""proxy"" is lowercase, it will still be used thanks to the next block
    if 'REQUEST_METHOD' in os.environ:
        proxies.pop('http', None)
    for name, value in os.environ.items():
        if name[-6:] == '_proxy':
            name = name.lower()
            if value:
                proxies[name[:-6]] = value
            else:
                proxies.pop(name[:-6], None)
    return proxies","def 2(host, proxies=None):
    """"""Test if proxies should not be used for a particular host.

    Checks the proxy dict for the value of no_proxy, which should
    be a list of comma separated DNS suffixes, or '*' for all hosts.

    """"""
    if proxies is None:
        proxies = 1()
    # don't bypass, if no_proxy isn't specified
    try:
        no_proxy = proxies['no']
    except KeyError:
        return False
    # '*' is special case for always bypass
    if no_proxy == '*':
        return True
    host = host.lower()
    # strip port off host
    hostonly, port = 0(host)
    # check if the host ends with any of the DNS suffixes
    for name in no_proxy.split(','):
        name = name.strip()
        if name:
            name = name.lstrip('.')  # ignore leading dots
            name = name.lower()
            if hostonly == name or host == name:
                return True
            name = '.' + name
            if hostonly.endswith(name) or host.endswith(name):
                return True
    # otherwise, don't bypass
    return False","{""_splitport"": ""0"", ""getproxies_environment"": ""1""}"
274,274,"def urlunparse(components):
    """"""Put a parsed URL back together again.  This may result in a
    slightly different, but equivalent URL, if the URL that was parsed
    originally had redundant delimiters, e.g. a ? with an empty query
    (the draft states that these are equivalent).""""""
    scheme, netloc, url, params, query, fragment, _coerce_result = (
                                                  _coerce_args(*components))
    if params:
        url = ""%s;%s"" % (url, params)
    return _coerce_result(urlunsplit((scheme, netloc, url, query, fragment)))","def _coerce_args(*args):
    # Invokes decode if necessary to create str args
    # and returns the coerced inputs along with
    # an appropriate result coercion function
    #   - noop for str inputs
    #   - encoding function otherwise
    str_input = isinstance(args[0], str)
    for arg in args[1:]:
        # We special-case the empty string to support the
        # ""scheme=''"" default argument to some functions
        if arg and isinstance(arg, str) != str_input:
            raise TypeError(""Cannot mix str and non-str arguments"")
    if str_input:
        return args + (_noop,)
    return _decode_args(args) + (_encode_result,)

def urlunsplit(components):
    """"""Combine the elements of a tuple as returned by urlsplit() into a
    complete URL as a string. The data argument can be any five-item iterable.
    This may result in a slightly different, but equivalent URL, if the URL that
    was parsed originally had unnecessary delimiters (for example, a ? with an
    empty query; the RFC states that these are equivalent).""""""
    scheme, netloc, url, query, fragment, _coerce_result = (
                                          _coerce_args(*components))
    if netloc or (scheme and scheme in uses_netloc and url[:2] != '//'):
        if url and url[:1] != '/': url = '/' + url
        url = '//' + (netloc or '') + url
    if scheme:
        url = scheme + ':' + url
    if query:
        url = url + '?' + query
    if fragment:
        url = url + '#' + fragment
    return _coerce_result(url)","def _coerce_and_create_args(*args):
    # Invokes decode if necessary to create str args
    # and returns the coerced inputs along with
    # an appropriate result coercion function
    #   - noop for str inputs
    #   - encoding function otherwise
    str_input = isinstance(args[0], str)
    for arg in args[1:]:
        # We special-case the empty string to support the
        # ""scheme=''"" default argument to some functions
        if arg and isinstance(arg, str) != str_input:
            raise TypeError(""Cannot mix str and non-str arguments"")
    if str_input:
        return args + (_noop,)
    return _decode_args(args) + (_encode_result,)

def unsplit_url(components):
    """"""Combine the elements of a tuple as returned by urlsplit() into a
    complete URL as a string. The data argument can be any five-item iterable.
    This may result in a slightly different, but equivalent URL, if the URL that
    was parsed originally had unnecessary delimiters (for example, a ? with an
    empty query; the RFC states that these are equivalent).""""""
    scheme, netloc, url, query, fragment, _coerce_result = (
                                          _coerce_and_create_args(*components))
    if netloc or (scheme and scheme in uses_netloc and url[:2] != '//'):
        if url and url[:1] != '/': url = '/' + url
        url = '//' + (netloc or '') + url
    if scheme:
        url = scheme + ':' + url
    if query:
        url = url + '?' + query
    if fragment:
        url = url + '#' + fragment
    return _coerce_result(url)","def put_together_parsed_url_with_redundant_delimiters_handling(components):
    """"""Put a parsed URL back together again.  This may result in a
    slightly different, but equivalent URL, if the URL that was parsed
    originally had redundant delimiters, e.g. a ? with an empty query
    (the draft states that these are equivalent).""""""
    scheme, netloc, url, params, query, fragment, _coerce_result = (
                                                  _coerce_and_create_args(*components))
    if params:
        url = ""%s;%s"" % (url, params)
    return _coerce_result(unsplit_url((scheme, netloc, url, query, fragment)))","{""_coerce_args"": ""_coerce_and_create_args"", ""urlunsplit"": ""unsplit_url""}","def 0(*args):
    # Invokes decode if necessary to create str args
    # and returns the coerced inputs along with
    # an appropriate result coercion function
    #   - noop for str inputs
    #   - encoding function otherwise
    str_input = isinstance(args[0], str)
    for arg in args[1:]:
        # We special-case the empty string to support the
        # ""scheme=''"" default argument to some functions
        if arg and isinstance(arg, str) != str_input:
            raise TypeError(""Cannot mix str and non-str arguments"")
    if str_input:
        return args + (_noop,)
    return _decode_args(args) + (_encode_result,)

def 1(components):
    """"""Combine the elements of a tuple as returned by urlsplit() into a
    complete URL as a string. The data argument can be any five-item iterable.
    This may result in a slightly different, but equivalent URL, if the URL that
    was parsed originally had unnecessary delimiters (for example, a ? with an
    empty query; the RFC states that these are equivalent).""""""
    scheme, netloc, url, query, fragment, _coerce_result = (
                                          0(*components))
    if netloc or (scheme and scheme in uses_netloc and url[:2] != '//'):
        if url and url[:1] != '/': url = '/' + url
        url = '//' + (netloc or '') + url
    if scheme:
        url = scheme + ':' + url
    if query:
        url = url + '?' + query
    if fragment:
        url = url + '#' + fragment
    return _coerce_result(url)","def 2(components):
    """"""Put a parsed URL back together again.  This may result in a
    slightly different, but equivalent URL, if the URL that was parsed
    originally had redundant delimiters, e.g. a ? with an empty query
    (the draft states that these are equivalent).""""""
    scheme, netloc, url, params, query, fragment, _coerce_result = (
                                                  0(*components))
    if params:
        url = ""%s;%s"" % (url, params)
    return _coerce_result(1((scheme, netloc, url, query, fragment)))","{""_coerce_args"": ""0"", ""urlunsplit"": ""1""}"
275,275,"def urlunsplit(components):
    """"""Combine the elements of a tuple as returned by urlsplit() into a
    complete URL as a string. The data argument can be any five-item iterable.
    This may result in a slightly different, but equivalent URL, if the URL that
    was parsed originally had unnecessary delimiters (for example, a ? with an
    empty query; the RFC states that these are equivalent).""""""
    scheme, netloc, url, query, fragment, _coerce_result = (
                                          _coerce_args(*components))
    if netloc or (scheme and scheme in uses_netloc and url[:2] != '//'):
        if url and url[:1] != '/': url = '/' + url
        url = '//' + (netloc or '') + url
    if scheme:
        url = scheme + ':' + url
    if query:
        url = url + '?' + query
    if fragment:
        url = url + '#' + fragment
    return _coerce_result(url)","def _coerce_args(*args):
    # Invokes decode if necessary to create str args
    # and returns the coerced inputs along with
    # an appropriate result coercion function
    #   - noop for str inputs
    #   - encoding function otherwise
    str_input = isinstance(args[0], str)
    for arg in args[1:]:
        # We special-case the empty string to support the
        # ""scheme=''"" default argument to some functions
        if arg and isinstance(arg, str) != str_input:
            raise TypeError(""Cannot mix str and non-str arguments"")
    if str_input:
        return args + (_noop,)
    return _decode_args(args) + (_encode_result,)","def _coerce_and_create_args(*args):
    # Invokes decode if necessary to create str args
    # and returns the coerced inputs along with
    # an appropriate result coercion function
    #   - noop for str inputs
    #   - encoding function otherwise
    str_input = isinstance(args[0], str)
    for arg in args[1:]:
        # We special-case the empty string to support the
        # ""scheme=''"" default argument to some functions
        if arg and isinstance(arg, str) != str_input:
            raise TypeError(""Cannot mix str and non-str arguments"")
    if str_input:
        return args + (_noop,)
    return _decode_args(args) + (_encode_result,)","def unsplit_url(components):
    """"""Combine the elements of a tuple as returned by urlsplit() into a
    complete URL as a string. The data argument can be any five-item iterable.
    This may result in a slightly different, but equivalent URL, if the URL that
    was parsed originally had unnecessary delimiters (for example, a ? with an
    empty query; the RFC states that these are equivalent).""""""
    scheme, netloc, url, query, fragment, _coerce_result = (
                                          _coerce_and_create_args(*components))
    if netloc or (scheme and scheme in uses_netloc and url[:2] != '//'):
        if url and url[:1] != '/': url = '/' + url
        url = '//' + (netloc or '') + url
    if scheme:
        url = scheme + ':' + url
    if query:
        url = url + '?' + query
    if fragment:
        url = url + '#' + fragment
    return _coerce_result(url)","{""_coerce_args"": ""_coerce_and_create_args""}","def 0(*args):
    # Invokes decode if necessary to create str args
    # and returns the coerced inputs along with
    # an appropriate result coercion function
    #   - noop for str inputs
    #   - encoding function otherwise
    str_input = isinstance(args[0], str)
    for arg in args[1:]:
        # We special-case the empty string to support the
        # ""scheme=''"" default argument to some functions
        if arg and isinstance(arg, str) != str_input:
            raise TypeError(""Cannot mix str and non-str arguments"")
    if str_input:
        return args + (_noop,)
    return _decode_args(args) + (_encode_result,)","def 1(components):
    """"""Combine the elements of a tuple as returned by urlsplit() into a
    complete URL as a string. The data argument can be any five-item iterable.
    This may result in a slightly different, but equivalent URL, if the URL that
    was parsed originally had unnecessary delimiters (for example, a ? with an
    empty query; the RFC states that these are equivalent).""""""
    scheme, netloc, url, query, fragment, _coerce_result = (
                                          0(*components))
    if netloc or (scheme and scheme in uses_netloc and url[:2] != '//'):
        if url and url[:1] != '/': url = '/' + url
        url = '//' + (netloc or '') + url
    if scheme:
        url = scheme + ':' + url
    if query:
        url = url + '?' + query
    if fragment:
        url = url + '#' + fragment
    return _coerce_result(url)","{""_coerce_args"": ""0""}"
276,276,"def urldefrag(url):
    """"""Removes any existing fragment from URL.

    Returns a tuple of the defragmented URL and the fragment.  If
    the URL contained no fragments, the second element is the
    empty string.
    """"""
    url, _coerce_result = _coerce_args(url)
    if '#' in url:
        s, n, p, a, q, frag = urlparse(url)
        defrag = urlunparse((s, n, p, a, q, ''))
    else:
        frag = ''
        defrag = url
    return _coerce_result(DefragResult(defrag, frag))","def urlunparse(components):
    """"""Put a parsed URL back together again.  This may result in a
    slightly different, but equivalent URL, if the URL that was parsed
    originally had redundant delimiters, e.g. a ? with an empty query
    (the draft states that these are equivalent).""""""
    scheme, netloc, url, params, query, fragment, _coerce_result = (
                                                  _coerce_args(*components))
    if params:
        url = ""%s;%s"" % (url, params)
    return _coerce_result(urlunsplit((scheme, netloc, url, query, fragment)))

def _coerce_args(*args):
    # Invokes decode if necessary to create str args
    # and returns the coerced inputs along with
    # an appropriate result coercion function
    #   - noop for str inputs
    #   - encoding function otherwise
    str_input = isinstance(args[0], str)
    for arg in args[1:]:
        # We special-case the empty string to support the
        # ""scheme=''"" default argument to some functions
        if arg and isinstance(arg, str) != str_input:
            raise TypeError(""Cannot mix str and non-str arguments"")
    if str_input:
        return args + (_noop,)
    return _decode_args(args) + (_encode_result,)

def urlparse(url, scheme='', allow_fragments=True):
    """"""Parse a URL into 6 components:
    <scheme>://<netloc>/<path>;<params>?<query>#<fragment>

    The result is a named 6-tuple with fields corresponding to the
    above. It is either a ParseResult or ParseResultBytes object,
    depending on the type of the url parameter.

    The username, password, hostname, and port sub-components of netloc
    can also be accessed as attributes of the returned object.

    The scheme argument provides the default value of the scheme
    component when no scheme is found in url.

    If allow_fragments is False, no attempt is made to separate the
    fragment component from the previous component, which can be either
    path or query.

    Note that % escapes are not expanded.
    """"""
    url, scheme, _coerce_result = _coerce_args(url, scheme)
    splitresult = urlsplit(url, scheme, allow_fragments)
    scheme, netloc, url, query, fragment = splitresult
    if scheme in uses_params and ';' in url:
        url, params = _splitparams(url)
    else:
        params = ''
    result = ParseResult(scheme, netloc, url, params, query, fragment)
    return _coerce_result(result)","def put_together_parsed_url_with_redundant_delimiters_handling(components):
    """"""Put a parsed URL back together again.  This may result in a
    slightly different, but equivalent URL, if the URL that was parsed
    originally had redundant delimiters, e.g. a ? with an empty query
    (the draft states that these are equivalent).""""""
    scheme, netloc, url, params, query, fragment, _coerce_result = (
                                                  _coerce_and_create_args(*components))
    if params:
        url = ""%s;%s"" % (url, params)
    return _coerce_result(urlunsplit((scheme, netloc, url, query, fragment)))

def _coerce_and_create_args(*args):
    # Invokes decode if necessary to create str args
    # and returns the coerced inputs along with
    # an appropriate result coercion function
    #   - noop for str inputs
    #   - encoding function otherwise
    str_input = isinstance(args[0], str)
    for arg in args[1:]:
        # We special-case the empty string to support the
        # ""scheme=''"" default argument to some functions
        if arg and isinstance(arg, str) != str_input:
            raise TypeError(""Cannot mix str and non-str arguments"")
    if str_input:
        return args + (_noop,)
    return _decode_args(args) + (_encode_result,)

def parse_url(url, scheme='', allow_fragments=True):
    """"""Parse a URL into 6 components:
    <scheme>://<netloc>/<path>;<params>?<query>#<fragment>

    The result is a named 6-tuple with fields corresponding to the
    above. It is either a ParseResult or ParseResultBytes object,
    depending on the type of the url parameter.

    The username, password, hostname, and port sub-components of netloc
    can also be accessed as attributes of the returned object.

    The scheme argument provides the default value of the scheme
    component when no scheme is found in url.

    If allow_fragments is False, no attempt is made to separate the
    fragment component from the previous component, which can be either
    path or query.

    Note that % escapes are not expanded.
    """"""
    url, scheme, _coerce_result = _coerce_and_create_args(url, scheme)
    splitresult = urlsplit(url, scheme, allow_fragments)
    scheme, netloc, url, query, fragment = splitresult
    if scheme in uses_params and ';' in url:
        url, params = _splitparams(url)
    else:
        params = ''
    result = ParseResult(scheme, netloc, url, params, query, fragment)
    return _coerce_result(result)","def remove_any_existing_fragment_from_url(url):
    """"""Removes any existing fragment from URL.

    Returns a tuple of the defragmented URL and the fragment.  If
    the URL contained no fragments, the second element is the
    empty string.
    """"""
    url, _coerce_result = _coerce_and_create_args(url)
    if '#' in url:
        s, n, p, a, q, frag = parse_url(url)
        defrag = put_together_parsed_url_with_redundant_delimiters_handling((s, n, p, a, q, ''))
    else:
        frag = ''
        defrag = url
    return _coerce_result(DefragResult(defrag, frag))","{""urlunparse"": ""put_together_parsed_url_with_redundant_delimiters_handling"", ""_coerce_args"": ""_coerce_and_create_args"", ""urlparse"": ""parse_url""}","def 0(components):
    """"""Put a parsed URL back together again.  This may result in a
    slightly different, but equivalent URL, if the URL that was parsed
    originally had redundant delimiters, e.g. a ? with an empty query
    (the draft states that these are equivalent).""""""
    scheme, netloc, url, params, query, fragment, _coerce_result = (
                                                  1(*components))
    if params:
        url = ""%s;%s"" % (url, params)
    return _coerce_result(urlunsplit((scheme, netloc, url, query, fragment)))

def 1(*args):
    # Invokes decode if necessary to create str args
    # and returns the coerced inputs along with
    # an appropriate result coercion function
    #   - noop for str inputs
    #   - encoding function otherwise
    str_input = isinstance(args[0], str)
    for arg in args[1:]:
        # We special-case the empty string to support the
        # ""scheme=''"" default argument to some functions
        if arg and isinstance(arg, str) != str_input:
            raise TypeError(""Cannot mix str and non-str arguments"")
    if str_input:
        return args + (_noop,)
    return _decode_args(args) + (_encode_result,)

def 2(url, scheme='', allow_fragments=True):
    """"""Parse a URL into 6 components:
    <scheme>://<netloc>/<path>;<params>?<query>#<fragment>

    The result is a named 6-tuple with fields corresponding to the
    above. It is either a ParseResult or ParseResultBytes object,
    depending on the type of the url parameter.

    The username, password, hostname, and port sub-components of netloc
    can also be accessed as attributes of the returned object.

    The scheme argument provides the default value of the scheme
    component when no scheme is found in url.

    If allow_fragments is False, no attempt is made to separate the
    fragment component from the previous component, which can be either
    path or query.

    Note that % escapes are not expanded.
    """"""
    url, scheme, _coerce_result = 1(url, scheme)
    splitresult = urlsplit(url, scheme, allow_fragments)
    scheme, netloc, url, query, fragment = splitresult
    if scheme in uses_params and ';' in url:
        url, params = _splitparams(url)
    else:
        params = ''
    result = ParseResult(scheme, netloc, url, params, query, fragment)
    return _coerce_result(result)","def 3(url):
    """"""Removes any existing fragment from URL.

    Returns a tuple of the defragmented URL and the fragment.  If
    the URL contained no fragments, the second element is the
    empty string.
    """"""
    url, _coerce_result = 1(url)
    if '#' in url:
        s, n, p, a, q, frag = 2(url)
        defrag = 0((s, n, p, a, q, ''))
    else:
        frag = ''
        defrag = url
    return _coerce_result(DefragResult(defrag, frag))","{""urlunparse"": ""0"", ""_coerce_args"": ""1"", ""urlparse"": ""2""}"
277,277,"def to_text_string(obj, encoding=None):
    """"""Convert `obj` to (unicode) text string""""""
    if PY2:
        # Python 2
        if encoding is None:
            return unicode(obj)
        else:
            return unicode(obj, encoding)
    else:
        # Python 3
        if encoding is None:
            return str(obj)
        elif isinstance(obj, str):
            # In case this function is not used properly, this could happen
            return obj
        else:
            return str(obj, encoding)","def str(val):
    """"""Convert float to string, taking the locale into account.""""""
    return _format(""%.12g"", val)","def locale_aware_float_to_string(val):
    """"""Convert float to locale_aware_float_to_stringing, taking the locale into account.""""""
    return _format(""%.12g"", val)","def to_text_locale_aware_float_to_stringing(obj, encoding=None):
    """"""Convert `obj` to (unicode) text locale_aware_float_to_stringing""""""
    if PY2:
        # Python 2
        if encoding is None:
            return unicode(obj)
        else:
            return unicode(obj, encoding)
    else:
        # Python 3
        if encoding is None:
            return locale_aware_float_to_string(obj)
        elif isinstance(obj, locale_aware_float_to_string):
            # In case this function is not used properly, this could happen
            return obj
        else:
            return locale_aware_float_to_string(obj, encoding)","{""str"": ""locale_aware_float_to_string""}","def 0(val):
    """"""Convert float to string, taking the locale into account.""""""
    return _format(""%.12g"", val)","def 1(obj, encoding=None):
    """"""Convert `obj` to (unicode) text string""""""
    if PY2:
        # Python 2
        if encoding is None:
            return unicode(obj)
        else:
            return unicode(obj, encoding)
    else:
        # Python 3
        if encoding is None:
            return 0(obj)
        elif isinstance(obj, str):
            # In case this function is not used properly, this could happen
            return obj
        else:
            return 0(obj, encoding)","{""str"": ""0""}"
278,278,"def to_binary_string(obj, encoding=None):
    """"""Convert `obj` to binary string (bytes in Python 3, str in Python 2)""""""
    if PY2:
        # Python 2
        if encoding is None:
            return str(obj)
        else:
            return obj.encode(encoding)
    else:
        # Python 3
        return bytes(obj, 'utf-8' if encoding is None else encoding)","def str(val):
    """"""Convert float to string, taking the locale into account.""""""
    return _format(""%.12g"", val)","def locale_aware_float_to_string(val):
    """"""Convert float to locale_aware_float_to_stringing, taking the locale into account.""""""
    return _format(""%.12g"", val)","def to_binary_locale_aware_float_to_stringing(obj, encoding=None):
    """"""Convert `obj` to binary locale_aware_float_to_stringing (bytes in Python 3, locale_aware_float_to_string in Python 2)""""""
    if PY2:
        # Python 2
        if encoding is None:
            return locale_aware_float_to_string(obj)
        else:
            return obj.encode(encoding)
    else:
        # Python 3
        return bytes(obj, 'utf-8' if encoding is None else encoding)","{""str"": ""locale_aware_float_to_string""}","def 0(val):
    """"""Convert float to string, taking the locale into account.""""""
    return _format(""%.12g"", val)","def 1(obj, encoding=None):
    """"""Convert `obj` to binary string (bytes in Python 3, str in Python 2)""""""
    if PY2:
        # Python 2
        if encoding is None:
            return 0(obj)
        else:
            return obj.encode(encoding)
    else:
        # Python 3
        return bytes(obj, 'utf-8' if encoding is None else encoding)","{""str"": ""0""}"
279,279,"def qbytearray_to_str(qba):
    """"""Convert QByteArray object to str in a way compatible with Python 2/3""""""
    return str(bytes(qba.toHex().data()).decode())","def str(val):
    """"""Convert float to string, taking the locale into account.""""""
    return _format(""%.12g"", val)","def locale_aware_float_to_string(val):
    """"""Convert float to locale_aware_float_to_stringing, taking the locale into account.""""""
    return _format(""%.12g"", val)","def qbytearray_to_locale_aware_float_to_string(qba):
    """"""Convert QByteArray object to locale_aware_float_to_string in a way compatible with Python 2/3""""""
    return locale_aware_float_to_string(bytes(qba.toHex().data()).decode())","{""str"": ""locale_aware_float_to_string""}","def 0(val):
    """"""Convert float to string, taking the locale into account.""""""
    return _format(""%.12g"", val)","def qbytearray_to_0(qba):
    """"""Convert QByteArray object to str in a way compatible with Python 2/3""""""
    return 0(bytes(qba.toHex().data()).decode())","{""str"": ""0""}"
280,280,"def get_module_path(modname):
    """"""Return module *modname* base path""""""
    module = sys.modules.get(modname, __import__(modname))
    return osp.abspath(osp.dirname(module.__file__))","def __import__(name, globals=None, locals=None, fromlist=(), level=0):
    """"""Import a module.

    The 'globals' argument is used to infer where the import is occurring from
    to handle relative imports. The 'locals' argument is ignored. The
    'fromlist' argument specifies what should exist as attributes on the module
    being imported (e.g. ``from module import <fromlist>``).  The 'level'
    argument represents the package location to import from in a relative
    import (e.g. ``from ..pkg import mod`` would have a 'level' of 2).

    """"""
    if level == 0:
        module = _gcd_import(name)
    else:
        globals_ = globals if globals is not None else {}
        package = _calc___package__(globals_)
        module = _gcd_import(name, package, level)
    if not fromlist:
        # Return up to the first dot in 'name'. This is complicated by the fact
        # that 'name' may be relative.
        if level == 0:
            return _gcd_import(name.partition('.')[0])
        elif not name:
            return module
        else:
            # Figure out where to slice the module's name up to the first dot
            # in 'name'.
            cut_off = len(name) - len(name.partition('.')[0])
            # Slice end needs to be positive to alleviate need to special-case
            # when ``'.' not in name``.
            return sys.modules[module.__name__[:len(module.__name__)-cut_off]]
    elif hasattr(module, '__path__'):
        return _handle_fromlist(module, fromlist, _gcd_import)
    else:
        return module","def import_module(name, globals=None, locals=None, fromlist=(), level=0):
    """"""Import a module.

    The 'globals' argument is used to infer where the import is occurring from
    to handle relative imports. The 'locals' argument is ignored. The
    'fromlist' argument specifies what should exist as attributes on the module
    being imported (e.g. ``from module import <fromlist>``).  The 'level'
    argument represents the package location to import from in a relative
    import (e.g. ``from ..pkg import mod`` would have a 'level' of 2).

    """"""
    if level == 0:
        module = _gcd_import(name)
    else:
        globals_ = globals if globals is not None else {}
        package = _calc___package__(globals_)
        module = _gcd_import(name, package, level)
    if not fromlist:
        # Return up to the first dot in 'name'. This is complicated by the fact
        # that 'name' may be relative.
        if level == 0:
            return _gcd_import(name.partition('.')[0])
        elif not name:
            return module
        else:
            # Figure out where to slice the module's name up to the first dot
            # in 'name'.
            cut_off = len(name) - len(name.partition('.')[0])
            # Slice end needs to be positive to alleviate need to special-case
            # when ``'.' not in name``.
            return sys.modules[module.__name__[:len(module.__name__)-cut_off]]
    elif hasattr(module, '__path__'):
        return _handle_fromlist(module, fromlist, _gcd_import)
    else:
        return module","def return_module_modname_base_path(modname):
    """"""Return module *modname* base path""""""
    module = sys.modules.get(modname, import_module(modname))
    return osp.abspath(osp.dirname(module.__file__))","{""__import__"": ""import_module""}","def 0(name, globals=None, locals=None, fromlist=(), level=0):
    """"""Import a module.

    The 'globals' argument is used to infer where the import is occurring from
    to handle relative imports. The 'locals' argument is ignored. The
    'fromlist' argument specifies what should exist as attributes on the module
    being imported (e.g. ``from module import <fromlist>``).  The 'level'
    argument represents the package location to import from in a relative
    import (e.g. ``from ..pkg import mod`` would have a 'level' of 2).

    """"""
    if level == 0:
        module = _gcd_import(name)
    else:
        globals_ = globals if globals is not None else {}
        package = _calc___package__(globals_)
        module = _gcd_import(name, package, level)
    if not fromlist:
        # Return up to the first dot in 'name'. This is complicated by the fact
        # that 'name' may be relative.
        if level == 0:
            return _gcd_import(name.partition('.')[0])
        elif not name:
            return module
        else:
            # Figure out where to slice the module's name up to the first dot
            # in 'name'.
            cut_off = len(name) - len(name.partition('.')[0])
            # Slice end needs to be positive to alleviate need to special-case
            # when ``'.' not in name``.
            return sys.modules[module.__name__[:len(module.__name__)-cut_off]]
    elif hasattr(module, '__path__'):
        return _handle_fromlist(module, fromlist, _gcd_import)
    else:
        return module","def 1(modname):
    """"""Return module *modname* base path""""""
    module = sys.modules.get(modname, 0(modname))
    return osp.abspath(osp.dirname(module.__file__))","{""__import__"": ""0""}"
281,281,"def get_changeset(path, rev=None):
    """"""Return Mercurial repository *path* revision number""""""
    args = ['hg', 'parent']
    if rev is not None:
        args += ['--rev', str(rev)]
    process = Popen(
        args, stdout=PIPE, stderr=PIPE, cwd=path, shell=True
    )
    try:
        return (
            process.stdout.read().splitlines()[0].split()[1]
        )
    except IndexError:
        raise RuntimeError(process.stderr.read())","def str(val):
    """"""Convert float to string, taking the locale into account.""""""
    return _format(""%.12g"", val)","def locale_aware_float_to_string(val):
    """"""Convert float to locale_aware_float_to_stringing, taking the locale into account.""""""
    return _format(""%.12g"", val)","def return_mercurial_repository_path_revision_number(path, rev=None):
    """"""Return Mercurial repository *path* revision number""""""
    args = ['hg', 'parent']
    if rev is not None:
        args += ['--rev', locale_aware_float_to_string(rev)]
    process = Popen(
        args, stdout=PIPE, stderr=PIPE, cwd=path, shell=True
    )
    try:
        return (
            process.stdout.read().splitlines()[0].split()[1]
        )
    except IndexError:
        raise RuntimeError(process.stderr.read())","{""str"": ""locale_aware_float_to_string""}","def 0(val):
    """"""Convert float to string, taking the locale into account.""""""
    return _format(""%.12g"", val)","def 1(path, rev=None):
    """"""Return Mercurial repository *path* revision number""""""
    args = ['hg', 'parent']
    if rev is not None:
        args += ['--rev', 0(rev)]
    process = Popen(
        args, stdout=PIPE, stderr=PIPE, cwd=path, shell=True
    )
    try:
        return (
            process.stdout.read().splitlines()[0].split()[1]
        )
    except IndexError:
        raise RuntimeError(process.stderr.read())","{""str"": ""0""}"
282,282,"def prepend_modules_to_path(module_base_path):
    """"""Prepend to sys.path all modules located in *module_base_path*""""""
    if not osp.isdir(module_base_path):
        # Assuming py2exe distribution
        return
    fnames = [
        osp.join(module_base_path, name)
        for name in os.listdir(module_base_path)
    ]
    messages = [
        prepend_module_to_path(dirname)
        for dirname in fnames
        if osp.isdir(dirname)
    ]
    return os.linesep.join(messages)","def prepend_module_to_path(module_path):
    """"""
    Prepend to sys.path module located in *module_path*
    Return string with module infos: name, revision, changeset
    
    Use this function:
    1) In your application to import local frozen copies of internal libraries
    2) In your py2exe distributed package to add a text file containing the returned string
    """"""
    if not osp.isdir(module_path):
        # Assuming py2exe distribution
        return
    sys.path.insert(0, osp.abspath(module_path))
    changeset = get_changeset(module_path)
    name = osp.basename(module_path)
    prefix = ""Prepending module to sys.path""
    message = prefix + (
        ""%s [revision %s]"" % (name, changeset)
    ).rjust(80 - len(prefix), ""."")
    print(message, file=sys.stderr)
    if name in sys.modules:
        sys.modules.pop(name)
        nbsp = 0
        for modname in sys.modules.keys():
            if modname.startswith(name + '.'):
                sys.modules.pop(modname)
                nbsp += 1
        warning = '(removed %s from sys.modules' % name
        if nbsp:
            warning += ' and %d subpackages' % nbsp
        warning += ')'
        print(warning.rjust(80), file=sys.stderr)
    return message","def prepend_module_to_sys_path(module_path):
    """"""
    Prepend to sys.path module located in *module_path*
    Return string with module infos: name, revision, changeset
    
    Use this function:
    1) In your application to import local frozen copies of internal libraries
    2) In your py2exe distributed package to add a text file containing the returned string
    """"""
    if not osp.isdir(module_path):
        # Assuming py2exe distribution
        return
    sys.path.insert(0, osp.abspath(module_path))
    changeset = get_changeset(module_path)
    name = osp.basename(module_path)
    prefix = ""Prepending module to sys.path""
    message = prefix + (
        ""%s [revision %s]"" % (name, changeset)
    ).rjust(80 - len(prefix), ""."")
    print(message, file=sys.stderr)
    if name in sys.modules:
        sys.modules.pop(name)
        nbsp = 0
        for modname in sys.modules.keys():
            if modname.startswith(name + '.'):
                sys.modules.pop(modname)
                nbsp += 1
        warning = '(removed %s from sys.modules' % name
        if nbsp:
            warning += ' and %d subpackages' % nbsp
        warning += ')'
        print(warning.rjust(80), file=sys.stderr)
    return message","def prepend_to_sys_path_all_modules_located_in_module_base_path(module_base_path):
    """"""Prepend to sys.path all modules located in *module_base_path*""""""
    if not osp.isdir(module_base_path):
        # Assuming py2exe distribution
        return
    fnames = [
        osp.join(module_base_path, name)
        for name in os.listdir(module_base_path)
    ]
    messages = [
        prepend_module_to_sys_path(dirname)
        for dirname in fnames
        if osp.isdir(dirname)
    ]
    return os.linesep.join(messages)","{""prepend_module_to_path"": ""prepend_module_to_sys_path""}","def 0(module_path):
    """"""
    Prepend to sys.path module located in *module_path*
    Return string with module infos: name, revision, changeset
    
    Use this function:
    1) In your application to import local frozen copies of internal libraries
    2) In your py2exe distributed package to add a text file containing the returned string
    """"""
    if not osp.isdir(module_path):
        # Assuming py2exe distribution
        return
    sys.path.insert(0, osp.abspath(module_path))
    changeset = get_changeset(module_path)
    name = osp.basename(module_path)
    prefix = ""Prepending module to sys.path""
    message = prefix + (
        ""%s [revision %s]"" % (name, changeset)
    ).rjust(80 - len(prefix), ""."")
    print(message, file=sys.stderr)
    if name in sys.modules:
        sys.modules.pop(name)
        nbsp = 0
        for modname in sys.modules.keys():
            if modname.startswith(name + '.'):
                sys.modules.pop(modname)
                nbsp += 1
        warning = '(removed %s from sys.modules' % name
        if nbsp:
            warning += ' and %d subpackages' % nbsp
        warning += ')'
        print(warning.rjust(80), file=sys.stderr)
    return message","def 1(module_base_path):
    """"""Prepend to sys.path all modules located in *module_base_path*""""""
    if not osp.isdir(module_base_path):
        # Assuming py2exe distribution
        return
    fnames = [
        osp.join(module_base_path, name)
        for name in os.listdir(module_base_path)
    ]
    messages = [
        0(dirname)
        for dirname in fnames
        if osp.isdir(dirname)
    ]
    return os.linesep.join(messages)","{""prepend_module_to_path"": ""0""}"
283,283,"def qapplication(translate=True):
    """"""Return QApplication instance
    Creates it if it doesn't already exist""""""
    if (
        sys.platform == ""darwin""
        and 'Spyder.app' in __file__
    ):
        SpyderApplication = MacApplication
    else:
        SpyderApplication = QApplication
    app = SpyderApplication.instance()
    if not app:
        # Set Application name for Gnome 3
        # https://groups.google.com/forum/#!topic/pyside/24qxvwfrRDs
        app = SpyderApplication(['Spyder'])
    if translate:
        install_translator(app)
    return app","def install_translator(qapp):
    """"""Install Qt translator to the QApplication instance""""""
    global QT_TRANSLATOR
    if QT_TRANSLATOR is None:
        qt_translator = QTranslator()
        if qt_translator.load(
            ""qt_"" + QLocale.system().name(),
            QLibraryInfo.location(
                QLibraryInfo.TranslationsPath
            ),
        ):
            QT_TRANSLATOR = (
                qt_translator
            )  # Keep reference alive
    if QT_TRANSLATOR is not None:
        qapp.installTranslator(QT_TRANSLATOR)","def install_qt_translator_to_qapplication(qapp):
    """"""Install Qt translator to the QApplication instance""""""
    global QT_TRANSLATOR
    if QT_TRANSLATOR is None:
        qt_translator = QTranslator()
        if qt_translator.load(
            ""qt_"" + QLocale.system().name(),
            QLibraryInfo.location(
                QLibraryInfo.TranslationsPath
            ),
        ):
            QT_TRANSLATOR = (
                qt_translator
            )  # Keep reference alive
    if QT_TRANSLATOR is not None:
        qapp.installTranslator(QT_TRANSLATOR)","def return_qapplication_instance(translate=True):
    """"""Return QApplication instance
    Creates it if it doesn't already exist""""""
    if (
        sys.platform == ""darwin""
        and 'Spyder.app' in __file__
    ):
        SpyderApplication = MacApplication
    else:
        SpyderApplication = QApplication
    app = SpyderApplication.instance()
    if not app:
        # Set Application name for Gnome 3
        # https://groups.google.com/forum/#!topic/pyside/24qxvwfrRDs
        app = SpyderApplication(['Spyder'])
    if translate:
        install_qt_translator_to_return_qapplication_instance(app)
    return app","{""install_translator"": ""install_qt_translator_to_qapplication""}","def 0(qapp):
    """"""Install Qt translator to the QApplication instance""""""
    global QT_TRANSLATOR
    if QT_TRANSLATOR is None:
        qt_translator = QTranslator()
        if qt_translator.load(
            ""qt_"" + QLocale.system().name(),
            QLibraryInfo.location(
                QLibraryInfo.TranslationsPath
            ),
        ):
            QT_TRANSLATOR = (
                qt_translator
            )  # Keep reference alive
    if QT_TRANSLATOR is not None:
        qapp.installTranslator(QT_TRANSLATOR)","def 1(translate=True):
    """"""Return QApplication instance
    Creates it if it doesn't already exist""""""
    if (
        sys.platform == ""darwin""
        and 'Spyder.app' in __file__
    ):
        SpyderApplication = MacApplication
    else:
        SpyderApplication = QApplication
    app = SpyderApplication.instance()
    if not app:
        # Set Application name for Gnome 3
        # https://groups.google.com/forum/#!topic/pyside/24qxvwfrRDs
        app = SpyderApplication(['Spyder'])
    if translate:
        0(app)
    return app","{""install_translator"": ""0""}"
284,284,"def keybinding(attr):
    """"""Return keybinding""""""
    ks = getattr(QKeySequence, attr)
    return from_qvariant(
        QKeySequence.keyBindings(ks)[0], str
    )","def from_qvariant(qobj=None, pytype=None):  # analysis:ignore
    """"""Convert QVariant object to Python object
    This is a transitional function from PyQt API #1 (QVariant exist)
    to PyQt API #2 and Pyside (QVariant does not exist)""""""
    return qobj","def convert_qvariant_to_python(qobj=None, pytype=None):  # analysis:ignore
    """"""Convert QVariant object to Python object
    This is a transitional function from PyQt API #1 (QVariant exist)
    to PyQt API #2 and Pyside (QVariant does not exist)""""""
    return qobj","def return_keybinding(attr):
    """"""Return return_keybinding""""""
    ks = getattr(QKeySequence, attr)
    return convert_qvariant_to_python(
        QKeySequence.keyBindings(ks)[0], str
    )","{""from_qvariant"": ""convert_qvariant_to_python""}","def 0(qobj=None, pytype=None):  # analysis:ignore
    """"""Convert QVariant object to Python object
    This is a transitional function from PyQt API #1 (QVariant exist)
    to PyQt API #2 and Pyside (QVariant does not exist)""""""
    return qobj","def 1(attr):
    """"""Return keybinding""""""
    ks = getattr(QKeySequence, attr)
    return 0(
        QKeySequence.keyBindings(ks)[0], str
    )","{""from_qvariant"": ""0""}"
285,285,"def onerror(function, path, excinfo):
    """"""Error handler for `shutil.rmtree`.

    If the error is due to an access error (read-only file), it
    attempts to add write permission and then retries.
    If the error is for another reason, it re-raises the error.

    Usage: `shutil.rmtree(path, onerror=onerror)""""""
    if not os.access(path, os.W_OK):
        # Is the error an access error?
        os.chmod(path, stat.S_IWUSR)
        function(path)
    else:
        raise","def function(a, b=Foo): pass","def function_with_default_argument_Foo(a, b=Foo): pass","def error_handler_for_shutil_rmtree(function_with_default_argument_Foo, path, excinfo):
    """"""Error handler for `shutil.rmtree`.

    If the error is due to an access error (read-only file), it
    attempts to add write permission and then retries.
    If the error is for another reason, it re-raises the error.

    Usage: `shutil.rmtree(path, error_handler_for_shutil_rmtree=error_handler_for_shutil_rmtree)""""""
    if not os.access(path, os.W_OK):
        # Is the error an access error?
        os.chmod(path, stat.S_IWUSR)
        function_with_default_argument_Foo(path)
    else:
        raise","{""function"": ""function_with_default_argument_Foo""}","def 0(a, b=Foo): pass","def 1(function, path, excinfo):
    """"""Error handler for `shutil.rmtree`.

    If the error is due to an access error (read-only file), it
    attempts to add write permission and then retries.
    If the error is for another reason, it re-raises the error.

    Usage: `shutil.rmtree(path, onerror=onerror)""""""
    if not os.access(path, os.W_OK):
        # Is the error an access error?
        os.chmod(path, stat.S_IWUSR)
        0(path)
    else:
        raise","{""function"": ""0""}"
286,286,"def onerror(function, path, excinfo):
    """"""Error handler for `shutil.rmtree`.

    If the error is due to an access error (read-only file), it
    attempts to add write permission and then retries.
    If the error is for another reason, it re-raises the error.

    Usage: `shutil.rmtree(path, onerror=onerror)""""""
    if not os.access(path, os.W_OK):
        # Is the error an access error?
        os.chmod(path, stat.S_IWUSR)
        function(path)
    else:
        raise","def function(a, b=Foo): pass","def function_with_default_argument_Foo(a, b=Foo): pass","def error_handler_for_shutil_rmtree(function_with_default_argument_Foo, path, excinfo):
    """"""Error handler for `shutil.rmtree`.

    If the error is due to an access error (read-only file), it
    attempts to add write permission and then retries.
    If the error is for another reason, it re-raises the error.

    Usage: `shutil.rmtree(path, error_handler_for_shutil_rmtree=error_handler_for_shutil_rmtree)""""""
    if not os.access(path, os.W_OK):
        # Is the error an access error?
        os.chmod(path, stat.S_IWUSR)
        function_with_default_argument_Foo(path)
    else:
        raise","{""function"": ""function_with_default_argument_Foo""}","def 0(a, b=Foo): pass","def 1(function, path, excinfo):
    """"""Error handler for `shutil.rmtree`.

    If the error is due to an access error (read-only file), it
    attempts to add write permission and then retries.
    If the error is for another reason, it re-raises the error.

    Usage: `shutil.rmtree(path, onerror=onerror)""""""
    if not os.access(path, os.W_OK):
        # Is the error an access error?
        os.chmod(path, stat.S_IWUSR)
        0(path)
    else:
        raise","{""function"": ""0""}"
287,287,"def to_text_string(obj, encoding=None):
    """"""Convert `obj` to (unicode) text string""""""
    if PY2:
        # Python 2
        if encoding is None:
            return unicode(obj)
        else:
            return unicode(obj, encoding)
    else:
        # Python 3
        if encoding is None:
            return str(obj)
        elif isinstance(obj, str):
            # In case this function is not used properly, this could happen
            return obj
        else:
            return str(obj, encoding)","def str(val):
    """"""Convert float to string, taking the locale into account.""""""
    return _format(""%.12g"", val)","def locale_aware_float_to_string(val):
    """"""Convert float to locale_aware_float_to_stringing, taking the locale into account.""""""
    return _format(""%.12g"", val)","def to_text_locale_aware_float_to_stringing(obj, encoding=None):
    """"""Convert `obj` to (unicode) text locale_aware_float_to_stringing""""""
    if PY2:
        # Python 2
        if encoding is None:
            return unicode(obj)
        else:
            return unicode(obj, encoding)
    else:
        # Python 3
        if encoding is None:
            return locale_aware_float_to_string(obj)
        elif isinstance(obj, locale_aware_float_to_string):
            # In case this function is not used properly, this could happen
            return obj
        else:
            return locale_aware_float_to_string(obj, encoding)","{""str"": ""locale_aware_float_to_string""}","def 0(val):
    """"""Convert float to string, taking the locale into account.""""""
    return _format(""%.12g"", val)","def 1(obj, encoding=None):
    """"""Convert `obj` to (unicode) text string""""""
    if PY2:
        # Python 2
        if encoding is None:
            return unicode(obj)
        else:
            return unicode(obj, encoding)
    else:
        # Python 3
        if encoding is None:
            return 0(obj)
        elif isinstance(obj, str):
            # In case this function is not used properly, this could happen
            return obj
        else:
            return 0(obj, encoding)","{""str"": ""0""}"
288,288,"def to_binary_string(obj, encoding=None):
    """"""Convert `obj` to binary string (bytes in Python 3, str in Python 2)""""""
    if PY2:
        # Python 2
        if encoding is None:
            return str(obj)
        else:
            return obj.encode(encoding)
    else:
        # Python 3
        return bytes(obj, 'utf-8' if encoding is None else encoding)","def str(val):
    """"""Convert float to string, taking the locale into account.""""""
    return _format(""%.12g"", val)","def locale_aware_float_to_string(val):
    """"""Convert float to locale_aware_float_to_stringing, taking the locale into account.""""""
    return _format(""%.12g"", val)","def to_binary_locale_aware_float_to_stringing(obj, encoding=None):
    """"""Convert `obj` to binary locale_aware_float_to_stringing (bytes in Python 3, locale_aware_float_to_string in Python 2)""""""
    if PY2:
        # Python 2
        if encoding is None:
            return locale_aware_float_to_string(obj)
        else:
            return obj.encode(encoding)
    else:
        # Python 3
        return bytes(obj, 'utf-8' if encoding is None else encoding)","{""str"": ""locale_aware_float_to_string""}","def 0(val):
    """"""Convert float to string, taking the locale into account.""""""
    return _format(""%.12g"", val)","def 1(obj, encoding=None):
    """"""Convert `obj` to binary string (bytes in Python 3, str in Python 2)""""""
    if PY2:
        # Python 2
        if encoding is None:
            return 0(obj)
        else:
            return obj.encode(encoding)
    else:
        # Python 3
        return bytes(obj, 'utf-8' if encoding is None else encoding)","{""str"": ""0""}"
289,289,"def qbytearray_to_str(qba):
    """"""Convert QByteArray object to str in a way compatible with Python 2/3""""""
    return str(bytes(qba.toHex().data()).decode())","def str(val):
    """"""Convert float to string, taking the locale into account.""""""
    return _format(""%.12g"", val)","def locale_aware_float_to_string(val):
    """"""Convert float to locale_aware_float_to_stringing, taking the locale into account.""""""
    return _format(""%.12g"", val)","def qbytearray_to_locale_aware_float_to_string(qba):
    """"""Convert QByteArray object to locale_aware_float_to_string in a way compatible with Python 2/3""""""
    return locale_aware_float_to_string(bytes(qba.toHex().data()).decode())","{""str"": ""locale_aware_float_to_string""}","def 0(val):
    """"""Convert float to string, taking the locale into account.""""""
    return _format(""%.12g"", val)","def qbytearray_to_0(qba):
    """"""Convert QByteArray object to str in a way compatible with Python 2/3""""""
    return 0(bytes(qba.toHex().data()).decode())","{""str"": ""0""}"
290,290,"def to_text_string(obj, encoding=None):
    """"""Convert `obj` to (unicode) text string""""""
    if PY2:
        # Python 2
        if encoding is None:
            return unicode(obj)
        else:
            return unicode(obj, encoding)
    else:
        # Python 3
        if encoding is None:
            return str(obj)
        elif isinstance(obj, str):
            # In case this function is not used properly, this could happen
            return obj
        else:
            return str(obj, encoding)","def str(val):
    """"""Convert float to string, taking the locale into account.""""""
    return _format(""%.12g"", val)","def locale_aware_float_to_string(val):
    """"""Convert float to locale_aware_float_to_stringing, taking the locale into account.""""""
    return _format(""%.12g"", val)","def to_text_locale_aware_float_to_stringing(obj, encoding=None):
    """"""Convert `obj` to (unicode) text locale_aware_float_to_stringing""""""
    if PY2:
        # Python 2
        if encoding is None:
            return unicode(obj)
        else:
            return unicode(obj, encoding)
    else:
        # Python 3
        if encoding is None:
            return locale_aware_float_to_string(obj)
        elif isinstance(obj, locale_aware_float_to_string):
            # In case this function is not used properly, this could happen
            return obj
        else:
            return locale_aware_float_to_string(obj, encoding)","{""str"": ""locale_aware_float_to_string""}","def 0(val):
    """"""Convert float to string, taking the locale into account.""""""
    return _format(""%.12g"", val)","def 1(obj, encoding=None):
    """"""Convert `obj` to (unicode) text string""""""
    if PY2:
        # Python 2
        if encoding is None:
            return unicode(obj)
        else:
            return unicode(obj, encoding)
    else:
        # Python 3
        if encoding is None:
            return 0(obj)
        elif isinstance(obj, str):
            # In case this function is not used properly, this could happen
            return obj
        else:
            return 0(obj, encoding)","{""str"": ""0""}"
291,291,"def to_text_string(obj, encoding=None):
    """"""Convert `obj` to (unicode) text string""""""
    if PY2:
        # Python 2
        if encoding is None:
            return unicode(obj)
        else:
            return unicode(obj, encoding)
    else:
        # Python 3
        if encoding is None:
            return str(obj)
        elif isinstance(obj, str):
            # In case this function is not used properly, this could happen
            return obj
        else:
            return str(obj, encoding)","def str(val):
    """"""Convert float to string, taking the locale into account.""""""
    return _format(""%.12g"", val)","def locale_aware_float_to_string(val):
    """"""Convert float to locale_aware_float_to_stringing, taking the locale into account.""""""
    return _format(""%.12g"", val)","def to_text_locale_aware_float_to_stringing(obj, encoding=None):
    """"""Convert `obj` to (unicode) text locale_aware_float_to_stringing""""""
    if PY2:
        # Python 2
        if encoding is None:
            return unicode(obj)
        else:
            return unicode(obj, encoding)
    else:
        # Python 3
        if encoding is None:
            return locale_aware_float_to_string(obj)
        elif isinstance(obj, locale_aware_float_to_string):
            # In case this function is not used properly, this could happen
            return obj
        else:
            return locale_aware_float_to_string(obj, encoding)","{""str"": ""locale_aware_float_to_string""}","def 0(val):
    """"""Convert float to string, taking the locale into account.""""""
    return _format(""%.12g"", val)","def 1(obj, encoding=None):
    """"""Convert `obj` to (unicode) text string""""""
    if PY2:
        # Python 2
        if encoding is None:
            return unicode(obj)
        else:
            return unicode(obj, encoding)
    else:
        # Python 3
        if encoding is None:
            return 0(obj)
        elif isinstance(obj, str):
            # In case this function is not used properly, this could happen
            return obj
        else:
            return 0(obj, encoding)","{""str"": ""0""}"
292,292,"def to_binary_string(obj, encoding=None):
    """"""Convert `obj` to binary string (bytes in Python 3, str in Python 2)""""""
    if PY2:
        # Python 2
        if encoding is None:
            return str(obj)
        else:
            return obj.encode(encoding)
    else:
        # Python 3
        return bytes(obj, 'utf-8' if encoding is None else encoding)","def str(val):
    """"""Convert float to string, taking the locale into account.""""""
    return _format(""%.12g"", val)","def locale_aware_float_to_string(val):
    """"""Convert float to locale_aware_float_to_stringing, taking the locale into account.""""""
    return _format(""%.12g"", val)","def to_binary_locale_aware_float_to_stringing(obj, encoding=None):
    """"""Convert `obj` to binary locale_aware_float_to_stringing (bytes in Python 3, locale_aware_float_to_string in Python 2)""""""
    if PY2:
        # Python 2
        if encoding is None:
            return locale_aware_float_to_string(obj)
        else:
            return obj.encode(encoding)
    else:
        # Python 3
        return bytes(obj, 'utf-8' if encoding is None else encoding)","{""str"": ""locale_aware_float_to_string""}","def 0(val):
    """"""Convert float to string, taking the locale into account.""""""
    return _format(""%.12g"", val)","def 1(obj, encoding=None):
    """"""Convert `obj` to binary string (bytes in Python 3, str in Python 2)""""""
    if PY2:
        # Python 2
        if encoding is None:
            return 0(obj)
        else:
            return obj.encode(encoding)
    else:
        # Python 3
        return bytes(obj, 'utf-8' if encoding is None else encoding)","{""str"": ""0""}"
293,293,"def qbytearray_to_str(qba):
    """"""Convert QByteArray object to str in a way compatible with Python 2/3""""""
    return str(bytes(qba.toHex().data()).decode())","def str(val):
    """"""Convert float to string, taking the locale into account.""""""
    return _format(""%.12g"", val)","def locale_aware_float_to_string(val):
    """"""Convert float to locale_aware_float_to_stringing, taking the locale into account.""""""
    return _format(""%.12g"", val)","def qbytearray_to_locale_aware_float_to_string(qba):
    """"""Convert QByteArray object to locale_aware_float_to_string in a way compatible with Python 2/3""""""
    return locale_aware_float_to_string(bytes(qba.toHex().data()).decode())","{""str"": ""locale_aware_float_to_string""}","def 0(val):
    """"""Convert float to string, taking the locale into account.""""""
    return _format(""%.12g"", val)","def qbytearray_to_0(qba):
    """"""Convert QByteArray object to str in a way compatible with Python 2/3""""""
    return 0(bytes(qba.toHex().data()).decode())","{""str"": ""0""}"
294,294,"def convert_requirements(requirements):
    """"""Yield Requires-Dist: strings for parsed requirements strings.""""""
    for req in requirements:
        parsed_requirement = pkg_resources.Requirement.parse(req)
        spec = requires_to_requires_dist(parsed_requirement)
        extras = "","".join(sorted(parsed_requirement.extras))
        if extras:
            extras = ""[%s]"" % extras
        yield (parsed_requirement.project_name + extras + spec)","def requires_to_requires_dist(requirement):
    """"""Return the version specifier for a requirement in PEP 345/566 fashion.""""""
    if getattr(requirement, 'url', None):
        return "" @ "" + requirement.url

    requires_dist = []
    for op, ver in requirement.specs:
        requires_dist.append(op + ver)
    if not requires_dist:
        return ''
    return "" (%s)"" % ','.join(sorted(requires_dist))","def get_version_specifier_for_requirement(requirement):
    """"""Return the version specifier for a requirement in PEP 345/566 fashion.""""""
    if getattr(requirement, 'url', None):
        return "" @ "" + requirement.url

    requires_dist = []
    for op, ver in requirement.specs:
        requires_dist.append(op + ver)
    if not requires_dist:
        return ''
    return "" (%s)"" % ','.join(sorted(requires_dist))","def generate_requires_dist_strings_for_parsed_requirements(requirements):
    """"""Yield Requires-Dist: strings for parsed requirements strings.""""""
    for req in requirements:
        parsed_requirement = pkg_resources.Requirement.parse(req)
        spec = get_version_specifier_for_requirement(parsed_requirement)
        extras = "","".join(sorted(parsed_requirement.extras))
        if extras:
            extras = ""[%s]"" % extras
        yield (parsed_requirement.project_name + extras + spec)","{""requires_to_requires_dist"": ""get_version_specifier_for_requirement""}","def 0(requirement):
    """"""Return the version specifier for a requirement in PEP 345/566 fashion.""""""
    if getattr(requirement, 'url', None):
        return "" @ "" + requirement.url

    requires_dist = []
    for op, ver in requirement.specs:
        requires_dist.append(op + ver)
    if not requires_dist:
        return ''
    return "" (%s)"" % ','.join(sorted(requires_dist))","def 1(requirements):
    """"""Yield Requires-Dist: strings for parsed requirements strings.""""""
    for req in requirements:
        parsed_requirement = pkg_resources.Requirement.parse(req)
        spec = 0(parsed_requirement)
        extras = "","".join(sorted(parsed_requirement.extras))
        if extras:
            extras = ""[%s]"" % extras
        yield (parsed_requirement.project_name + extras + spec)","{""requires_to_requires_dist"": ""0""}"
295,295,"def generate_requirements(extras_require):
    """"""
    Convert requirements from a setup()-style dictionary to ('Requires-Dist', 'requirement')
    and ('Provides-Extra', 'extra') tuples.

    extras_require is a dictionary of {extra: [requirements]} as passed to setup(),
    using the empty extra {'': [requirements]} to hold install_requires.
    """"""
    for extra, depends in extras_require.items():
        condition = ''
        extra = extra or ''
        if ':' in extra:  # setuptools extra:condition syntax
            extra, condition = extra.split(':', 1)

        extra = pkg_resources.safe_extra(extra)
        if extra:
            yield 'Provides-Extra', extra
            if condition:
                condition = ""("" + condition + "") and ""
            condition += ""extra == '%s'"" % extra

        if condition:
            condition = ' ; ' + condition

        for new_req in convert_requirements(depends):
            yield 'Requires-Dist', new_req + condition","def convert_requirements(requirements):
    """"""Yield Requires-Dist: strings for parsed requirements strings.""""""
    for req in requirements:
        parsed_requirement = pkg_resources.Requirement.parse(req)
        spec = requires_to_requires_dist(parsed_requirement)
        extras = "","".join(sorted(parsed_requirement.extras))
        if extras:
            extras = ""[%s]"" % extras
        yield (parsed_requirement.project_name + extras + spec)","def generate_requires_dist_strings_for_parsed_requirements(requirements):
    """"""Yield Requires-Dist: strings for parsed requirements strings.""""""
    for req in requirements:
        parsed_requirement = pkg_resources.Requirement.parse(req)
        spec = requires_to_requires_dist(parsed_requirement)
        extras = "","".join(sorted(parsed_requirement.extras))
        if extras:
            extras = ""[%s]"" % extras
        yield (parsed_requirement.project_name + extras + spec)","def convert_requirements_from_setup_style_dictionary(extras_require):
    """"""
    Convert requirements from a setup()-style dictionary to ('Requires-Dist', 'requirement')
    and ('Provides-Extra', 'extra') tuples.

    extras_require is a dictionary of {extra: [requirements]} as passed to setup(),
    using the empty extra {'': [requirements]} to hold install_requires.
    """"""
    for extra, depends in extras_require.items():
        condition = ''
        extra = extra or ''
        if ':' in extra:  # setuptools extra:condition syntax
            extra, condition = extra.split(':', 1)

        extra = pkg_resources.safe_extra(extra)
        if extra:
            yield 'Provides-Extra', extra
            if condition:
                condition = ""("" + condition + "") and ""
            condition += ""extra == '%s'"" % extra

        if condition:
            condition = ' ; ' + condition

        for new_req in generate_requires_dist_strings_for_parsed_requirements(depends):
            yield 'Requires-Dist', new_req + condition","{""convert_requirements"": ""generate_requires_dist_strings_for_parsed_requirements""}","def 0(requirements):
    """"""Yield Requires-Dist: strings for parsed requirements strings.""""""
    for req in requirements:
        parsed_requirement = pkg_resources.Requirement.parse(req)
        spec = requires_to_requires_dist(parsed_requirement)
        extras = "","".join(sorted(parsed_requirement.extras))
        if extras:
            extras = ""[%s]"" % extras
        yield (parsed_requirement.project_name + extras + spec)","def 1(extras_require):
    """"""
    Convert requirements from a setup()-style dictionary to ('Requires-Dist', 'requirement')
    and ('Provides-Extra', 'extra') tuples.

    extras_require is a dictionary of {extra: [requirements]} as passed to setup(),
    using the empty extra {'': [requirements]} to hold install_requires.
    """"""
    for extra, depends in extras_require.items():
        condition = ''
        extra = extra or ''
        if ':' in extra:  # setuptools extra:condition syntax
            extra, condition = extra.split(':', 1)

        extra = pkg_resources.safe_extra(extra)
        if extra:
            yield 'Provides-Extra', extra
            if condition:
                condition = ""("" + condition + "") and ""
            condition += ""extra == '%s'"" % extra

        if condition:
            condition = ' ; ' + condition

        for new_req in 0(depends):
            yield 'Requires-Dist', new_req + condition","{""convert_requirements"": ""0""}"
296,296,"def dedent_description(pkg_info):
    """"""
    Dedent and convert pkg_info['Description'] to Unicode.
    """"""
    description = pkg_info['Description']

    # Python 3 Unicode handling, sorta.
    surrogates = False
    if not isinstance(description, str):
        surrogates = True
        description = pkginfo_unicode(pkg_info, 'Description')

    description_lines = description.splitlines()
    description_dedent = '\n'.join(
        # if the first line of long_description is blank,
        # the first line here will be indented.
        (description_lines[0].lstrip(),
         textwrap.dedent('\n'.join(description_lines[1:])),
         '\n'))

    if surrogates:
        description_dedent = description_dedent \
            .encode(""utf8"") \
            .decode(""ascii"", ""surrogateescape"")

    return description_dedent","def pkginfo_unicode(pkg_info, field):
    """"""Hack to coax Unicode out of an email Message() - Python 3.3+""""""
    text = pkg_info[field]
    field = field.lower()
    if not isinstance(text, str):
        for item in pkg_info.raw_items():
            if item[0].lower() == field:
                text = item[1].encode('ascii', 'surrogateescape') \
                    .decode('utf-8')
                break

    return text","def coax_unicode_from_pkginfo(pkg_info, field):
    """"""Hack to coax Unicode out of an email Message() - Python 3.3+""""""
    text = pkg_info[field]
    field = field.lower()
    if not isinstance(text, str):
        for item in pkg_info.raw_items():
            if item[0].lower() == field:
                text = item[1].encode('ascii', 'surrogateescape') \
                    .decode('utf-8')
                break

    return text","def dedent_and_convert_pkg_info_description_to_unicode(pkg_info):
    """"""
    Dedent and convert pkg_info['Description'] to Unicode.
    """"""
    description = pkg_info['Description']

    # Python 3 Unicode handling, sorta.
    surrogates = False
    if not isinstance(description, str):
        surrogates = True
        description = coax_unicode_from_pkginfo(pkg_info, 'Description')

    description_lines = description.splitlines()
    description_dedent = '\n'.join(
        # if the first line of long_description is blank,
        # the first line here will be indented.
        (description_lines[0].lstrip(),
         textwrap.dedent('\n'.join(description_lines[1:])),
         '\n'))

    if surrogates:
        description_dedent = description_dedent \
            .encode(""utf8"") \
            .decode(""ascii"", ""surrogateescape"")

    return description_dedent","{""pkginfo_unicode"": ""coax_unicode_from_pkginfo""}","def 0(pkg_info, field):
    """"""Hack to coax Unicode out of an email Message() - Python 3.3+""""""
    text = pkg_info[field]
    field = field.lower()
    if not isinstance(text, str):
        for item in pkg_info.raw_items():
            if item[0].lower() == field:
                text = item[1].encode('ascii', 'surrogateescape') \
                    .decode('utf-8')
                break

    return text","def 1(pkg_info):
    """"""
    Dedent and convert pkg_info['Description'] to Unicode.
    """"""
    description = pkg_info['Description']

    # Python 3 Unicode handling, sorta.
    surrogates = False
    if not isinstance(description, str):
        surrogates = True
        description = 0(pkg_info, 'Description')

    description_lines = description.splitlines()
    description_dedent = '\n'.join(
        # if the first line of long_description is blank,
        # the first line here will be indented.
        (description_lines[0].lstrip(),
         textwrap.dedent('\n'.join(description_lines[1:])),
         '\n'))

    if surrogates:
        description_dedent = description_dedent \
            .encode(""utf8"") \
            .decode(""ascii"", ""surrogateescape"")

    return description_dedent","{""pkginfo_unicode"": ""0""}"
297,297,"def _should_cache(
    req: InstallRequirement,
) -> Optional[bool]:
    """"""
    Return whether a built InstallRequirement can be stored in the persistent
    wheel cache, assuming the wheel cache is available, and _should_build()
    has determined a wheel needs to be built.
    """"""
    if req.editable or not req.source_dir:
        # never cache editable requirements
        return False

    if req.link and req.link.is_vcs:
        # VCS checkout. Do not cache
        # unless it points to an immutable commit hash.
        assert not req.editable
        assert req.source_dir
        vcs_backend = vcs.get_backend_for_scheme(req.link.scheme)
        assert vcs_backend
        if vcs_backend.is_immutable_rev_checkout(req.link.url, req.source_dir):
            return True
        return False

    assert req.link
    base, ext = req.link.splitext()
    if _contains_egg_info(base):
        return True

    # Otherwise, do not cache.
    return False","def _contains_egg_info(s: str) -> bool:
    """"""Determine whether the string looks like an egg_info.

    :param s: The string to parse. E.g. foo-2.1
    """"""
    return bool(_egg_info_re.search(s))","def _contains_egg_info(s: str) -> bool:
    """"""Determine whether the string looks like an egg_info.

    :param s: The string to parse. E.g. foo-2.1
    """"""
    return bool(_egg_info_re.search(s))","def can_cache_built_requirement(
    req: InstallRequirement,
) -> Optional[bool]:
    """"""
    Return whether a built InstallRequirement can be stored in the persistent
    wheel cache, assuming the wheel cache is available, and _should_build()
    has determined a wheel needs to be built.
    """"""
    if req.editable or not req.source_dir:
        # never cache editable requirements
        return False

    if req.link and req.link.is_vcs:
        # VCS checkout. Do not cache
        # unless it points to an immutable commit hash.
        assert not req.editable
        assert req.source_dir
        vcs_backend = vcs.get_backend_for_scheme(req.link.scheme)
        assert vcs_backend
        if vcs_backend.is_immutable_rev_checkout(req.link.url, req.source_dir):
            return True
        return False

    assert req.link
    base, ext = req.link.splitext()
    if _contains_egg_info(base):
        return True

    # Otherwise, do not cache.
    return False",{},"def _contains_egg_info(s: str) -> bool:
    """"""Determine whether the string looks like an egg_info.

    :param s: The string to parse. E.g. foo-2.1
    """"""
    return bool(_egg_info_re.search(s))","def 1(
    req: InstallRequirement,
) -> Optional[bool]:
    """"""
    Return whether a built InstallRequirement can be stored in the persistent
    wheel cache, assuming the wheel cache is available, and _should_build()
    has determined a wheel needs to be built.
    """"""
    if req.editable or not req.source_dir:
        # never cache editable requirements
        return False

    if req.link and req.link.is_vcs:
        # VCS checkout. Do not cache
        # unless it points to an immutable commit hash.
        assert not req.editable
        assert req.source_dir
        vcs_backend = vcs.get_backend_for_scheme(req.link.scheme)
        assert vcs_backend
        if vcs_backend.is_immutable_rev_checkout(req.link.url, req.source_dir):
            return True
        return False

    assert req.link
    base, ext = req.link.splitext()
    if _contains_egg_info(base):
        return True

    # Otherwise, do not cache.
    return False",{}
298,298,"def _get_cache_dir(
    req: InstallRequirement,
    wheel_cache: WheelCache,
) -> str:
    """"""Return the persistent or temporary cache directory where the built
    wheel need to be stored.
    """"""
    cache_available = bool(wheel_cache.cache_dir)
    assert req.link
    if cache_available and _should_cache(req):
        cache_dir = wheel_cache.get_path_for_link(req.link)
    else:
        cache_dir = wheel_cache.get_ephem_path_for_link(req.link)
    return cache_dir","def _should_cache(
    req: InstallRequirement,
) -> Optional[bool]:
    """"""
    Return whether a built InstallRequirement can be stored in the persistent
    wheel cache, assuming the wheel cache is available, and _should_build()
    has determined a wheel needs to be built.
    """"""
    if req.editable or not req.source_dir:
        # never cache editable requirements
        return False

    if req.link and req.link.is_vcs:
        # VCS checkout. Do not cache
        # unless it points to an immutable commit hash.
        assert not req.editable
        assert req.source_dir
        vcs_backend = vcs.get_backend_for_scheme(req.link.scheme)
        assert vcs_backend
        if vcs_backend.is_immutable_rev_checkout(req.link.url, req.source_dir):
            return True
        return False

    assert req.link
    base, ext = req.link.splitext()
    if _contains_egg_info(base):
        return True

    # Otherwise, do not cache.
    return False","def can_cache_built_requirement(
    req: InstallRequirement,
) -> Optional[bool]:
    """"""
    Return whether a built InstallRequirement can be stored in the persistent
    wheel cache, assuming the wheel cache is available, and _should_build()
    has determined a wheel needs to be built.
    """"""
    if req.editable or not req.source_dir:
        # never cache editable requirements
        return False

    if req.link and req.link.is_vcs:
        # VCS checkout. Do not cache
        # unless it points to an immutable commit hash.
        assert not req.editable
        assert req.source_dir
        vcs_backend = vcs.get_backend_for_scheme(req.link.scheme)
        assert vcs_backend
        if vcs_backend.is_immutable_rev_checkout(req.link.url, req.source_dir):
            return True
        return False

    assert req.link
    base, ext = req.link.splitext()
    if _contains_egg_info(base):
        return True

    # Otherwise, do not cache.
    return False","def return_persistent_or_temporary_cache_directory(
    req: InstallRequirement,
    wheel_cache: WheelCache,
) -> str:
    """"""Return the persistent or temporary cache directory where the built
    wheel need to be stored.
    """"""
    cache_available = bool(wheel_cache.cache_dir)
    assert req.link
    if cache_available and can_cache_built_requirement(req):
        cache_dir = wheel_cache.get_path_for_link(req.link)
    else:
        cache_dir = wheel_cache.get_ephem_path_for_link(req.link)
    return cache_dir","{""_should_cache"": ""can_cache_built_requirement""}","def 0(
    req: InstallRequirement,
) -> Optional[bool]:
    """"""
    Return whether a built InstallRequirement can be stored in the persistent
    wheel cache, assuming the wheel cache is available, and _should_build()
    has determined a wheel needs to be built.
    """"""
    if req.editable or not req.source_dir:
        # never cache editable requirements
        return False

    if req.link and req.link.is_vcs:
        # VCS checkout. Do not cache
        # unless it points to an immutable commit hash.
        assert not req.editable
        assert req.source_dir
        vcs_backend = vcs.get_backend_for_scheme(req.link.scheme)
        assert vcs_backend
        if vcs_backend.is_immutable_rev_checkout(req.link.url, req.source_dir):
            return True
        return False

    assert req.link
    base, ext = req.link.splitext()
    if _contains_egg_info(base):
        return True

    # Otherwise, do not cache.
    return False","def 1(
    req: InstallRequirement,
    wheel_cache: WheelCache,
) -> str:
    """"""Return the persistent or temporary cache directory where the built
    wheel need to be stored.
    """"""
    cache_available = bool(wheel_cache.cache_dir)
    assert req.link
    if cache_available and 0(req):
        cache_dir = wheel_cache.get_path_for_link(req.link)
    else:
        cache_dir = wheel_cache.get_ephem_path_for_link(req.link)
    return cache_dir","{""_should_cache"": ""0""}"
299,299,"def was_installed_by_pip(pkg: str) -> bool:
    """"""Checks whether pkg was installed by pip

    This is used not to display the upgrade message when pip is in fact
    installed by system package manager, such as dnf on Fedora.
    """"""
    dist = get_default_environment().get_distribution(pkg)
    return dist is not None and ""pip"" == dist.installer","def get_default_environment() -> BaseEnvironment:
    """"""Get the default representation for the current environment.

    This returns an Environment instance from the chosen backend. The default
    Environment instance should be built from ``sys.path`` and may use caching
    to share instance state accorss calls.
    """"""
    return select_backend().Environment.default()","def get_default_environment() -> BaseEnvironment:
    """"""Get the default representation for the current environment.

    This returns an Environment instance from the chosen backend. The default
    Environment instance should be built from ``sys.path`` and may use caching
    to share instance state accorss calls.
    """"""
    return select_backend().Environment.default()","def was_installed_by_pip(pkg: str) -> bool:
    """"""Checks whether pkg was installed by pip

    This is used not to display the upgrade message when pip is in fact
    installed by system package manager, such as dnf on Fedora.
    """"""
    dist = get_default_environment().get_distribution(pkg)
    return dist is not None and ""pip"" == dist.installer",{},"def get_default_environment() -> BaseEnvironment:
    """"""Get the default representation for the current environment.

    This returns an Environment instance from the chosen backend. The default
    Environment instance should be built from ``sys.path`` and may use caching
    to share instance state accorss calls.
    """"""
    return select_backend().Environment.default()","def was_installed_by_pip(pkg: str) -> bool:
    """"""Checks whether pkg was installed by pip

    This is used not to display the upgrade message when pip is in fact
    installed by system package manager, such as dnf on Fedora.
    """"""
    dist = get_default_environment().get_distribution(pkg)
    return dist is not None and ""pip"" == dist.installer",{}
300,300,"def pip_self_version_check(session: PipSession, options: optparse.Values) -> None:
    """"""Check for an update for pip.

    Limit the frequency of checks to once per week. State is stored either in
    the active virtualenv or in the user's USER_CACHE_DIR keyed off the prefix
    of the pip script path.
    """"""
    installed_dist = get_default_environment().get_distribution(""pip"")
    if not installed_dist:
        return

    try:
        upgrade_prompt = _self_version_check_logic(
            state=SelfCheckState(cache_dir=options.cache_dir),
            current_time=datetime.datetime.utcnow(),
            local_version=installed_dist.version,
            get_remote_version=functools.partial(
                _get_current_remote_pip_version, session, options
            ),
        )
        if upgrade_prompt is not None:
            logger.info(""[present-rich] %s"", upgrade_prompt)
    except Exception:
        logger.warning(""There was an error checking the latest version of pip."")
        logger.debug(""See below for error"", exc_info=True)","def _self_version_check_logic(
    *,
    state: SelfCheckState,
    current_time: datetime.datetime,
    local_version: DistributionVersion,
    get_remote_version: Callable[[], str],
) -> Optional[UpgradePrompt]:
    remote_version_str = state.get(current_time)
    if remote_version_str is None:
        remote_version_str = get_remote_version()
        state.set(remote_version_str, current_time)

    remote_version = parse_version(remote_version_str)
    logger.debug(""Remote version of pip: %s"", remote_version)
    logger.debug(""Local version of pip:  %s"", local_version)

    pip_installed_by_pip = was_installed_by_pip(""pip"")
    logger.debug(""Was pip installed by pip? %s"", pip_installed_by_pip)
    if not pip_installed_by_pip:
        return None  # Only suggest upgrade if pip is installed by pip.

    local_version_is_older = (
        local_version < remote_version
        and local_version.base_version != remote_version.base_version
    )
    if local_version_is_older:
        return UpgradePrompt(old=str(local_version), new=remote_version_str)

    return None

def get_default_environment() -> BaseEnvironment:
    """"""Get the default representation for the current environment.

    This returns an Environment instance from the chosen backend. The default
    Environment instance should be built from ``sys.path`` and may use caching
    to share instance state accorss calls.
    """"""
    return select_backend().Environment.default()","def _self_version_check_logic(
    *,
    state: SelfCheckState,
    current_time: datetime.datetime,
    local_version: DistributionVersion,
    get_remote_version: Callable[[], str],
) -> Optional[UpgradePrompt]:
    remote_version_str = state.get(current_time)
    if remote_version_str is None:
        remote_version_str = get_remote_version()
        state.set(remote_version_str, current_time)

    remote_version = parse_version(remote_version_str)
    logger.debug(""Remote version of pip: %s"", remote_version)
    logger.debug(""Local version of pip:  %s"", local_version)

    pip_installed_by_pip = was_installed_by_pip(""pip"")
    logger.debug(""Was pip installed by pip? %s"", pip_installed_by_pip)
    if not pip_installed_by_pip:
        return None  # Only suggest upgrade if pip is installed by pip.

    local_version_is_older = (
        local_version < remote_version
        and local_version.base_version != remote_version.base_version
    )
    if local_version_is_older:
        return UpgradePrompt(old=str(local_version), new=remote_version_str)

    return None

def get_default_environment() -> BaseEnvironment:
    """"""Get the default representation for the current environment.

    This returns an Environment instance from the chosen backend. The default
    Environment instance should be built from ``sys.path`` and may use caching
    to share instance state accorss calls.
    """"""
    return select_backend().Environment.default()","def pip_self_version_check(session: PipSession, options: optparse.Values) -> None:
    """"""Check for an update for pip.

    Limit the frequency of checks to once per week. State is stored either in
    the active virtualenv or in the user's USER_CACHE_DIR keyed off the prefix
    of the pip script path.
    """"""
    installed_dist = get_default_environment().get_distribution(""pip"")
    if not installed_dist:
        return

    try:
        upgrade_prompt = _self_version_check_logic(
            state=SelfCheckState(cache_dir=options.cache_dir),
            current_time=datetime.datetime.utcnow(),
            local_version=installed_dist.version,
            get_remote_version=functools.partial(
                _get_current_remote_pip_version, session, options
            ),
        )
        if upgrade_prompt is not None:
            logger.info(""[present-rich] %s"", upgrade_prompt)
    except Exception:
        logger.warning(""There was an error checking the latest version of pip."")
        logger.debug(""See below for error"", exc_info=True)",{},"def _self_version_check_logic(
    *,
    state: SelfCheckState,
    current_time: datetime.datetime,
    local_version: DistributionVersion,
    get_remote_version: Callable[[], str],
) -> Optional[UpgradePrompt]:
    remote_version_str = state.get(current_time)
    if remote_version_str is None:
        remote_version_str = get_remote_version()
        state.set(remote_version_str, current_time)

    remote_version = parse_version(remote_version_str)
    logger.debug(""Remote version of pip: %s"", remote_version)
    logger.debug(""Local version of pip:  %s"", local_version)

    pip_installed_by_pip = was_installed_by_pip(""pip"")
    logger.debug(""Was pip installed by pip? %s"", pip_installed_by_pip)
    if not pip_installed_by_pip:
        return None  # Only suggest upgrade if pip is installed by pip.

    local_version_is_older = (
        local_version < remote_version
        and local_version.base_version != remote_version.base_version
    )
    if local_version_is_older:
        return UpgradePrompt(old=str(local_version), new=remote_version_str)

    return None

def get_default_environment() -> BaseEnvironment:
    """"""Get the default representation for the current environment.

    This returns an Environment instance from the chosen backend. The default
    Environment instance should be built from ``sys.path`` and may use caching
    to share instance state accorss calls.
    """"""
    return select_backend().Environment.default()","def pip_self_version_check(session: PipSession, options: optparse.Values) -> None:
    """"""Check for an update for pip.

    Limit the frequency of checks to once per week. State is stored either in
    the active virtualenv or in the user's USER_CACHE_DIR keyed off the prefix
    of the pip script path.
    """"""
    installed_dist = get_default_environment().get_distribution(""pip"")
    if not installed_dist:
        return

    try:
        upgrade_prompt = _self_version_check_logic(
            state=SelfCheckState(cache_dir=options.cache_dir),
            current_time=datetime.datetime.utcnow(),
            local_version=installed_dist.version,
            get_remote_version=functools.partial(
                _get_current_remote_pip_version, session, options
            ),
        )
        if upgrade_prompt is not None:
            logger.info(""[present-rich] %s"", upgrade_prompt)
    except Exception:
        logger.warning(""There was an error checking the latest version of pip."")
        logger.debug(""See below for error"", exc_info=True)",{}
301,301,"def get_keyring_auth(url: Optional[str], username: Optional[str]) -> Optional[AuthInfo]:
    """"""Return the tuple auth for a given url from keyring.""""""
    global keyring
    if not url or not keyring:
        return None

    try:
        try:
            get_credential = keyring.get_credential
        except AttributeError:
            pass
        else:
            logger.debug(""Getting credentials from keyring for %s"", url)
            cred = get_credential(url, username)
            if cred is not None:
                return cred.username, cred.password
            return None

        if username:
            logger.debug(""Getting password from keyring for %s"", url)
            password = keyring.get_password(url, username)
            if password:
                return username, password

    except Exception as exc:
        logger.warning(
            ""Keyring is skipped due to an exception: %s"",
            str(exc),
        )
        keyring = None  # type: ignore[assignment]
    return None","def str(val):
    """"""Convert float to string, taking the locale into account.""""""
    return _format(""%.12g"", val)","def locale_aware_float_to_string(val):
    """"""Convert float to locale_aware_float_to_stringing, taking the locale into account.""""""
    return _format(""%.12g"", val)","def get_keyring_auth(url: Optional[locale_aware_float_to_string], username: Optional[locale_aware_float_to_string]) -> Optional[AuthInfo]:
    """"""Return the tuple auth for a given url from keyring.""""""
    global keyring
    if not url or not keyring:
        return None

    try:
        try:
            get_credential = keyring.get_credential
        except AttributeError:
            pass
        else:
            logger.debug(""Getting credentials from keyring for %s"", url)
            cred = get_credential(url, username)
            if cred is not None:
                return cred.username, cred.password
            return None

        if username:
            logger.debug(""Getting password from keyring for %s"", url)
            password = keyring.get_password(url, username)
            if password:
                return username, password

    except Exception as exc:
        logger.warning(
            ""Keyring is skipped due to an exception: %s"",
            locale_aware_float_to_string(exc),
        )
        keyring = None  # type: ignore[assignment]
    return None","{""str"": ""locale_aware_float_to_string""}","def 0(val):
    """"""Convert float to string, taking the locale into account.""""""
    return _format(""%.12g"", val)","def get_keyring_auth(url: Optional[str], username: Optional[str]) -> Optional[AuthInfo]:
    """"""Return the tuple auth for a given url from keyring.""""""
    global keyring
    if not url or not keyring:
        return None

    try:
        try:
            get_credential = keyring.get_credential
        except AttributeError:
            pass
        else:
            logger.debug(""Getting credentials from keyring for %s"", url)
            cred = get_credential(url, username)
            if cred is not None:
                return cred.username, cred.password
            return None

        if username:
            logger.debug(""Getting password from keyring for %s"", url)
            password = keyring.get_password(url, username)
            if password:
                return username, password

    except Exception as exc:
        logger.warning(
            ""Keyring is skipped due to an exception: %s"",
            0(exc),
        )
        keyring = None  # type: ignore[assignment]
    return None","{""str"": ""0""}"
302,302,"def parse_content_disposition(content_disposition: str, default_filename: str) -> str:
    """"""
    Parse the ""filename"" value from a Content-Disposition header, and
    return the default filename if the result is empty.
    """"""
    _type, params = cgi.parse_header(content_disposition)
    filename = params.get(""filename"")
    if filename:
        # We need to sanitize the filename to prevent directory traversal
        # in case the filename contains "".."" path parts.
        filename = sanitize_content_filename(filename)
    return filename or default_filename","def sanitize_content_filename(filename: str) -> str:
    """"""
    Sanitize the ""filename"" value from a Content-Disposition header.
    """"""
    return os.path.basename(filename)","def sanitize_content_filename(filename: str) -> str:
    """"""
    Sanitize the ""filename"" value from a Content-Disposition header.
    """"""
    return os.path.basename(filename)","def parse_content_disposition(content_disposition: str, default_filename: str) -> str:
    """"""
    Parse the ""filename"" value from a Content-Disposition header, and
    return the default filename if the result is empty.
    """"""
    _type, params = cgi.parse_header(content_disposition)
    filename = params.get(""filename"")
    if filename:
        # We need to sanitize the filename to prevent directory traversal
        # in case the filename contains "".."" path parts.
        filename = sanitize_content_filename(filename)
    return filename or default_filename",{},"def sanitize_content_filename(filename: str) -> str:
    """"""
    Sanitize the ""filename"" value from a Content-Disposition header.
    """"""
    return os.path.basename(filename)","def parse_content_disposition(content_disposition: str, default_filename: str) -> str:
    """"""
    Parse the ""filename"" value from a Content-Disposition header, and
    return the default filename if the result is empty.
    """"""
    _type, params = cgi.parse_header(content_disposition)
    filename = params.get(""filename"")
    if filename:
        # We need to sanitize the filename to prevent directory traversal
        # in case the filename contains "".."" path parts.
        filename = sanitize_content_filename(filename)
    return filename or default_filename",{}
303,303,"def backup_dir(dir: str, ext: str = "".bak"") -> str:
    """"""Figure out the name of a directory to back up the given dir to
    (adding .bak, .bak2, etc)""""""
    n = 1
    extension = ext
    while os.path.exists(dir + extension):
        n += 1
        extension = ext + str(n)
    return dir + extension","def str(val):
    """"""Convert float to string, taking the locale into account.""""""
    return _format(""%.12g"", val)","def locale_aware_float_to_string(val):
    """"""Convert float to locale_aware_float_to_stringing, taking the locale into account.""""""
    return _format(""%.12g"", val)","def figure_out_name_of_directory_to_back_up_given_dir_to(dir: locale_aware_float_to_string, ext: locale_aware_float_to_string = "".bak"") -> locale_aware_float_to_string:
    """"""Figure out the name of a directory to back up the given dir to
    (adding .bak, .bak2, etc)""""""
    n = 1
    extension = ext
    while os.path.exists(dir + extension):
        n += 1
        extension = ext + locale_aware_float_to_string(n)
    return dir + extension","{""str"": ""locale_aware_float_to_string""}","def 0(val):
    """"""Convert float to string, taking the locale into account.""""""
    return _format(""%.12g"", val)","def 1(dir: str, ext: str = "".bak"") -> str:
    """"""Figure out the name of a directory to back up the given dir to
    (adding .bak, .bak2, etc)""""""
    n = 1
    extension = ext
    while os.path.exists(dir + extension):
        n += 1
        extension = ext + 0(n)
    return dir + extension","{""str"": ""0""}"
304,304,"def ask_input(message: str) -> str:
    """"""Ask for input interactively.""""""
    _check_no_input(message)
    return input(message)","def _check_no_input(message: str) -> None:
    """"""Raise an error if no input is allowed.""""""
    if os.environ.get(""PIP_NO_INPUT""):
        raise Exception(
            f""No input was expected ($PIP_NO_INPUT set); question: {message}""
        )

def input(files=None, inplace=False, backup="""", *, mode=""r"", openhook=None,
          encoding=None, errors=None):
    """"""Return an instance of the FileInput class, which can be iterated.

    The parameters are passed to the constructor of the FileInput class.
    The returned instance, in addition to being an iterator,
    keeps global state for the functions of this module,.
    """"""
    global _state
    if _state and _state._file:
        raise RuntimeError(""input() already active"")
    _state = FileInput(files, inplace, backup, mode=mode, openhook=openhook,
                       encoding=encoding, errors=errors)
    return _state","def check_for_no_create_file_input_instance_and_raise_error(message: str) -> None:
    """"""Raise an error if no create_file_input_instance is allowed.""""""
    if os.environ.get(""PIP_NO_INPUT""):
        raise Exception(
            f""No create_file_input_instance was expected ($PIP_NO_INPUT set); question: {message}""
        )

def create_file_input_instance(files=None, inplace=False, backup="""", *, mode=""r"", openhook=None,
          encoding=None, errors=None):
    """"""Return an instance of the FileInput class, which can be iterated.

    The parameters are passed to the constructor of the FileInput class.
    The returned instance, in addition to being an iterator,
    keeps global state for the functions of this module,.
    """"""
    global _state
    if _state and _state._file:
        raise RuntimeError(""create_file_input_instance() already active"")
    _state = FileInput(files, inplace, backup, mode=mode, openhook=openhook,
                       encoding=encoding, errors=errors)
    return _state","def ask_create_file_input_instance(message: str) -> str:
    """"""Ask for create_file_input_instance interactively.""""""
    check_for_no_create_file_input_instance_and_raise_error(message)
    return create_file_input_instance(message)","{""_check_no_input"": ""check_for_no_input_and_raise_error"", ""input"": ""create_file_input_instance""}","def 0(message: str) -> None:
    """"""Raise an error if no input is allowed.""""""
    if os.environ.get(""PIP_NO_INPUT""):
        raise Exception(
            f""No input was expected ($PIP_NO_INPUT set); question: {message}""
        )

def 1(files=None, inplace=False, backup="""", *, mode=""r"", openhook=None,
          encoding=None, errors=None):
    """"""Return an instance of the FileInput class, which can be iterated.

    The parameters are passed to the constructor of the FileInput class.
    The returned instance, in addition to being an iterator,
    keeps global state for the functions of this module,.
    """"""
    global _state
    if _state and _state._file:
        raise RuntimeError(""1() already active"")
    _state = FileInput(files, inplace, backup, mode=mode, openhook=openhook,
                       encoding=encoding, errors=errors)
    return _state","def ask_1(message: str) -> str:
    """"""Ask for input interactively.""""""
    0(message)
    return 1(message)","{""_check_no_input"": ""0"", ""input"": ""1""}"
305,305,"def ask_password(message: str) -> str:
    """"""Ask for a password interactively.""""""
    _check_no_input(message)
    return getpass.getpass(message)","def _check_no_input(message: str) -> None:
    """"""Raise an error if no input is allowed.""""""
    if os.environ.get(""PIP_NO_INPUT""):
        raise Exception(
            f""No input was expected ($PIP_NO_INPUT set); question: {message}""
        )","def check_for_no_input_and_raise_error(message: str) -> None:
    """"""Raise an error if no input is allowed.""""""
    if os.environ.get(""PIP_NO_INPUT""):
        raise Exception(
            f""No input was expected ($PIP_NO_INPUT set); question: {message}""
        )","def ask_password(message: str) -> str:
    """"""Ask for a password interactively.""""""
    check_for_no_input_and_raise_error(message)
    return getpass.getpass(message)","{""_check_no_input"": ""check_for_no_input_and_raise_error""}","def 0(message: str) -> None:
    """"""Raise an error if no input is allowed.""""""
    if os.environ.get(""PIP_NO_INPUT""):
        raise Exception(
            f""No input was expected ($PIP_NO_INPUT set); question: {message}""
        )","def ask_password(message: str) -> str:
    """"""Ask for a password interactively.""""""
    0(message)
    return getpass.getpass(message)","{""_check_no_input"": ""0""}"
306,306,"def parse_netloc(netloc: str) -> Tuple[str, Optional[int]]:
    """"""
    Return the host-port pair from a netloc.
    """"""
    url = build_url_from_netloc(netloc)
    parsed = urllib.parse.urlparse(url)
    return parsed.hostname, parsed.port","def build_url_from_netloc(netloc: str, scheme: str = ""https"") -> str:
    """"""
    Build a full URL from a netloc.
    """"""
    if netloc.count("":"") >= 2 and ""@"" not in netloc and ""["" not in netloc:
        # It must be a bare IPv6 address, so wrap it with brackets.
        netloc = f""[{netloc}]""
    return f""{scheme}://{netloc}""","def build_url_from_netloc(netloc: str, scheme: str = ""https"") -> str:
    """"""
    Build a full URL from a netloc.
    """"""
    if netloc.count("":"") >= 2 and ""@"" not in netloc and ""["" not in netloc:
        # It must be a bare IPv6 address, so wrap it with brackets.
        netloc = f""[{netloc}]""
    return f""{scheme}://{netloc}""","def parse_netloc(netloc: str) -> Tuple[str, Optional[int]]:
    """"""
    Return the host-port pair from a netloc.
    """"""
    url = build_url_from_netloc(netloc)
    parsed = urllib.parse.urlparse(url)
    return parsed.hostname, parsed.port",{},"def build_url_from_netloc(netloc: str, scheme: str = ""https"") -> str:
    """"""
    Build a full URL from a netloc.
    """"""
    if netloc.count("":"") >= 2 and ""@"" not in netloc and ""["" not in netloc:
        # It must be a bare IPv6 address, so wrap it with brackets.
        netloc = f""[{netloc}]""
    return f""{scheme}://{netloc}""","def parse_netloc(netloc: str) -> Tuple[str, Optional[int]]:
    """"""
    Return the host-port pair from a netloc.
    """"""
    url = build_url_from_netloc(netloc)
    parsed = urllib.parse.urlparse(url)
    return parsed.hostname, parsed.port",{}
307,307,"def redact_netloc(netloc: str) -> str:
    """"""
    Replace the sensitive data in a netloc with ""****"", if it exists.

    For example:
        - ""user:pass@example.com"" returns ""user:****@example.com""
        - ""accesstoken@example.com"" returns ""****@example.com""
    """"""
    netloc, (user, password) = split_auth_from_netloc(netloc)
    if user is None:
        return netloc
    if password is None:
        user = ""****""
        password = """"
    else:
        user = urllib.parse.quote(user)
        password = "":****""
    return ""{user}{password}@{netloc}"".format(
        user=user, password=password, netloc=netloc
    )","def split_auth_from_netloc(netloc: str) -> NetlocTuple:
    """"""
    Parse out and remove the auth information from a netloc.

    Returns: (netloc, (username, password)).
    """"""
    if ""@"" not in netloc:
        return netloc, (None, None)

    # Split from the right because that's how urllib.parse.urlsplit()
    # behaves if more than one @ is present (which can be checked using
    # the password attribute of urlsplit()'s return value).
    auth, netloc = netloc.rsplit(""@"", 1)
    pw: Optional[str] = None
    if "":"" in auth:
        # Split from the left because that's how urllib.parse.urlsplit()
        # behaves if more than one : is present (which again can be checked
        # using the password attribute of the return value)
        user, pw = auth.split("":"", 1)
    else:
        user, pw = auth, None

    user = urllib.parse.unquote(user)
    if pw is not None:
        pw = urllib.parse.unquote(pw)

    return netloc, (user, pw)","def split_auth_from_netloc(netloc: str) -> NetlocTuple:
    """"""
    Parse out and remove the auth information from a netloc.

    Returns: (netloc, (username, password)).
    """"""
    if ""@"" not in netloc:
        return netloc, (None, None)

    # Split from the right because that's how urllib.parse.urlsplit()
    # behaves if more than one @ is present (which can be checked using
    # the password attribute of urlsplit()'s return value).
    auth, netloc = netloc.rsplit(""@"", 1)
    pw: Optional[str] = None
    if "":"" in auth:
        # Split from the left because that's how urllib.parse.urlsplit()
        # behaves if more than one : is present (which again can be checked
        # using the password attribute of the return value)
        user, pw = auth.split("":"", 1)
    else:
        user, pw = auth, None

    user = urllib.parse.unquote(user)
    if pw is not None:
        pw = urllib.parse.unquote(pw)

    return netloc, (user, pw)","def redact_netloc(netloc: str) -> str:
    """"""
    Replace the sensitive data in a netloc with ""****"", if it exists.

    For example:
        - ""user:pass@example.com"" returns ""user:****@example.com""
        - ""accesstoken@example.com"" returns ""****@example.com""
    """"""
    netloc, (user, password) = split_auth_from_netloc(netloc)
    if user is None:
        return netloc
    if password is None:
        user = ""****""
        password = """"
    else:
        user = urllib.parse.quote(user)
        password = "":****""
    return ""{user}{password}@{netloc}"".format(
        user=user, password=password, netloc=netloc
    )",{},"def split_auth_from_netloc(netloc: str) -> NetlocTuple:
    """"""
    Parse out and remove the auth information from a netloc.

    Returns: (netloc, (username, password)).
    """"""
    if ""@"" not in netloc:
        return netloc, (None, None)

    # Split from the right because that's how urllib.parse.urlsplit()
    # behaves if more than one @ is present (which can be checked using
    # the password attribute of urlsplit()'s return value).
    auth, netloc = netloc.rsplit(""@"", 1)
    pw: Optional[str] = None
    if "":"" in auth:
        # Split from the left because that's how urllib.parse.urlsplit()
        # behaves if more than one : is present (which again can be checked
        # using the password attribute of the return value)
        user, pw = auth.split("":"", 1)
    else:
        user, pw = auth, None

    user = urllib.parse.unquote(user)
    if pw is not None:
        pw = urllib.parse.unquote(pw)

    return netloc, (user, pw)","def redact_netloc(netloc: str) -> str:
    """"""
    Replace the sensitive data in a netloc with ""****"", if it exists.

    For example:
        - ""user:pass@example.com"" returns ""user:****@example.com""
        - ""accesstoken@example.com"" returns ""****@example.com""
    """"""
    netloc, (user, password) = split_auth_from_netloc(netloc)
    if user is None:
        return netloc
    if password is None:
        user = ""****""
        password = """"
    else:
        user = urllib.parse.quote(user)
        password = "":****""
    return ""{user}{password}@{netloc}"".format(
        user=user, password=password, netloc=netloc
    )",{}
308,308,"def split_auth_netloc_from_url(url: str) -> Tuple[str, str, Tuple[str, str]]:
    """"""
    Parse a url into separate netloc, auth, and url with no auth.

    Returns: (url_without_auth, netloc, (username, password))
    """"""
    url_without_auth, (netloc, auth) = _transform_url(url, _get_netloc)
    return url_without_auth, netloc, auth","def _transform_url(
    url: str, transform_netloc: Callable[[str], Tuple[Any, ...]]
) -> Tuple[str, NetlocTuple]:
    """"""Transform and replace netloc in a url.

    transform_netloc is a function taking the netloc and returning a
    tuple. The first element of this tuple is the new netloc. The
    entire tuple is returned.

    Returns a tuple containing the transformed url as item 0 and the
    original tuple returned by transform_netloc as item 1.
    """"""
    purl = urllib.parse.urlsplit(url)
    netloc_tuple = transform_netloc(purl.netloc)
    # stripped url
    url_pieces = (purl.scheme, netloc_tuple[0], purl.path, purl.query, purl.fragment)
    surl = urllib.parse.urlunsplit(url_pieces)
    return surl, cast(""NetlocTuple"", netloc_tuple)","def _transform_url(
    url: str, transform_netloc: Callable[[str], Tuple[Any, ...]]
) -> Tuple[str, NetlocTuple]:
    """"""Transform and replace netloc in a url.

    transform_netloc is a function taking the netloc and returning a
    tuple. The first element of this tuple is the new netloc. The
    entire tuple is returned.

    Returns a tuple containing the transformed url as item 0 and the
    original tuple returned by transform_netloc as item 1.
    """"""
    purl = urllib.parse.urlsplit(url)
    netloc_tuple = transform_netloc(purl.netloc)
    # stripped url
    url_pieces = (purl.scheme, netloc_tuple[0], purl.path, purl.query, purl.fragment)
    surl = urllib.parse.urlunsplit(url_pieces)
    return surl, cast(""NetlocTuple"", netloc_tuple)","def split_auth_netloc_from_url(url: str) -> Tuple[str, str, Tuple[str, str]]:
    """"""
    Parse a url into separate netloc, auth, and url with no auth.

    Returns: (url_without_auth, netloc, (username, password))
    """"""
    url_without_auth, (netloc, auth) = _transform_url(url, _get_netloc)
    return url_without_auth, netloc, auth",{},"def _transform_url(
    url: str, transform_netloc: Callable[[str], Tuple[Any, ...]]
) -> Tuple[str, NetlocTuple]:
    """"""Transform and replace netloc in a url.

    transform_netloc is a function taking the netloc and returning a
    tuple. The first element of this tuple is the new netloc. The
    entire tuple is returned.

    Returns a tuple containing the transformed url as item 0 and the
    original tuple returned by transform_netloc as item 1.
    """"""
    purl = urllib.parse.urlsplit(url)
    netloc_tuple = transform_netloc(purl.netloc)
    # stripped url
    url_pieces = (purl.scheme, netloc_tuple[0], purl.path, purl.query, purl.fragment)
    surl = urllib.parse.urlunsplit(url_pieces)
    return surl, cast(""NetlocTuple"", netloc_tuple)","def split_auth_netloc_from_url(url: str) -> Tuple[str, str, Tuple[str, str]]:
    """"""
    Parse a url into separate netloc, auth, and url with no auth.

    Returns: (url_without_auth, netloc, (username, password))
    """"""
    url_without_auth, (netloc, auth) = _transform_url(url, _get_netloc)
    return url_without_auth, netloc, auth",{}
309,309,"def remove_auth_from_url(url: str) -> str:
    """"""Return a copy of url with 'username:password@' removed.""""""
    # username/pass params are passed to subversion through flags
    # and are not recognized in the url.
    return _transform_url(url, _get_netloc)[0]","def _transform_url(
    url: str, transform_netloc: Callable[[str], Tuple[Any, ...]]
) -> Tuple[str, NetlocTuple]:
    """"""Transform and replace netloc in a url.

    transform_netloc is a function taking the netloc and returning a
    tuple. The first element of this tuple is the new netloc. The
    entire tuple is returned.

    Returns a tuple containing the transformed url as item 0 and the
    original tuple returned by transform_netloc as item 1.
    """"""
    purl = urllib.parse.urlsplit(url)
    netloc_tuple = transform_netloc(purl.netloc)
    # stripped url
    url_pieces = (purl.scheme, netloc_tuple[0], purl.path, purl.query, purl.fragment)
    surl = urllib.parse.urlunsplit(url_pieces)
    return surl, cast(""NetlocTuple"", netloc_tuple)","def _transform_url(
    url: str, transform_netloc: Callable[[str], Tuple[Any, ...]]
) -> Tuple[str, NetlocTuple]:
    """"""Transform and replace netloc in a url.

    transform_netloc is a function taking the netloc and returning a
    tuple. The first element of this tuple is the new netloc. The
    entire tuple is returned.

    Returns a tuple containing the transformed url as item 0 and the
    original tuple returned by transform_netloc as item 1.
    """"""
    purl = urllib.parse.urlsplit(url)
    netloc_tuple = transform_netloc(purl.netloc)
    # stripped url
    url_pieces = (purl.scheme, netloc_tuple[0], purl.path, purl.query, purl.fragment)
    surl = urllib.parse.urlunsplit(url_pieces)
    return surl, cast(""NetlocTuple"", netloc_tuple)","def remove_auth_from_url(url: str) -> str:
    """"""Return a copy of url with 'username:password@' removed.""""""
    # username/pass params are passed to subversion through flags
    # and are not recognized in the url.
    return _transform_url(url, _get_netloc)[0]",{},"def _transform_url(
    url: str, transform_netloc: Callable[[str], Tuple[Any, ...]]
) -> Tuple[str, NetlocTuple]:
    """"""Transform and replace netloc in a url.

    transform_netloc is a function taking the netloc and returning a
    tuple. The first element of this tuple is the new netloc. The
    entire tuple is returned.

    Returns a tuple containing the transformed url as item 0 and the
    original tuple returned by transform_netloc as item 1.
    """"""
    purl = urllib.parse.urlsplit(url)
    netloc_tuple = transform_netloc(purl.netloc)
    # stripped url
    url_pieces = (purl.scheme, netloc_tuple[0], purl.path, purl.query, purl.fragment)
    surl = urllib.parse.urlunsplit(url_pieces)
    return surl, cast(""NetlocTuple"", netloc_tuple)","def remove_auth_from_url(url: str) -> str:
    """"""Return a copy of url with 'username:password@' removed.""""""
    # username/pass params are passed to subversion through flags
    # and are not recognized in the url.
    return _transform_url(url, _get_netloc)[0]",{}
310,310,"def redact_auth_from_url(url: str) -> str:
    """"""Replace the password in a given url with ****.""""""
    return _transform_url(url, _redact_netloc)[0]","def _transform_url(
    url: str, transform_netloc: Callable[[str], Tuple[Any, ...]]
) -> Tuple[str, NetlocTuple]:
    """"""Transform and replace netloc in a url.

    transform_netloc is a function taking the netloc and returning a
    tuple. The first element of this tuple is the new netloc. The
    entire tuple is returned.

    Returns a tuple containing the transformed url as item 0 and the
    original tuple returned by transform_netloc as item 1.
    """"""
    purl = urllib.parse.urlsplit(url)
    netloc_tuple = transform_netloc(purl.netloc)
    # stripped url
    url_pieces = (purl.scheme, netloc_tuple[0], purl.path, purl.query, purl.fragment)
    surl = urllib.parse.urlunsplit(url_pieces)
    return surl, cast(""NetlocTuple"", netloc_tuple)","def _transform_url(
    url: str, transform_netloc: Callable[[str], Tuple[Any, ...]]
) -> Tuple[str, NetlocTuple]:
    """"""Transform and replace netloc in a url.

    transform_netloc is a function taking the netloc and returning a
    tuple. The first element of this tuple is the new netloc. The
    entire tuple is returned.

    Returns a tuple containing the transformed url as item 0 and the
    original tuple returned by transform_netloc as item 1.
    """"""
    purl = urllib.parse.urlsplit(url)
    netloc_tuple = transform_netloc(purl.netloc)
    # stripped url
    url_pieces = (purl.scheme, netloc_tuple[0], purl.path, purl.query, purl.fragment)
    surl = urllib.parse.urlunsplit(url_pieces)
    return surl, cast(""NetlocTuple"", netloc_tuple)","def redact_auth_from_url(url: str) -> str:
    """"""Replace the password in a given url with ****.""""""
    return _transform_url(url, _redact_netloc)[0]",{},"def _transform_url(
    url: str, transform_netloc: Callable[[str], Tuple[Any, ...]]
) -> Tuple[str, NetlocTuple]:
    """"""Transform and replace netloc in a url.

    transform_netloc is a function taking the netloc and returning a
    tuple. The first element of this tuple is the new netloc. The
    entire tuple is returned.

    Returns a tuple containing the transformed url as item 0 and the
    original tuple returned by transform_netloc as item 1.
    """"""
    purl = urllib.parse.urlsplit(url)
    netloc_tuple = transform_netloc(purl.netloc)
    # stripped url
    url_pieces = (purl.scheme, netloc_tuple[0], purl.path, purl.query, purl.fragment)
    surl = urllib.parse.urlunsplit(url_pieces)
    return surl, cast(""NetlocTuple"", netloc_tuple)","def redact_auth_from_url(url: str) -> str:
    """"""Replace the password in a given url with ****.""""""
    return _transform_url(url, _redact_netloc)[0]",{}
311,311,"def egg_link_path_from_sys_path(raw_name: str) -> Optional[str]:
    """"""
    Look for a .egg-link file for project name, by walking sys.path.
    """"""
    egg_link_name = _egg_link_name(raw_name)
    for path_item in sys.path:
        egg_link = os.path.join(path_item, egg_link_name)
        if os.path.isfile(egg_link):
            return egg_link
    return None","def _egg_link_name(raw_name: str) -> str:
    """"""
    Convert a Name metadata value to a .egg-link name, by applying
    the same substitution as pkg_resources's safe_name function.
    Note: we cannot use canonicalize_name because it has a different logic.
    """"""
    return re.sub(""[^A-Za-z0-9.]+"", ""-"", raw_name) + "".egg-link""","def _egg_link_name(raw_name: str) -> str:
    """"""
    Convert a Name metadata value to a .egg-link name, by applying
    the same substitution as pkg_resources's safe_name function.
    Note: we cannot use canonicalize_name because it has a different logic.
    """"""
    return re.sub(""[^A-Za-z0-9.]+"", ""-"", raw_name) + "".egg-link""","def look_for_egg_link_file_for_project_name_by_walking_sys_path(raw_name: str) -> Optional[str]:
    """"""
    Look for a .egg-link file for project name, by walking sys.path.
    """"""
    egg_link_name = _egg_link_name(raw_name)
    for path_item in sys.path:
        egg_link = os.path.join(path_item, egg_link_name)
        if os.path.isfile(egg_link):
            return egg_link
    return None",{},"def _egg_link_name(raw_name: str) -> str:
    """"""
    Convert a Name metadata value to a .egg-link name, by applying
    the same substitution as pkg_resources's safe_name function.
    Note: we cannot use canonicalize_name because it has a different logic.
    """"""
    return re.sub(""[^A-Za-z0-9.]+"", ""-"", raw_name) + "".egg-link""","def 1(raw_name: str) -> Optional[str]:
    """"""
    Look for a .egg-link file for project name, by walking sys.path.
    """"""
    egg_link_name = _egg_link_name(raw_name)
    for path_item in sys.path:
        egg_link = os.path.join(path_item, egg_link_name)
        if os.path.isfile(egg_link):
            return egg_link
    return None",{}
312,312,"def format_command_args(args: Union[List[str], CommandArgs]) -> str:
    """"""
    Format command arguments for display.
    """"""
    # For HiddenText arguments, display the redacted form by calling str().
    # Also, we don't apply str() to arguments that aren't HiddenText since
    # this can trigger a UnicodeDecodeError in Python 2 if the argument
    # has type unicode and includes a non-ascii character.  (The type
    # checker doesn't ensure the annotations are correct in all cases.)
    return "" "".join(
        shlex.quote(str(arg)) if isinstance(arg, HiddenText) else shlex.quote(arg)
        for arg in args
    )","def str(val):
    """"""Convert float to string, taking the locale into account.""""""
    return _format(""%.12g"", val)","def locale_aware_float_to_string(val):
    """"""Convert float to locale_aware_float_to_stringing, taking the locale into account.""""""
    return _format(""%.12g"", val)","def format_command_args(args: Union[List[locale_aware_float_to_string], CommandArgs]) -> locale_aware_float_to_string:
    """"""
    Format command arguments for display.
    """"""
    # For HiddenText arguments, display the redacted form by calling locale_aware_float_to_string().
    # Also, we don't apply locale_aware_float_to_string() to arguments that aren't HiddenText since
    # this can trigger a UnicodeDecodeError in Python 2 if the argument
    # has type unicode and includes a non-ascii character.  (The type
    # checker doesn't ensure the annotations are correct in all cases.)
    return "" "".join(
        shlex.quote(locale_aware_float_to_string(arg)) if isinstance(arg, HiddenText) else shlex.quote(arg)
        for arg in args
    )","{""str"": ""locale_aware_float_to_string""}","def 0(val):
    """"""Convert float to string, taking the locale into account.""""""
    return _format(""%.12g"", val)","def format_command_args(args: Union[List[str], CommandArgs]) -> str:
    """"""
    Format command arguments for display.
    """"""
    # For HiddenText arguments, display the redacted form by calling 0().
    # Also, we don't apply 0() to arguments that aren't HiddenText since
    # this can trigger a UnicodeDecodeError in Python 2 if the argument
    # has type unicode and includes a non-ascii character.  (The type
    # checker doesn't ensure the annotations are correct in all cases.)
    return "" "".join(
        shlex.quote(0(arg)) if isinstance(arg, HiddenText) else shlex.quote(arg)
        for arg in args
    )","{""str"": ""0""}"
313,313,"def runner_with_spinner_message(message: str) -> Callable[..., None]:
    """"""Provide a subprocess_runner that shows a spinner message.

    Intended for use with for pep517's Pep517HookCaller. Thus, the runner has
    an API that matches what's expected by Pep517HookCaller.subprocess_runner.
    """"""

    def runner(
        cmd: List[str],
        cwd: Optional[str] = None,
        extra_environ: Optional[Mapping[str, Any]] = None,
    ) -> None:
        with open_spinner(message) as spinner:
            call_subprocess(
                cmd,
                command_desc=message,
                cwd=cwd,
                extra_environ=extra_environ,
                spinner=spinner,
            )

    return runner","def call_subprocess(
    cmd: Union[List[str], CommandArgs],
    show_stdout: bool = False,
    cwd: Optional[str] = None,
    on_returncode: 'Literal[""raise"", ""warn"", ""ignore""]' = ""raise"",
    extra_ok_returncodes: Optional[Iterable[int]] = None,
    extra_environ: Optional[Mapping[str, Any]] = None,
    unset_environ: Optional[Iterable[str]] = None,
    spinner: Optional[SpinnerInterface] = None,
    log_failed_cmd: Optional[bool] = True,
    stdout_only: Optional[bool] = False,
    *,
    command_desc: str,
) -> str:
    """"""
    Args:
      show_stdout: if true, use INFO to log the subprocess's stderr and
        stdout streams.  Otherwise, use DEBUG.  Defaults to False.
      extra_ok_returncodes: an iterable of integer return codes that are
        acceptable, in addition to 0. Defaults to None, which means [].
      unset_environ: an iterable of environment variable names to unset
        prior to calling subprocess.Popen().
      log_failed_cmd: if false, failed commands are not logged, only raised.
      stdout_only: if true, return only stdout, else return both. When true,
        logging of both stdout and stderr occurs when the subprocess has
        terminated, else logging occurs as subprocess output is produced.
    """"""
    if extra_ok_returncodes is None:
        extra_ok_returncodes = []
    if unset_environ is None:
        unset_environ = []
    # Most places in pip use show_stdout=False. What this means is--
    #
    # - We connect the child's output (combined stderr and stdout) to a
    #   single pipe, which we read.
    # - We log this output to stderr at DEBUG level as it is received.
    # - If DEBUG logging isn't enabled (e.g. if --verbose logging wasn't
    #   requested), then we show a spinner so the user can still see the
    #   subprocess is in progress.
    # - If the subprocess exits with an error, we log the output to stderr
    #   at ERROR level if it hasn't already been displayed to the console
    #   (e.g. if --verbose logging wasn't enabled).  This way we don't log
    #   the output to the console twice.
    #
    # If show_stdout=True, then the above is still done, but with DEBUG
    # replaced by INFO.
    if show_stdout:
        # Then log the subprocess output at INFO level.
        log_subprocess: Callable[..., None] = subprocess_logger.info
        used_level = logging.INFO
    else:
        # Then log the subprocess output using VERBOSE.  This also ensures
        # it will be logged to the log file (aka user_log), if enabled.
        log_subprocess = subprocess_logger.verbose
        used_level = VERBOSE

    # Whether the subprocess will be visible in the console.
    showing_subprocess = subprocess_logger.getEffectiveLevel() <= used_level

    # Only use the spinner if we're not showing the subprocess output
    # and we have a spinner.
    use_spinner = not showing_subprocess and spinner is not None

    log_subprocess(""Running command %s"", command_desc)
    env = os.environ.copy()
    if extra_environ:
        env.update(extra_environ)
    for name in unset_environ:
        env.pop(name, None)
    try:
        proc = subprocess.Popen(
            # Convert HiddenText objects to the underlying str.
            reveal_command_args(cmd),
            stdin=subprocess.PIPE,
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT if not stdout_only else subprocess.PIPE,
            cwd=cwd,
            env=env,
            errors=""backslashreplace"",
        )
    except Exception as exc:
        if log_failed_cmd:
            subprocess_logger.critical(
                ""Error %s while executing command %s"",
                exc,
                command_desc,
            )
        raise
    all_output = []
    if not stdout_only:
        assert proc.stdout
        assert proc.stdin
        proc.stdin.close()
        # In this mode, stdout and stderr are in the same pipe.
        while True:
            line: str = proc.stdout.readline()
            if not line:
                break
            line = line.rstrip()
            all_output.append(line + ""\n"")

            # Show the line immediately.
            log_subprocess(line)
            # Update the spinner.
            if use_spinner:
                assert spinner
                spinner.spin()
        try:
            proc.wait()
        finally:
            if proc.stdout:
                proc.stdout.close()
        output = """".join(all_output)
    else:
        # In this mode, stdout and stderr are in different pipes.
        # We must use communicate() which is the only safe way to read both.
        out, err = proc.communicate()
        # log line by line to preserve pip log indenting
        for out_line in out.splitlines():
            log_subprocess(out_line)
        all_output.append(out)
        for err_line in err.splitlines():
            log_subprocess(err_line)
        all_output.append(err)
        output = out

    proc_had_error = proc.returncode and proc.returncode not in extra_ok_returncodes
    if use_spinner:
        assert spinner
        if proc_had_error:
            spinner.finish(""error"")
        else:
            spinner.finish(""done"")
    if proc_had_error:
        if on_returncode == ""raise"":
            error = InstallationSubprocessError(
                command_description=command_desc,
                exit_code=proc.returncode,
                output_lines=all_output if not showing_subprocess else None,
            )
            if log_failed_cmd:
                subprocess_logger.error(""[present-rich] %s"", error)
                subprocess_logger.verbose(
                    ""[bold magenta]full command[/]: [blue]%s[/]"",
                    escape(format_command_args(cmd)),
                    extra={""markup"": True},
                )
                subprocess_logger.verbose(
                    ""[bold magenta]cwd[/]: %s"",
                    escape(cwd or ""[inherit]""),
                    extra={""markup"": True},
                )

            raise error
        elif on_returncode == ""warn"":
            subprocess_logger.warning(
                'Command ""%s"" had error code %s in %s',
                command_desc,
                proc.returncode,
                cwd,
            )
        elif on_returncode == ""ignore"":
            pass
        else:
            raise ValueError(f""Invalid value: on_returncode={on_returncode!r}"")
    return output","def execute_subprocess_and_handle_return_code(
    cmd: Union[List[str], CommandArgs],
    show_stdout: bool = False,
    cwd: Optional[str] = None,
    on_returncode: 'Literal[""raise"", ""warn"", ""ignore""]' = ""raise"",
    extra_ok_returncodes: Optional[Iterable[int]] = None,
    extra_environ: Optional[Mapping[str, Any]] = None,
    unset_environ: Optional[Iterable[str]] = None,
    spinner: Optional[SpinnerInterface] = None,
    log_failed_cmd: Optional[bool] = True,
    stdout_only: Optional[bool] = False,
    *,
    command_desc: str,
) -> str:
    """"""
    Args:
      show_stdout: if true, use INFO to log the subprocess's stderr and
        stdout streams.  Otherwise, use DEBUG.  Defaults to False.
      extra_ok_returncodes: an iterable of integer return codes that are
        acceptable, in addition to 0. Defaults to None, which means [].
      unset_environ: an iterable of environment variable names to unset
        prior to calling subprocess.Popen().
      log_failed_cmd: if false, failed commands are not logged, only raised.
      stdout_only: if true, return only stdout, else return both. When true,
        logging of both stdout and stderr occurs when the subprocess has
        terminated, else logging occurs as subprocess output is produced.
    """"""
    if extra_ok_returncodes is None:
        extra_ok_returncodes = []
    if unset_environ is None:
        unset_environ = []
    # Most places in pip use show_stdout=False. What this means is--
    #
    # - We connect the child's output (combined stderr and stdout) to a
    #   single pipe, which we read.
    # - We log this output to stderr at DEBUG level as it is received.
    # - If DEBUG logging isn't enabled (e.g. if --verbose logging wasn't
    #   requested), then we show a spinner so the user can still see the
    #   subprocess is in progress.
    # - If the subprocess exits with an error, we log the output to stderr
    #   at ERROR level if it hasn't already been displayed to the console
    #   (e.g. if --verbose logging wasn't enabled).  This way we don't log
    #   the output to the console twice.
    #
    # If show_stdout=True, then the above is still done, but with DEBUG
    # replaced by INFO.
    if show_stdout:
        # Then log the subprocess output at INFO level.
        log_subprocess: Callable[..., None] = subprocess_logger.info
        used_level = logging.INFO
    else:
        # Then log the subprocess output using VERBOSE.  This also ensures
        # it will be logged to the log file (aka user_log), if enabled.
        log_subprocess = subprocess_logger.verbose
        used_level = VERBOSE

    # Whether the subprocess will be visible in the console.
    showing_subprocess = subprocess_logger.getEffectiveLevel() <= used_level

    # Only use the spinner if we're not showing the subprocess output
    # and we have a spinner.
    use_spinner = not showing_subprocess and spinner is not None

    log_subprocess(""Running command %s"", command_desc)
    env = os.environ.copy()
    if extra_environ:
        env.update(extra_environ)
    for name in unset_environ:
        env.pop(name, None)
    try:
        proc = subprocess.Popen(
            # Convert HiddenText objects to the underlying str.
            reveal_command_args(cmd),
            stdin=subprocess.PIPE,
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT if not stdout_only else subprocess.PIPE,
            cwd=cwd,
            env=env,
            errors=""backslashreplace"",
        )
    except Exception as exc:
        if log_failed_cmd:
            subprocess_logger.critical(
                ""Error %s while executing command %s"",
                exc,
                command_desc,
            )
        raise
    all_output = []
    if not stdout_only:
        assert proc.stdout
        assert proc.stdin
        proc.stdin.close()
        # In this mode, stdout and stderr are in the same pipe.
        while True:
            line: str = proc.stdout.readline()
            if not line:
                break
            line = line.rstrip()
            all_output.append(line + ""\n"")

            # Show the line immediately.
            log_subprocess(line)
            # Update the spinner.
            if use_spinner:
                assert spinner
                spinner.spin()
        try:
            proc.wait()
        finally:
            if proc.stdout:
                proc.stdout.close()
        output = """".join(all_output)
    else:
        # In this mode, stdout and stderr are in different pipes.
        # We must use communicate() which is the only safe way to read both.
        out, err = proc.communicate()
        # log line by line to preserve pip log indenting
        for out_line in out.splitlines():
            log_subprocess(out_line)
        all_output.append(out)
        for err_line in err.splitlines():
            log_subprocess(err_line)
        all_output.append(err)
        output = out

    proc_had_error = proc.returncode and proc.returncode not in extra_ok_returncodes
    if use_spinner:
        assert spinner
        if proc_had_error:
            spinner.finish(""error"")
        else:
            spinner.finish(""done"")
    if proc_had_error:
        if on_returncode == ""raise"":
            error = InstallationSubprocessError(
                command_description=command_desc,
                exit_code=proc.returncode,
                output_lines=all_output if not showing_subprocess else None,
            )
            if log_failed_cmd:
                subprocess_logger.error(""[present-rich] %s"", error)
                subprocess_logger.verbose(
                    ""[bold magenta]full command[/]: [blue]%s[/]"",
                    escape(format_command_args(cmd)),
                    extra={""markup"": True},
                )
                subprocess_logger.verbose(
                    ""[bold magenta]cwd[/]: %s"",
                    escape(cwd or ""[inherit]""),
                    extra={""markup"": True},
                )

            raise error
        elif on_returncode == ""warn"":
            subprocess_logger.warning(
                'Command ""%s"" had error code %s in %s',
                command_desc,
                proc.returncode,
                cwd,
            )
        elif on_returncode == ""ignore"":
            pass
        else:
            raise ValueError(f""Invalid value: on_returncode={on_returncode!r}"")
    return output","def runner_with_spinner_message(message: str) -> Callable[..., None]:
    """"""Provide a subprocess_runner that shows a spinner message.

    Intended for use with for pep517's Pep517HookCaller. Thus, the runner has
    an API that matches what's expected by Pep517HookCaller.subprocess_runner.
    """"""

    def runner(
        cmd: List[str],
        cwd: Optional[str] = None,
        extra_environ: Optional[Mapping[str, Any]] = None,
    ) -> None:
        with open_spinner(message) as spinner:
            execute_subprocess_and_handle_return_code(
                cmd,
                command_desc=message,
                cwd=cwd,
                extra_environ=extra_environ,
                spinner=spinner,
            )

    return runner","{""call_subprocess"": ""execute_subprocess_and_handle_return_code""}","def 0(
    cmd: Union[List[str], CommandArgs],
    show_stdout: bool = False,
    cwd: Optional[str] = None,
    on_returncode: 'Literal[""raise"", ""warn"", ""ignore""]' = ""raise"",
    extra_ok_returncodes: Optional[Iterable[int]] = None,
    extra_environ: Optional[Mapping[str, Any]] = None,
    unset_environ: Optional[Iterable[str]] = None,
    spinner: Optional[SpinnerInterface] = None,
    log_failed_cmd: Optional[bool] = True,
    stdout_only: Optional[bool] = False,
    *,
    command_desc: str,
) -> str:
    """"""
    Args:
      show_stdout: if true, use INFO to log the subprocess's stderr and
        stdout streams.  Otherwise, use DEBUG.  Defaults to False.
      extra_ok_returncodes: an iterable of integer return codes that are
        acceptable, in addition to 0. Defaults to None, which means [].
      unset_environ: an iterable of environment variable names to unset
        prior to calling subprocess.Popen().
      log_failed_cmd: if false, failed commands are not logged, only raised.
      stdout_only: if true, return only stdout, else return both. When true,
        logging of both stdout and stderr occurs when the subprocess has
        terminated, else logging occurs as subprocess output is produced.
    """"""
    if extra_ok_returncodes is None:
        extra_ok_returncodes = []
    if unset_environ is None:
        unset_environ = []
    # Most places in pip use show_stdout=False. What this means is--
    #
    # - We connect the child's output (combined stderr and stdout) to a
    #   single pipe, which we read.
    # - We log this output to stderr at DEBUG level as it is received.
    # - If DEBUG logging isn't enabled (e.g. if --verbose logging wasn't
    #   requested), then we show a spinner so the user can still see the
    #   subprocess is in progress.
    # - If the subprocess exits with an error, we log the output to stderr
    #   at ERROR level if it hasn't already been displayed to the console
    #   (e.g. if --verbose logging wasn't enabled).  This way we don't log
    #   the output to the console twice.
    #
    # If show_stdout=True, then the above is still done, but with DEBUG
    # replaced by INFO.
    if show_stdout:
        # Then log the subprocess output at INFO level.
        log_subprocess: Callable[..., None] = subprocess_logger.info
        used_level = logging.INFO
    else:
        # Then log the subprocess output using VERBOSE.  This also ensures
        # it will be logged to the log file (aka user_log), if enabled.
        log_subprocess = subprocess_logger.verbose
        used_level = VERBOSE

    # Whether the subprocess will be visible in the console.
    showing_subprocess = subprocess_logger.getEffectiveLevel() <= used_level

    # Only use the spinner if we're not showing the subprocess output
    # and we have a spinner.
    use_spinner = not showing_subprocess and spinner is not None

    log_subprocess(""Running command %s"", command_desc)
    env = os.environ.copy()
    if extra_environ:
        env.update(extra_environ)
    for name in unset_environ:
        env.pop(name, None)
    try:
        proc = subprocess.Popen(
            # Convert HiddenText objects to the underlying str.
            reveal_command_args(cmd),
            stdin=subprocess.PIPE,
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT if not stdout_only else subprocess.PIPE,
            cwd=cwd,
            env=env,
            errors=""backslashreplace"",
        )
    except Exception as exc:
        if log_failed_cmd:
            subprocess_logger.critical(
                ""Error %s while executing command %s"",
                exc,
                command_desc,
            )
        raise
    all_output = []
    if not stdout_only:
        assert proc.stdout
        assert proc.stdin
        proc.stdin.close()
        # In this mode, stdout and stderr are in the same pipe.
        while True:
            line: str = proc.stdout.readline()
            if not line:
                break
            line = line.rstrip()
            all_output.append(line + ""\n"")

            # Show the line immediately.
            log_subprocess(line)
            # Update the spinner.
            if use_spinner:
                assert spinner
                spinner.spin()
        try:
            proc.wait()
        finally:
            if proc.stdout:
                proc.stdout.close()
        output = """".join(all_output)
    else:
        # In this mode, stdout and stderr are in different pipes.
        # We must use communicate() which is the only safe way to read both.
        out, err = proc.communicate()
        # log line by line to preserve pip log indenting
        for out_line in out.splitlines():
            log_subprocess(out_line)
        all_output.append(out)
        for err_line in err.splitlines():
            log_subprocess(err_line)
        all_output.append(err)
        output = out

    proc_had_error = proc.returncode and proc.returncode not in extra_ok_returncodes
    if use_spinner:
        assert spinner
        if proc_had_error:
            spinner.finish(""error"")
        else:
            spinner.finish(""done"")
    if proc_had_error:
        if on_returncode == ""raise"":
            error = InstallationSubprocessError(
                command_description=command_desc,
                exit_code=proc.returncode,
                output_lines=all_output if not showing_subprocess else None,
            )
            if log_failed_cmd:
                subprocess_logger.error(""[present-rich] %s"", error)
                subprocess_logger.verbose(
                    ""[bold magenta]full command[/]: [blue]%s[/]"",
                    escape(format_command_args(cmd)),
                    extra={""markup"": True},
                )
                subprocess_logger.verbose(
                    ""[bold magenta]cwd[/]: %s"",
                    escape(cwd or ""[inherit]""),
                    extra={""markup"": True},
                )

            raise error
        elif on_returncode == ""warn"":
            subprocess_logger.warning(
                'Command ""%s"" had error code %s in %s',
                command_desc,
                proc.returncode,
                cwd,
            )
        elif on_returncode == ""ignore"":
            pass
        else:
            raise ValueError(f""Invalid value: on_returncode={on_returncode!r}"")
    return output","def runner_with_spinner_message(message: str) -> Callable[..., None]:
    """"""Provide a subprocess_runner that shows a spinner message.

    Intended for use with for pep517's Pep517HookCaller. Thus, the runner has
    an API that matches what's expected by Pep517HookCaller.subprocess_runner.
    """"""

    def runner(
        cmd: List[str],
        cwd: Optional[str] = None,
        extra_environ: Optional[Mapping[str, Any]] = None,
    ) -> None:
        with open_spinner(message) as spinner:
            0(
                cmd,
                command_desc=message,
                cwd=cwd,
                extra_environ=extra_environ,
                spinner=spinner,
            )

    return runner","{""call_subprocess"": ""0""}"
314,314,"def test_writable_dir(path: str) -> bool:
    """"""Check if a directory is writable.

    Uses os.access() on POSIX, tries creating files on Windows.
    """"""
    # If the directory doesn't exist, find the closest parent that does.
    while not os.path.isdir(path):
        parent = os.path.dirname(path)
        if parent == path:
            break  # Should never get here, but infinite loops are bad
        path = parent

    if os.name == ""posix"":
        return os.access(path, os.W_OK)

    return _test_writable_dir_win(path)","def _test_writable_dir_win(path: str) -> bool:
    # os.access doesn't work on Windows: http://bugs.python.org/issue2528
    # and we can't use tempfile: http://bugs.python.org/issue22107
    basename = ""accesstest_deleteme_fishfingers_custard_""
    alphabet = ""abcdefghijklmnopqrstuvwxyz0123456789""
    for _ in range(10):
        name = basename + """".join(random.choice(alphabet) for _ in range(6))
        file = os.path.join(path, name)
        try:
            fd = os.open(file, os.O_RDWR | os.O_CREAT | os.O_EXCL)
        except FileExistsError:
            pass
        except PermissionError:
            # This could be because there's a directory with the same name.
            # But it's highly unlikely there's a directory called that,
            # so we'll assume it's because the parent dir is not writable.
            # This could as well be because the parent dir is not readable,
            # due to non-privileged user access.
            return False
        else:
            os.close(fd)
            os.unlink(file)
            return True

    # This should never be reached
    raise OSError(""Unexpected condition testing for writable directory"")","def is_writable_directory_on_windows(path: str) -> bool:
    # os.access doesn't work on Windows: http://bugs.python.org/issue2528
    # and we can't use tempfile: http://bugs.python.org/issue22107
    basename = ""accesstest_deleteme_fishfingers_custard_""
    alphabet = ""abcdefghijklmnopqrstuvwxyz0123456789""
    for _ in range(10):
        name = basename + """".join(random.choice(alphabet) for _ in range(6))
        file = os.path.join(path, name)
        try:
            fd = os.open(file, os.O_RDWR | os.O_CREAT | os.O_EXCL)
        except FileExistsError:
            pass
        except PermissionError:
            # This could be because there's a directory with the same name.
            # But it's highly unlikely there's a directory called that,
            # so we'll assume it's because the parent dir is not writable.
            # This could as well be because the parent dir is not readable,
            # due to non-privileged user access.
            return False
        else:
            os.close(fd)
            os.unlink(file)
            return True

    # This should never be reached
    raise OSError(""Unexpected condition testing for writable directory"")","def check_if_directory_is_writable(path: str) -> bool:
    """"""Check if a directory is writable.

    Uses os.access() on POSIX, tries creating files on Windows.
    """"""
    # If the directory doesn't exist, find the closest parent that does.
    while not os.path.isdir(path):
        parent = os.path.dirname(path)
        if parent == path:
            break  # Should never get here, but infinite loops are bad
        path = parent

    if os.name == ""posix"":
        return os.access(path, os.W_OK)

    return is_writable_directory_on_windows(path)","{""_test_writable_dir_win"": ""is_writable_directory_on_windows""}","def 0(path: str) -> bool:
    # os.access doesn't work on Windows: http://bugs.python.org/issue2528
    # and we can't use tempfile: http://bugs.python.org/issue22107
    basename = ""accesstest_deleteme_fishfingers_custard_""
    alphabet = ""abcdefghijklmnopqrstuvwxyz0123456789""
    for _ in range(10):
        name = basename + """".join(random.choice(alphabet) for _ in range(6))
        file = os.path.join(path, name)
        try:
            fd = os.open(file, os.O_RDWR | os.O_CREAT | os.O_EXCL)
        except FileExistsError:
            pass
        except PermissionError:
            # This could be because there's a directory with the same name.
            # But it's highly unlikely there's a directory called that,
            # so we'll assume it's because the parent dir is not writable.
            # This could as well be because the parent dir is not readable,
            # due to non-privileged user access.
            return False
        else:
            os.close(fd)
            os.unlink(file)
            return True

    # This should never be reached
    raise OSError(""Unexpected condition testing for writable directory"")","def 1(path: str) -> bool:
    """"""Check if a directory is writable.

    Uses os.access() on POSIX, tries creating files on Windows.
    """"""
    # If the directory doesn't exist, find the closest parent that does.
    while not os.path.isdir(path):
        parent = os.path.dirname(path)
        if parent == path:
            break  # Should never get here, but infinite loops are bad
        path = parent

    if os.name == ""posix"":
        return os.access(path, os.W_OK)

    return 0(path)","{""_test_writable_dir_win"": ""0""}"
315,315,"def get_best_invocation_for_this_pip() -> str:
    """"""Try to figure out the best way to invoke pip in the current environment.""""""
    binary_directory = ""Scripts"" if WINDOWS else ""bin""
    binary_prefix = os.path.join(sys.prefix, binary_directory)

    # Try to use pip[X[.Y]] names, if those executables for this environment are
    # the first on PATH with that name.
    path_parts = os.path.normcase(os.environ.get(""PATH"", """")).split(os.pathsep)
    exe_are_in_PATH = os.path.normcase(binary_prefix) in path_parts
    if exe_are_in_PATH:
        for exe_name in _EXECUTABLE_NAMES:
            found_executable = shutil.which(exe_name)
            if found_executable and os.path.samefile(
                found_executable,
                os.path.join(binary_prefix, exe_name),
            ):
                return exe_name

    # Use the `-m` invocation, if there's no ""nice"" invocation.
    return f""{get_best_invocation_for_this_python()} -m pip""","def get_best_invocation_for_this_python() -> str:
    """"""Try to figure out the best way to invoke the current Python.""""""
    exe = sys.executable
    exe_name = os.path.basename(exe)

    # Try to use the basename, if it's the first executable.
    found_executable = shutil.which(exe_name)
    if found_executable and os.path.samefile(found_executable, exe):
        return exe_name

    # Use the full executable name, because we couldn't find something simpler.
    return exe","def get_current_python_invocation() -> str:
    """"""Try to figure out the best way to invoke the current Python.""""""
    exe = sys.executable
    exe_name = os.path.basename(exe)

    # Try to use the basename, if it's the first executable.
    found_executable = shutil.which(exe_name)
    if found_executable and os.path.samefile(found_executable, exe):
        return exe_name

    # Use the full executable name, because we couldn't find something simpler.
    return exe","def try_to_figure_out_best_way_to_invoke_pip_in_current_environment() -> str:
    """"""Try to figure out the best way to invoke pip in the current environment.""""""
    binary_directory = ""Scripts"" if WINDOWS else ""bin""
    binary_prefix = os.path.join(sys.prefix, binary_directory)

    # Try to use pip[X[.Y]] names, if those executables for this environment are
    # the first on PATH with that name.
    path_parts = os.path.normcase(os.environ.get(""PATH"", """")).split(os.pathsep)
    exe_are_in_PATH = os.path.normcase(binary_prefix) in path_parts
    if exe_are_in_PATH:
        for exe_name in _EXECUTABLE_NAMES:
            found_executable = shutil.which(exe_name)
            if found_executable and os.path.samefile(
                found_executable,
                os.path.join(binary_prefix, exe_name),
            ):
                return exe_name

    # Use the `-m` invocation, if there's no ""nice"" invocation.
    return f""{get_current_python_invocation()} -m pip""","{""get_best_invocation_for_this_python"": ""get_current_python_invocation""}","def 0() -> str:
    """"""Try to figure out the best way to invoke the current Python.""""""
    exe = sys.executable
    exe_name = os.path.basename(exe)

    # Try to use the basename, if it's the first executable.
    found_executable = shutil.which(exe_name)
    if found_executable and os.path.samefile(found_executable, exe):
        return exe_name

    # Use the full executable name, because we couldn't find something simpler.
    return exe","def 1() -> str:
    """"""Try to figure out the best way to invoke pip in the current environment.""""""
    binary_directory = ""Scripts"" if WINDOWS else ""bin""
    binary_prefix = os.path.join(sys.prefix, binary_directory)

    # Try to use pip[X[.Y]] names, if those executables for this environment are
    # the first on PATH with that name.
    path_parts = os.path.normcase(os.environ.get(""PATH"", """")).split(os.pathsep)
    exe_are_in_PATH = os.path.normcase(binary_prefix) in path_parts
    if exe_are_in_PATH:
        for exe_name in _EXECUTABLE_NAMES:
            found_executable = shutil.which(exe_name)
            if found_executable and os.path.samefile(
                found_executable,
                os.path.join(binary_prefix, exe_name),
            ):
                return exe_name

    # Use the `-m` invocation, if there's no ""nice"" invocation.
    return f""{0()} -m pip""","{""get_best_invocation_for_this_python"": ""0""}"
316,316,"def running_under_virtualenv() -> bool:
    """"""Return True if we're running inside a virtualenv, False otherwise.""""""
    return _running_under_venv() or _running_under_regular_virtualenv()","def _running_under_regular_virtualenv() -> bool:
    """"""Checks if sys.real_prefix is set.

    This handles virtual environments created with pypa's virtualenv.
    """"""
    # pypa/virtualenv case
    return hasattr(sys, ""real_prefix"")

def _running_under_venv() -> bool:
    """"""Checks if sys.base_prefix and sys.prefix match.

    This handles PEP 405 compliant virtual environments.
    """"""
    return sys.prefix != getattr(sys, ""base_prefix"", sys.prefix)","def _running_under_regular_virtualenv() -> bool:
    """"""Checks if sys.real_prefix is set.

    This handles virtual environments created with pypa's virtualenv.
    """"""
    # pypa/virtualenv case
    return hasattr(sys, ""real_prefix"")

def _running_under_venv() -> bool:
    """"""Checks if sys.base_prefix and sys.prefix match.

    This handles PEP 405 compliant virtual environments.
    """"""
    return sys.prefix != getattr(sys, ""base_prefix"", sys.prefix)","def running_under_virtualenv() -> bool:
    """"""Return True if we're running inside a virtualenv, False otherwise.""""""
    return _running_under_venv() or _running_under_regular_virtualenv()",{},"def _running_under_regular_virtualenv() -> bool:
    """"""Checks if sys.real_prefix is set.

    This handles virtual environments created with pypa's virtualenv.
    """"""
    # pypa/virtualenv case
    return hasattr(sys, ""real_prefix"")

def _running_under_venv() -> bool:
    """"""Checks if sys.base_prefix and sys.prefix match.

    This handles PEP 405 compliant virtual environments.
    """"""
    return sys.prefix != getattr(sys, ""base_prefix"", sys.prefix)","def running_under_virtualenv() -> bool:
    """"""Return True if we're running inside a virtualenv, False otherwise.""""""
    return _running_under_venv() or _running_under_regular_virtualenv()",{}
317,317,"def _no_global_under_venv() -> bool:
    """"""Check `{sys.prefix}/pyvenv.cfg` for system site-packages inclusion

    PEP 405 specifies that when system site-packages are not supposed to be
    visible from a virtual environment, `pyvenv.cfg` must contain the following
    line:

        include-system-site-packages = false

    Additionally, log a warning if accessing the file fails.
    """"""
    cfg_lines = _get_pyvenv_cfg_lines()
    if cfg_lines is None:
        # We're not in a ""sane"" venv, so assume there is no system
        # site-packages access (since that's PEP 405's default state).
        logger.warning(
            ""Could not access 'pyvenv.cfg' despite a virtual environment ""
            ""being active. Assuming global site-packages is not accessible ""
            ""in this environment.""
        )
        return True

    for line in cfg_lines:
        match = _INCLUDE_SYSTEM_SITE_PACKAGES_REGEX.match(line)
        if match is not None and match.group(""value"") == ""false"":
            return True
    return False","def _get_pyvenv_cfg_lines() -> Optional[List[str]]:
    """"""Reads {sys.prefix}/pyvenv.cfg and returns its contents as list of lines

    Returns None, if it could not read/access the file.
    """"""
    pyvenv_cfg_file = os.path.join(sys.prefix, ""pyvenv.cfg"")
    try:
        # Although PEP 405 does not specify, the built-in venv module always
        # writes with UTF-8. (pypa/pip#8717)
        with open(pyvenv_cfg_file, encoding=""utf-8"") as f:
            return f.read().splitlines()  # avoids trailing newlines
    except OSError:
        return None","def _get_pyvenv_cfg_lines() -> Optional[List[str]]:
    """"""Reads {sys.prefix}/pyvenv.cfg and returns its contents as list of lines

    Returns None, if it could not read/access the file.
    """"""
    pyvenv_cfg_file = os.path.join(sys.prefix, ""pyvenv.cfg"")
    try:
        # Although PEP 405 does not specify, the built-in venv module always
        # writes with UTF-8. (pypa/pip#8717)
        with open(pyvenv_cfg_file, encoding=""utf-8"") as f:
            return f.read().splitlines()  # avoids trailing newlines
    except OSError:
        return None","def _no_global_under_venv() -> bool:
    """"""Check `{sys.prefix}/pyvenv.cfg` for system site-packages inclusion

    PEP 405 specifies that when system site-packages are not supposed to be
    visible from a virtual environment, `pyvenv.cfg` must contain the following
    line:

        include-system-site-packages = false

    Additionally, log a warning if accessing the file fails.
    """"""
    cfg_lines = _get_pyvenv_cfg_lines()
    if cfg_lines is None:
        # We're not in a ""sane"" venv, so assume there is no system
        # site-packages access (since that's PEP 405's default state).
        logger.warning(
            ""Could not access 'pyvenv.cfg' despite a virtual environment ""
            ""being active. Assuming global site-packages is not accessible ""
            ""in this environment.""
        )
        return True

    for line in cfg_lines:
        match = _INCLUDE_SYSTEM_SITE_PACKAGES_REGEX.match(line)
        if match is not None and match.group(""value"") == ""false"":
            return True
    return False",{},"def _get_pyvenv_cfg_lines() -> Optional[List[str]]:
    """"""Reads {sys.prefix}/pyvenv.cfg and returns its contents as list of lines

    Returns None, if it could not read/access the file.
    """"""
    pyvenv_cfg_file = os.path.join(sys.prefix, ""pyvenv.cfg"")
    try:
        # Although PEP 405 does not specify, the built-in venv module always
        # writes with UTF-8. (pypa/pip#8717)
        with open(pyvenv_cfg_file, encoding=""utf-8"") as f:
            return f.read().splitlines()  # avoids trailing newlines
    except OSError:
        return None","def _no_global_under_venv() -> bool:
    """"""Check `{sys.prefix}/pyvenv.cfg` for system site-packages inclusion

    PEP 405 specifies that when system site-packages are not supposed to be
    visible from a virtual environment, `pyvenv.cfg` must contain the following
    line:

        include-system-site-packages = false

    Additionally, log a warning if accessing the file fails.
    """"""
    cfg_lines = _get_pyvenv_cfg_lines()
    if cfg_lines is None:
        # We're not in a ""sane"" venv, so assume there is no system
        # site-packages access (since that's PEP 405's default state).
        logger.warning(
            ""Could not access 'pyvenv.cfg' despite a virtual environment ""
            ""being active. Assuming global site-packages is not accessible ""
            ""in this environment.""
        )
        return True

    for line in cfg_lines:
        match = _INCLUDE_SYSTEM_SITE_PACKAGES_REGEX.match(line)
        if match is not None and match.group(""value"") == ""false"":
            return True
    return False",{}
318,318,"def virtualenv_no_global() -> bool:
    """"""Returns a boolean, whether running in venv with no system site-packages.""""""
    # PEP 405 compliance needs to be checked first since virtualenv >=20 would
    # return True for both checks, but is only able to use the PEP 405 config.
    if _running_under_venv():
        return _no_global_under_venv()

    if _running_under_regular_virtualenv():
        return _no_global_under_regular_virtualenv()

    return False","def _running_under_regular_virtualenv() -> bool:
    """"""Checks if sys.real_prefix is set.

    This handles virtual environments created with pypa's virtualenv.
    """"""
    # pypa/virtualenv case
    return hasattr(sys, ""real_prefix"")

def _no_global_under_regular_virtualenv() -> bool:
    """"""Check if ""no-global-site-packages.txt"" exists beside site.py

    This mirrors logic in pypa/virtualenv for determining whether system
    site-packages are visible in the virtual environment.
    """"""
    site_mod_dir = os.path.dirname(os.path.abspath(site.__file__))
    no_global_site_packages_file = os.path.join(
        site_mod_dir,
        ""no-global-site-packages.txt"",
    )
    return os.path.exists(no_global_site_packages_file)

def _running_under_venv() -> bool:
    """"""Checks if sys.base_prefix and sys.prefix match.

    This handles PEP 405 compliant virtual environments.
    """"""
    return sys.prefix != getattr(sys, ""base_prefix"", sys.prefix)

def _no_global_under_venv() -> bool:
    """"""Check `{sys.prefix}/pyvenv.cfg` for system site-packages inclusion

    PEP 405 specifies that when system site-packages are not supposed to be
    visible from a virtual environment, `pyvenv.cfg` must contain the following
    line:

        include-system-site-packages = false

    Additionally, log a warning if accessing the file fails.
    """"""
    cfg_lines = _get_pyvenv_cfg_lines()
    if cfg_lines is None:
        # We're not in a ""sane"" venv, so assume there is no system
        # site-packages access (since that's PEP 405's default state).
        logger.warning(
            ""Could not access 'pyvenv.cfg' despite a virtual environment ""
            ""being active. Assuming global site-packages is not accessible ""
            ""in this environment.""
        )
        return True

    for line in cfg_lines:
        match = _INCLUDE_SYSTEM_SITE_PACKAGES_REGEX.match(line)
        if match is not None and match.group(""value"") == ""false"":
            return True
    return False","def _running_under_regular_virtualenv() -> bool:
    """"""Checks if sys.real_prefix is set.

    This handles virtual environments created with pypa's virtualenv.
    """"""
    # pypa/virtualenv case
    return hasattr(sys, ""real_prefix"")

def _no_global_under_regular_virtualenv() -> bool:
    """"""Check if ""no-global-site-packages.txt"" exists beside site.py

    This mirrors logic in pypa/virtualenv for determining whether system
    site-packages are visible in the virtual environment.
    """"""
    site_mod_dir = os.path.dirname(os.path.abspath(site.__file__))
    no_global_site_packages_file = os.path.join(
        site_mod_dir,
        ""no-global-site-packages.txt"",
    )
    return os.path.exists(no_global_site_packages_file)

def _running_under_venv() -> bool:
    """"""Checks if sys.base_prefix and sys.prefix match.

    This handles PEP 405 compliant virtual environments.
    """"""
    return sys.prefix != getattr(sys, ""base_prefix"", sys.prefix)

def _no_global_under_venv() -> bool:
    """"""Check `{sys.prefix}/pyvenv.cfg` for system site-packages inclusion

    PEP 405 specifies that when system site-packages are not supposed to be
    visible from a virtual environment, `pyvenv.cfg` must contain the following
    line:

        include-system-site-packages = false

    Additionally, log a warning if accessing the file fails.
    """"""
    cfg_lines = _get_pyvenv_cfg_lines()
    if cfg_lines is None:
        # We're not in a ""sane"" venv, so assume there is no system
        # site-packages access (since that's PEP 405's default state).
        logger.warning(
            ""Could not access 'pyvenv.cfg' despite a virtual environment ""
            ""being active. Assuming global site-packages is not accessible ""
            ""in this environment.""
        )
        return True

    for line in cfg_lines:
        match = _INCLUDE_SYSTEM_SITE_PACKAGES_REGEX.match(line)
        if match is not None and match.group(""value"") == ""false"":
            return True
    return False","def check_if_running_under_venv_and_return_no_global_flag() -> bool:
    """"""Returns a boolean, whether running in venv with no system site-packages.""""""
    # PEP 405 compliance needs to be checked first since virtualenv >=20 would
    # return True for both checks, but is only able to use the PEP 405 config.
    if _running_under_venv():
        return _no_global_under_venv()

    if _running_under_regular_virtualenv():
        return _no_global_under_regular_virtualenv()

    return False",{},"def _running_under_regular_virtualenv() -> bool:
    """"""Checks if sys.real_prefix is set.

    This handles virtual environments created with pypa's virtualenv.
    """"""
    # pypa/virtualenv case
    return hasattr(sys, ""real_prefix"")

def _no_global_under_regular_virtualenv() -> bool:
    """"""Check if ""no-global-site-packages.txt"" exists beside site.py

    This mirrors logic in pypa/virtualenv for determining whether system
    site-packages are visible in the virtual environment.
    """"""
    site_mod_dir = os.path.dirname(os.path.abspath(site.__file__))
    no_global_site_packages_file = os.path.join(
        site_mod_dir,
        ""no-global-site-packages.txt"",
    )
    return os.path.exists(no_global_site_packages_file)

def _running_under_venv() -> bool:
    """"""Checks if sys.base_prefix and sys.prefix match.

    This handles PEP 405 compliant virtual environments.
    """"""
    return sys.prefix != getattr(sys, ""base_prefix"", sys.prefix)

def _no_global_under_venv() -> bool:
    """"""Check `{sys.prefix}/pyvenv.cfg` for system site-packages inclusion

    PEP 405 specifies that when system site-packages are not supposed to be
    visible from a virtual environment, `pyvenv.cfg` must contain the following
    line:

        include-system-site-packages = false

    Additionally, log a warning if accessing the file fails.
    """"""
    cfg_lines = _get_pyvenv_cfg_lines()
    if cfg_lines is None:
        # We're not in a ""sane"" venv, so assume there is no system
        # site-packages access (since that's PEP 405's default state).
        logger.warning(
            ""Could not access 'pyvenv.cfg' despite a virtual environment ""
            ""being active. Assuming global site-packages is not accessible ""
            ""in this environment.""
        )
        return True

    for line in cfg_lines:
        match = _INCLUDE_SYSTEM_SITE_PACKAGES_REGEX.match(line)
        if match is not None and match.group(""value"") == ""false"":
            return True
    return False","def 1() -> bool:
    """"""Returns a boolean, whether running in venv with no system site-packages.""""""
    # PEP 405 compliance needs to be checked first since virtualenv >=20 would
    # return True for both checks, but is only able to use the PEP 405 config.
    if _running_under_venv():
        return _no_global_under_venv()

    if _running_under_regular_virtualenv():
        return _no_global_under_regular_virtualenv()

    return False",{}
319,319,"def glibc_version_string() -> Optional[str]:
    ""Returns glibc version string, or None if not using glibc.""
    return glibc_version_string_confstr() or glibc_version_string_ctypes()","def glibc_version_string_confstr() -> Optional[str]:
    ""Primary implementation of glibc_version_string using os.confstr.""
    # os.confstr is quite a bit faster than ctypes.DLL. It's also less likely
    # to be broken or missing. This strategy is used in the standard library
    # platform module:
    # https://github.com/python/cpython/blob/fcf1d003bf4f0100c9d0921ff3d70e1127ca1b71/Lib/platform.py#L175-L183
    if sys.platform == ""win32"":
        return None
    try:
        # os.confstr(""CS_GNU_LIBC_VERSION"") returns a string like ""glibc 2.17"":
        _, version = os.confstr(""CS_GNU_LIBC_VERSION"").split()
    except (AttributeError, OSError, ValueError):
        # os.confstr() or CS_GNU_LIBC_VERSION not available (or a bad value)...
        return None
    return version

def glibc_version_string_ctypes() -> Optional[str]:
    ""Fallback implementation of glibc_version_string using ctypes.""

    try:
        import ctypes
    except ImportError:
        return None

    # ctypes.CDLL(None) internally calls dlopen(NULL), and as the dlopen
    # manpage says, ""If filename is NULL, then the returned handle is for the
    # main program"". This way we can let the linker do the work to figure out
    # which libc our process is actually using.
    process_namespace = ctypes.CDLL(None)
    try:
        gnu_get_libc_version = process_namespace.gnu_get_libc_version
    except AttributeError:
        # Symbol doesn't exist -> therefore, we are not linked to
        # glibc.
        return None

    # Call gnu_get_libc_version, which returns a string like ""2.5""
    gnu_get_libc_version.restype = ctypes.c_char_p
    version_str = gnu_get_libc_version()
    # py2 / py3 compatibility:
    if not isinstance(version_str, str):
        version_str = version_str.decode(""ascii"")

    return version_str","def get_glibc_version_string_using_confstr() -> Optional[str]:
    ""Primary implementation of glibc_version_string using os.confstr.""
    # os.confstr is quite a bit faster than ctypes.DLL. It's also less likely
    # to be broken or missing. This strategy is used in the standard library
    # platform module:
    # https://github.com/python/cpython/blob/fcf1d003bf4f0100c9d0921ff3d70e1127ca1b71/Lib/platform.py#L175-L183
    if sys.platform == ""win32"":
        return None
    try:
        # os.confstr(""CS_GNU_LIBC_VERSION"") returns a string like ""glibc 2.17"":
        _, version = os.confstr(""CS_GNU_LIBC_VERSION"").split()
    except (AttributeError, OSError, ValueError):
        # os.confstr() or CS_GNU_LIBC_VERSION not available (or a bad value)...
        return None
    return version

def get_glibc_version_string_using_ctypes() -> Optional[str]:
    ""Fallback implementation of glibc_version_string using ctypes.""

    try:
        import ctypes
    except ImportError:
        return None

    # ctypes.CDLL(None) internally calls dlopen(NULL), and as the dlopen
    # manpage says, ""If filename is NULL, then the returned handle is for the
    # main program"". This way we can let the linker do the work to figure out
    # which libc our process is actually using.
    process_namespace = ctypes.CDLL(None)
    try:
        gnu_get_libc_version = process_namespace.gnu_get_libc_version
    except AttributeError:
        # Symbol doesn't exist -> therefore, we are not linked to
        # glibc.
        return None

    # Call gnu_get_libc_version, which returns a string like ""2.5""
    gnu_get_libc_version.restype = ctypes.c_char_p
    version_str = gnu_get_libc_version()
    # py2 / py3 compatibility:
    if not isinstance(version_str, str):
        version_str = version_str.decode(""ascii"")

    return version_str","def glibc_version_string() -> Optional[str]:
    ""Returns glibc version string, or None if not using glibc.""
    return get_glibc_version_string_using_confstr() or get_glibc_version_string_using_ctypes()","{""glibc_version_string_confstr"": ""get_glibc_version_string_using_confstr"", ""glibc_version_string_ctypes"": ""get_glibc_version_string_using_ctypes""}","def 0() -> Optional[str]:
    ""Primary implementation of glibc_version_string using os.confstr.""
    # os.confstr is quite a bit faster than ctypes.DLL. It's also less likely
    # to be broken or missing. This strategy is used in the standard library
    # platform module:
    # https://github.com/python/cpython/blob/fcf1d003bf4f0100c9d0921ff3d70e1127ca1b71/Lib/platform.py#L175-L183
    if sys.platform == ""win32"":
        return None
    try:
        # os.confstr(""CS_GNU_LIBC_VERSION"") returns a string like ""glibc 2.17"":
        _, version = os.confstr(""CS_GNU_LIBC_VERSION"").split()
    except (AttributeError, OSError, ValueError):
        # os.confstr() or CS_GNU_LIBC_VERSION not available (or a bad value)...
        return None
    return version

def 1() -> Optional[str]:
    ""Fallback implementation of glibc_version_string using ctypes.""

    try:
        import ctypes
    except ImportError:
        return None

    # ctypes.CDLL(None) internally calls dlopen(NULL), and as the dlopen
    # manpage says, ""If filename is NULL, then the returned handle is for the
    # main program"". This way we can let the linker do the work to figure out
    # which libc our process is actually using.
    process_namespace = ctypes.CDLL(None)
    try:
        gnu_get_libc_version = process_namespace.gnu_get_libc_version
    except AttributeError:
        # Symbol doesn't exist -> therefore, we are not linked to
        # glibc.
        return None

    # Call gnu_get_libc_version, which returns a string like ""2.5""
    gnu_get_libc_version.restype = ctypes.c_char_p
    version_str = gnu_get_libc_version()
    # py2 / py3 compatibility:
    if not isinstance(version_str, str):
        version_str = version_str.decode(""ascii"")

    return version_str","def glibc_version_string() -> Optional[str]:
    ""Returns glibc version string, or None if not using glibc.""
    return 0() or 1()","{""glibc_version_string_confstr"": ""0"", ""glibc_version_string_ctypes"": ""1""}"
320,320,"def libc_ver() -> Tuple[str, str]:
    """"""Try to determine the glibc version

    Returns a tuple of strings (lib, version) which default to empty strings
    in case the lookup fails.
    """"""
    glibc_version = glibc_version_string()
    if glibc_version is None:
        return ("""", """")
    else:
        return (""glibc"", glibc_version)","def glibc_version_string() -> Optional[str]:
    ""Returns glibc version string, or None if not using glibc.""
    return glibc_version_string_confstr() or glibc_version_string_ctypes()","def glibc_version_string() -> Optional[str]:
    ""Returns glibc version string, or None if not using glibc.""
    return glibc_version_string_confstr() or glibc_version_string_ctypes()","def libc_ver() -> Tuple[str, str]:
    """"""Try to determine the glibc version

    Returns a tuple of strings (lib, version) which default to empty strings
    in case the lookup fails.
    """"""
    glibc_version = glibc_version_string()
    if glibc_version is None:
        return ("""", """")
    else:
        return (""glibc"", glibc_version)",{},"def glibc_version_string() -> Optional[str]:
    ""Returns glibc version string, or None if not using glibc.""
    return glibc_version_string_confstr() or glibc_version_string_ctypes()","def libc_ver() -> Tuple[str, str]:
    """"""Try to determine the glibc version

    Returns a tuple of strings (lib, version) which default to empty strings
    in case the lookup fails.
    """"""
    glibc_version = glibc_version_string()
    if glibc_version is None:
        return ("""", """")
    else:
        return (""glibc"", glibc_version)",{}
321,321,"def wheel_metadata(source: ZipFile, dist_info_dir: str) -> Message:
    """"""Return the WHEEL metadata of an extracted wheel, if possible.
    Otherwise, raise UnsupportedWheel.
    """"""
    path = f""{dist_info_dir}/WHEEL""
    # Zip file path separators must be /
    wheel_contents = read_wheel_metadata_file(source, path)

    try:
        wheel_text = wheel_contents.decode()
    except UnicodeDecodeError as e:
        raise UnsupportedWheel(f""error decoding {path!r}: {e!r}"")

    # FeedParser (used by Parser) does not raise any exceptions. The returned
    # message may have .defects populated, but for backwards-compatibility we
    # currently ignore them.
    return Parser().parsestr(wheel_text)","def read_wheel_metadata_file(source: ZipFile, path: str) -> bytes:
    try:
        return source.read(path)
        # BadZipFile for general corruption, KeyError for missing entry,
        # and RuntimeError for password-protected files
    except (BadZipFile, KeyError, RuntimeError) as e:
        raise UnsupportedWheel(f""could not read {path!r} file: {e!r}"")","def read_wheel_metadata_file(source: ZipFile, path: str) -> bytes:
    try:
        return source.read(path)
        # BadZipFile for general corruption, KeyError for missing entry,
        # and RuntimeError for password-protected files
    except (BadZipFile, KeyError, RuntimeError) as e:
        raise UnsupportedWheel(f""could not read {path!r} file: {e!r}"")","def wheel_metadata(source: ZipFile, dist_info_dir: str) -> Message:
    """"""Return the WHEEL metadata of an extracted wheel, if possible.
    Otherwise, raise UnsupportedWheel.
    """"""
    path = f""{dist_info_dir}/WHEEL""
    # Zip file path separators must be /
    wheel_contents = read_wheel_metadata_file(source, path)

    try:
        wheel_text = wheel_contents.decode()
    except UnicodeDecodeError as e:
        raise UnsupportedWheel(f""error decoding {path!r}: {e!r}"")

    # FeedParser (used by Parser) does not raise any exceptions. The returned
    # message may have .defects populated, but for backwards-compatibility we
    # currently ignore them.
    return Parser().parsestr(wheel_text)",{},"def read_wheel_metadata_file(source: ZipFile, path: str) -> bytes:
    try:
        return source.read(path)
        # BadZipFile for general corruption, KeyError for missing entry,
        # and RuntimeError for password-protected files
    except (BadZipFile, KeyError, RuntimeError) as e:
        raise UnsupportedWheel(f""could not read {path!r} file: {e!r}"")","def wheel_metadata(source: ZipFile, dist_info_dir: str) -> Message:
    """"""Return the WHEEL metadata of an extracted wheel, if possible.
    Otherwise, raise UnsupportedWheel.
    """"""
    path = f""{dist_info_dir}/WHEEL""
    # Zip file path separators must be /
    wheel_contents = read_wheel_metadata_file(source, path)

    try:
        wheel_text = wheel_contents.decode()
    except UnicodeDecodeError as e:
        raise UnsupportedWheel(f""error decoding {path!r}: {e!r}"")

    # FeedParser (used by Parser) does not raise any exceptions. The returned
    # message may have .defects populated, but for backwards-compatibility we
    # currently ignore them.
    return Parser().parsestr(wheel_text)",{}
322,322,"def has_leading_dir(paths: Iterable[str]) -> bool:
    """"""Returns true if all the paths have the same leading path name
    (i.e., everything is in one subdirectory in an archive)""""""
    common_prefix = None
    for path in paths:
        prefix, rest = split_leading_dir(path)
        if not prefix:
            return False
        elif common_prefix is None:
            common_prefix = prefix
        elif prefix != common_prefix:
            return False
    return True","def split_leading_dir(path: str) -> List[str]:
    path = path.lstrip(""/"").lstrip(""\\"")
    if ""/"" in path and (
        (""\\"" in path and path.find(""/"") < path.find(""\\"")) or ""\\"" not in path
    ):
        return path.split(""/"", 1)
    elif ""\\"" in path:
        return path.split(""\\"", 1)
    else:
        return [path, """"]","def split_first_directory_from_path(path: str) -> List[str]:
    path = path.lstrip(""/"").lstrip(""\\"")
    if ""/"" in path and (
        (""\\"" in path and path.find(""/"") < path.find(""\\"")) or ""\\"" not in path
    ):
        return path.split(""/"", 1)
    elif ""\\"" in path:
        return path.split(""\\"", 1)
    else:
        return [path, """"]","def has_leading_dir(paths: Iterable[str]) -> bool:
    """"""Returns true if all the paths have the same leading path name
    (i.e., everything is in one subdirectory in an archive)""""""
    common_prefix = None
    for path in paths:
        prefix, rest = split_first_directory_from_path(path)
        if not prefix:
            return False
        elif common_prefix is None:
            common_prefix = prefix
        elif prefix != common_prefix:
            return False
    return True","{""split_leading_dir"": ""split_first_directory_from_path""}","def 0(path: str) -> List[str]:
    path = path.lstrip(""/"").lstrip(""\\"")
    if ""/"" in path and (
        (""\\"" in path and path.find(""/"") < path.find(""\\"")) or ""\\"" not in path
    ):
        return path.split(""/"", 1)
    elif ""\\"" in path:
        return path.split(""\\"", 1)
    else:
        return [path, """"]","def has_leading_dir(paths: Iterable[str]) -> bool:
    """"""Returns true if all the paths have the same leading path name
    (i.e., everything is in one subdirectory in an archive)""""""
    common_prefix = None
    for path in paths:
        prefix, rest = 0(path)
        if not prefix:
            return False
        elif common_prefix is None:
            common_prefix = prefix
        elif prefix != common_prefix:
            return False
    return True","{""split_leading_dir"": ""0""}"
323,323,"def _handle_python_version(
    option: Option, opt_str: str, value: str, parser: OptionParser
) -> None:
    """"""
    Handle a provided --python-version value.
    """"""
    version_info, error_msg = _convert_python_version(value)
    if error_msg is not None:
        msg = ""invalid --python-version value: {!r}: {}"".format(
            value,
            error_msg,
        )
        raise_option_error(parser, option=option, msg=msg)

    parser.values.python_version = version_info","def _convert_python_version(value: str) -> Tuple[Tuple[int, ...], Optional[str]]:
    """"""
    Convert a version string like ""3"", ""37"", or ""3.7.3"" into a tuple of ints.

    :return: A 2-tuple (version_info, error_msg), where `error_msg` is
        non-None if and only if there was a parsing error.
    """"""
    if not value:
        # The empty string is the same as not providing a value.
        return (None, None)

    parts = value.split(""."")
    if len(parts) > 3:
        return ((), ""at most three version parts are allowed"")

    if len(parts) == 1:
        # Then we are in the case of ""3"" or ""37"".
        value = parts[0]
        if len(value) > 1:
            parts = [value[0], value[1:]]

    try:
        version_info = tuple(int(part) for part in parts)
    except ValueError:
        return ((), ""each version part must be an integer"")

    return (version_info, None)

def raise_option_error(parser: OptionParser, option: Option, msg: str) -> None:
    """"""
    Raise an option parsing error using parser.error().

    Args:
      parser: an OptionParser instance.
      option: an Option instance.
      msg: the error text.
    """"""
    msg = f""{option} error: {msg}""
    msg = textwrap.fill("" "".join(msg.split()))
    parser.error(msg)","def _convert_python_version(value: str) -> Tuple[Tuple[int, ...], Optional[str]]:
    """"""
    Convert a version string like ""3"", ""37"", or ""3.7.3"" into a tuple of ints.

    :return: A 2-tuple (version_info, error_msg), where `error_msg` is
        non-None if and only if there was a parsing error.
    """"""
    if not value:
        # The empty string is the same as not providing a value.
        return (None, None)

    parts = value.split(""."")
    if len(parts) > 3:
        return ((), ""at most three version parts are allowed"")

    if len(parts) == 1:
        # Then we are in the case of ""3"" or ""37"".
        value = parts[0]
        if len(value) > 1:
            parts = [value[0], value[1:]]

    try:
        version_info = tuple(int(part) for part in parts)
    except ValueError:
        return ((), ""each version part must be an integer"")

    return (version_info, None)

def raise_option_error(parser: OptionParser, option: Option, msg: str) -> None:
    """"""
    Raise an option parsing error using parser.error().

    Args:
      parser: an OptionParser instance.
      option: an Option instance.
      msg: the error text.
    """"""
    msg = f""{option} error: {msg}""
    msg = textwrap.fill("" "".join(msg.split()))
    parser.error(msg)","def _handle_python_version(
    option: Option, opt_str: str, value: str, parser: OptionParser
) -> None:
    """"""
    Handle a provided --python-version value.
    """"""
    version_info, error_msg = _convert_python_version(value)
    if error_msg is not None:
        msg = ""invalid --python-version value: {!r}: {}"".format(
            value,
            error_msg,
        )
        raise_option_error(parser, option=option, msg=msg)

    parser.values.python_version = version_info",{},"def _convert_python_version(value: str) -> Tuple[Tuple[int, ...], Optional[str]]:
    """"""
    Convert a version string like ""3"", ""37"", or ""3.7.3"" into a tuple of ints.

    :return: A 2-tuple (version_info, error_msg), where `error_msg` is
        non-None if and only if there was a parsing error.
    """"""
    if not value:
        # The empty string is the same as not providing a value.
        return (None, None)

    parts = value.split(""."")
    if len(parts) > 3:
        return ((), ""at most three version parts are allowed"")

    if len(parts) == 1:
        # Then we are in the case of ""3"" or ""37"".
        value = parts[0]
        if len(value) > 1:
            parts = [value[0], value[1:]]

    try:
        version_info = tuple(int(part) for part in parts)
    except ValueError:
        return ((), ""each version part must be an integer"")

    return (version_info, None)

def raise_option_error(parser: OptionParser, option: Option, msg: str) -> None:
    """"""
    Raise an option parsing error using parser.error().

    Args:
      parser: an OptionParser instance.
      option: an Option instance.
      msg: the error text.
    """"""
    msg = f""{option} error: {msg}""
    msg = textwrap.fill("" "".join(msg.split()))
    parser.error(msg)","def _handle_python_version(
    option: Option, opt_str: str, value: str, parser: OptionParser
) -> None:
    """"""
    Handle a provided --python-version value.
    """"""
    version_info, error_msg = _convert_python_version(value)
    if error_msg is not None:
        msg = ""invalid --python-version value: {!r}: {}"".format(
            value,
            error_msg,
        )
        raise_option_error(parser, option=option, msg=msg)

    parser.values.python_version = version_info",{}
324,324,"def create_main_parser() -> ConfigOptionParser:
    """"""Creates and returns the main parser for pip's CLI""""""

    parser = ConfigOptionParser(
        usage=""\n%prog <command> [options]"",
        add_help_option=False,
        formatter=UpdatingDefaultsHelpFormatter(),
        name=""global"",
        prog=get_prog(),
    )
    parser.disable_interspersed_args()

    parser.version = get_pip_version()

    # add the general options
    gen_opts = cmdoptions.make_option_group(cmdoptions.general_group, parser)
    parser.add_option_group(gen_opts)

    # so the help formatter knows
    parser.main = True  # type: ignore

    # create command listing for description
    description = [""""] + [
        f""{name:27} {command_info.summary}""
        for name, command_info in commands_dict.items()
    ]
    parser.description = ""\n"".join(description)

    return parser","def get_prog() -> str:
    try:
        prog = os.path.basename(sys.argv[0])
        if prog in (""__main__.py"", ""-c""):
            return f""{sys.executable} -m pip""
        else:
            return prog
    except (AttributeError, TypeError, IndexError):
        pass
    return ""pip""

def get_pip_version() -> str:
    pip_pkg_dir = os.path.join(os.path.dirname(__file__), "".."", "".."")
    pip_pkg_dir = os.path.abspath(pip_pkg_dir)

    return ""pip {} from {} (python {})"".format(
        __version__,
        pip_pkg_dir,
        get_major_minor_version(),
    )","def get_program_name() -> str:
    try:
        prog = os.path.basename(sys.argv[0])
        if prog in (""__main__.py"", ""-c""):
            return f""{sys.executable} -m pip""
        else:
            return prog
    except (AttributeError, TypeError, IndexError):
        pass
    return ""pip""

def get_pip_version() -> str:
    pip_pkg_dir = os.path.join(os.path.dirname(__file__), "".."", "".."")
    pip_pkg_dir = os.path.abspath(pip_pkg_dir)

    return ""pip {} from {} (python {})"".format(
        __version__,
        pip_pkg_dir,
        get_major_minor_version(),
    )","def create_main_parser() -> ConfigOptionParser:
    """"""Creates and returns the main parser for pip's CLI""""""

    parser = ConfigOptionParser(
        usage=""\n%prog <command> [options]"",
        add_help_option=False,
        formatter=UpdatingDefaultsHelpFormatter(),
        name=""global"",
        prog=get_program_name(),
    )
    parser.disable_interspersed_args()

    parser.version = get_pip_version()

    # add the general options
    gen_opts = cmdoptions.make_option_group(cmdoptions.general_group, parser)
    parser.add_option_group(gen_opts)

    # so the help formatter knows
    parser.main = True  # type: ignore

    # create command listing for description
    description = [""""] + [
        f""{name:27} {command_info.summary}""
        for name, command_info in commands_dict.items()
    ]
    parser.description = ""\n"".join(description)

    return parser","{""get_prog"": ""get_program_name""}","def 0() -> str:
    try:
        prog = os.path.basename(sys.argv[0])
        if prog in (""__main__.py"", ""-c""):
            return f""{sys.executable} -m pip""
        else:
            return prog
    except (AttributeError, TypeError, IndexError):
        pass
    return ""pip""

def get_pip_version() -> str:
    pip_pkg_dir = os.path.join(os.path.dirname(__file__), "".."", "".."")
    pip_pkg_dir = os.path.abspath(pip_pkg_dir)

    return ""pip {} from {} (python {})"".format(
        __version__,
        pip_pkg_dir,
        get_major_minor_version(),
    )","def create_main_parser() -> ConfigOptionParser:
    """"""Creates and returns the main parser for pip's CLI""""""

    parser = ConfigOptionParser(
        usage=""\n%prog <command> [options]"",
        add_help_option=False,
        formatter=UpdatingDefaultsHelpFormatter(),
        name=""global"",
        prog=0(),
    )
    parser.disable_interspersed_args()

    parser.version = get_pip_version()

    # add the general options
    gen_opts = cmdoptions.make_option_group(cmdoptions.general_group, parser)
    parser.add_option_group(gen_opts)

    # so the help formatter knows
    parser.main = True  # type: ignore

    # create command listing for description
    description = [""""] + [
        f""{name:27} {command_info.summary}""
        for name, command_info in commands_dict.items()
    ]
    parser.description = ""\n"".join(description)

    return parser","{""get_prog"": ""0""}"
325,325,"def warn_if_run_as_root() -> None:
    """"""Output a warning for sudo users on Unix.

    In a virtual environment, sudo pip still writes to virtualenv.
    On Windows, users may run pip as Administrator without issues.
    This warning only applies to Unix root users outside of virtualenv.
    """"""
    if running_under_virtualenv():
        return
    if not hasattr(os, ""getuid""):
        return
    # On Windows, there are no ""system managed"" Python packages. Installing as
    # Administrator via pip is the correct way of updating system environments.
    #
    # We choose sys.platform over utils.compat.WINDOWS here to enable Mypy platform
    # checks: https://mypy.readthedocs.io/en/stable/common_issues.html
    if sys.platform == ""win32"" or sys.platform == ""cygwin"":
        return

    if os.getuid() != 0:
        return

    logger.warning(
        ""Running pip as the 'root' user can result in broken permissions and ""
        ""conflicting behaviour with the system package manager. ""
        ""It is recommended to use a virtual environment instead: ""
        ""https://pip.pypa.io/warnings/venv""
    )","def running_under_virtualenv() -> bool:
    """"""Return True if we're running inside a virtualenv, False otherwise.""""""
    return _running_under_venv() or _running_under_regular_virtualenv()","def running_under_virtualenv() -> bool:
    """"""Return True if we're running inside a virtualenv, False otherwise.""""""
    return _running_under_venv() or _running_under_regular_virtualenv()","def issue_warning_for_sudo_users_on_unix() -> None:
    """"""Output a warning for sudo users on Unix.

    In a virtual environment, sudo pip still writes to virtualenv.
    On Windows, users may run pip as Administrator without issues.
    This warning only applies to Unix root users outside of virtualenv.
    """"""
    if running_under_virtualenv():
        return
    if not hasattr(os, ""getuid""):
        return
    # On Windows, there are no ""system managed"" Python packages. Installing as
    # Administrator via pip is the correct way of updating system environments.
    #
    # We choose sys.platform over utils.compat.WINDOWS here to enable Mypy platform
    # checks: https://mypy.readthedocs.io/en/stable/common_issues.html
    if sys.platform == ""win32"" or sys.platform == ""cygwin"":
        return

    if os.getuid() != 0:
        return

    logger.warning(
        ""Running pip as the 'root' user can result in broken permissions and ""
        ""conflicting behaviour with the system package manager. ""
        ""It is recommended to use a virtual environment instead: ""
        ""https://pip.pypa.io/warnings/venv""
    )",{},"def running_under_virtualenv() -> bool:
    """"""Return True if we're running inside a virtualenv, False otherwise.""""""
    return _running_under_venv() or _running_under_regular_virtualenv()","def 1() -> None:
    """"""Output a warning for sudo users on Unix.

    In a virtual environment, sudo pip still writes to virtualenv.
    On Windows, users may run pip as Administrator without issues.
    This warning only applies to Unix root users outside of virtualenv.
    """"""
    if running_under_virtualenv():
        return
    if not hasattr(os, ""getuid""):
        return
    # On Windows, there are no ""system managed"" Python packages. Installing as
    # Administrator via pip is the correct way of updating system environments.
    #
    # We choose sys.platform over utils.compat.WINDOWS here to enable Mypy platform
    # checks: https://mypy.readthedocs.io/en/stable/common_issues.html
    if sys.platform == ""win32"" or sys.platform == ""cygwin"":
        return

    if os.getuid() != 0:
        return

    logger.warning(
        ""Running pip as the 'root' user can result in broken permissions and ""
        ""conflicting behaviour with the system package manager. ""
        ""It is recommended to use a virtual environment instead: ""
        ""https://pip.pypa.io/warnings/venv""
    )",{}
326,326,"def create_package_set_from_installed() -> Tuple[PackageSet, bool]:
    """"""Converts a list of distributions into a PackageSet.""""""
    package_set = {}
    problems = False
    env = get_default_environment()
    for dist in env.iter_installed_distributions(local_only=False, skip=()):
        name = dist.canonical_name
        try:
            dependencies = list(dist.iter_dependencies())
            package_set[name] = PackageDetails(dist.version, dependencies)
        except (OSError, ValueError) as e:
            # Don't crash on unreadable or broken metadata.
            logger.warning(""Error parsing requirements for %s: %s"", name, e)
            problems = True
    return package_set, problems","def get_default_environment() -> BaseEnvironment:
    """"""Get the default representation for the current environment.

    This returns an Environment instance from the chosen backend. The default
    Environment instance should be built from ``sys.path`` and may use caching
    to share instance state accorss calls.
    """"""
    return select_backend().Environment.default()","def get_default_environment() -> BaseEnvironment:
    """"""Get the default representation for the current environment.

    This returns an Environment instance from the chosen backend. The default
    Environment instance should be built from ``sys.path`` and may use caching
    to share instance state accorss calls.
    """"""
    return select_backend().Environment.default()","def generate_set_of_installed_packages_with_dependency_details() -> Tuple[PackageSet, bool]:
    """"""Converts a list of distributions into a PackageSet.""""""
    package_set = {}
    problems = False
    env = get_default_environment()
    for dist in env.iter_installed_distributions(local_only=False, skip=()):
        name = dist.canonical_name
        try:
            dependencies = list(dist.iter_dependencies())
            package_set[name] = PackageDetails(dist.version, dependencies)
        except (OSError, ValueError) as e:
            # Don't crash on unreadable or broken metadata.
            logger.warning(""Error parsing requirements for %s: %s"", name, e)
            problems = True
    return package_set, problems",{},"def get_default_environment() -> BaseEnvironment:
    """"""Get the default representation for the current environment.

    This returns an Environment instance from the chosen backend. The default
    Environment instance should be built from ``sys.path`` and may use caching
    to share instance state accorss calls.
    """"""
    return select_backend().Environment.default()","def 1() -> Tuple[PackageSet, bool]:
    """"""Converts a list of distributions into a PackageSet.""""""
    package_set = {}
    problems = False
    env = get_default_environment()
    for dist in env.iter_installed_distributions(local_only=False, skip=()):
        name = dist.canonical_name
        try:
            dependencies = list(dist.iter_dependencies())
            package_set[name] = PackageDetails(dist.version, dependencies)
        except (OSError, ValueError) as e:
            # Don't crash on unreadable or broken metadata.
            logger.warning(""Error parsing requirements for %s: %s"", name, e)
            problems = True
    return package_set, problems",{}
327,327,"def check_install_conflicts(to_install: List[InstallRequirement]) -> ConflictDetails:
    """"""For checking if the dependency graph would be consistent after \
    installing given requirements
    """"""
    # Start from the current state
    package_set, _ = create_package_set_from_installed()
    # Install packages
    would_be_installed = _simulate_installation_of(to_install, package_set)

    # Only warn about directly-dependent packages; create a whitelist of them
    whitelist = _create_whitelist(would_be_installed, package_set)

    return (
        package_set,
        check_package_set(
            package_set, should_ignore=lambda name: name not in whitelist
        ),
    )","def create_package_set_from_installed() -> Tuple[PackageSet, bool]:
    """"""Converts a list of distributions into a PackageSet.""""""
    package_set = {}
    problems = False
    env = get_default_environment()
    for dist in env.iter_installed_distributions(local_only=False, skip=()):
        name = dist.canonical_name
        try:
            dependencies = list(dist.iter_dependencies())
            package_set[name] = PackageDetails(dist.version, dependencies)
        except (OSError, ValueError) as e:
            # Don't crash on unreadable or broken metadata.
            logger.warning(""Error parsing requirements for %s: %s"", name, e)
            problems = True
    return package_set, problems

def _create_whitelist(
    would_be_installed: Set[NormalizedName], package_set: PackageSet
) -> Set[NormalizedName]:
    packages_affected = set(would_be_installed)

    for package_name in package_set:
        if package_name in packages_affected:
            continue

        for req in package_set[package_name].dependencies:
            if canonicalize_name(req.name) in packages_affected:
                packages_affected.add(package_name)
                break

    return packages_affected

def _simulate_installation_of(
    to_install: List[InstallRequirement], package_set: PackageSet
) -> Set[NormalizedName]:
    """"""Computes the version of packages after installing to_install.""""""
    # Keep track of packages that were installed
    installed = set()

    # Modify it as installing requirement_set would (assuming no errors)
    for inst_req in to_install:
        abstract_dist = make_distribution_for_install_requirement(inst_req)
        dist = abstract_dist.get_metadata_distribution()
        name = dist.canonical_name
        package_set[name] = PackageDetails(dist.version, list(dist.iter_dependencies()))

        installed.add(name)

    return installed

def check_package_set(
    package_set: PackageSet, should_ignore: Optional[Callable[[str], bool]] = None
) -> CheckResult:
    """"""Check if a package set is consistent

    If should_ignore is passed, it should be a callable that takes a
    package name and returns a boolean.
    """"""

    missing = {}
    conflicting = {}

    for package_name, package_detail in package_set.items():
        # Info about dependencies of package_name
        missing_deps: Set[Missing] = set()
        conflicting_deps: Set[Conflicting] = set()

        if should_ignore and should_ignore(package_name):
            continue

        for req in package_detail.dependencies:
            name = canonicalize_name(req.name)

            # Check if it's missing
            if name not in package_set:
                missed = True
                if req.marker is not None:
                    missed = req.marker.evaluate()
                if missed:
                    missing_deps.add((name, req))
                continue

            # Check if there's a conflict
            version = package_set[name].version
            if not req.specifier.contains(version, prereleases=True):
                conflicting_deps.add((name, version, req))

        if missing_deps:
            missing[package_name] = sorted(missing_deps, key=str)
        if conflicting_deps:
            conflicting[package_name] = sorted(conflicting_deps, key=str)

    return missing, conflicting","def generate_set_of_installed_packages_with_dependency_details() -> Tuple[PackageSet, bool]:
    """"""Converts a list of distributions into a PackageSet.""""""
    package_set = {}
    problems = False
    env = get_default_environment()
    for dist in env.iter_installed_distributions(local_only=False, skip=()):
        name = dist.canonical_name
        try:
            dependencies = list(dist.iter_dependencies())
            package_set[name] = PackageDetails(dist.version, dependencies)
        except (OSError, ValueError) as e:
            # Don't crash on unreadable or broken metadata.
            logger.warning(""Error parsing requirements for %s: %s"", name, e)
            problems = True
    return package_set, problems

def derive_whitelist_based_on_installed_and_provided_packages(
    would_be_installed: Set[NormalizedName], package_set: PackageSet
) -> Set[NormalizedName]:
    packages_affected = set(would_be_installed)

    for package_name in package_set:
        if package_name in packages_affected:
            continue

        for req in package_set[package_name].dependencies:
            if canonicalize_name(req.name) in packages_affected:
                packages_affected.add(package_name)
                break

    return packages_affected

def _simulate_installation_of(
    to_install: List[InstallRequirement], package_set: PackageSet
) -> Set[NormalizedName]:
    """"""Computes the version of packages after installing to_install.""""""
    # Keep track of packages that were installed
    installed = set()

    # Modify it as installing requirement_set would (assuming no errors)
    for inst_req in to_install:
        abstract_dist = make_distribution_for_install_requirement(inst_req)
        dist = abstract_dist.get_metadata_distribution()
        name = dist.canonical_name
        package_set[name] = PackageDetails(dist.version, list(dist.iter_dependencies()))

        installed.add(name)

    return installed

def check_package_set(
    package_set: PackageSet, should_ignore: Optional[Callable[[str], bool]] = None
) -> CheckResult:
    """"""Check if a package set is consistent

    If should_ignore is passed, it should be a callable that takes a
    package name and returns a boolean.
    """"""

    missing = {}
    conflicting = {}

    for package_name, package_detail in package_set.items():
        # Info about dependencies of package_name
        missing_deps: Set[Missing] = set()
        conflicting_deps: Set[Conflicting] = set()

        if should_ignore and should_ignore(package_name):
            continue

        for req in package_detail.dependencies:
            name = canonicalize_name(req.name)

            # Check if it's missing
            if name not in package_set:
                missed = True
                if req.marker is not None:
                    missed = req.marker.evaluate()
                if missed:
                    missing_deps.add((name, req))
                continue

            # Check if there's a conflict
            version = package_set[name].version
            if not req.specifier.contains(version, prereleases=True):
                conflicting_deps.add((name, version, req))

        if missing_deps:
            missing[package_name] = sorted(missing_deps, key=str)
        if conflicting_deps:
            conflicting[package_name] = sorted(conflicting_deps, key=str)

    return missing, conflicting","def check_install_conflicts(to_install: List[InstallRequirement]) -> ConflictDetails:
    """"""For checking if the dependency graph would be consistent after \
    installing given requirements
    """"""
    # Start from the current state
    package_set, _ = generate_set_of_installed_packages_with_dependency_details()
    # Install packages
    would_be_installed = _simulate_installation_of(to_install, package_set)

    # Only warn about directly-dependent packages; create a whitelist of them
    whitelist = derive_whitelist_based_on_installed_and_provided_packages(would_be_installed, package_set)

    return (
        package_set,
        check_package_set(
            package_set, should_ignore=lambda name: name not in whitelist
        ),
    )","{""create_package_set_from_installed"": ""generate_set_of_installed_packages_with_dependency_details"", ""_create_whitelist"": ""derive_whitelist_based_on_installed_and_provided_packages""}","def 0() -> Tuple[PackageSet, bool]:
    """"""Converts a list of distributions into a PackageSet.""""""
    package_set = {}
    problems = False
    env = get_default_environment()
    for dist in env.iter_installed_distributions(local_only=False, skip=()):
        name = dist.canonical_name
        try:
            dependencies = list(dist.iter_dependencies())
            package_set[name] = PackageDetails(dist.version, dependencies)
        except (OSError, ValueError) as e:
            # Don't crash on unreadable or broken metadata.
            logger.warning(""Error parsing requirements for %s: %s"", name, e)
            problems = True
    return package_set, problems

def 1(
    would_be_installed: Set[NormalizedName], package_set: PackageSet
) -> Set[NormalizedName]:
    packages_affected = set(would_be_installed)

    for package_name in package_set:
        if package_name in packages_affected:
            continue

        for req in package_set[package_name].dependencies:
            if canonicalize_name(req.name) in packages_affected:
                packages_affected.add(package_name)
                break

    return packages_affected

def _simulate_installation_of(
    to_install: List[InstallRequirement], package_set: PackageSet
) -> Set[NormalizedName]:
    """"""Computes the version of packages after installing to_install.""""""
    # Keep track of packages that were installed
    installed = set()

    # Modify it as installing requirement_set would (assuming no errors)
    for inst_req in to_install:
        abstract_dist = make_distribution_for_install_requirement(inst_req)
        dist = abstract_dist.get_metadata_distribution()
        name = dist.canonical_name
        package_set[name] = PackageDetails(dist.version, list(dist.iter_dependencies()))

        installed.add(name)

    return installed

def check_package_set(
    package_set: PackageSet, should_ignore: Optional[Callable[[str], bool]] = None
) -> CheckResult:
    """"""Check if a package set is consistent

    If should_ignore is passed, it should be a callable that takes a
    package name and returns a boolean.
    """"""

    missing = {}
    conflicting = {}

    for package_name, package_detail in package_set.items():
        # Info about dependencies of package_name
        missing_deps: Set[Missing] = set()
        conflicting_deps: Set[Conflicting] = set()

        if should_ignore and should_ignore(package_name):
            continue

        for req in package_detail.dependencies:
            name = canonicalize_name(req.name)

            # Check if it's missing
            if name not in package_set:
                missed = True
                if req.marker is not None:
                    missed = req.marker.evaluate()
                if missed:
                    missing_deps.add((name, req))
                continue

            # Check if there's a conflict
            version = package_set[name].version
            if not req.specifier.contains(version, prereleases=True):
                conflicting_deps.add((name, version, req))

        if missing_deps:
            missing[package_name] = sorted(missing_deps, key=str)
        if conflicting_deps:
            conflicting[package_name] = sorted(conflicting_deps, key=str)

    return missing, conflicting","def check_install_conflicts(to_install: List[InstallRequirement]) -> ConflictDetails:
    """"""For checking if the dependency graph would be consistent after \
    installing given requirements
    """"""
    # Start from the current state
    package_set, _ = 0()
    # Install packages
    would_be_installed = _simulate_installation_of(to_install, package_set)

    # Only warn about directly-dependent packages; create a whitelist of them
    whitelist = 1(would_be_installed, package_set)

    return (
        package_set,
        check_package_set(
            package_set, should_ignore=lambda name: name not in whitelist
        ),
    )","{""create_package_set_from_installed"": ""0"", ""_create_whitelist"": ""1""}"
328,328,"def _simulate_installation_of(
    to_install: List[InstallRequirement], package_set: PackageSet
) -> Set[NormalizedName]:
    """"""Computes the version of packages after installing to_install.""""""
    # Keep track of packages that were installed
    installed = set()

    # Modify it as installing requirement_set would (assuming no errors)
    for inst_req in to_install:
        abstract_dist = make_distribution_for_install_requirement(inst_req)
        dist = abstract_dist.get_metadata_distribution()
        name = dist.canonical_name
        package_set[name] = PackageDetails(dist.version, list(dist.iter_dependencies()))

        installed.add(name)

    return installed","def make_distribution_for_install_requirement(
    install_req: InstallRequirement,
) -> AbstractDistribution:
    """"""Returns a Distribution for the given InstallRequirement""""""
    # Editable requirements will always be source distributions. They use the
    # legacy logic until we create a modern standard for them.
    if install_req.editable:
        return SourceDistribution(install_req)

    # If it's a wheel, it's a WheelDistribution
    if install_req.is_wheel:
        return WheelDistribution(install_req)

    # Otherwise, a SourceDistribution
    return SourceDistribution(install_req)","def make_distribution_for_install_requirement(
    install_req: InstallRequirement,
) -> AbstractDistribution:
    """"""Returns a Distribution for the given InstallRequirement""""""
    # Editable requirements will always be source distributions. They use the
    # legacy logic until we create a modern standard for them.
    if install_req.editable:
        return SourceDistribution(install_req)

    # If it's a wheel, it's a WheelDistribution
    if install_req.is_wheel:
        return WheelDistribution(install_req)

    # Otherwise, a SourceDistribution
    return SourceDistribution(install_req)","def _simulate_installation_of(
    to_install: List[InstallRequirement], package_set: PackageSet
) -> Set[NormalizedName]:
    """"""Computes the version of packages after installing to_install.""""""
    # Keep track of packages that were installed
    installed = set()

    # Modify it as installing requirement_set would (assuming no errors)
    for inst_req in to_install:
        abstract_dist = make_distribution_for_install_requirement(inst_req)
        dist = abstract_dist.get_metadata_distribution()
        name = dist.canonical_name
        package_set[name] = PackageDetails(dist.version, list(dist.iter_dependencies()))

        installed.add(name)

    return installed",{},"def make_distribution_for_install_requirement(
    install_req: InstallRequirement,
) -> AbstractDistribution:
    """"""Returns a Distribution for the given InstallRequirement""""""
    # Editable requirements will always be source distributions. They use the
    # legacy logic until we create a modern standard for them.
    if install_req.editable:
        return SourceDistribution(install_req)

    # If it's a wheel, it's a WheelDistribution
    if install_req.is_wheel:
        return WheelDistribution(install_req)

    # Otherwise, a SourceDistribution
    return SourceDistribution(install_req)","def _simulate_installation_of(
    to_install: List[InstallRequirement], package_set: PackageSet
) -> Set[NormalizedName]:
    """"""Computes the version of packages after installing to_install.""""""
    # Keep track of packages that were installed
    installed = set()

    # Modify it as installing requirement_set would (assuming no errors)
    for inst_req in to_install:
        abstract_dist = make_distribution_for_install_requirement(inst_req)
        dist = abstract_dist.get_metadata_distribution()
        name = dist.canonical_name
        package_set[name] = PackageDetails(dist.version, list(dist.iter_dependencies()))

        installed.add(name)

    return installed",{}
329,329,"def _get_prepared_distribution(
    req: InstallRequirement,
    build_tracker: BuildTracker,
    finder: PackageFinder,
    build_isolation: bool,
    check_build_deps: bool,
) -> BaseDistribution:
    """"""Prepare a distribution for installation.""""""
    abstract_dist = make_distribution_for_install_requirement(req)
    with build_tracker.track(req):
        abstract_dist.prepare_distribution_metadata(
            finder, build_isolation, check_build_deps
        )
    return abstract_dist.get_metadata_distribution()","def make_distribution_for_install_requirement(
    install_req: InstallRequirement,
) -> AbstractDistribution:
    """"""Returns a Distribution for the given InstallRequirement""""""
    # Editable requirements will always be source distributions. They use the
    # legacy logic until we create a modern standard for them.
    if install_req.editable:
        return SourceDistribution(install_req)

    # If it's a wheel, it's a WheelDistribution
    if install_req.is_wheel:
        return WheelDistribution(install_req)

    # Otherwise, a SourceDistribution
    return SourceDistribution(install_req)","def make_distribution_for_install_requirement(
    install_req: InstallRequirement,
) -> AbstractDistribution:
    """"""Returns a Distribution for the given InstallRequirement""""""
    # Editable requirements will always be source distributions. They use the
    # legacy logic until we create a modern standard for them.
    if install_req.editable:
        return SourceDistribution(install_req)

    # If it's a wheel, it's a WheelDistribution
    if install_req.is_wheel:
        return WheelDistribution(install_req)

    # Otherwise, a SourceDistribution
    return SourceDistribution(install_req)","def prepare_distribution_for_install_requirement_with_tracker(
    req: InstallRequirement,
    build_tracker: BuildTracker,
    finder: PackageFinder,
    build_isolation: bool,
    check_build_deps: bool,
) -> BaseDistribution:
    """"""Prepare a distribution for installation.""""""
    abstract_dist = make_distribution_for_install_requirement(req)
    with build_tracker.track(req):
        abstract_dist.prepare_distribution_metadata(
            finder, build_isolation, check_build_deps
        )
    return abstract_dist.get_metadata_distribution()",{},"def make_distribution_for_install_requirement(
    install_req: InstallRequirement,
) -> AbstractDistribution:
    """"""Returns a Distribution for the given InstallRequirement""""""
    # Editable requirements will always be source distributions. They use the
    # legacy logic until we create a modern standard for them.
    if install_req.editable:
        return SourceDistribution(install_req)

    # If it's a wheel, it's a WheelDistribution
    if install_req.is_wheel:
        return WheelDistribution(install_req)

    # Otherwise, a SourceDistribution
    return SourceDistribution(install_req)","def 2(
    req: InstallRequirement,
    build_tracker: BuildTracker,
    finder: PackageFinder,
    build_isolation: bool,
    check_build_deps: bool,
) -> BaseDistribution:
    """"""Prepare a distribution for installation.""""""
    abstract_dist = make_distribution_for_install_requirement(req)
    with build_tracker.track(req):
        abstract_dist.prepare_distribution_metadata(
            finder, build_isolation, check_build_deps
        )
    return abstract_dist.get_metadata_distribution()",{}
330,330,"def get_file_url(
    link: Link, download_dir: Optional[str] = None, hashes: Optional[Hashes] = None
) -> File:
    """"""Get file and optionally check its hash.""""""
    # If a download dir is specified, is the file already there and valid?
    already_downloaded_path = None
    if download_dir:
        already_downloaded_path = _check_download_dir(link, download_dir, hashes)

    if already_downloaded_path:
        from_path = already_downloaded_path
    else:
        from_path = link.file_path

    # If --require-hashes is off, `hashes` is either empty, the
    # link's embedded hash, or MissingHashes; it is required to
    # match. If --require-hashes is on, we are satisfied by any
    # hash in `hashes` matching: a URL-based or an option-based
    # one; no internet-sourced hash will be in `hashes`.
    if hashes:
        hashes.check_against_path(from_path)
    return File(from_path, None)","def _check_download_dir(
    link: Link, download_dir: str, hashes: Optional[Hashes]
) -> Optional[str]:
    """"""Check download_dir for previously downloaded file with correct hash
    If a correct file is found return its path else None
    """"""
    download_path = os.path.join(download_dir, link.filename)

    if not os.path.exists(download_path):
        return None

    # If already downloaded, does its hash match?
    logger.info(""File was already downloaded %s"", download_path)
    if hashes:
        try:
            hashes.check_against_path(download_path)
        except HashMismatch:
            logger.warning(
                ""Previously-downloaded file %s has bad hash. Re-downloading."",
                download_path,
            )
            os.unlink(download_path)
            return None
    return download_path","def verify_download_directory_for_existing_file_with_correct_hash(
    link: Link, download_dir: str, hashes: Optional[Hashes]
) -> Optional[str]:
    """"""Check download_dir for previously downloaded file with correct hash
    If a correct file is found return its path else None
    """"""
    download_path = os.path.join(download_dir, link.filename)

    if not os.path.exists(download_path):
        return None

    # If already downloaded, does its hash match?
    logger.info(""File was already downloaded %s"", download_path)
    if hashes:
        try:
            hashes.check_against_path(download_path)
        except HashMismatch:
            logger.warning(
                ""Previously-downloaded file %s has bad hash. Re-downloading."",
                download_path,
            )
            os.unlink(download_path)
            return None
    return download_path","def get_file_url(
    link: Link, download_dir: Optional[str] = None, hashes: Optional[Hashes] = None
) -> File:
    """"""Get file and optionally check its hash.""""""
    # If a download dir is specified, is the file already there and valid?
    already_downloaded_path = None
    if download_dir:
        already_downloaded_path = verify_download_directory_for_existing_file_with_correct_hash(link, download_dir, hashes)

    if already_downloaded_path:
        from_path = already_downloaded_path
    else:
        from_path = link.file_path

    # If --require-hashes is off, `hashes` is either empty, the
    # link's embedded hash, or MissingHashes; it is required to
    # match. If --require-hashes is on, we are satisfied by any
    # hash in `hashes` matching: a URL-based or an option-based
    # one; no internet-sourced hash will be in `hashes`.
    if hashes:
        hashes.check_against_path(from_path)
    return File(from_path, None)","{""_check_download_dir"": ""verify_download_directory_for_existing_file_with_correct_hash""}","def 0(
    link: Link, download_dir: str, hashes: Optional[Hashes]
) -> Optional[str]:
    """"""Check download_dir for previously downloaded file with correct hash
    If a correct file is found return its path else None
    """"""
    download_path = os.path.join(download_dir, link.filename)

    if not os.path.exists(download_path):
        return None

    # If already downloaded, does its hash match?
    logger.info(""File was already downloaded %s"", download_path)
    if hashes:
        try:
            hashes.check_against_path(download_path)
        except HashMismatch:
            logger.warning(
                ""Previously-downloaded file %s has bad hash. Re-downloading."",
                download_path,
            )
            os.unlink(download_path)
            return None
    return download_path","def get_file_url(
    link: Link, download_dir: Optional[str] = None, hashes: Optional[Hashes] = None
) -> File:
    """"""Get file and optionally check its hash.""""""
    # If a download dir is specified, is the file already there and valid?
    already_downloaded_path = None
    if download_dir:
        already_downloaded_path = 0(link, download_dir, hashes)

    if already_downloaded_path:
        from_path = already_downloaded_path
    else:
        from_path = link.file_path

    # If --require-hashes is off, `hashes` is either empty, the
    # link's embedded hash, or MissingHashes; it is required to
    # match. If --require-hashes is on, we are satisfied by any
    # hash in `hashes` matching: a URL-based or an option-based
    # one; no internet-sourced hash will be in `hashes`.
    if hashes:
        hashes.check_against_path(from_path)
    return File(from_path, None)","{""_check_download_dir"": ""0""}"
331,331,"def install_editable(
    install_options: List[str],
    global_options: Sequence[str],
    prefix: Optional[str],
    home: Optional[str],
    use_user_site: bool,
    name: str,
    setup_py_path: str,
    isolated: bool,
    build_env: BuildEnvironment,
    unpacked_source_directory: str,
) -> None:
    """"""Install a package in editable mode. Most arguments are pass-through
    to setuptools.
    """"""
    logger.info(""Running setup.py develop for %s"", name)

    args = make_setuptools_develop_args(
        setup_py_path,
        global_options=global_options,
        install_options=install_options,
        no_user_config=isolated,
        prefix=prefix,
        home=home,
        use_user_site=use_user_site,
    )

    with indent_log():
        with build_env:
            call_subprocess(
                args,
                command_desc=""python setup.py develop"",
                cwd=unpacked_source_directory,
            )","def make_setuptools_develop_args(
    setup_py_path: str,
    global_options: Sequence[str],
    install_options: Sequence[str],
    no_user_config: bool,
    prefix: Optional[str],
    home: Optional[str],
    use_user_site: bool,
) -> List[str]:
    assert not (use_user_site and prefix)

    args = make_setuptools_shim_args(
        setup_py_path,
        global_options=global_options,
        no_user_config=no_user_config,
    )

    args += [""develop"", ""--no-deps""]

    args += install_options

    if prefix:
        args += [""--prefix"", prefix]
    if home is not None:
        args += [""--install-dir"", home]

    if use_user_site:
        args += [""--user"", ""--prefix=""]

    return args

def call_subprocess(
    cmd: Union[List[str], CommandArgs],
    show_stdout: bool = False,
    cwd: Optional[str] = None,
    on_returncode: 'Literal[""raise"", ""warn"", ""ignore""]' = ""raise"",
    extra_ok_returncodes: Optional[Iterable[int]] = None,
    extra_environ: Optional[Mapping[str, Any]] = None,
    unset_environ: Optional[Iterable[str]] = None,
    spinner: Optional[SpinnerInterface] = None,
    log_failed_cmd: Optional[bool] = True,
    stdout_only: Optional[bool] = False,
    *,
    command_desc: str,
) -> str:
    """"""
    Args:
      show_stdout: if true, use INFO to log the subprocess's stderr and
        stdout streams.  Otherwise, use DEBUG.  Defaults to False.
      extra_ok_returncodes: an iterable of integer return codes that are
        acceptable, in addition to 0. Defaults to None, which means [].
      unset_environ: an iterable of environment variable names to unset
        prior to calling subprocess.Popen().
      log_failed_cmd: if false, failed commands are not logged, only raised.
      stdout_only: if true, return only stdout, else return both. When true,
        logging of both stdout and stderr occurs when the subprocess has
        terminated, else logging occurs as subprocess output is produced.
    """"""
    if extra_ok_returncodes is None:
        extra_ok_returncodes = []
    if unset_environ is None:
        unset_environ = []
    # Most places in pip use show_stdout=False. What this means is--
    #
    # - We connect the child's output (combined stderr and stdout) to a
    #   single pipe, which we read.
    # - We log this output to stderr at DEBUG level as it is received.
    # - If DEBUG logging isn't enabled (e.g. if --verbose logging wasn't
    #   requested), then we show a spinner so the user can still see the
    #   subprocess is in progress.
    # - If the subprocess exits with an error, we log the output to stderr
    #   at ERROR level if it hasn't already been displayed to the console
    #   (e.g. if --verbose logging wasn't enabled).  This way we don't log
    #   the output to the console twice.
    #
    # If show_stdout=True, then the above is still done, but with DEBUG
    # replaced by INFO.
    if show_stdout:
        # Then log the subprocess output at INFO level.
        log_subprocess: Callable[..., None] = subprocess_logger.info
        used_level = logging.INFO
    else:
        # Then log the subprocess output using VERBOSE.  This also ensures
        # it will be logged to the log file (aka user_log), if enabled.
        log_subprocess = subprocess_logger.verbose
        used_level = VERBOSE

    # Whether the subprocess will be visible in the console.
    showing_subprocess = subprocess_logger.getEffectiveLevel() <= used_level

    # Only use the spinner if we're not showing the subprocess output
    # and we have a spinner.
    use_spinner = not showing_subprocess and spinner is not None

    log_subprocess(""Running command %s"", command_desc)
    env = os.environ.copy()
    if extra_environ:
        env.update(extra_environ)
    for name in unset_environ:
        env.pop(name, None)
    try:
        proc = subprocess.Popen(
            # Convert HiddenText objects to the underlying str.
            reveal_command_args(cmd),
            stdin=subprocess.PIPE,
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT if not stdout_only else subprocess.PIPE,
            cwd=cwd,
            env=env,
            errors=""backslashreplace"",
        )
    except Exception as exc:
        if log_failed_cmd:
            subprocess_logger.critical(
                ""Error %s while executing command %s"",
                exc,
                command_desc,
            )
        raise
    all_output = []
    if not stdout_only:
        assert proc.stdout
        assert proc.stdin
        proc.stdin.close()
        # In this mode, stdout and stderr are in the same pipe.
        while True:
            line: str = proc.stdout.readline()
            if not line:
                break
            line = line.rstrip()
            all_output.append(line + ""\n"")

            # Show the line immediately.
            log_subprocess(line)
            # Update the spinner.
            if use_spinner:
                assert spinner
                spinner.spin()
        try:
            proc.wait()
        finally:
            if proc.stdout:
                proc.stdout.close()
        output = """".join(all_output)
    else:
        # In this mode, stdout and stderr are in different pipes.
        # We must use communicate() which is the only safe way to read both.
        out, err = proc.communicate()
        # log line by line to preserve pip log indenting
        for out_line in out.splitlines():
            log_subprocess(out_line)
        all_output.append(out)
        for err_line in err.splitlines():
            log_subprocess(err_line)
        all_output.append(err)
        output = out

    proc_had_error = proc.returncode and proc.returncode not in extra_ok_returncodes
    if use_spinner:
        assert spinner
        if proc_had_error:
            spinner.finish(""error"")
        else:
            spinner.finish(""done"")
    if proc_had_error:
        if on_returncode == ""raise"":
            error = InstallationSubprocessError(
                command_description=command_desc,
                exit_code=proc.returncode,
                output_lines=all_output if not showing_subprocess else None,
            )
            if log_failed_cmd:
                subprocess_logger.error(""[present-rich] %s"", error)
                subprocess_logger.verbose(
                    ""[bold magenta]full command[/]: [blue]%s[/]"",
                    escape(format_command_args(cmd)),
                    extra={""markup"": True},
                )
                subprocess_logger.verbose(
                    ""[bold magenta]cwd[/]: %s"",
                    escape(cwd or ""[inherit]""),
                    extra={""markup"": True},
                )

            raise error
        elif on_returncode == ""warn"":
            subprocess_logger.warning(
                'Command ""%s"" had error code %s in %s',
                command_desc,
                proc.returncode,
                cwd,
            )
        elif on_returncode == ""ignore"":
            pass
        else:
            raise ValueError(f""Invalid value: on_returncode={on_returncode!r}"")
    return output","def generate_setuptools_develop_arguments_with_options(
    setup_py_path: str,
    global_options: Sequence[str],
    install_options: Sequence[str],
    no_user_config: bool,
    prefix: Optional[str],
    home: Optional[str],
    use_user_site: bool,
) -> List[str]:
    assert not (use_user_site and prefix)

    args = make_setuptools_shim_args(
        setup_py_path,
        global_options=global_options,
        no_user_config=no_user_config,
    )

    args += [""develop"", ""--no-deps""]

    args += install_options

    if prefix:
        args += [""--prefix"", prefix]
    if home is not None:
        args += [""--install-dir"", home]

    if use_user_site:
        args += [""--user"", ""--prefix=""]

    return args

def call_subprocess(
    cmd: Union[List[str], CommandArgs],
    show_stdout: bool = False,
    cwd: Optional[str] = None,
    on_returncode: 'Literal[""raise"", ""warn"", ""ignore""]' = ""raise"",
    extra_ok_returncodes: Optional[Iterable[int]] = None,
    extra_environ: Optional[Mapping[str, Any]] = None,
    unset_environ: Optional[Iterable[str]] = None,
    spinner: Optional[SpinnerInterface] = None,
    log_failed_cmd: Optional[bool] = True,
    stdout_only: Optional[bool] = False,
    *,
    command_desc: str,
) -> str:
    """"""
    Args:
      show_stdout: if true, use INFO to log the subprocess's stderr and
        stdout streams.  Otherwise, use DEBUG.  Defaults to False.
      extra_ok_returncodes: an iterable of integer return codes that are
        acceptable, in addition to 0. Defaults to None, which means [].
      unset_environ: an iterable of environment variable names to unset
        prior to calling subprocess.Popen().
      log_failed_cmd: if false, failed commands are not logged, only raised.
      stdout_only: if true, return only stdout, else return both. When true,
        logging of both stdout and stderr occurs when the subprocess has
        terminated, else logging occurs as subprocess output is produced.
    """"""
    if extra_ok_returncodes is None:
        extra_ok_returncodes = []
    if unset_environ is None:
        unset_environ = []
    # Most places in pip use show_stdout=False. What this means is--
    #
    # - We connect the child's output (combined stderr and stdout) to a
    #   single pipe, which we read.
    # - We log this output to stderr at DEBUG level as it is received.
    # - If DEBUG logging isn't enabled (e.g. if --verbose logging wasn't
    #   requested), then we show a spinner so the user can still see the
    #   subprocess is in progress.
    # - If the subprocess exits with an error, we log the output to stderr
    #   at ERROR level if it hasn't already been displayed to the console
    #   (e.g. if --verbose logging wasn't enabled).  This way we don't log
    #   the output to the console twice.
    #
    # If show_stdout=True, then the above is still done, but with DEBUG
    # replaced by INFO.
    if show_stdout:
        # Then log the subprocess output at INFO level.
        log_subprocess: Callable[..., None] = subprocess_logger.info
        used_level = logging.INFO
    else:
        # Then log the subprocess output using VERBOSE.  This also ensures
        # it will be logged to the log file (aka user_log), if enabled.
        log_subprocess = subprocess_logger.verbose
        used_level = VERBOSE

    # Whether the subprocess will be visible in the console.
    showing_subprocess = subprocess_logger.getEffectiveLevel() <= used_level

    # Only use the spinner if we're not showing the subprocess output
    # and we have a spinner.
    use_spinner = not showing_subprocess and spinner is not None

    log_subprocess(""Running command %s"", command_desc)
    env = os.environ.copy()
    if extra_environ:
        env.update(extra_environ)
    for name in unset_environ:
        env.pop(name, None)
    try:
        proc = subprocess.Popen(
            # Convert HiddenText objects to the underlying str.
            reveal_command_args(cmd),
            stdin=subprocess.PIPE,
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT if not stdout_only else subprocess.PIPE,
            cwd=cwd,
            env=env,
            errors=""backslashreplace"",
        )
    except Exception as exc:
        if log_failed_cmd:
            subprocess_logger.critical(
                ""Error %s while executing command %s"",
                exc,
                command_desc,
            )
        raise
    all_output = []
    if not stdout_only:
        assert proc.stdout
        assert proc.stdin
        proc.stdin.close()
        # In this mode, stdout and stderr are in the same pipe.
        while True:
            line: str = proc.stdout.readline()
            if not line:
                break
            line = line.rstrip()
            all_output.append(line + ""\n"")

            # Show the line immediately.
            log_subprocess(line)
            # Update the spinner.
            if use_spinner:
                assert spinner
                spinner.spin()
        try:
            proc.wait()
        finally:
            if proc.stdout:
                proc.stdout.close()
        output = """".join(all_output)
    else:
        # In this mode, stdout and stderr are in different pipes.
        # We must use communicate() which is the only safe way to read both.
        out, err = proc.communicate()
        # log line by line to preserve pip log indenting
        for out_line in out.splitlines():
            log_subprocess(out_line)
        all_output.append(out)
        for err_line in err.splitlines():
            log_subprocess(err_line)
        all_output.append(err)
        output = out

    proc_had_error = proc.returncode and proc.returncode not in extra_ok_returncodes
    if use_spinner:
        assert spinner
        if proc_had_error:
            spinner.finish(""error"")
        else:
            spinner.finish(""done"")
    if proc_had_error:
        if on_returncode == ""raise"":
            error = InstallationSubprocessError(
                command_description=command_desc,
                exit_code=proc.returncode,
                output_lines=all_output if not showing_subprocess else None,
            )
            if log_failed_cmd:
                subprocess_logger.error(""[present-rich] %s"", error)
                subprocess_logger.verbose(
                    ""[bold magenta]full command[/]: [blue]%s[/]"",
                    escape(format_command_args(cmd)),
                    extra={""markup"": True},
                )
                subprocess_logger.verbose(
                    ""[bold magenta]cwd[/]: %s"",
                    escape(cwd or ""[inherit]""),
                    extra={""markup"": True},
                )

            raise error
        elif on_returncode == ""warn"":
            subprocess_logger.warning(
                'Command ""%s"" had error code %s in %s',
                command_desc,
                proc.returncode,
                cwd,
            )
        elif on_returncode == ""ignore"":
            pass
        else:
            raise ValueError(f""Invalid value: on_returncode={on_returncode!r}"")
    return output","def run_setup_py_develop_for_package_in_editable_mode(
    install_options: List[str],
    global_options: Sequence[str],
    prefix: Optional[str],
    home: Optional[str],
    use_user_site: bool,
    name: str,
    setup_py_path: str,
    isolated: bool,
    build_env: BuildEnvironment,
    unpacked_source_directory: str,
) -> None:
    """"""Install a package in editable mode. Most arguments are pass-through
    to setuptools.
    """"""
    logger.info(""Running setup.py develop for %s"", name)

    args = generate_setuptools_develop_arguments_with_options(
        setup_py_path,
        global_options=global_options,
        install_options=install_options,
        no_user_config=isolated,
        prefix=prefix,
        home=home,
        use_user_site=use_user_site,
    )

    with indent_log():
        with build_env:
            call_subprocess(
                args,
                command_desc=""python setup.py develop"",
                cwd=unpacked_source_directory,
            )","{""make_setuptools_develop_args"": ""generate_setuptools_develop_arguments_with_options""}","def 0(
    setup_py_path: str,
    global_options: Sequence[str],
    install_options: Sequence[str],
    no_user_config: bool,
    prefix: Optional[str],
    home: Optional[str],
    use_user_site: bool,
) -> List[str]:
    assert not (use_user_site and prefix)

    args = make_setuptools_shim_args(
        setup_py_path,
        global_options=global_options,
        no_user_config=no_user_config,
    )

    args += [""develop"", ""--no-deps""]

    args += install_options

    if prefix:
        args += [""--prefix"", prefix]
    if home is not None:
        args += [""--install-dir"", home]

    if use_user_site:
        args += [""--user"", ""--prefix=""]

    return args

def call_subprocess(
    cmd: Union[List[str], CommandArgs],
    show_stdout: bool = False,
    cwd: Optional[str] = None,
    on_returncode: 'Literal[""raise"", ""warn"", ""ignore""]' = ""raise"",
    extra_ok_returncodes: Optional[Iterable[int]] = None,
    extra_environ: Optional[Mapping[str, Any]] = None,
    unset_environ: Optional[Iterable[str]] = None,
    spinner: Optional[SpinnerInterface] = None,
    log_failed_cmd: Optional[bool] = True,
    stdout_only: Optional[bool] = False,
    *,
    command_desc: str,
) -> str:
    """"""
    Args:
      show_stdout: if true, use INFO to log the subprocess's stderr and
        stdout streams.  Otherwise, use DEBUG.  Defaults to False.
      extra_ok_returncodes: an iterable of integer return codes that are
        acceptable, in addition to 0. Defaults to None, which means [].
      unset_environ: an iterable of environment variable names to unset
        prior to calling subprocess.Popen().
      log_failed_cmd: if false, failed commands are not logged, only raised.
      stdout_only: if true, return only stdout, else return both. When true,
        logging of both stdout and stderr occurs when the subprocess has
        terminated, else logging occurs as subprocess output is produced.
    """"""
    if extra_ok_returncodes is None:
        extra_ok_returncodes = []
    if unset_environ is None:
        unset_environ = []
    # Most places in pip use show_stdout=False. What this means is--
    #
    # - We connect the child's output (combined stderr and stdout) to a
    #   single pipe, which we read.
    # - We log this output to stderr at DEBUG level as it is received.
    # - If DEBUG logging isn't enabled (e.g. if --verbose logging wasn't
    #   requested), then we show a spinner so the user can still see the
    #   subprocess is in progress.
    # - If the subprocess exits with an error, we log the output to stderr
    #   at ERROR level if it hasn't already been displayed to the console
    #   (e.g. if --verbose logging wasn't enabled).  This way we don't log
    #   the output to the console twice.
    #
    # If show_stdout=True, then the above is still done, but with DEBUG
    # replaced by INFO.
    if show_stdout:
        # Then log the subprocess output at INFO level.
        log_subprocess: Callable[..., None] = subprocess_logger.info
        used_level = logging.INFO
    else:
        # Then log the subprocess output using VERBOSE.  This also ensures
        # it will be logged to the log file (aka user_log), if enabled.
        log_subprocess = subprocess_logger.verbose
        used_level = VERBOSE

    # Whether the subprocess will be visible in the console.
    showing_subprocess = subprocess_logger.getEffectiveLevel() <= used_level

    # Only use the spinner if we're not showing the subprocess output
    # and we have a spinner.
    use_spinner = not showing_subprocess and spinner is not None

    log_subprocess(""Running command %s"", command_desc)
    env = os.environ.copy()
    if extra_environ:
        env.update(extra_environ)
    for name in unset_environ:
        env.pop(name, None)
    try:
        proc = subprocess.Popen(
            # Convert HiddenText objects to the underlying str.
            reveal_command_args(cmd),
            stdin=subprocess.PIPE,
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT if not stdout_only else subprocess.PIPE,
            cwd=cwd,
            env=env,
            errors=""backslashreplace"",
        )
    except Exception as exc:
        if log_failed_cmd:
            subprocess_logger.critical(
                ""Error %s while executing command %s"",
                exc,
                command_desc,
            )
        raise
    all_output = []
    if not stdout_only:
        assert proc.stdout
        assert proc.stdin
        proc.stdin.close()
        # In this mode, stdout and stderr are in the same pipe.
        while True:
            line: str = proc.stdout.readline()
            if not line:
                break
            line = line.rstrip()
            all_output.append(line + ""\n"")

            # Show the line immediately.
            log_subprocess(line)
            # Update the spinner.
            if use_spinner:
                assert spinner
                spinner.spin()
        try:
            proc.wait()
        finally:
            if proc.stdout:
                proc.stdout.close()
        output = """".join(all_output)
    else:
        # In this mode, stdout and stderr are in different pipes.
        # We must use communicate() which is the only safe way to read both.
        out, err = proc.communicate()
        # log line by line to preserve pip log indenting
        for out_line in out.splitlines():
            log_subprocess(out_line)
        all_output.append(out)
        for err_line in err.splitlines():
            log_subprocess(err_line)
        all_output.append(err)
        output = out

    proc_had_error = proc.returncode and proc.returncode not in extra_ok_returncodes
    if use_spinner:
        assert spinner
        if proc_had_error:
            spinner.finish(""error"")
        else:
            spinner.finish(""done"")
    if proc_had_error:
        if on_returncode == ""raise"":
            error = InstallationSubprocessError(
                command_description=command_desc,
                exit_code=proc.returncode,
                output_lines=all_output if not showing_subprocess else None,
            )
            if log_failed_cmd:
                subprocess_logger.error(""[present-rich] %s"", error)
                subprocess_logger.verbose(
                    ""[bold magenta]full command[/]: [blue]%s[/]"",
                    escape(format_command_args(cmd)),
                    extra={""markup"": True},
                )
                subprocess_logger.verbose(
                    ""[bold magenta]cwd[/]: %s"",
                    escape(cwd or ""[inherit]""),
                    extra={""markup"": True},
                )

            raise error
        elif on_returncode == ""warn"":
            subprocess_logger.warning(
                'Command ""%s"" had error code %s in %s',
                command_desc,
                proc.returncode,
                cwd,
            )
        elif on_returncode == ""ignore"":
            pass
        else:
            raise ValueError(f""Invalid value: on_returncode={on_returncode!r}"")
    return output","def 1(
    install_options: List[str],
    global_options: Sequence[str],
    prefix: Optional[str],
    home: Optional[str],
    use_user_site: bool,
    name: str,
    setup_py_path: str,
    isolated: bool,
    build_env: BuildEnvironment,
    unpacked_source_directory: str,
) -> None:
    """"""Install a package in editable mode. Most arguments are pass-through
    to setuptools.
    """"""
    logger.info(""Running setup.py develop for %s"", name)

    args = 0(
        setup_py_path,
        global_options=global_options,
        install_options=install_options,
        no_user_config=isolated,
        prefix=prefix,
        home=home,
        use_user_site=use_user_site,
    )

    with indent_log():
        with build_env:
            call_subprocess(
                args,
                command_desc=""python setup.py develop"",
                cwd=unpacked_source_directory,
            )","{""make_setuptools_develop_args"": ""0""}"
332,332,"def format_command_result(
    command_args: List[str],
    command_output: str,
) -> str:
    """"""Format command information for logging.""""""
    command_desc = format_command_args(command_args)
    text = f""Command arguments: {command_desc}\n""

    if not command_output:
        text += ""Command output: None""
    elif logger.getEffectiveLevel() > logging.DEBUG:
        text += ""Command output: [use --verbose to show]""
    else:
        if not command_output.endswith(""\n""):
            command_output += ""\n""
        text += f""Command output:\n{command_output}""

    return text","def format_command_args(args: Union[List[str], CommandArgs]) -> str:
    """"""
    Format command arguments for display.
    """"""
    # For HiddenText arguments, display the redacted form by calling str().
    # Also, we don't apply str() to arguments that aren't HiddenText since
    # this can trigger a UnicodeDecodeError in Python 2 if the argument
    # has type unicode and includes a non-ascii character.  (The type
    # checker doesn't ensure the annotations are correct in all cases.)
    return "" "".join(
        shlex.quote(str(arg)) if isinstance(arg, HiddenText) else shlex.quote(arg)
        for arg in args
    )","def format_command_args(args: Union[List[str], CommandArgs]) -> str:
    """"""
    Format command arguments for display.
    """"""
    # For HiddenText arguments, display the redacted form by calling str().
    # Also, we don't apply str() to arguments that aren't HiddenText since
    # this can trigger a UnicodeDecodeError in Python 2 if the argument
    # has type unicode and includes a non-ascii character.  (The type
    # checker doesn't ensure the annotations are correct in all cases.)
    return "" "".join(
        shlex.quote(str(arg)) if isinstance(arg, HiddenText) else shlex.quote(arg)
        for arg in args
    )","def prepare_command_result_for_logging(
    command_args: List[str],
    command_output: str,
) -> str:
    """"""Format command information for logging.""""""
    command_desc = format_command_args(command_args)
    text = f""Command arguments: {command_desc}\n""

    if not command_output:
        text += ""Command output: None""
    elif logger.getEffectiveLevel() > logging.DEBUG:
        text += ""Command output: [use --verbose to show]""
    else:
        if not command_output.endswith(""\n""):
            command_output += ""\n""
        text += f""Command output:\n{command_output}""

    return text",{},"def format_command_args(args: Union[List[str], CommandArgs]) -> str:
    """"""
    Format command arguments for display.
    """"""
    # For HiddenText arguments, display the redacted form by calling str().
    # Also, we don't apply str() to arguments that aren't HiddenText since
    # this can trigger a UnicodeDecodeError in Python 2 if the argument
    # has type unicode and includes a non-ascii character.  (The type
    # checker doesn't ensure the annotations are correct in all cases.)
    return "" "".join(
        shlex.quote(str(arg)) if isinstance(arg, HiddenText) else shlex.quote(arg)
        for arg in args
    )","def 1(
    command_args: List[str],
    command_output: str,
) -> str:
    """"""Format command information for logging.""""""
    command_desc = format_command_args(command_args)
    text = f""Command arguments: {command_desc}\n""

    if not command_output:
        text += ""Command output: None""
    elif logger.getEffectiveLevel() > logging.DEBUG:
        text += ""Command output: [use --verbose to show]""
    else:
        if not command_output.endswith(""\n""):
            command_output += ""\n""
        text += f""Command output:\n{command_output}""

    return text",{}
333,333,"def get_legacy_build_wheel_path(
    names: List[str],
    temp_dir: str,
    name: str,
    command_args: List[str],
    command_output: str,
) -> Optional[str]:
    """"""Return the path to the wheel in the temporary build directory.""""""
    # Sort for determinism.
    names = sorted(names)
    if not names:
        msg = (""Legacy build of wheel for {!r} created no files.\n"").format(name)
        msg += format_command_result(command_args, command_output)
        logger.warning(msg)
        return None

    if len(names) > 1:
        msg = (
            ""Legacy build of wheel for {!r} created more than one file.\n""
            ""Filenames (choosing first): {}\n""
        ).format(name, names)
        msg += format_command_result(command_args, command_output)
        logger.warning(msg)

    return os.path.join(temp_dir, names[0])","def format_command_result(
    command_args: List[str],
    command_output: str,
) -> str:
    """"""Format command information for logging.""""""
    command_desc = format_command_args(command_args)
    text = f""Command arguments: {command_desc}\n""

    if not command_output:
        text += ""Command output: None""
    elif logger.getEffectiveLevel() > logging.DEBUG:
        text += ""Command output: [use --verbose to show]""
    else:
        if not command_output.endswith(""\n""):
            command_output += ""\n""
        text += f""Command output:\n{command_output}""

    return text","def prepare_command_result_for_logging(
    command_args: List[str],
    command_output: str,
) -> str:
    """"""Format command information for logging.""""""
    command_desc = format_command_args(command_args)
    text = f""Command arguments: {command_desc}\n""

    if not command_output:
        text += ""Command output: None""
    elif logger.getEffectiveLevel() > logging.DEBUG:
        text += ""Command output: [use --verbose to show]""
    else:
        if not command_output.endswith(""\n""):
            command_output += ""\n""
        text += f""Command output:\n{command_output}""

    return text","def return_path_to_legacy_wheel_in_temp_dir(
    names: List[str],
    temp_dir: str,
    name: str,
    command_args: List[str],
    command_output: str,
) -> Optional[str]:
    """"""Return the path to the wheel in the temporary build directory.""""""
    # Sort for determinism.
    names = sorted(names)
    if not names:
        msg = (""Legacy build of wheel for {!r} created no files.\n"").format(name)
        msg += prepare_command_result_for_logging(command_args, command_output)
        logger.warning(msg)
        return None

    if len(names) > 1:
        msg = (
            ""Legacy build of wheel for {!r} created more than one file.\n""
            ""Filenames (choosing first): {}\n""
        ).format(name, names)
        msg += prepare_command_result_for_logging(command_args, command_output)
        logger.warning(msg)

    return os.path.join(temp_dir, names[0])","{""format_command_result"": ""prepare_command_result_for_logging""}","def 0(
    command_args: List[str],
    command_output: str,
) -> str:
    """"""Format command information for logging.""""""
    command_desc = format_command_args(command_args)
    text = f""Command arguments: {command_desc}\n""

    if not command_output:
        text += ""Command output: None""
    elif logger.getEffectiveLevel() > logging.DEBUG:
        text += ""Command output: [use --verbose to show]""
    else:
        if not command_output.endswith(""\n""):
            command_output += ""\n""
        text += f""Command output:\n{command_output}""

    return text","def 1(
    names: List[str],
    temp_dir: str,
    name: str,
    command_args: List[str],
    command_output: str,
) -> Optional[str]:
    """"""Return the path to the wheel in the temporary build directory.""""""
    # Sort for determinism.
    names = sorted(names)
    if not names:
        msg = (""Legacy build of wheel for {!r} created no files.\n"").format(name)
        msg += 0(command_args, command_output)
        logger.warning(msg)
        return None

    if len(names) > 1:
        msg = (
            ""Legacy build of wheel for {!r} created more than one file.\n""
            ""Filenames (choosing first): {}\n""
        ).format(name, names)
        msg += 0(command_args, command_output)
        logger.warning(msg)

    return os.path.join(temp_dir, names[0])","{""format_command_result"": ""0""}"
334,334,"def generate_editable_metadata(
    build_env: BuildEnvironment, backend: Pep517HookCaller, details: str
) -> str:
    """"""Generate metadata using mechanisms described in PEP 660.

    Returns the generated metadata directory.
    """"""
    metadata_tmpdir = TempDirectory(kind=""modern-metadata"", globally_managed=True)

    metadata_dir = metadata_tmpdir.path

    with build_env:
        # Note that Pep517HookCaller implements a fallback for
        # prepare_metadata_for_build_wheel/editable, so we don't have to
        # consider the possibility that this hook doesn't exist.
        runner = runner_with_spinner_message(
            ""Preparing editable metadata (pyproject.toml)""
        )
        with backend.subprocess_runner(runner):
            try:
                distinfo_dir = backend.prepare_metadata_for_build_editable(metadata_dir)
            except InstallationSubprocessError as error:
                raise MetadataGenerationFailed(package_details=details) from error

    return os.path.join(metadata_dir, distinfo_dir)","def runner_with_spinner_message(message: str) -> Callable[..., None]:
    """"""Provide a subprocess_runner that shows a spinner message.

    Intended for use with for pep517's Pep517HookCaller. Thus, the runner has
    an API that matches what's expected by Pep517HookCaller.subprocess_runner.
    """"""

    def runner(
        cmd: List[str],
        cwd: Optional[str] = None,
        extra_environ: Optional[Mapping[str, Any]] = None,
    ) -> None:
        with open_spinner(message) as spinner:
            call_subprocess(
                cmd,
                command_desc=message,
                cwd=cwd,
                extra_environ=extra_environ,
                spinner=spinner,
            )

    return runner","def runner_with_spinner_message(message: str) -> Callable[..., None]:
    """"""Provide a subprocess_runner that shows a spinner message.

    Intended for use with for pep517's Pep517HookCaller. Thus, the runner has
    an API that matches what's expected by Pep517HookCaller.subprocess_runner.
    """"""

    def runner(
        cmd: List[str],
        cwd: Optional[str] = None,
        extra_environ: Optional[Mapping[str, Any]] = None,
    ) -> None:
        with open_spinner(message) as spinner:
            call_subprocess(
                cmd,
                command_desc=message,
                cwd=cwd,
                extra_environ=extra_environ,
                spinner=spinner,
            )

    return runner","def generate_metadata_for_build_editable_using_pep660(
    build_env: BuildEnvironment, backend: Pep517HookCaller, details: str
) -> str:
    """"""Generate metadata using mechanisms described in PEP 660.

    Returns the generated metadata directory.
    """"""
    metadata_tmpdir = TempDirectory(kind=""modern-metadata"", globally_managed=True)

    metadata_dir = metadata_tmpdir.path

    with build_env:
        # Note that Pep517HookCaller implements a fallback for
        # prepare_metadata_for_build_wheel/editable, so we don't have to
        # consider the possibility that this hook doesn't exist.
        runner = runner_with_spinner_message(
            ""Preparing editable metadata (pyproject.toml)""
        )
        with backend.subprocess_runner(runner):
            try:
                distinfo_dir = backend.prepare_metadata_for_build_editable(metadata_dir)
            except InstallationSubprocessError as error:
                raise MetadataGenerationFailed(package_details=details) from error

    return os.path.join(metadata_dir, distinfo_dir)",{},"def runner_with_spinner_message(message: str) -> Callable[..., None]:
    """"""Provide a subprocess_runner that shows a spinner message.

    Intended for use with for pep517's Pep517HookCaller. Thus, the runner has
    an API that matches what's expected by Pep517HookCaller.subprocess_runner.
    """"""

    def runner(
        cmd: List[str],
        cwd: Optional[str] = None,
        extra_environ: Optional[Mapping[str, Any]] = None,
    ) -> None:
        with open_spinner(message) as spinner:
            call_subprocess(
                cmd,
                command_desc=message,
                cwd=cwd,
                extra_environ=extra_environ,
                spinner=spinner,
            )

    return runner","def 1(
    build_env: BuildEnvironment, backend: Pep517HookCaller, details: str
) -> str:
    """"""Generate metadata using mechanisms described in PEP 660.

    Returns the generated metadata directory.
    """"""
    metadata_tmpdir = TempDirectory(kind=""modern-metadata"", globally_managed=True)

    metadata_dir = metadata_tmpdir.path

    with build_env:
        # Note that Pep517HookCaller implements a fallback for
        # prepare_metadata_for_build_wheel/editable, so we don't have to
        # consider the possibility that this hook doesn't exist.
        runner = runner_with_spinner_message(
            ""Preparing editable metadata (pyproject.toml)""
        )
        with backend.subprocess_runner(runner):
            try:
                distinfo_dir = backend.prepare_metadata_for_build_editable(metadata_dir)
            except InstallationSubprocessError as error:
                raise MetadataGenerationFailed(package_details=details) from error

    return os.path.join(metadata_dir, distinfo_dir)",{}
335,335,"def build_wheel_editable(
    name: str,
    backend: Pep517HookCaller,
    metadata_directory: str,
    tempd: str,
) -> Optional[str]:
    """"""Build one InstallRequirement using the PEP 660 build process.

    Returns path to wheel if successfully built. Otherwise, returns None.
    """"""
    assert metadata_directory is not None
    try:
        logger.debug(""Destination directory: %s"", tempd)

        runner = runner_with_spinner_message(
            f""Building editable for {name} (pyproject.toml)""
        )
        with backend.subprocess_runner(runner):
            try:
                wheel_name = backend.build_editable(
                    tempd,
                    metadata_directory=metadata_directory,
                )
            except HookMissing as e:
                logger.error(
                    ""Cannot build editable %s because the build ""
                    ""backend does not have the %s hook"",
                    name,
                    e,
                )
                return None
    except Exception:
        logger.error(""Failed building editable for %s"", name)
        return None
    return os.path.join(tempd, wheel_name)","def runner_with_spinner_message(message: str) -> Callable[..., None]:
    """"""Provide a subprocess_runner that shows a spinner message.

    Intended for use with for pep517's Pep517HookCaller. Thus, the runner has
    an API that matches what's expected by Pep517HookCaller.subprocess_runner.
    """"""

    def runner(
        cmd: List[str],
        cwd: Optional[str] = None,
        extra_environ: Optional[Mapping[str, Any]] = None,
    ) -> None:
        with open_spinner(message) as spinner:
            call_subprocess(
                cmd,
                command_desc=message,
                cwd=cwd,
                extra_environ=extra_environ,
                spinner=spinner,
            )

    return runner","def runner_with_spinner_message(message: str) -> Callable[..., None]:
    """"""Provide a subprocess_runner that shows a spinner message.

    Intended for use with for pep517's Pep517HookCaller. Thus, the runner has
    an API that matches what's expected by Pep517HookCaller.subprocess_runner.
    """"""

    def runner(
        cmd: List[str],
        cwd: Optional[str] = None,
        extra_environ: Optional[Mapping[str, Any]] = None,
    ) -> None:
        with open_spinner(message) as spinner:
            call_subprocess(
                cmd,
                command_desc=message,
                cwd=cwd,
                extra_environ=extra_environ,
                spinner=spinner,
            )

    return runner","def build_wheel_for_install_requirement_using_pep660(
    name: str,
    backend: Pep517HookCaller,
    metadata_directory: str,
    tempd: str,
) -> Optional[str]:
    """"""Build one InstallRequirement using the PEP 660 build process.

    Returns path to wheel if successfully built. Otherwise, returns None.
    """"""
    assert metadata_directory is not None
    try:
        logger.debug(""Destination directory: %s"", tempd)

        runner = runner_with_spinner_message(
            f""Building editable for {name} (pyproject.toml)""
        )
        with backend.subprocess_runner(runner):
            try:
                wheel_name = backend.build_editable(
                    tempd,
                    metadata_directory=metadata_directory,
                )
            except HookMissing as e:
                logger.error(
                    ""Cannot build editable %s because the build ""
                    ""backend does not have the %s hook"",
                    name,
                    e,
                )
                return None
    except Exception:
        logger.error(""Failed building editable for %s"", name)
        return None
    return os.path.join(tempd, wheel_name)",{},"def runner_with_spinner_message(message: str) -> Callable[..., None]:
    """"""Provide a subprocess_runner that shows a spinner message.

    Intended for use with for pep517's Pep517HookCaller. Thus, the runner has
    an API that matches what's expected by Pep517HookCaller.subprocess_runner.
    """"""

    def runner(
        cmd: List[str],
        cwd: Optional[str] = None,
        extra_environ: Optional[Mapping[str, Any]] = None,
    ) -> None:
        with open_spinner(message) as spinner:
            call_subprocess(
                cmd,
                command_desc=message,
                cwd=cwd,
                extra_environ=extra_environ,
                spinner=spinner,
            )

    return runner","def 1(
    name: str,
    backend: Pep517HookCaller,
    metadata_directory: str,
    tempd: str,
) -> Optional[str]:
    """"""Build one InstallRequirement using the PEP 660 build process.

    Returns path to wheel if successfully built. Otherwise, returns None.
    """"""
    assert metadata_directory is not None
    try:
        logger.debug(""Destination directory: %s"", tempd)

        runner = runner_with_spinner_message(
            f""Building editable for {name} (pyproject.toml)""
        )
        with backend.subprocess_runner(runner):
            try:
                wheel_name = backend.build_editable(
                    tempd,
                    metadata_directory=metadata_directory,
                )
            except HookMissing as e:
                logger.error(
                    ""Cannot build editable %s because the build ""
                    ""backend does not have the %s hook"",
                    name,
                    e,
                )
                return None
    except Exception:
        logger.error(""Failed building editable for %s"", name)
        return None
    return os.path.join(tempd, wheel_name)",{}
336,336,"def build_wheel_pep517(
    name: str,
    backend: Pep517HookCaller,
    metadata_directory: str,
    tempd: str,
) -> Optional[str]:
    """"""Build one InstallRequirement using the PEP 517 build process.

    Returns path to wheel if successfully built. Otherwise, returns None.
    """"""
    assert metadata_directory is not None
    try:
        logger.debug(""Destination directory: %s"", tempd)

        runner = runner_with_spinner_message(
            f""Building wheel for {name} (pyproject.toml)""
        )
        with backend.subprocess_runner(runner):
            wheel_name = backend.build_wheel(
                tempd,
                metadata_directory=metadata_directory,
            )
    except Exception:
        logger.error(""Failed building wheel for %s"", name)
        return None
    return os.path.join(tempd, wheel_name)","def runner_with_spinner_message(message: str) -> Callable[..., None]:
    """"""Provide a subprocess_runner that shows a spinner message.

    Intended for use with for pep517's Pep517HookCaller. Thus, the runner has
    an API that matches what's expected by Pep517HookCaller.subprocess_runner.
    """"""

    def runner(
        cmd: List[str],
        cwd: Optional[str] = None,
        extra_environ: Optional[Mapping[str, Any]] = None,
    ) -> None:
        with open_spinner(message) as spinner:
            call_subprocess(
                cmd,
                command_desc=message,
                cwd=cwd,
                extra_environ=extra_environ,
                spinner=spinner,
            )

    return runner

def error(msg, *args, **kwargs):
    """"""
    Log a message with severity 'ERROR' on the root logger. If the logger has
    no handlers, call basicConfig() to add a console handler with a pre-defined
    format.
    """"""
    if len(root.handlers) == 0:
        basicConfig()
    root.error(msg, *args, **kwargs)","def execute_runner_with_spinner_displaying_message(message: str) -> Callable[..., None]:
    """"""Provide a subprocess_runner that shows a spinner message.

    Intended for use with for pep517's Pep517HookCaller. Thus, the runner has
    an API that matches what's expected by Pep517HookCaller.subprocess_runner.
    """"""

    def runner(
        cmd: List[str],
        cwd: Optional[str] = None,
        extra_environ: Optional[Mapping[str, Any]] = None,
    ) -> None:
        with open_spinner(message) as spinner:
            call_subprocess(
                cmd,
                command_desc=message,
                cwd=cwd,
                extra_environ=extra_environ,
                spinner=spinner,
            )

    return runner

def error(msg, *args, **kwargs):
    """"""
    Log a message with severity 'ERROR' on the root logger. If the logger has
    no handlers, call basicConfig() to add a console handler with a pre-defined
    format.
    """"""
    if len(root.handlers) == 0:
        basicConfig()
    root.error(msg, *args, **kwargs)","def build_wheel_for_install_requirement_using_pep517(
    name: str,
    backend: Pep517HookCaller,
    metadata_directory: str,
    tempd: str,
) -> Optional[str]:
    """"""Build one InstallRequirement using the PEP 517 build process.

    Returns path to wheel if successfully built. Otherwise, returns None.
    """"""
    assert metadata_directory is not None
    try:
        logger.debug(""Destination directory: %s"", tempd)

        runner = execute_runner_with_spinner_displaying_message(
            f""Building wheel for {name} (pyproject.toml)""
        )
        with backend.subprocess_runner(runner):
            wheel_name = backend.build_wheel(
                tempd,
                metadata_directory=metadata_directory,
            )
    except Exception:
        logger.error(""Failed building wheel for %s"", name)
        return None
    return os.path.join(tempd, wheel_name)","{""runner_with_spinner_message"": ""execute_runner_with_spinner_displaying_message""}","def 0(message: str) -> Callable[..., None]:
    """"""Provide a subprocess_runner that shows a spinner message.

    Intended for use with for pep517's Pep517HookCaller. Thus, the runner has
    an API that matches what's expected by Pep517HookCaller.subprocess_runner.
    """"""

    def runner(
        cmd: List[str],
        cwd: Optional[str] = None,
        extra_environ: Optional[Mapping[str, Any]] = None,
    ) -> None:
        with open_spinner(message) as spinner:
            call_subprocess(
                cmd,
                command_desc=message,
                cwd=cwd,
                extra_environ=extra_environ,
                spinner=spinner,
            )

    return runner

def error(msg, *args, **kwargs):
    """"""
    Log a message with severity 'ERROR' on the root logger. If the logger has
    no handlers, call basicConfig() to add a console handler with a pre-defined
    format.
    """"""
    if len(root.handlers) == 0:
        basicConfig()
    root.error(msg, *args, **kwargs)","def 1(
    name: str,
    backend: Pep517HookCaller,
    metadata_directory: str,
    tempd: str,
) -> Optional[str]:
    """"""Build one InstallRequirement using the PEP 517 build process.

    Returns path to wheel if successfully built. Otherwise, returns None.
    """"""
    assert metadata_directory is not None
    try:
        logger.debug(""Destination directory: %s"", tempd)

        runner = 0(
            f""Building wheel for {name} (pyproject.toml)""
        )
        with backend.subprocess_runner(runner):
            wheel_name = backend.build_wheel(
                tempd,
                metadata_directory=metadata_directory,
            )
    except Exception:
        logger.error(""Failed building wheel for %s"", name)
        return None
    return os.path.join(tempd, wheel_name)","{""runner_with_spinner_message"": ""0""}"
337,337,"def preprocess(content: str) -> ReqFileLines:
    """"""Split, filter, and join lines, and return a line iterator

    :param content: the content of the requirements file
    """"""
    lines_enum: ReqFileLines = enumerate(content.splitlines(), start=1)
    lines_enum = join_lines(lines_enum)
    lines_enum = ignore_comments(lines_enum)
    lines_enum = expand_env_variables(lines_enum)
    return lines_enum","def enumerate():
    """"""Return a list of all Thread objects currently alive.

    The list includes daemonic threads, dummy thread objects created by
    current_thread(), and the main thread. It excludes terminated threads and
    threads that have not yet been started.

    """"""
    with _active_limbo_lock:
        return list(_active.values()) + list(_limbo.values())

def join_lines(lines_enum: ReqFileLines) -> ReqFileLines:
    """"""Joins a line ending in '\' with the previous line (except when following
    comments).  The joined line takes on the index of the first line.
    """"""
    primary_line_number = None
    new_line: List[str] = []
    for line_number, line in lines_enum:
        if not line.endswith(""\\"") or COMMENT_RE.match(line):
            if COMMENT_RE.match(line):
                # this ensures comments are always matched later
                line = "" "" + line
            if new_line:
                new_line.append(line)
                assert primary_line_number is not None
                yield primary_line_number, """".join(new_line)
                new_line = []
            else:
                yield line_number, line
        else:
            if not new_line:
                primary_line_number = line_number
            new_line.append(line.strip(""\\""))

    # last line contains \
    if new_line:
        assert primary_line_number is not None
        yield primary_line_number, """".join(new_line)

    # TODO: handle space after '\'.

def expand_env_variables(lines_enum: ReqFileLines) -> ReqFileLines:
    """"""Replace all environment variables that can be retrieved via `os.getenv`.

    The only allowed format for environment variables defined in the
    requirement file is `${MY_VARIABLE_1}` to ensure two things:

    1. Strings that contain a `$` aren't accidentally (partially) expanded.
    2. Ensure consistency across platforms for requirement files.

    These points are the result of a discussion on the `github pull
    request #3514 <https://github.com/pypa/pip/pull/3514>`_.

    Valid characters in variable names follow the `POSIX standard
    <http://pubs.opengroup.org/onlinepubs/9699919799/>`_ and are limited
    to uppercase letter, digits and the `_` (underscore).
    """"""
    for line_number, line in lines_enum:
        for env_var, var_name in ENV_VAR_RE.findall(line):
            value = os.getenv(var_name)
            if not value:
                continue

            line = line.replace(env_var, value)

        yield line_number, line

def ignore_comments(lines_enum: ReqFileLines) -> ReqFileLines:
    """"""
    Strips comments and filter empty lines.
    """"""
    for line_number, line in lines_enum:
        line = COMMENT_RE.sub("""", line)
        line = line.strip()
        if line:
            yield line_number, line","def get_alive_threads():
    """"""Return a list of all Thread objects currently alive.

    The list includes daemonic threads, dummy thread objects created by
    current_thread(), and the main thread. It excludes terminated threads and
    threads that have not yet been started.

    """"""
    with _active_limbo_lock:
        return list(_active.values()) + list(_limbo.values())

def join_lines_ending_with_backslash_except_after_comments(lines_enum: ReqFileLines) -> ReqFileLines:
    """"""Joins a line ending in '\' with the previous line (except when following
    comments).  The joined line takes on the index of the first line.
    """"""
    primary_line_number = None
    new_line: List[str] = []
    for line_number, line in lines_enum:
        if not line.endswith(""\\"") or COMMENT_RE.match(line):
            if COMMENT_RE.match(line):
                # this ensures comments are always matched later
                line = "" "" + line
            if new_line:
                new_line.append(line)
                assert primary_line_number is not None
                yield primary_line_number, """".join(new_line)
                new_line = []
            else:
                yield line_number, line
        else:
            if not new_line:
                primary_line_number = line_number
            new_line.append(line.strip(""\\""))

    # last line contains \
    if new_line:
        assert primary_line_number is not None
        yield primary_line_number, """".join(new_line)

    # TODO: handle space after '\'.

def expand_environment_variables_in_lines_enum(lines_enum: ReqFileLines) -> ReqFileLines:
    """"""Replace all environment variables that can be retrieved via `os.getenv`.

    The only allowed format for environment variables defined in the
    requirement file is `${MY_VARIABLE_1}` to ensure two things:

    1. Strings that contain a `$` aren't accidentally (partially) expanded.
    2. Ensure consistency across platforms for requirement files.

    These points are the result of a discussion on the `github pull
    request #3514 <https://github.com/pypa/pip/pull/3514>`_.

    Valid characters in variable names follow the `POSIX standard
    <http://pubs.opengroup.org/onlinepubs/9699919799/>`_ and are limited
    to uppercase letter, digits and the `_` (underscore).
    """"""
    for line_number, line in lines_enum:
        for env_var, var_name in ENV_VAR_RE.findall(line):
            value = os.getenv(var_name)
            if not value:
                continue

            line = line.replace(env_var, value)

        yield line_number, line

def ignore_comments(lines_enum: ReqFileLines) -> ReqFileLines:
    """"""
    Strips comments and filter empty lines.
    """"""
    for line_number, line in lines_enum:
        line = COMMENT_RE.sub("""", line)
        line = line.strip()
        if line:
            yield line_number, line","def preprocess(content: str) -> ReqFileLines:
    """"""Split, filter, and join lines, and return a line iterator

    :param content: the content of the requirements file
    """"""
    lines_enum: ReqFileLines = get_alive_threads(content.splitlines(), start=1)
    lines_enum = join_lines_ending_with_backslash_except_after_comments(lines_enum)
    lines_enum = ignore_comments(lines_enum)
    lines_enum = expand_environment_variables_in_lines_enum(lines_enum)
    return lines_enum","{""enumerate"": ""get_alive_threads"", ""join_lines"": ""join_lines_ending_with_backslash_except_after_comments"", ""expand_env_variables"": ""expand_environment_variables_in_lines_enum""}","def 0():
    """"""Return a list of all Thread objects currently alive.

    The list includes daemonic threads, dummy thread objects created by
    current_thread(), and the main thread. It excludes terminated threads and
    threads that have not yet been started.

    """"""
    with _active_limbo_lock:
        return list(_active.values()) + list(_limbo.values())

def 1(lines_enum: ReqFileLines) -> ReqFileLines:
    """"""Joins a line ending in '\' with the previous line (except when following
    comments).  The joined line takes on the index of the first line.
    """"""
    primary_line_number = None
    new_line: List[str] = []
    for line_number, line in lines_enum:
        if not line.endswith(""\\"") or COMMENT_RE.match(line):
            if COMMENT_RE.match(line):
                # this ensures comments are always matched later
                line = "" "" + line
            if new_line:
                new_line.append(line)
                assert primary_line_number is not None
                yield primary_line_number, """".join(new_line)
                new_line = []
            else:
                yield line_number, line
        else:
            if not new_line:
                primary_line_number = line_number
            new_line.append(line.strip(""\\""))

    # last line contains \
    if new_line:
        assert primary_line_number is not None
        yield primary_line_number, """".join(new_line)

    # TODO: handle space after '\'.

def 2(lines_enum: ReqFileLines) -> ReqFileLines:
    """"""Replace all environment variables that can be retrieved via `os.getenv`.

    The only allowed format for environment variables defined in the
    requirement file is `${MY_VARIABLE_1}` to ensure two things:

    1. Strings that contain a `$` aren't accidentally (partially) expanded.
    2. Ensure consistency across platforms for requirement files.

    These points are the result of a discussion on the `github pull
    request #3514 <https://github.com/pypa/pip/pull/3514>`_.

    Valid characters in variable names follow the `POSIX standard
    <http://pubs.opengroup.org/onlinepubs/9699919799/>`_ and are limited
    to uppercase letter, digits and the `_` (underscore).
    """"""
    for line_number, line in lines_enum:
        for env_var, var_name in ENV_VAR_RE.findall(line):
            value = os.getenv(var_name)
            if not value:
                continue

            line = line.replace(env_var, value)

        yield line_number, line

def ignore_comments(lines_enum: ReqFileLines) -> ReqFileLines:
    """"""
    Strips comments and filter empty lines.
    """"""
    for line_number, line in lines_enum:
        line = COMMENT_RE.sub("""", line)
        line = line.strip()
        if line:
            yield line_number, line","def preprocess(content: str) -> ReqFileLines:
    """"""Split, filter, and join lines, and return a line iterator

    :param content: the content of the requirements file
    """"""
    lines_enum: ReqFileLines = 0(content.splitlines(), start=1)
    lines_enum = 1(lines_enum)
    lines_enum = ignore_comments(lines_enum)
    lines_enum = 2(lines_enum)
    return lines_enum","{""enumerate"": ""0"", ""join_lines"": ""1"", ""expand_env_variables"": ""2""}"
338,338,"def deduce_helpful_msg(req: str) -> str:
    """"""Returns helpful msg in case requirements file does not exist,
    or cannot be parsed.

    :params req: Requirements file path
    """"""
    if not os.path.exists(req):
        return f"" File '{req}' does not exist.""
    msg = "" The path does exist. ""
    # Try to parse and check if it is a requirements file.
    try:
        check_first_requirement_in_file(req)
    except InvalidRequirement:
        logger.debug(""Cannot parse '%s' as requirements file"", req)
    else:
        msg += (
            f""The argument you provided ""
            f""({req}) appears to be a""
            f"" requirements file. If that is the""
            f"" case, use the '-r' flag to install""
            f"" the packages specified within it.""
        )
    return msg","def check_first_requirement_in_file(filename: str) -> None:
    """"""Check if file is parsable as a requirements file.

    This is heavily based on ``pkg_resources.parse_requirements``, but
    simplified to just check the first meaningful line.

    :raises InvalidRequirement: If the first meaningful line cannot be parsed
        as an requirement.
    """"""
    with open(filename, encoding=""utf-8"", errors=""ignore"") as f:
        # Create a steppable iterator, so we can handle \-continuations.
        lines = (
            line
            for line in (line.strip() for line in f)
            if line and not line.startswith(""#"")  # Skip blank lines/comments.
        )

        for line in lines:
            # Drop comments -- a hash without a space may be in a URL.
            if "" #"" in line:
                line = line[: line.find("" #"")]
            # If there is a line continuation, drop it, and append the next line.
            if line.endswith(""\\""):
                line = line[:-2].strip() + next(lines, """")
            Requirement(line)
            return","def validate_first_line_as_requirements_file(filename: str) -> None:
    """"""Check if file is parsable as a requirements file.

    This is heavily based on ``pkg_resources.parse_requirements``, but
    simplified to just check the first meaningful line.

    :raises InvalidRequirement: If the first meaningful line cannot be parsed
        as an requirement.
    """"""
    with open(filename, encoding=""utf-8"", errors=""ignore"") as f:
        # Create a steppable iterator, so we can handle \-continuations.
        lines = (
            line
            for line in (line.strip() for line in f)
            if line and not line.startswith(""#"")  # Skip blank lines/comments.
        )

        for line in lines:
            # Drop comments -- a hash without a space may be in a URL.
            if "" #"" in line:
                line = line[: line.find("" #"")]
            # If there is a line continuation, drop it, and append the next line.
            if line.endswith(""\\""):
                line = line[:-2].strip() + next(lines, """")
            Requirement(line)
            return","def return_helpful_message_for_requirements_file(req: str) -> str:
    """"""Returns helpful msg in case requirements file does not exist,
    or cannot be parsed.

    :params req: Requirements file path
    """"""
    if not os.path.exists(req):
        return f"" File '{req}' does not exist.""
    msg = "" The path does exist. ""
    # Try to parse and check if it is a requirements file.
    try:
        validate_first_line_as_requirements_file(req)
    except InvalidRequirement:
        logger.debug(""Cannot parse '%s' as requirements file"", req)
    else:
        msg += (
            f""The argument you provided ""
            f""({req}) appears to be a""
            f"" requirements file. If that is the""
            f"" case, use the '-r' flag to install""
            f"" the packages specified within it.""
        )
    return msg","{""check_first_requirement_in_file"": ""validate_first_line_as_requirements_file""}","def 0(filename: str) -> None:
    """"""Check if file is parsable as a requirements file.

    This is heavily based on ``pkg_resources.parse_requirements``, but
    simplified to just check the first meaningful line.

    :raises InvalidRequirement: If the first meaningful line cannot be parsed
        as an requirement.
    """"""
    with open(filename, encoding=""utf-8"", errors=""ignore"") as f:
        # Create a steppable iterator, so we can handle \-continuations.
        lines = (
            line
            for line in (line.strip() for line in f)
            if line and not line.startswith(""#"")  # Skip blank lines/comments.
        )

        for line in lines:
            # Drop comments -- a hash without a space may be in a URL.
            if "" #"" in line:
                line = line[: line.find("" #"")]
            # If there is a line continuation, drop it, and append the next line.
            if line.endswith(""\\""):
                line = line[:-2].strip() + next(lines, """")
            Requirement(line)
            return","def 1(req: str) -> str:
    """"""Returns helpful msg in case requirements file does not exist,
    or cannot be parsed.

    :params req: Requirements file path
    """"""
    if not os.path.exists(req):
        return f"" File '{req}' does not exist.""
    msg = "" The path does exist. ""
    # Try to parse and check if it is a requirements file.
    try:
        0(req)
    except InvalidRequirement:
        logger.debug(""Cannot parse '%s' as requirements file"", req)
    else:
        msg += (
            f""The argument you provided ""
            f""({req}) appears to be a""
            f"" requirements file. If that is the""
            f"" case, use the '-r' flag to install""
            f"" the packages specified within it.""
        )
    return msg","{""check_first_requirement_in_file"": ""0""}"
339,339,"def is_url(name: str) -> bool:
    """"""
    Return true if the name looks like a URL.
    """"""
    scheme = get_url_scheme(name)
    if scheme is None:
        return False
    return scheme in [""http"", ""https"", ""file"", ""ftp""] + vcs.all_schemes","def get_url_scheme(url: str) -> Optional[str]:
    if "":"" not in url:
        return None
    return url.split("":"", 1)[0].lower()","def get_url_scheme(url: str) -> Optional[str]:
    if "":"" not in url:
        return None
    return url.split("":"", 1)[0].lower()","def is_url(name: str) -> bool:
    """"""
    Return true if the name looks like a URL.
    """"""
    scheme = get_url_scheme(name)
    if scheme is None:
        return False
    return scheme in [""http"", ""https"", ""file"", ""ftp""] + vcs.all_schemes",{},"def get_url_scheme(url: str) -> Optional[str]:
    if "":"" not in url:
        return None
    return url.split("":"", 1)[0].lower()","def is_url(name: str) -> bool:
    """"""
    Return true if the name looks like a URL.
    """"""
    scheme = get_url_scheme(name)
    if scheme is None:
        return False
    return scheme in [""http"", ""https"", ""file"", ""ftp""] + vcs.all_schemes",{}
340,340,"def find_path_to_project_root_from_repo_root(
    location: str, repo_root: str
) -> Optional[str]:
    """"""
    Find the the Python project's root by searching up the filesystem from
    `location`. Return the path to project root relative to `repo_root`.
    Return None if the project root is `repo_root`, or cannot be found.
    """"""
    # find project root.
    orig_location = location
    while not is_installable_dir(location):
        last_location = location
        location = os.path.dirname(location)
        if location == last_location:
            # We've traversed up to the root of the filesystem without
            # finding a Python project.
            logger.warning(
                ""Could not find a Python project for directory %s (tried all ""
                ""parent directories)"",
                orig_location,
            )
            return None

    if os.path.samefile(repo_root, location):
        return None

    return os.path.relpath(location, repo_root)","def is_installable_dir(path: str) -> bool:
    """"""Is path is a directory containing pyproject.toml or setup.py?

    If pyproject.toml exists, this is a PEP 517 project. Otherwise we look for
    a legacy setuptools layout by identifying setup.py. We don't check for the
    setup.cfg because using it without setup.py is only available for PEP 517
    projects, which are already covered by the pyproject.toml check.
    """"""
    if not os.path.isdir(path):
        return False
    if os.path.isfile(os.path.join(path, ""pyproject.toml"")):
        return True
    if os.path.isfile(os.path.join(path, ""setup.py"")):
        return True
    return False","def identify_directory_as_installable_project(path: str) -> bool:
    """"""Is path is a directory containing pyproject.toml or setup.py?

    If pyproject.toml exists, this is a PEP 517 project. Otherwise we look for
    a legacy setuptools layout by identifying setup.py. We don't check for the
    setup.cfg because using it without setup.py is only available for PEP 517
    projects, which are already covered by the pyproject.toml check.
    """"""
    if not os.path.isdir(path):
        return False
    if os.path.isfile(os.path.join(path, ""pyproject.toml"")):
        return True
    if os.path.isfile(os.path.join(path, ""setup.py"")):
        return True
    return False","def find_project_root_relative_to_repo_root(
    location: str, repo_root: str
) -> Optional[str]:
    """"""
    Find the the Python project's root by searching up the filesystem from
    `location`. Return the path to project root relative to `repo_root`.
    Return None if the project root is `repo_root`, or cannot be found.
    """"""
    # find project root.
    orig_location = location
    while not identify_directory_as_installable_project(location):
        last_location = location
        location = os.path.dirname(location)
        if location == last_location:
            # We've traversed up to the root of the filesystem without
            # finding a Python project.
            logger.warning(
                ""Could not find a Python project for directory %s (tried all ""
                ""parent directories)"",
                orig_location,
            )
            return None

    if os.path.samefile(repo_root, location):
        return None

    return os.path.relpath(location, repo_root)","{""is_installable_dir"": ""identify_directory_as_installable_project""}","def 0(path: str) -> bool:
    """"""Is path is a directory containing pyproject.toml or setup.py?

    If pyproject.toml exists, this is a PEP 517 project. Otherwise we look for
    a legacy setuptools layout by identifying setup.py. We don't check for the
    setup.cfg because using it without setup.py is only available for PEP 517
    projects, which are already covered by the pyproject.toml check.
    """"""
    if not os.path.isdir(path):
        return False
    if os.path.isfile(os.path.join(path, ""pyproject.toml"")):
        return True
    if os.path.isfile(os.path.join(path, ""setup.py"")):
        return True
    return False","def 1(
    location: str, repo_root: str
) -> Optional[str]:
    """"""
    Find the the Python project's root by searching up the filesystem from
    `location`. Return the path to project root relative to `repo_root`.
    Return None if the project root is `repo_root`, or cannot be found.
    """"""
    # find project root.
    orig_location = location
    while not 0(location):
        last_location = location
        location = os.path.dirname(location)
        if location == last_location:
            # We've traversed up to the root of the filesystem without
            # finding a Python project.
            logger.warning(
                ""Could not find a Python project for directory %s (tried all ""
                ""parent directories)"",
                orig_location,
            )
            return None

    if os.path.samefile(repo_root, location):
        return None

    return os.path.relpath(location, repo_root)","{""is_installable_dir"": ""0""}"
341,341,"def _looks_like_apple_library(path: str) -> bool:
    """"""Apple patches sysconfig to *always* look under */Library/Python*.""""""
    if sys.platform[:6] != ""darwin"":
        return False
    return path == f""/Library/Python/{get_major_minor_version()}/site-packages""","def get_major_minor_version() -> str:
    """"""
    Return the major-minor version of the current Python as a string, e.g.
    ""3.7"" or ""3.10"".
    """"""
    return ""{}.{}"".format(*sys.version_info)","def get_major_minor_version() -> str:
    """"""
    Return the major-minor version of the current Python as a string, e.g.
    ""3.7"" or ""3.10"".
    """"""
    return ""{}.{}"".format(*sys.version_info)","def _looks_like_apple_library(path: str) -> bool:
    """"""Apple patches sysconfig to *always* look under */Library/Python*.""""""
    if sys.platform[:6] != ""darwin"":
        return False
    return path == f""/Library/Python/{get_major_minor_version()}/site-packages""",{},"def get_major_minor_version() -> str:
    """"""
    Return the major-minor version of the current Python as a string, e.g.
    ""3.7"" or ""3.10"".
    """"""
    return ""{}.{}"".format(*sys.version_info)","def _looks_like_apple_library(path: str) -> bool:
    """"""Apple patches sysconfig to *always* look under */Library/Python*.""""""
    if sys.platform[:6] != ""darwin"":
        return False
    return path == f""/Library/Python/{get_major_minor_version()}/site-packages""",{}
342,342,"def _infer_user() -> str:
    """"""Try to find a user scheme for the current platform.""""""
    if _PREFERRED_SCHEME_API:
        return _PREFERRED_SCHEME_API(""user"")
    if is_osx_framework() and not running_under_virtualenv():
        suffixed = ""osx_framework_user""
    else:
        suffixed = f""{os.name}_user""
    if suffixed in _AVAILABLE_SCHEMES:
        return suffixed
    if ""posix_user"" not in _AVAILABLE_SCHEMES:  # User scheme unavailable.
        raise UserInstallationInvalid()
    return ""posix_user""","def running_under_virtualenv() -> bool:
    """"""Return True if we're running inside a virtualenv, False otherwise.""""""
    return _running_under_venv() or _running_under_regular_virtualenv()","def running_under_virtualenv() -> bool:
    """"""Return True if we're running inside a virtualenv, False otherwise.""""""
    return _running_under_venv() or _running_under_regular_virtualenv()","def try_to_find_user_scheme_for_current_platform() -> str:
    """"""Try to find a user scheme for the current platform.""""""
    if _PREFERRED_SCHEME_API:
        return _PREFERRED_SCHEME_API(""user"")
    if is_osx_framework() and not running_under_virtualenv():
        suffixed = ""osx_framework_user""
    else:
        suffixed = f""{os.name}_user""
    if suffixed in _AVAILABLE_SCHEMES:
        return suffixed
    if ""posix_user"" not in _AVAILABLE_SCHEMES:  # User scheme unavailable.
        raise UserInstallationInvalid()
    return ""posix_user""",{},"def running_under_virtualenv() -> bool:
    """"""Return True if we're running inside a virtualenv, False otherwise.""""""
    return _running_under_venv() or _running_under_regular_virtualenv()","def 1() -> str:
    """"""Try to find a user scheme for the current platform.""""""
    if _PREFERRED_SCHEME_API:
        return _PREFERRED_SCHEME_API(""user"")
    if is_osx_framework() and not running_under_virtualenv():
        suffixed = ""osx_framework_user""
    else:
        suffixed = f""{os.name}_user""
    if suffixed in _AVAILABLE_SCHEMES:
        return suffixed
    if ""posix_user"" not in _AVAILABLE_SCHEMES:  # User scheme unavailable.
        raise UserInstallationInvalid()
    return ""posix_user""",{}
343,343,"def _ensure_html_response(url: str, session: PipSession) -> None:
    """"""Send a HEAD request to the URL, and ensure the response contains HTML.

    Raises `_NotHTTP` if the URL is not available for a HEAD request, or
    `_NotHTML` if the content type is not text/html.
    """"""
    scheme, netloc, path, query, fragment = urllib.parse.urlsplit(url)
    if scheme not in {""http"", ""https""}:
        raise _NotHTTP()

    resp = session.head(url, allow_redirects=True)
    raise_for_status(resp)

    _ensure_html_header(resp)","def raise_for_status(resp: Response) -> None:
    http_error_msg = """"
    if isinstance(resp.reason, bytes):
        # We attempt to decode utf-8 first because some servers
        # choose to localize their reason strings. If the string
        # isn't utf-8, we fall back to iso-8859-1 for all other
        # encodings.
        try:
            reason = resp.reason.decode(""utf-8"")
        except UnicodeDecodeError:
            reason = resp.reason.decode(""iso-8859-1"")
    else:
        reason = resp.reason

    if 400 <= resp.status_code < 500:
        http_error_msg = (
            f""{resp.status_code} Client Error: {reason} for url: {resp.url}""
        )

    elif 500 <= resp.status_code < 600:
        http_error_msg = (
            f""{resp.status_code} Server Error: {reason} for url: {resp.url}""
        )

    if http_error_msg:
        raise NetworkConnectionError(http_error_msg, response=resp)

def _ensure_html_header(response: Response) -> None:
    """"""Check the Content-Type header to ensure the response contains HTML.

    Raises `_NotHTML` if the content type is not text/html.
    """"""
    content_type = response.headers.get(""Content-Type"", """")
    if not content_type.lower().startswith(""text/html""):
        raise _NotHTML(content_type, response.request.method)","def check_and_raise_for_status(resp: Response) -> None:
    http_error_msg = """"
    if isinstance(resp.reason, bytes):
        # We attempt to decode utf-8 first because some servers
        # choose to localize their reason strings. If the string
        # isn't utf-8, we fall back to iso-8859-1 for all other
        # encodings.
        try:
            reason = resp.reason.decode(""utf-8"")
        except UnicodeDecodeError:
            reason = resp.reason.decode(""iso-8859-1"")
    else:
        reason = resp.reason

    if 400 <= resp.status_code < 500:
        http_error_msg = (
            f""{resp.status_code} Client Error: {reason} for url: {resp.url}""
        )

    elif 500 <= resp.status_code < 600:
        http_error_msg = (
            f""{resp.status_code} Server Error: {reason} for url: {resp.url}""
        )

    if http_error_msg:
        raise NetworkConnectionError(http_error_msg, response=resp)

def validate_and_ensure_html_header(response: Response) -> None:
    """"""Check the Content-Type header to ensure the response contains HTML.

    Raises `_NotHTML` if the content type is not text/html.
    """"""
    content_type = response.headers.get(""Content-Type"", """")
    if not content_type.lower().startswith(""text/html""):
        raise _NotHTML(content_type, response.request.method)","def _ensure_html_response(url: str, session: PipSession) -> None:
    """"""Send a HEAD request to the URL, and ensure the response contains HTML.

    Raises `_NotHTTP` if the URL is not available for a HEAD request, or
    `_NotHTML` if the content type is not text/html.
    """"""
    scheme, netloc, path, query, fragment = urllib.parse.urlsplit(url)
    if scheme not in {""http"", ""https""}:
        raise _NotHTTP()

    resp = session.head(url, allow_redirects=True)
    check_and_raise_for_status(resp)

    validate_and_ensure_html_header(resp)","{""raise_for_status"": ""check_and_raise_for_status"", ""_ensure_html_header"": ""validate_and_ensure_html_header""}","def 0(resp: Response) -> None:
    http_error_msg = """"
    if isinstance(resp.reason, bytes):
        # We attempt to decode utf-8 first because some servers
        # choose to localize their reason strings. If the string
        # isn't utf-8, we fall back to iso-8859-1 for all other
        # encodings.
        try:
            reason = resp.reason.decode(""utf-8"")
        except UnicodeDecodeError:
            reason = resp.reason.decode(""iso-8859-1"")
    else:
        reason = resp.reason

    if 400 <= resp.status_code < 500:
        http_error_msg = (
            f""{resp.status_code} Client Error: {reason} for url: {resp.url}""
        )

    elif 500 <= resp.status_code < 600:
        http_error_msg = (
            f""{resp.status_code} Server Error: {reason} for url: {resp.url}""
        )

    if http_error_msg:
        raise NetworkConnectionError(http_error_msg, response=resp)

def 1(response: Response) -> None:
    """"""Check the Content-Type header to ensure the response contains HTML.

    Raises `_NotHTML` if the content type is not text/html.
    """"""
    content_type = response.headers.get(""Content-Type"", """")
    if not content_type.lower().startswith(""text/html""):
        raise _NotHTML(content_type, response.request.method)","def _ensure_html_response(url: str, session: PipSession) -> None:
    """"""Send a HEAD request to the URL, and ensure the response contains HTML.

    Raises `_NotHTTP` if the URL is not available for a HEAD request, or
    `_NotHTML` if the content type is not text/html.
    """"""
    scheme, netloc, path, query, fragment = urllib.parse.urlsplit(url)
    if scheme not in {""http"", ""https""}:
        raise _NotHTTP()

    resp = session.head(url, allow_redirects=True)
    0(resp)

    1(resp)","{""raise_for_status"": ""0"", ""_ensure_html_header"": ""1""}"
344,344,"def _clean_url_path(path: str, is_local_path: bool) -> str:
    """"""
    Clean the path portion of a URL.
    """"""
    if is_local_path:
        clean_func = _clean_file_url_path
    else:
        clean_func = _clean_url_path_part

    # Split on the reserved characters prior to cleaning so that
    # revision strings in VCS URLs are properly preserved.
    parts = _reserved_chars_re.split(path)

    cleaned_parts = []
    for to_clean, reserved in pairwise(itertools.chain(parts, [""""])):
        cleaned_parts.append(clean_func(to_clean))
        # Normalize %xx escapes (e.g. %2f -> %2F)
        cleaned_parts.append(reserved.upper())

    return """".join(cleaned_parts)","def pairwise(iterable: Iterable[Any]) -> Iterator[Tuple[Any, Any]]:
    """"""
    Return paired elements.

    For example:
        s -> (s0, s1), (s2, s3), (s4, s5), ...
    """"""
    iterable = iter(iterable)
    return zip_longest(iterable, iterable)","def pairwise(iterable: Iterable[Any]) -> Iterator[Tuple[Any, Any]]:
    """"""
    Return paired elements.

    For example:
        s -> (s0, s1), (s2, s3), (s4, s5), ...
    """"""
    iterable = iter(iterable)
    return zip_longest(iterable, iterable)","def sanitize_url_path(path: str, is_local_path: bool) -> str:
    """"""
    Clean the path portion of a URL.
    """"""
    if is_local_path:
        clean_func = _clean_file_url_path
    else:
        clean_func = sanitize_url_path_part

    # Split on the reserved characters prior to cleaning so that
    # revision strings in VCS URLs are properly preserved.
    parts = _reserved_chars_re.split(path)

    cleaned_parts = []
    for to_clean, reserved in pairwise(itertools.chain(parts, [""""])):
        cleaned_parts.append(clean_func(to_clean))
        # Normalize %xx escapes (e.g. %2f -> %2F)
        cleaned_parts.append(reserved.upper())

    return """".join(cleaned_parts)",{},"def pairwise(iterable: Iterable[Any]) -> Iterator[Tuple[Any, Any]]:
    """"""
    Return paired elements.

    For example:
        s -> (s0, s1), (s2, s3), (s4, s5), ...
    """"""
    iterable = iter(iterable)
    return zip_longest(iterable, iterable)","def 2(path: str, is_local_path: bool) -> str:
    """"""
    Clean the path portion of a URL.
    """"""
    if is_local_path:
        clean_func = _clean_file_url_path
    else:
        clean_func = _clean_url_path_part

    # Split on the reserved characters prior to cleaning so that
    # revision strings in VCS URLs are properly preserved.
    parts = _reserved_chars_re.split(path)

    cleaned_parts = []
    for to_clean, reserved in pairwise(itertools.chain(parts, [""""])):
        cleaned_parts.append(clean_func(to_clean))
        # Normalize %xx escapes (e.g. %2f -> %2F)
        cleaned_parts.append(reserved.upper())

    return """".join(cleaned_parts)",{}
345,345,"def _clean_link(url: str) -> str:
    """"""
    Make sure a link is fully quoted.
    For example, if ' ' occurs in the URL, it will be replaced with ""%20"",
    and without double-quoting other characters.
    """"""
    # Split the URL into parts according to the general structure
    # `scheme://netloc/path;parameters?query#fragment`.
    result = urllib.parse.urlparse(url)
    # If the netloc is empty, then the URL refers to a local filesystem path.
    is_local_path = not result.netloc
    path = _clean_url_path(result.path, is_local_path=is_local_path)
    return urllib.parse.urlunparse(result._replace(path=path))","def _clean_url_path(path: str, is_local_path: bool) -> str:
    """"""
    Clean the path portion of a URL.
    """"""
    if is_local_path:
        clean_func = _clean_file_url_path
    else:
        clean_func = _clean_url_path_part

    # Split on the reserved characters prior to cleaning so that
    # revision strings in VCS URLs are properly preserved.
    parts = _reserved_chars_re.split(path)

    cleaned_parts = []
    for to_clean, reserved in pairwise(itertools.chain(parts, [""""])):
        cleaned_parts.append(clean_func(to_clean))
        # Normalize %xx escapes (e.g. %2f -> %2F)
        cleaned_parts.append(reserved.upper())

    return """".join(cleaned_parts)","def sanitize_url_path(path: str, is_local_path: bool) -> str:
    """"""
    Clean the path portion of a URL.
    """"""
    if is_local_path:
        clean_func = _clean_file_url_path
    else:
        clean_func = sanitize_url_path_part

    # Split on the reserved characters prior to cleaning so that
    # revision strings in VCS URLs are properly preserved.
    parts = _reserved_chars_re.split(path)

    cleaned_parts = []
    for to_clean, reserved in pairwise(itertools.chain(parts, [""""])):
        cleaned_parts.append(clean_func(to_clean))
        # Normalize %xx escapes (e.g. %2f -> %2F)
        cleaned_parts.append(reserved.upper())

    return """".join(cleaned_parts)","def _clean_link(url: str) -> str:
    """"""
    Make sure a link is fully quoted.
    For example, if ' ' occurs in the URL, it will be replaced with ""%20"",
    and without double-quoting other characters.
    """"""
    # Split the URL into parts according to the general structure
    # `scheme://netloc/path;parameters?query#fragment`.
    result = urllib.parse.urlparse(url)
    # If the netloc is empty, then the URL refers to a local filesystem path.
    is_local_path = not result.netloc
    path = sanitize_url_path(result.path, is_local_path=is_local_path)
    return urllib.parse.urlunparse(result._replace(path=path))","{""_clean_url_path"": ""sanitize_url_path""}","def 0(path: str, is_local_path: bool) -> str:
    """"""
    Clean the path portion of a URL.
    """"""
    if is_local_path:
        clean_func = _clean_file_url_path
    else:
        clean_func = _clean_url_path_part

    # Split on the reserved characters prior to cleaning so that
    # revision strings in VCS URLs are properly preserved.
    parts = _reserved_chars_re.split(path)

    cleaned_parts = []
    for to_clean, reserved in pairwise(itertools.chain(parts, [""""])):
        cleaned_parts.append(clean_func(to_clean))
        # Normalize %xx escapes (e.g. %2f -> %2F)
        cleaned_parts.append(reserved.upper())

    return """".join(cleaned_parts)","def _clean_link(url: str) -> str:
    """"""
    Make sure a link is fully quoted.
    For example, if ' ' occurs in the URL, it will be replaced with ""%20"",
    and without double-quoting other characters.
    """"""
    # Split the URL into parts according to the general structure
    # `scheme://netloc/path;parameters?query#fragment`.
    result = urllib.parse.urlparse(url)
    # If the netloc is empty, then the URL refers to a local filesystem path.
    is_local_path = not result.netloc
    path = 0(result.path, is_local_path=is_local_path)
    return urllib.parse.urlunparse(result._replace(path=path))","{""_clean_url_path"": ""0""}"
346,346,"def with_cached_html_pages(fn: ParseLinks) -> ParseLinks:
    """"""
    Given a function that parses an Iterable[Link] from an HTMLPage, cache the
    function's result (keyed by CacheablePageContent), unless the HTMLPage
    `page` has `page.cache_link_parsing == False`.
    """"""

    @functools.lru_cache(maxsize=None)
    def wrapper(
        cacheable_page: CacheablePageContent, use_deprecated_html5lib: bool
    ) -> List[Link]:
        return list(fn(cacheable_page.page, use_deprecated_html5lib))

    @functools.wraps(fn)
    def wrapper_wrapper(page: ""HTMLPage"", use_deprecated_html5lib: bool) -> List[Link]:
        if page.cache_link_parsing:
            return wrapper(CacheablePageContent(page), use_deprecated_html5lib)
        return list(fn(page, use_deprecated_html5lib))

    return wrapper_wrapper","def wrapper(func, /, *args, **kwds):
    """"""Wrapper function that initializes curses and calls another function,
    restoring normal keyboard/screen behavior on error.
    The callable object 'func' is then passed the main window 'stdscr'
    as its first argument, followed by any other arguments passed to
    wrapper().
    """"""

    try:
        # Initialize curses
        stdscr = initscr()

        # Turn off echoing of keys, and enter cbreak mode,
        # where no buffering is performed on keyboard input
        noecho()
        cbreak()

        # In keypad mode, escape sequences for special keys
        # (like the cursor keys) will be interpreted and
        # a special value like curses.KEY_LEFT will be returned
        stdscr.keypad(1)

        # Start color, too.  Harmless if the terminal doesn't have
        # color; user can test with has_color() later on.  The try/catch
        # works around a minor bit of over-conscientiousness in the curses
        # module -- the error return from C start_color() is ignorable.
        try:
            start_color()
        except:
            pass

        return func(stdscr, *args, **kwds)
    finally:
        # Set everything back to normal
        if 'stdscr' in locals():
            stdscr.keypad(0)
            echo()
            nocbreak()
            endwin()","def initialize_curses_and_call_function(func, /, *args, **kwds):
    """"""Wrapper function that initializes curses and calls another function,
    restoring normal keyboard/screen behavior on error.
    The callable object 'func' is then passed the main window 'stdscr'
    as its first argument, followed by any other arguments passed to
    initialize_curses_and_call_function().
    """"""

    try:
        # Initialize curses
        stdscr = initscr()

        # Turn off echoing of keys, and enter cbreak mode,
        # where no buffering is performed on keyboard input
        noecho()
        cbreak()

        # In keypad mode, escape sequences for special keys
        # (like the cursor keys) will be interpreted and
        # a special value like curses.KEY_LEFT will be returned
        stdscr.keypad(1)

        # Start color, too.  Harmless if the terminal doesn't have
        # color; user can test with has_color() later on.  The try/catch
        # works around a minor bit of over-conscientiousness in the curses
        # module -- the error return from C start_color() is ignorable.
        try:
            start_color()
        except:
            pass

        return func(stdscr, *args, **kwds)
    finally:
        # Set everything back to normal
        if 'stdscr' in locals():
            stdscr.keypad(0)
            echo()
            nocbreak()
            endwin()","def with_cached_html_pages(fn: ParseLinks) -> ParseLinks:
    """"""
    Given a function that parses an Iterable[Link] from an HTMLPage, cache the
    function's result (keyed by CacheablePageContent), unless the HTMLPage
    `page` has `page.cache_link_parsing == False`.
    """"""

    @functools.lru_cache(maxsize=None)
    def initialize_curses_and_call_function(
        cacheable_page: CacheablePageContent, use_deprecated_html5lib: bool
    ) -> List[Link]:
        return list(fn(cacheable_page.page, use_deprecated_html5lib))

    @functools.wraps(fn)
    def initialize_curses_and_call_function_initialize_curses_and_call_function(page: ""HTMLPage"", use_deprecated_html5lib: bool) -> List[Link]:
        if page.cache_link_parsing:
            return initialize_curses_and_call_function(CacheablePageContent(page), use_deprecated_html5lib)
        return list(fn(page, use_deprecated_html5lib))

    return initialize_curses_and_call_function_initialize_curses_and_call_function","{""wrapper"": ""initialize_curses_and_call_function""}","def 0(func, /, *args, **kwds):
    """"""Wrapper function that initializes curses and calls another function,
    restoring normal keyboard/screen behavior on error.
    The callable object 'func' is then passed the main window 'stdscr'
    as its first argument, followed by any other arguments passed to
    0().
    """"""

    try:
        # Initialize curses
        stdscr = initscr()

        # Turn off echoing of keys, and enter cbreak mode,
        # where no buffering is performed on keyboard input
        noecho()
        cbreak()

        # In keypad mode, escape sequences for special keys
        # (like the cursor keys) will be interpreted and
        # a special value like curses.KEY_LEFT will be returned
        stdscr.keypad(1)

        # Start color, too.  Harmless if the terminal doesn't have
        # color; user can test with has_color() later on.  The try/catch
        # works around a minor bit of over-conscientiousness in the curses
        # module -- the error return from C start_color() is ignorable.
        try:
            start_color()
        except:
            pass

        return func(stdscr, *args, **kwds)
    finally:
        # Set everything back to normal
        if 'stdscr' in locals():
            stdscr.keypad(0)
            echo()
            nocbreak()
            endwin()","def with_cached_html_pages(fn: ParseLinks) -> ParseLinks:
    """"""
    Given a function that parses an Iterable[Link] from an HTMLPage, cache the
    function's result (keyed by CacheablePageContent), unless the HTMLPage
    `page` has `page.cache_link_parsing == False`.
    """"""

    @functools.lru_cache(maxsize=None)
    def 0(
        cacheable_page: CacheablePageContent, use_deprecated_html5lib: bool
    ) -> List[Link]:
        return list(fn(cacheable_page.page, use_deprecated_html5lib))

    @functools.wraps(fn)
    def wrapper_0(page: ""HTMLPage"", use_deprecated_html5lib: bool) -> List[Link]:
        if page.cache_link_parsing:
            return 0(CacheablePageContent(page), use_deprecated_html5lib)
        return list(fn(page, use_deprecated_html5lib))

    return wrapper_wrapper","{""wrapper"": ""0""}"
347,347,"def _parse_links_html5lib(page: ""HTMLPage"") -> Iterable[Link]:
    """"""
    Parse an HTML document, and yield its anchor elements as Link objects.

    TODO: Remove when `html5lib` is dropped.
    """"""
    document = html5lib.parse(
        page.content,
        transport_encoding=page.encoding,
        namespaceHTMLElements=False,
    )

    url = page.url
    base_url = _determine_base_url(document, url)
    for anchor in document.findall("".//a""):
        link = _create_link_from_element(
            anchor.attrib,
            page_url=url,
            base_url=base_url,
        )
        if link is None:
            continue
        yield link","def _create_link_from_element(
    element_attribs: Dict[str, Optional[str]],
    page_url: str,
    base_url: str,
) -> Optional[Link]:
    """"""
    Convert an anchor element's attributes in a simple repository page to a Link.
    """"""
    href = element_attribs.get(""href"")
    if not href:
        return None

    url = _clean_link(urllib.parse.urljoin(base_url, href))
    pyrequire = element_attribs.get(""data-requires-python"")
    yanked_reason = element_attribs.get(""data-yanked"")

    link = Link(
        url,
        comes_from=page_url,
        requires_python=pyrequire,
        yanked_reason=yanked_reason,
    )

    return link

def _determine_base_url(document: HTMLElement, page_url: str) -> str:
    """"""Determine the HTML document's base URL.

    This looks for a ``<base>`` tag in the HTML document. If present, its href
    attribute denotes the base URL of anchor tags in the document. If there is
    no such tag (or if it does not have a valid href attribute), the HTML
    file's URL is used as the base URL.

    :param document: An HTML document representation. The current
        implementation expects the result of ``html5lib.parse()``.
    :param page_url: The URL of the HTML document.

    TODO: Remove when `html5lib` is dropped.
    """"""
    for base in document.findall("".//base""):
        href = base.get(""href"")
        if href is not None:
            return href
    return page_url","def convert_anchor_element_attributes_to_link(
    element_attribs: Dict[str, Optional[str]],
    page_url: str,
    base_url: str,
) -> Optional[Link]:
    """"""
    Convert an anchor element's attributes in a simple repository page to a Link.
    """"""
    href = element_attribs.get(""href"")
    if not href:
        return None

    url = _clean_link(urllib.parse.urljoin(base_url, href))
    pyrequire = element_attribs.get(""data-requires-python"")
    yanked_reason = element_attribs.get(""data-yanked"")

    link = Link(
        url,
        comes_from=page_url,
        requires_python=pyrequire,
        yanked_reason=yanked_reason,
    )

    return link

def _determine_base_url(document: HTMLElement, page_url: str) -> str:
    """"""Determine the HTML document's base URL.

    This looks for a ``<base>`` tag in the HTML document. If present, its href
    attribute denotes the base URL of anchor tags in the document. If there is
    no such tag (or if it does not have a valid href attribute), the HTML
    file's URL is used as the base URL.

    :param document: An HTML document representation. The current
        implementation expects the result of ``html5lib.parse()``.
    :param page_url: The URL of the HTML document.

    TODO: Remove when `html5lib` is dropped.
    """"""
    for base in document.findall("".//base""):
        href = base.get(""href"")
        if href is not None:
            return href
    return page_url","def parse_html_document_and_yield_anchor_elements(page: ""HTMLPage"") -> Iterable[Link]:
    """"""
    Parse an HTML document, and yield its anchor elements as Link objects.

    TODO: Remove when `html5lib` is dropped.
    """"""
    document = html5lib.parse(
        page.content,
        transport_encoding=page.encoding,
        namespaceHTMLElements=False,
    )

    url = page.url
    base_url = _determine_base_url(document, url)
    for anchor in document.findall("".//a""):
        link = convert_anchor_element_attributes_to_link(
            anchor.attrib,
            page_url=url,
            base_url=base_url,
        )
        if link is None:
            continue
        yield link","{""_create_link_from_element"": ""convert_anchor_element_attributes_to_link""}","def 0(
    element_attribs: Dict[str, Optional[str]],
    page_url: str,
    base_url: str,
) -> Optional[Link]:
    """"""
    Convert an anchor element's attributes in a simple repository page to a Link.
    """"""
    href = element_attribs.get(""href"")
    if not href:
        return None

    url = _clean_link(urllib.parse.urljoin(base_url, href))
    pyrequire = element_attribs.get(""data-requires-python"")
    yanked_reason = element_attribs.get(""data-yanked"")

    link = Link(
        url,
        comes_from=page_url,
        requires_python=pyrequire,
        yanked_reason=yanked_reason,
    )

    return link

def _determine_base_url(document: HTMLElement, page_url: str) -> str:
    """"""Determine the HTML document's base URL.

    This looks for a ``<base>`` tag in the HTML document. If present, its href
    attribute denotes the base URL of anchor tags in the document. If there is
    no such tag (or if it does not have a valid href attribute), the HTML
    file's URL is used as the base URL.

    :param document: An HTML document representation. The current
        implementation expects the result of ``html5lib.parse()``.
    :param page_url: The URL of the HTML document.

    TODO: Remove when `html5lib` is dropped.
    """"""
    for base in document.findall("".//base""):
        href = base.get(""href"")
        if href is not None:
            return href
    return page_url","def 1(page: ""HTMLPage"") -> Iterable[Link]:
    """"""
    Parse an HTML document, and yield its anchor elements as Link objects.

    TODO: Remove when `html5lib` is dropped.
    """"""
    document = html5lib.parse(
        page.content,
        transport_encoding=page.encoding,
        namespaceHTMLElements=False,
    )

    url = page.url
    base_url = _determine_base_url(document, url)
    for anchor in document.findall("".//a""):
        link = 0(
            anchor.attrib,
            page_url=url,
            base_url=base_url,
        )
        if link is None:
            continue
        yield link","{""_create_link_from_element"": ""0""}"
348,348,"def _extract_version_from_fragment(fragment: str, canonical_name: str) -> Optional[str]:
    """"""Parse the version string from a <package>+<version> filename
    ""fragment"" (stem) or egg fragment.

    :param fragment: The string to parse. E.g. foo-2.1
    :param canonical_name: The canonicalized name of the package this
        belongs to.
    """"""
    try:
        version_start = _find_name_version_sep(fragment, canonical_name) + 1
    except ValueError:
        return None
    version = fragment[version_start:]
    if not version:
        return None
    return version","def _find_name_version_sep(fragment: str, canonical_name: str) -> int:
    """"""Find the separator's index based on the package's canonical name.

    :param fragment: A <package>+<version> filename ""fragment"" (stem) or
        egg fragment.
    :param canonical_name: The package's canonical name.

    This function is needed since the canonicalized name does not necessarily
    have the same length as the egg info's name part. An example::

    >>> fragment = 'foo__bar-1.0'
    >>> canonical_name = 'foo-bar'
    >>> _find_name_version_sep(fragment, canonical_name)
    8
    """"""
    # Project name and version must be separated by one single dash. Find all
    # occurrences of dashes; if the string in front of it matches the canonical
    # name, this is the one separating the name and version parts.
    for i, c in enumerate(fragment):
        if c != ""-"":
            continue
        if canonicalize_name(fragment[:i]) == canonical_name:
            return i
    raise ValueError(f""{fragment} does not match {canonical_name}"")","def locate_separator_index_based_on_canonical_name(fragment: str, canonical_name: str) -> int:
    """"""Find the separator's index based on the package's canonical name.

    :param fragment: A <package>+<version> filename ""fragment"" (stem) or
        egg fragment.
    :param canonical_name: The package's canonical name.

    This function is needed since the canonicalized name does not necessarily
    have the same length as the egg info's name part. An example::

    >>> fragment = 'foo__bar-1.0'
    >>> canonical_name = 'foo-bar'
    >>> locate_separator_index_based_on_canonical_name(fragment, canonical_name)
    8
    """"""
    # Project name and version must be separated by one single dash. Find all
    # occurrences of dashes; if the string in front of it matches the canonical
    # name, this is the one separating the name and version parts.
    for i, c in enumerate(fragment):
        if c != ""-"":
            continue
        if canonicalize_name(fragment[:i]) == canonical_name:
            return i
    raise ValueError(f""{fragment} does not match {canonical_name}"")","def _extract_version_from_fragment(fragment: str, canonical_name: str) -> Optional[str]:
    """"""Parse the version string from a <package>+<version> filename
    ""fragment"" (stem) or egg fragment.

    :param fragment: The string to parse. E.g. foo-2.1
    :param canonical_name: The canonicalized name of the package this
        belongs to.
    """"""
    try:
        version_start = locate_separator_index_based_on_canonical_name(fragment, canonical_name) + 1
    except ValueError:
        return None
    version = fragment[version_start:]
    if not version:
        return None
    return version","{""_find_name_version_sep"": ""locate_separator_index_based_on_canonical_name""}","def 0(fragment: str, canonical_name: str) -> int:
    """"""Find the separator's index based on the package's canonical name.

    :param fragment: A <package>+<version> filename ""fragment"" (stem) or
        egg fragment.
    :param canonical_name: The package's canonical name.

    This function is needed since the canonicalized name does not necessarily
    have the same length as the egg info's name part. An example::

    >>> fragment = 'foo__bar-1.0'
    >>> canonical_name = 'foo-bar'
    >>> 0(fragment, canonical_name)
    8
    """"""
    # Project name and version must be separated by one single dash. Find all
    # occurrences of dashes; if the string in front of it matches the canonical
    # name, this is the one separating the name and version parts.
    for i, c in enumerate(fragment):
        if c != ""-"":
            continue
        if canonicalize_name(fragment[:i]) == canonical_name:
            return i
    raise ValueError(f""{fragment} does not match {canonical_name}"")","def _extract_version_from_fragment(fragment: str, canonical_name: str) -> Optional[str]:
    """"""Parse the version string from a <package>+<version> filename
    ""fragment"" (stem) or egg fragment.

    :param fragment: The string to parse. E.g. foo-2.1
    :param canonical_name: The canonicalized name of the package this
        belongs to.
    """"""
    try:
        version_start = 0(fragment, canonical_name) + 1
    except ValueError:
        return None
    version = fragment[version_start:]
    if not version:
        return None
    return version","{""_find_name_version_sep"": ""0""}"
349,349,"def get_similar_commands(name: str) -> Optional[str]:
    """"""Command name auto-correct.""""""
    from difflib import get_close_matches

    name = name.lower()

    close_commands = get_close_matches(name, commands_dict.keys())

    if close_commands:
        return close_commands[0]
    else:
        return None","def get_close_matches(word, possibilities, n=3, cutoff=0.6):
    """"""Use SequenceMatcher to return list of the best ""good enough"" matches.

    word is a sequence for which close matches are desired (typically a
    string).

    possibilities is a list of sequences against which to match word
    (typically a list of strings).

    Optional arg n (default 3) is the maximum number of close matches to
    return.  n must be > 0.

    Optional arg cutoff (default 0.6) is a float in [0, 1].  Possibilities
    that don't score at least that similar to word are ignored.

    The best (no more than n) matches among the possibilities are returned
    in a list, sorted by similarity score, most similar first.

    >>> get_close_matches(""appel"", [""ape"", ""apple"", ""peach"", ""puppy""])
    ['apple', 'ape']
    >>> import keyword as _keyword
    >>> get_close_matches(""wheel"", _keyword.kwlist)
    ['while']
    >>> get_close_matches(""Apple"", _keyword.kwlist)
    []
    >>> get_close_matches(""accept"", _keyword.kwlist)
    ['except']
    """"""

    if not n >  0:
        raise ValueError(""n must be > 0: %r"" % (n,))
    if not 0.0 <= cutoff <= 1.0:
        raise ValueError(""cutoff must be in [0.0, 1.0]: %r"" % (cutoff,))
    result = []
    s = SequenceMatcher()
    s.set_seq2(word)
    for x in possibilities:
        s.set_seq1(x)
        if s.real_quick_ratio() >= cutoff and \
           s.quick_ratio() >= cutoff and \
           s.ratio() >= cutoff:
            result.append((s.ratio(), x))

    # Move the best scorers to head of list
    result = _nlargest(n, result)
    # Strip scores for the best n matches
    return [x for score, x in result]","def retrieve_similar_matches_using_sequence_matcher(word, possibilities, n=3, cutoff=0.6):
    """"""Use SequenceMatcher to return list of the best ""good enough"" matches.

    word is a sequence for which close matches are desired (typically a
    string).

    possibilities is a list of sequences against which to match word
    (typically a list of strings).

    Optional arg n (default 3) is the maximum number of close matches to
    return.  n must be > 0.

    Optional arg cutoff (default 0.6) is a float in [0, 1].  Possibilities
    that don't score at least that similar to word are ignored.

    The best (no more than n) matches among the possibilities are returned
    in a list, sorted by similarity score, most similar first.

    >>> retrieve_similar_matches_using_sequence_matcher(""appel"", [""ape"", ""apple"", ""peach"", ""puppy""])
    ['apple', 'ape']
    >>> import keyword as _keyword
    >>> retrieve_similar_matches_using_sequence_matcher(""wheel"", _keyword.kwlist)
    ['while']
    >>> retrieve_similar_matches_using_sequence_matcher(""Apple"", _keyword.kwlist)
    []
    >>> retrieve_similar_matches_using_sequence_matcher(""accept"", _keyword.kwlist)
    ['except']
    """"""

    if not n >  0:
        raise ValueError(""n must be > 0: %r"" % (n,))
    if not 0.0 <= cutoff <= 1.0:
        raise ValueError(""cutoff must be in [0.0, 1.0]: %r"" % (cutoff,))
    result = []
    s = SequenceMatcher()
    s.set_seq2(word)
    for x in possibilities:
        s.set_seq1(x)
        if s.real_quick_ratio() >= cutoff and \
           s.quick_ratio() >= cutoff and \
           s.ratio() >= cutoff:
            result.append((s.ratio(), x))

    # Move the best scorers to head of list
    result = _nlargest(n, result)
    # Strip scores for the best n matches
    return [x for score, x in result]","def get_similar_commands(name: str) -> Optional[str]:
    """"""Command name auto-correct.""""""
    from difflib import retrieve_similar_matches_using_sequence_matcher

    name = name.lower()

    close_commands = retrieve_similar_matches_using_sequence_matcher(name, commands_dict.keys())

    if close_commands:
        return close_commands[0]
    else:
        return None","{""get_close_matches"": ""retrieve_similar_matches_using_sequence_matcher""}","def 0(word, possibilities, n=3, cutoff=0.6):
    """"""Use SequenceMatcher to return list of the best ""good enough"" matches.

    word is a sequence for which close matches are desired (typically a
    string).

    possibilities is a list of sequences against which to match word
    (typically a list of strings).

    Optional arg n (default 3) is the maximum number of close matches to
    return.  n must be > 0.

    Optional arg cutoff (default 0.6) is a float in [0, 1].  Possibilities
    that don't score at least that similar to word are ignored.

    The best (no more than n) matches among the possibilities are returned
    in a list, sorted by similarity score, most similar first.

    >>> 0(""appel"", [""ape"", ""apple"", ""peach"", ""puppy""])
    ['apple', 'ape']
    >>> import keyword as _keyword
    >>> 0(""wheel"", _keyword.kwlist)
    ['while']
    >>> 0(""Apple"", _keyword.kwlist)
    []
    >>> 0(""accept"", _keyword.kwlist)
    ['except']
    """"""

    if not n >  0:
        raise ValueError(""n must be > 0: %r"" % (n,))
    if not 0.0 <= cutoff <= 1.0:
        raise ValueError(""cutoff must be in [0.0, 1.0]: %r"" % (cutoff,))
    result = []
    s = SequenceMatcher()
    s.set_seq2(word)
    for x in possibilities:
        s.set_seq1(x)
        if s.real_quick_ratio() >= cutoff and \
           s.quick_ratio() >= cutoff and \
           s.ratio() >= cutoff:
            result.append((s.ratio(), x))

    # Move the best scorers to head of list
    result = _nlargest(n, result)
    # Strip scores for the best n matches
    return [x for score, x in result]","def get_similar_commands(name: str) -> Optional[str]:
    """"""Command name auto-correct.""""""
    from difflib import get_close_matches

    name = name.lower()

    close_commands = 0(name, commands_dict.keys())

    if close_commands:
        return close_commands[0]
    else:
        return None","{""get_close_matches"": ""0""}"
350,350,"def transform_hits(hits: List[Dict[str, str]]) -> List[""TransformedHit""]:
    """"""
    The list from pypi is really a list of versions. We want a list of
    packages with the list of versions stored inline. This converts the
    list from pypi into one we can use.
    """"""
    packages: Dict[str, ""TransformedHit""] = OrderedDict()
    for hit in hits:
        name = hit[""name""]
        summary = hit[""summary""]
        version = hit[""version""]

        if name not in packages.keys():
            packages[name] = {
                ""name"": name,
                ""summary"": summary,
                ""versions"": [version],
            }
        else:
            packages[name][""versions""].append(version)

            # if this is the highest version, replace summary and score
            if version == highest_version(packages[name][""versions""]):
                packages[name][""summary""] = summary

    return list(packages.values())","def highest_version(versions: List[str]) -> str:
    return max(versions, key=parse_version)","def highest_version(versions: List[str]) -> str:
    return max(versions, key=parse_version)","def convert_list_of_versions_into_list_of_packages(hits: List[Dict[str, str]]) -> List[""TransformedHit""]:
    """"""
    The list from pypi is really a list of versions. We want a list of
    packages with the list of versions stored inline. This converts the
    list from pypi into one we can use.
    """"""
    packages: Dict[str, ""TransformedHit""] = OrderedDict()
    for hit in hits:
        name = hit[""name""]
        summary = hit[""summary""]
        version = hit[""version""]

        if name not in packages.keys():
            packages[name] = {
                ""name"": name,
                ""summary"": summary,
                ""versions"": [version],
            }
        else:
            packages[name][""versions""].append(version)

            # if this is the highest version, replace summary and score
            if version == highest_version(packages[name][""versions""]):
                packages[name][""summary""] = summary

    return list(packages.values())",{},"def highest_version(versions: List[str]) -> str:
    return max(versions, key=parse_version)","def 1(hits: List[Dict[str, str]]) -> List[""TransformedHit""]:
    """"""
    The list from pypi is really a list of versions. We want a list of
    packages with the list of versions stored inline. This converts the
    list from pypi into one we can use.
    """"""
    packages: Dict[str, ""TransformedHit""] = OrderedDict()
    for hit in hits:
        name = hit[""name""]
        summary = hit[""summary""]
        version = hit[""version""]

        if name not in packages.keys():
            packages[name] = {
                ""name"": name,
                ""summary"": summary,
                ""versions"": [version],
            }
        else:
            packages[name][""versions""].append(version)

            # if this is the highest version, replace summary and score
            if version == highest_version(packages[name][""versions""]):
                packages[name][""summary""] = summary

    return list(packages.values())",{}
351,351,"def _import_module(name):
    """"""Import module, returning the module after the last dot.""""""
    __import__(name)
    return sys.modules[name]","def __import__(name, globals=None, locals=None, fromlist=(), level=0):
    """"""Import a module.

    The 'globals' argument is used to infer where the import is occurring from
    to handle relative imports. The 'locals' argument is ignored. The
    'fromlist' argument specifies what should exist as attributes on the module
    being imported (e.g. ``from module import <fromlist>``).  The 'level'
    argument represents the package location to import from in a relative
    import (e.g. ``from ..pkg import mod`` would have a 'level' of 2).

    """"""
    if level == 0:
        module = _gcd_import(name)
    else:
        globals_ = globals if globals is not None else {}
        package = _calc___package__(globals_)
        module = _gcd_import(name, package, level)
    if not fromlist:
        # Return up to the first dot in 'name'. This is complicated by the fact
        # that 'name' may be relative.
        if level == 0:
            return _gcd_import(name.partition('.')[0])
        elif not name:
            return module
        else:
            # Figure out where to slice the module's name up to the first dot
            # in 'name'.
            cut_off = len(name) - len(name.partition('.')[0])
            # Slice end needs to be positive to alleviate need to special-case
            # when ``'.' not in name``.
            return sys.modules[module.__name__[:len(module.__name__)-cut_off]]
    elif hasattr(module, '__path__'):
        return _handle_fromlist(module, fromlist, _gcd_import)
    else:
        return module","def import_module(name, globals=None, locals=None, fromlist=(), level=0):
    """"""Import a module.

    The 'globals' argument is used to infer where the import is occurring from
    to handle relative imports. The 'locals' argument is ignored. The
    'fromlist' argument specifies what should exist as attributes on the module
    being imported (e.g. ``from module import <fromlist>``).  The 'level'
    argument represents the package location to import from in a relative
    import (e.g. ``from ..pkg import mod`` would have a 'level' of 2).

    """"""
    if level == 0:
        module = _gcd_import(name)
    else:
        globals_ = globals if globals is not None else {}
        package = _calc___package__(globals_)
        module = _gcd_import(name, package, level)
    if not fromlist:
        # Return up to the first dot in 'name'. This is complicated by the fact
        # that 'name' may be relative.
        if level == 0:
            return _gcd_import(name.partition('.')[0])
        elif not name:
            return module
        else:
            # Figure out where to slice the module's name up to the first dot
            # in 'name'.
            cut_off = len(name) - len(name.partition('.')[0])
            # Slice end needs to be positive to alleviate need to special-case
            # when ``'.' not in name``.
            return sys.modules[module.__name__[:len(module.__name__)-cut_off]]
    elif hasattr(module, '__path__'):
        return _handle_fromlist(module, fromlist, _gcd_import)
    else:
        return module","def import_module_and_return_last_module(name):
    """"""Import module, returning the module after the last dot.""""""
    import_module(name)
    return sys.modules[name]","{""__import__"": ""import_module""}","def 0(name, globals=None, locals=None, fromlist=(), level=0):
    """"""Import a module.

    The 'globals' argument is used to infer where the import is occurring from
    to handle relative imports. The 'locals' argument is ignored. The
    'fromlist' argument specifies what should exist as attributes on the module
    being imported (e.g. ``from module import <fromlist>``).  The 'level'
    argument represents the package location to import from in a relative
    import (e.g. ``from ..pkg import mod`` would have a 'level' of 2).

    """"""
    if level == 0:
        module = _gcd_import(name)
    else:
        globals_ = globals if globals is not None else {}
        package = _calc___package__(globals_)
        module = _gcd_import(name, package, level)
    if not fromlist:
        # Return up to the first dot in 'name'. This is complicated by the fact
        # that 'name' may be relative.
        if level == 0:
            return _gcd_import(name.partition('.')[0])
        elif not name:
            return module
        else:
            # Figure out where to slice the module's name up to the first dot
            # in 'name'.
            cut_off = len(name) - len(name.partition('.')[0])
            # Slice end needs to be positive to alleviate need to special-case
            # when ``'.' not in name``.
            return sys.modules[module.__name__[:len(module.__name__)-cut_off]]
    elif hasattr(module, '__path__'):
        return _handle_fromlist(module, fromlist, _gcd_import)
    else:
        return module","def 1(name):
    """"""Import module, returning the module after the last dot.""""""
    0(name)
    return sys.modules[name]","{""__import__"": ""0""}"
352,352,"def lookup(label):
    """"""
    Look for an encoding by its label.
    This is the specs `get an encoding
    <http://encoding.spec.whatwg.org/#concept-encoding-get>`_ algorithm.
    Supported labels are listed there.

    :param label: A string.
    :returns:
        An :class:`Encoding` object, or :obj:`None` for an unknown label.

    """"""
    # Only strip ASCII whitespace: U+0009, U+000A, U+000C, U+000D, and U+0020.
    label = ascii_lower(label.strip('\t\n\f\r '))
    name = LABELS.get(label)
    if name is None:
        return None
    encoding = CACHE.get(name)
    if encoding is None:
        if name == 'x-user-defined':
            from .x_user_defined import codec_info
        else:
            python_name = PYTHON_NAMES.get(name, name)
            # Any python_name value that gets to here should be valid.
            codec_info = codecs.lookup(python_name)
        encoding = Encoding(name, codec_info)
        CACHE[name] = encoding
    return encoding","def ascii_lower(string):
    r""""""Transform (only) ASCII letters to lower case: A-Z is mapped to a-z.

    :param string: An Unicode string.
    :returns: A new Unicode string.

    This is used for `ASCII case-insensitive
    <http://encoding.spec.whatwg.org/#ascii-case-insensitive>`_
    matching of encoding labels.
    The same matching is also used, among other things,
    for `CSS keywords <http://dev.w3.org/csswg/css-values/#keywords>`_.

    This is different from the :meth:`~py:str.lower` method of Unicode strings
    which also affect non-ASCII characters,
    sometimes mapping them into the ASCII range:

        >>> keyword = u'Bac\N{KELVIN SIGN}ground'
        >>> assert keyword.lower() == u'background'
        >>> assert ascii_lower(keyword) != keyword.lower()
        >>> assert ascii_lower(keyword) == u'bac\N{KELVIN SIGN}ground'

    """"""
    # This turns out to be faster than unicode.translate()
    return string.encode('utf8').lower().decode('utf8')","def transform_ascii_to_lower_with_unicode_support(string):
    r""""""Transform (only) ASCII letters to lower case: A-Z is mapped to a-z.

    :param string: An Unicode string.
    :returns: A new Unicode string.

    This is used for `ASCII case-insensitive
    <http://encoding.spec.whatwg.org/#ascii-case-insensitive>`_
    matching of encoding labels.
    The same matching is also used, among other things,
    for `CSS keywords <http://dev.w3.org/csswg/css-values/#keywords>`_.

    This is different from the :meth:`~py:str.lower` method of Unicode strings
    which also affect non-ASCII characters,
    sometimes mapping them into the ASCII range:

        >>> keyword = u'Bac\N{KELVIN SIGN}ground'
        >>> assert keyword.lower() == u'background'
        >>> assert transform_ascii_to_lower_with_unicode_support(keyword) != keyword.lower()
        >>> assert transform_ascii_to_lower_with_unicode_support(keyword) == u'bac\N{KELVIN SIGN}ground'

    """"""
    # This turns out to be faster than unicode.translate()
    return string.encode('utf8').lower().decode('utf8')","def look_for_encoding_by_label(label):
    """"""
    Look for an encoding by its label.
    This is the specs `get an encoding
    <http://encoding.spec.whatwg.org/#concept-encoding-get>`_ algorithm.
    Supported labels are listed there.

    :param label: A string.
    :returns:
        An :class:`Encoding` object, or :obj:`None` for an unknown label.

    """"""
    # Only strip ASCII whitespace: U+0009, U+000A, U+000C, U+000D, and U+0020.
    label = transform_ascii_to_lower_with_unicode_support(label.strip('\t\n\f\r '))
    name = LABELS.get(label)
    if name is None:
        return None
    encoding = CACHE.get(name)
    if encoding is None:
        if name == 'x-user-defined':
            from .x_user_defined import codec_info
        else:
            python_name = PYTHON_NAMES.get(name, name)
            # Any python_name value that gets to here should be valid.
            codec_info = codecs.look_for_encoding_by_label(python_name)
        encoding = Encoding(name, codec_info)
        CACHE[name] = encoding
    return encoding","{""ascii_lower"": ""transform_ascii_to_lower_with_unicode_support""}","def 0(string):
    r""""""Transform (only) ASCII letters to lower case: A-Z is mapped to a-z.

    :param string: An Unicode string.
    :returns: A new Unicode string.

    This is used for `ASCII case-insensitive
    <http://encoding.spec.whatwg.org/#ascii-case-insensitive>`_
    matching of encoding labels.
    The same matching is also used, among other things,
    for `CSS keywords <http://dev.w3.org/csswg/css-values/#keywords>`_.

    This is different from the :meth:`~py:str.lower` method of Unicode strings
    which also affect non-ASCII characters,
    sometimes mapping them into the ASCII range:

        >>> keyword = u'Bac\N{KELVIN SIGN}ground'
        >>> assert keyword.lower() == u'background'
        >>> assert 0(keyword) != keyword.lower()
        >>> assert 0(keyword) == u'bac\N{KELVIN SIGN}ground'

    """"""
    # This turns out to be faster than unicode.translate()
    return string.encode('utf8').lower().decode('utf8')","def 1(label):
    """"""
    Look for an encoding by its label.
    This is the specs `get an encoding
    <http://encoding.spec.whatwg.org/#concept-encoding-get>`_ algorithm.
    Supported labels are listed there.

    :param label: A string.
    :returns:
        An :class:`Encoding` object, or :obj:`None` for an unknown label.

    """"""
    # Only strip ASCII whitespace: U+0009, U+000A, U+000C, U+000D, and U+0020.
    label = 0(label.strip('\t\n\f\r '))
    name = LABELS.get(label)
    if name is None:
        return None
    encoding = CACHE.get(name)
    if encoding is None:
        if name == 'x-user-defined':
            from .x_user_defined import codec_info
        else:
            python_name = PYTHON_NAMES.get(name, name)
            # Any python_name value that gets to here should be valid.
            codec_info = codecs.1(python_name)
        encoding = Encoding(name, codec_info)
        CACHE[name] = encoding
    return encoding","{""ascii_lower"": ""0""}"
353,353,"def iter_decode(input, fallback_encoding, errors='replace'):
    """"""
    ""Pull""-based decoder.

    :param input:
        An iterable of byte strings.

        The input is first consumed just enough to determine the encoding
        based on the precense of a BOM,
        then consumed on demand when the return value is.
    :param fallback_encoding:
        An :class:`Encoding` object or a label string.
        The encoding to use if :obj:`input` does note have a BOM.
    :param errors: Type of error handling. See :func:`codecs.register`.
    :raises: :exc:`~exceptions.LookupError` for an unknown encoding label.
    :returns:
        An ``(output, encoding)`` tuple.
        :obj:`output` is an iterable of Unicode strings,
        :obj:`encoding` is the :obj:`Encoding` that is being used.

    """"""

    decoder = IncrementalDecoder(fallback_encoding, errors)
    generator = _iter_decode_generator(input, decoder)
    encoding = next(generator)
    return generator, encoding","def _iter_decode_generator(input, decoder):
    """"""Return a generator that first yields the :obj:`Encoding`,
    then yields output chukns as Unicode strings.

    """"""
    decode = decoder.decode
    input = iter(input)
    for chunck in input:
        output = decode(chunck)
        if output:
            assert decoder.encoding is not None
            yield decoder.encoding
            yield output
            break
    else:
        # Input exhausted without determining the encoding
        output = decode(b'', final=True)
        assert decoder.encoding is not None
        yield decoder.encoding
        if output:
            yield output
        return

    for chunck in input:
        output = decode(chunck)
        if output:
            yield output
    output = decode(b'', final=True)
    if output:
        yield output","def generate_decoded_output_iterator_using_decoder(input, decoder):
    """"""Return a generator that first yields the :obj:`Encoding`,
    then yields output chukns as Unicode strings.

    """"""
    decode = decoder.decode
    input = iter(input)
    for chunck in input:
        output = decode(chunck)
        if output:
            assert decoder.encoding is not None
            yield decoder.encoding
            yield output
            break
    else:
        # Input exhausted without determining the encoding
        output = decode(b'', final=True)
        assert decoder.encoding is not None
        yield decoder.encoding
        if output:
            yield output
        return

    for chunck in input:
        output = decode(chunck)
        if output:
            yield output
    output = decode(b'', final=True)
    if output:
        yield output","def pull_based_decoder_for_unicode_strings(input, fallback_encoding, errors='replace'):
    """"""
    ""Pull""-based decoder.

    :param input:
        An iterable of byte strings.

        The input is first consumed just enough to determine the encoding
        based on the precense of a BOM,
        then consumed on demand when the return value is.
    :param fallback_encoding:
        An :class:`Encoding` object or a label string.
        The encoding to use if :obj:`input` does note have a BOM.
    :param errors: Type of error handling. See :func:`codecs.register`.
    :raises: :exc:`~exceptions.LookupError` for an unknown encoding label.
    :returns:
        An ``(output, encoding)`` tuple.
        :obj:`output` is an iterable of Unicode strings,
        :obj:`encoding` is the :obj:`Encoding` that is being used.

    """"""

    decoder = IncrementalDecoder(fallback_encoding, errors)
    generator = generate_decoded_output_iterator_using_decoder(input, decoder)
    encoding = next(generator)
    return generator, encoding","{""_iter_decode_generator"": ""generate_decoded_output_iterator_using_decoder""}","def 0(input, decoder):
    """"""Return a generator that first yields the :obj:`Encoding`,
    then yields output chukns as Unicode strings.

    """"""
    decode = decoder.decode
    input = iter(input)
    for chunck in input:
        output = decode(chunck)
        if output:
            assert decoder.encoding is not None
            yield decoder.encoding
            yield output
            break
    else:
        # Input exhausted without determining the encoding
        output = decode(b'', final=True)
        assert decoder.encoding is not None
        yield decoder.encoding
        if output:
            yield output
        return

    for chunck in input:
        output = decode(chunck)
        if output:
            yield output
    output = decode(b'', final=True)
    if output:
        yield output","def 1(input, fallback_encoding, errors='replace'):
    """"""
    ""Pull""-based decoder.

    :param input:
        An iterable of byte strings.

        The input is first consumed just enough to determine the encoding
        based on the precense of a BOM,
        then consumed on demand when the return value is.
    :param fallback_encoding:
        An :class:`Encoding` object or a label string.
        The encoding to use if :obj:`input` does note have a BOM.
    :param errors: Type of error handling. See :func:`codecs.register`.
    :raises: :exc:`~exceptions.LookupError` for an unknown encoding label.
    :returns:
        An ``(output, encoding)`` tuple.
        :obj:`output` is an iterable of Unicode strings,
        :obj:`encoding` is the :obj:`Encoding` that is being used.

    """"""

    decoder = IncrementalDecoder(fallback_encoding, errors)
    generator = 0(input, decoder)
    encoding = next(generator)
    return generator, encoding","{""_iter_decode_generator"": ""0""}"
354,354,"def iter_encode(input, encoding=UTF8, errors='strict'):
    """"""
    Pull-based encoder.

    :param input: An iterable of Unicode strings.
    :param encoding: An :class:`Encoding` object or a label string.
    :param errors: Type of error handling. See :func:`codecs.register`.
    :raises: :exc:`~exceptions.LookupError` for an unknown encoding label.
    :returns: An iterable of byte strings.

    """"""
    # Fail early if `encoding` is an invalid label.
    encode = IncrementalEncoder(encoding, errors).encode
    return _iter_encode_generator(input, encode)","def _iter_encode_generator(input, encode):
    for chunck in input:
        output = encode(chunck)
        if output:
            yield output
    output = encode('', final=True)
    if output:
        yield output","def iterate_and_encode_generator(input, encode):
    for chunck in input:
        output = encode(chunck)
        if output:
            yield output
    output = encode('', final=True)
    if output:
        yield output","def pull_based_encoder_for_unicode_strings(input, encoding=UTF8, errors='strict'):
    """"""
    Pull-based encoder.

    :param input: An iterable of Unicode strings.
    :param encoding: An :class:`Encoding` object or a label string.
    :param errors: Type of error handling. See :func:`codecs.register`.
    :raises: :exc:`~exceptions.LookupError` for an unknown encoding label.
    :returns: An iterable of byte strings.

    """"""
    # Fail early if `encoding` is an invalid label.
    encode = IncrementalEncoder(encoding, errors).encode
    return iterate_and_encode_generator(input, encode)","{""_iter_encode_generator"": ""iterate_and_encode_generator""}","def 0(input, encode):
    for chunck in input:
        output = encode(chunck)
        if output:
            yield output
    output = encode('', final=True)
    if output:
        yield output","def 1(input, encoding=UTF8, errors='strict'):
    """"""
    Pull-based encoder.

    :param input: An iterable of Unicode strings.
    :param encoding: An :class:`Encoding` object or a label string.
    :param errors: Type of error handling. See :func:`codecs.register`.
    :raises: :exc:`~exceptions.LookupError` for an unknown encoding label.
    :returns: An iterable of byte strings.

    """"""
    # Fail early if `encoding` is an invalid label.
    encode = IncrementalEncoder(encoding, errors).encode
    return 0(input, encode)","{""_iter_encode_generator"": ""0""}"
355,355,"def get_filetype_from_buffer(buf, max_lines=5):
    """"""
    Scan the buffer for modelines and return filetype if one is found.
    """"""
    lines = buf.splitlines()
    for l in lines[-1:-max_lines-1:-1]:
        ret = get_filetype_from_line(l)
        if ret:
            return ret
    for i in range(max_lines, -1, -1):
        if i < len(lines):
            ret = get_filetype_from_line(lines[i])
            if ret:
                return ret

    return None","def get_filetype_from_line(l):
    m = modeline_re.search(l)
    if m:
        return m.group(1)","def determine_filetype_from_last_line(l):
    m = modeline_re.search(l)
    if m:
        return m.group(1)","def determine_filetype_from_buffer(buf, max_lines=5):
    """"""
    Scan the buffer for modelines and return filetype if one is found.
    """"""
    lines = buf.splitlines()
    for l in lines[-1:-max_lines-1:-1]:
        ret = determine_filetype_from_last_line(l)
        if ret:
            return ret
    for i in range(max_lines, -1, -1):
        if i < len(lines):
            ret = determine_filetype_from_last_line(lines[i])
            if ret:
                return ret

    return None","{""get_filetype_from_line"": ""determine_filetype_from_last_line""}","def 0(l):
    m = modeline_re.search(l)
    if m:
        return m.group(1)","def 1(buf, max_lines=5):
    """"""
    Scan the buffer for modelines and return filetype if one is found.
    """"""
    lines = buf.splitlines()
    for l in lines[-1:-max_lines-1:-1]:
        ret = 0(l)
        if ret:
            return ret
    for i in range(max_lines, -1, -1):
        if i < len(lines):
            ret = 0(lines[i])
            if ret:
                return ret

    return None","{""get_filetype_from_line"": ""0""}"
356,356,"def html_doctype_matches(text):
    """"""Check if the file looks like it has a html doctype.""""""
    return doctype_matches(text, r'html')","def doctype_matches(text, regex):
    """"""Check if the doctype matches a regular expression (if present).

    Note that this method only checks the first part of a DOCTYPE.
    eg: 'html PUBLIC ""-//W3C//DTD XHTML 1.0 Strict//EN""'
    """"""
    m = doctype_lookup_re.search(text)
    if m is None:
        return False
    doctype = m.group(1)
    return re.compile(regex, re.I).match(doctype.strip()) is not None","def check_if_doctype_matches_regex(text, regex):
    """"""Check if the doctype matches a regular expression (if present).

    Note that this method only checks the first part of a DOCTYPE.
    eg: 'html PUBLIC ""-//W3C//DTD XHTML 1.0 Strict//EN""'
    """"""
    m = doctype_lookup_re.search(text)
    if m is None:
        return False
    doctype = m.group(1)
    return re.compile(regex, re.I).match(doctype.strip()) is not None","def html_check_if_doctype_matches_regex(text):
    """"""Check if the file looks like it has a html doctype.""""""
    return check_if_doctype_matches_regex(text, r'html')","{""doctype_matches"": ""check_if_doctype_matches_regex""}","def 0(text, regex):
    """"""Check if the doctype matches a regular expression (if present).

    Note that this method only checks the first part of a DOCTYPE.
    eg: 'html PUBLIC ""-//W3C//DTD XHTML 1.0 Strict//EN""'
    """"""
    m = doctype_lookup_re.search(text)
    if m is None:
        return False
    doctype = m.group(1)
    return re.compile(regex, re.I).match(doctype.strip()) is not None","def html_0(text):
    """"""Check if the file looks like it has a html doctype.""""""
    return 0(text, r'html')","{""doctype_matches"": ""0""}"
357,357,"def guess_decode_from_terminal(text, term):
    """"""Decode *text* coming from terminal *term*.

    First try the terminal encoding, if given.
    Then try UTF-8.  Then try the preferred locale encoding.
    Fall back to latin-1, which always works.
    """"""
    if getattr(term, 'encoding', None):
        try:
            text = text.decode(term.encoding)
        except UnicodeDecodeError:
            pass
        else:
            return text, term.encoding
    return guess_decode(text)","def guess_decode(text):
    """"""Decode *text* with guessed encoding.

    First try UTF-8; this should fail for non-UTF-8 encodings.
    Then try the preferred locale encoding.
    Fall back to latin-1, which always works.
    """"""
    try:
        text = text.decode('utf-8')
        return text, 'utf-8'
    except UnicodeDecodeError:
        try:
            import locale
            prefencoding = locale.getpreferredencoding()
            text = text.decode()
            return text, prefencoding
        except (UnicodeDecodeError, LookupError):
            text = text.decode('latin1')
            return text, 'latin1'","def attempt_decoding_with_guess(text):
    """"""Decode *text* with guessed encoding.

    First try UTF-8; this should fail for non-UTF-8 encodings.
    Then try the preferred locale encoding.
    Fall back to latin-1, which always works.
    """"""
    try:
        text = text.decode('utf-8')
        return text, 'utf-8'
    except UnicodeDecodeError:
        try:
            import locale
            prefencoding = locale.getpreferredencoding()
            text = text.decode()
            return text, prefencoding
        except (UnicodeDecodeError, LookupError):
            text = text.decode('latin1')
            return text, 'latin1'","def attempt_decoding_with_guess_from_terminal(text, term):
    """"""Decode *text* coming from terminal *term*.

    First try the terminal encoding, if given.
    Then try UTF-8.  Then try the preferred locale encoding.
    Fall back to latin-1, which always works.
    """"""
    if getattr(term, 'encoding', None):
        try:
            text = text.decode(term.encoding)
        except UnicodeDecodeError:
            pass
        else:
            return text, term.encoding
    return attempt_decoding_with_guess(text)","{""guess_decode"": ""attempt_decoding_with_guess""}","def 0(text):
    """"""Decode *text* with guessed encoding.

    First try UTF-8; this should fail for non-UTF-8 encodings.
    Then try the preferred locale encoding.
    Fall back to latin-1, which always works.
    """"""
    try:
        text = text.decode('utf-8')
        return text, 'utf-8'
    except UnicodeDecodeError:
        try:
            import locale
            prefencoding = locale.getpreferredencoding()
            text = text.decode()
            return text, prefencoding
        except (UnicodeDecodeError, LookupError):
            text = text.decode('latin1')
            return text, 'latin1'","def 1(text, term):
    """"""Decode *text* coming from terminal *term*.

    First try the terminal encoding, if given.
    Then try UTF-8.  Then try the preferred locale encoding.
    Fall back to latin-1, which always works.
    """"""
    if getattr(term, 'encoding', None):
        try:
            text = text.decode(term.encoding)
        except UnicodeDecodeError:
            pass
        else:
            return text, term.encoding
    return 0(text)","{""guess_decode"": ""0""}"
358,358,"def bygroups(*args):
    """"""
    Callback that yields multiple actions for each group in the match.
    """"""
    def callback(lexer, match, ctx=None):
        for i, action in enumerate(args):
            if action is None:
                continue
            elif type(action) is _TokenType:
                data = match.group(i + 1)
                if data:
                    yield match.start(i + 1), action, data
            else:
                data = match.group(i + 1)
                if data is not None:
                    if ctx:
                        ctx.pos = match.start(i + 1)
                    for item in action(lexer,
                                       _PseudoMatch(match.start(i + 1), data), ctx):
                        if item:
                            yield item
        if ctx:
            ctx.pos = match.end()
    return callback","def enumerate():
    """"""Return a list of all Thread objects currently alive.

    The list includes daemonic threads, dummy thread objects created by
    current_thread(), and the main thread. It excludes terminated threads and
    threads that have not yet been started.

    """"""
    with _active_limbo_lock:
        return list(_active.values()) + list(_limbo.values())","def get_alive_threads():
    """"""Return a list of all Thread objects currently alive.

    The list includes daemonic threads, dummy thread objects created by
    current_thread(), and the main thread. It excludes terminated threads and
    threads that have not yet been started.

    """"""
    with _active_limbo_lock:
        return list(_active.values()) + list(_limbo.values())","def callback_that_processes_matched_groups_that_yields_actions_for_each_group_in_match(*args):
    """"""
    Callback that yields multiple actions for each group in the match.
    """"""
    def callback_that_processes_matched_groups(lexer, match, ctx=None):
        for i, action in get_alive_threads(args):
            if action is None:
                continue
            elif type(action) is _TokenType:
                data = match.group(i + 1)
                if data:
                    yield match.start(i + 1), action, data
            else:
                data = match.group(i + 1)
                if data is not None:
                    if ctx:
                        ctx.pos = match.start(i + 1)
                    for item in action(lexer,
                                       _PseudoMatch(match.start(i + 1), data), ctx):
                        if item:
                            yield item
        if ctx:
            ctx.pos = match.end()
    return callback_that_processes_matched_groups","{""enumerate"": ""get_alive_threads""}","def 0():
    """"""Return a list of all Thread objects currently alive.

    The list includes daemonic threads, dummy thread objects created by
    current_thread(), and the main thread. It excludes terminated threads and
    threads that have not yet been started.

    """"""
    with _active_limbo_lock:
        return list(_active.values()) + list(_limbo.values())","def 1(*args):
    """"""
    Callback that yields multiple actions for each group in the match.
    """"""
    def 2(lexer, match, ctx=None):
        for i, action in 0(args):
            if action is None:
                continue
            elif type(action) is _TokenType:
                data = match.group(i + 1)
                if data:
                    yield match.start(i + 1), action, data
            else:
                data = match.group(i + 1)
                if data is not None:
                    if ctx:
                        ctx.pos = match.start(i + 1)
                    for item in action(lexer,
                                       _PseudoMatch(match.start(i + 1), data), ctx):
                        if item:
                            yield item
        if ctx:
            ctx.pos = match.end()
    return callback","{""enumerate"": ""0""}"
359,359,"def regex_opt(strings, prefix='', suffix=''):
    """"""Return a compiled regex that matches any string in the given list.

    The strings to match must be literal strings, not regexes.  They will be
    regex-escaped.

    *prefix* and *suffix* are pre- and appended to the final regex.
    """"""
    strings = sorted(strings)
    return prefix + regex_opt_inner(strings, '(') + suffix","def regex_opt_inner(strings, open_paren):
    """"""Return a regex that matches any string in the sorted list of strings.""""""
    close_paren = open_paren and ')' or ''
    # print strings, repr(open_paren)
    if not strings:
        # print '-> nothing left'
        return ''
    first = strings[0]
    if len(strings) == 1:
        # print '-> only 1 string'
        return open_paren + escape(first) + close_paren
    if not first:
        # print '-> first string empty'
        return open_paren + regex_opt_inner(strings[1:], '(?:') \
            + '?' + close_paren
    if len(first) == 1:
        # multiple one-char strings? make a charset
        oneletter = []
        rest = []
        for s in strings:
            if len(s) == 1:
                oneletter.append(s)
            else:
                rest.append(s)
        if len(oneletter) > 1:  # do we have more than one oneletter string?
            if rest:
                # print '-> 1-character + rest'
                return open_paren + regex_opt_inner(rest, '') + '|' \
                    + make_charset(oneletter) + close_paren
            # print '-> only 1-character'
            return open_paren + make_charset(oneletter) + close_paren
    prefix = commonprefix(strings)
    if prefix:
        plen = len(prefix)
        # we have a prefix for all strings
        # print '-> prefix:', prefix
        return open_paren + escape(prefix) \
            + regex_opt_inner([s[plen:] for s in strings], '(?:') \
            + close_paren
    # is there a suffix?
    strings_rev = [s[::-1] for s in strings]
    suffix = commonprefix(strings_rev)
    if suffix:
        slen = len(suffix)
        # print '-> suffix:', suffix[::-1]
        return open_paren \
            + regex_opt_inner(sorted(s[:-slen] for s in strings), '(?:') \
            + escape(suffix[::-1]) + close_paren
    # recurse on common 1-string prefixes
    # print '-> last resort'
    return open_paren + \
        '|'.join(regex_opt_inner(list(group[1]), '')
                 for group in groupby(strings, lambda s: s[0] == first[0])) \
        + close_paren","def generate_inner_regex_for_string_list(strings, open_paren):
    """"""Return a regex that matches any string in the sorted list of strings.""""""
    close_paren = open_paren and ')' or ''
    # print strings, repr(open_paren)
    if not strings:
        # print '-> nothing left'
        return ''
    first = strings[0]
    if len(strings) == 1:
        # print '-> only 1 string'
        return open_paren + escape(first) + close_paren
    if not first:
        # print '-> first string empty'
        return open_paren + generate_inner_regex_for_string_list(strings[1:], '(?:') \
            + '?' + close_paren
    if len(first) == 1:
        # multiple one-char strings? make a charset
        oneletter = []
        rest = []
        for s in strings:
            if len(s) == 1:
                oneletter.append(s)
            else:
                rest.append(s)
        if len(oneletter) > 1:  # do we have more than one oneletter string?
            if rest:
                # print '-> 1-character + rest'
                return open_paren + generate_inner_regex_for_string_list(rest, '') + '|' \
                    + make_charset(oneletter) + close_paren
            # print '-> only 1-character'
            return open_paren + make_charset(oneletter) + close_paren
    prefix = commonprefix(strings)
    if prefix:
        plen = len(prefix)
        # we have a prefix for all strings
        # print '-> prefix:', prefix
        return open_paren + escape(prefix) \
            + generate_inner_regex_for_string_list([s[plen:] for s in strings], '(?:') \
            + close_paren
    # is there a suffix?
    strings_rev = [s[::-1] for s in strings]
    suffix = commonprefix(strings_rev)
    if suffix:
        slen = len(suffix)
        # print '-> suffix:', suffix[::-1]
        return open_paren \
            + generate_inner_regex_for_string_list(sorted(s[:-slen] for s in strings), '(?:') \
            + escape(suffix[::-1]) + close_paren
    # recurse on common 1-string prefixes
    # print '-> last resort'
    return open_paren + \
        '|'.join(generate_inner_regex_for_string_list(list(group[1]), '')
                 for group in groupby(strings, lambda s: s[0] == first[0])) \
        + close_paren","def compiled_regex_matching_strings_in_list(strings, prefix='', suffix=''):
    """"""Return a compiled regex that matches any string in the given list.

    The strings to match must be literal strings, not regexes.  They will be
    regex-escaped.

    *prefix* and *suffix* are pre- and appended to the final regex.
    """"""
    strings = sorted(strings)
    return prefix + generate_inner_regex_for_string_list(strings, '(') + suffix","{""regex_opt_inner"": ""generate_inner_regex_for_string_list""}","def 0(strings, open_paren):
    """"""Return a regex that matches any string in the sorted list of strings.""""""
    close_paren = open_paren and ')' or ''
    # print strings, repr(open_paren)
    if not strings:
        # print '-> nothing left'
        return ''
    first = strings[0]
    if len(strings) == 1:
        # print '-> only 1 string'
        return open_paren + escape(first) + close_paren
    if not first:
        # print '-> first string empty'
        return open_paren + 0(strings[1:], '(?:') \
            + '?' + close_paren
    if len(first) == 1:
        # multiple one-char strings? make a charset
        oneletter = []
        rest = []
        for s in strings:
            if len(s) == 1:
                oneletter.append(s)
            else:
                rest.append(s)
        if len(oneletter) > 1:  # do we have more than one oneletter string?
            if rest:
                # print '-> 1-character + rest'
                return open_paren + 0(rest, '') + '|' \
                    + make_charset(oneletter) + close_paren
            # print '-> only 1-character'
            return open_paren + make_charset(oneletter) + close_paren
    prefix = commonprefix(strings)
    if prefix:
        plen = len(prefix)
        # we have a prefix for all strings
        # print '-> prefix:', prefix
        return open_paren + escape(prefix) \
            + 0([s[plen:] for s in strings], '(?:') \
            + close_paren
    # is there a suffix?
    strings_rev = [s[::-1] for s in strings]
    suffix = commonprefix(strings_rev)
    if suffix:
        slen = len(suffix)
        # print '-> suffix:', suffix[::-1]
        return open_paren \
            + 0(sorted(s[:-slen] for s in strings), '(?:') \
            + escape(suffix[::-1]) + close_paren
    # recurse on common 1-string prefixes
    # print '-> last resort'
    return open_paren + \
        '|'.join(0(list(group[1]), '')
                 for group in groupby(strings, lambda s: s[0] == first[0])) \
        + close_paren","def 1(strings, prefix='', suffix=''):
    """"""Return a compiled regex that matches any string in the given list.

    The strings to match must be literal strings, not regexes.  They will be
    regex-escaped.

    *prefix* and *suffix* are pre- and appended to the final regex.
    """"""
    strings = sorted(strings)
    return prefix + 0(strings, '(') + suffix","{""regex_opt_inner"": ""0""}"
360,360,"def apply_filters(stream, filters, lexer=None):
    """"""
    Use this method to apply an iterable of filters to
    a stream. If lexer is given it's forwarded to the
    filter, otherwise the filter receives `None`.
    """"""
    def _apply(filter_, stream):
        yield from filter_.filter(lexer, stream)
    for filter_ in filters:
        stream = _apply(filter_, stream)
    return stream","def _apply(
    dist: ""Distribution"", filepath: _Path,
    other_files: Iterable[_Path] = (),
    ignore_option_errors: bool = False,
) -> Tuple[""ConfigHandler"", ...]:
    """"""Read configuration from ``filepath`` and applies to the ``dist`` object.""""""
    from setuptools.dist import _Distribution

    filepath = os.path.abspath(filepath)

    if not os.path.isfile(filepath):
        raise DistutilsFileError('Configuration file %s does not exist.' % filepath)

    current_directory = os.getcwd()
    os.chdir(os.path.dirname(filepath))
    filenames = [*other_files, filepath]

    try:
        _Distribution.parse_config_files(dist, filenames=filenames)
        handlers = parse_configuration(
            dist, dist.command_options, ignore_option_errors=ignore_option_errors
        )
        dist._finalize_license_files()
    finally:
        os.chdir(current_directory)

    return handlers","def apply_configuration_to_distribution(
    dist: ""Distribution"", filepath: _Path,
    other_files: Iterable[_Path] = (),
    ignore_option_errors: bool = False,
) -> Tuple[""ConfigHandler"", ...]:
    """"""Read configuration from ``filepath`` and applies to the ``dist`` object.""""""
    from setuptools.dist import _Distribution

    filepath = os.path.abspath(filepath)

    if not os.path.isfile(filepath):
        raise DistutilsFileError('Configuration file %s does not exist.' % filepath)

    current_directory = os.getcwd()
    os.chdir(os.path.dirname(filepath))
    filenames = [*other_files, filepath]

    try:
        _Distribution.parse_config_files(dist, filenames=filenames)
        handlers = parse_configuration(
            dist, dist.command_options, ignore_option_errors=ignore_option_errors
        )
        dist._finalize_license_files()
    finally:
        os.chdir(current_directory)

    return handlers","def apply_filters_to_stream_using_given_filters(stream, filters, lexer=None):
    """"""
    Use this method to apply an iterable of filters to
    a stream. If lexer is given it's forwarded to the
    filter, otherwise the filter receives `None`.
    """"""
    def apply_configuration_to_distribution(filter_, stream):
        yield from filter_.filter(lexer, stream)
    for filter_ in filters:
        stream = apply_configuration_to_distribution(filter_, stream)
    return stream","{""_apply"": ""apply_configuration_to_distribution""}","def 0(
    dist: ""Distribution"", filepath: _Path,
    other_files: Iterable[_Path] = (),
    ignore_option_errors: bool = False,
) -> Tuple[""ConfigHandler"", ...]:
    """"""Read configuration from ``filepath`` and applies to the ``dist`` object.""""""
    from setuptools.dist import _Distribution

    filepath = os.path.abspath(filepath)

    if not os.path.isfile(filepath):
        raise DistutilsFileError('Configuration file %s does not exist.' % filepath)

    current_directory = os.getcwd()
    os.chdir(os.path.dirname(filepath))
    filenames = [*other_files, filepath]

    try:
        _Distribution.parse_config_files(dist, filenames=filenames)
        handlers = parse_configuration(
            dist, dist.command_options, ignore_option_errors=ignore_option_errors
        )
        dist._finalize_license_files()
    finally:
        os.chdir(current_directory)

    return handlers","def 1(stream, filters, lexer=None):
    """"""
    Use this method to apply an iterable of filters to
    a stream. If lexer is given it's forwarded to the
    filter, otherwise the filter receives `None`.
    """"""
    def 0(filter_, stream):
        yield from filter_.filter(lexer, stream)
    for filter_ in filters:
        stream = 0(filter_, stream)
    return stream","{""_apply"": ""0""}"
361,361,"def find_filter_class(filtername):
    """"""Lookup a filter by name. Return None if not found.""""""
    if filtername in FILTERS:
        return FILTERS[filtername]
    for name, cls in find_plugin_filters():
        if name == filtername:
            return cls
    return None","def find_plugin_filters():
    for entrypoint in iter_entry_points(FILTER_ENTRY_POINT):
        yield entrypoint.name, entrypoint.load()","def discover_and_return_plugin_filters():
    for entrypoint in iter_entry_points(FILTER_ENTRY_POINT):
        yield entrypoint.name, entrypoint.load()","def search_for_filter_class_by_name(filtername):
    """"""Lookup a filter by name. Return None if not found.""""""
    if filtername in FILTERS:
        return FILTERS[filtername]
    for name, cls in discover_and_return_plugin_filters():
        if name == filtername:
            return cls
    return None","{""find_plugin_filters"": ""discover_and_return_plugin_filters""}","def 0():
    for entrypoint in iter_entry_points(FILTER_ENTRY_POINT):
        yield entrypoint.name, entrypoint.load()","def 1(filtername):
    """"""Lookup a filter by name. Return None if not found.""""""
    if filtername in FILTERS:
        return FILTERS[filtername]
    for name, cls in 0():
        if name == filtername:
            return cls
    return None","{""find_plugin_filters"": ""0""}"
362,362,"def get_filter_by_name(filtername, **options):
    """"""Return an instantiated filter.

    Options are passed to the filter initializer if wanted.
    Raise a ClassNotFound if not found.
    """"""
    cls = find_filter_class(filtername)
    if cls:
        return cls(**options)
    else:
        raise ClassNotFound('filter %r not found' % filtername)","def find_filter_class(filtername):
    """"""Lookup a filter by name. Return None if not found.""""""
    if filtername in FILTERS:
        return FILTERS[filtername]
    for name, cls in find_plugin_filters():
        if name == filtername:
            return cls
    return None","def search_for_filter_class_by_name(filtername):
    """"""Lookup a filter by name. Return None if not found.""""""
    if filtername in FILTERS:
        return FILTERS[filtername]
    for name, cls in find_plugin_filters():
        if name == filtername:
            return cls
    return None","def return_instantiated_filter_by_name(filtername, **options):
    """"""Return an instantiated filter.

    Options are passed to the filter initializer if wanted.
    Raise a ClassNotFound if not found.
    """"""
    cls = search_for_filter_class_by_name(filtername)
    if cls:
        return cls(**options)
    else:
        raise ClassNotFound('filter %r not found' % filtername)","{""find_filter_class"": ""search_for_filter_class_by_name""}","def 0(filtername):
    """"""Lookup a filter by name. Return None if not found.""""""
    if filtername in FILTERS:
        return FILTERS[filtername]
    for name, cls in find_plugin_filters():
        if name == filtername:
            return cls
    return None","def 1(filtername, **options):
    """"""Return an instantiated filter.

    Options are passed to the filter initializer if wanted.
    Raise a ClassNotFound if not found.
    """"""
    cls = 0(filtername)
    if cls:
        return cls(**options)
    else:
        raise ClassNotFound('filter %r not found' % filtername)","{""find_filter_class"": ""0""}"
363,363,"def get_all_filters():
    """"""Return a generator of all filter names.""""""
    yield from FILTERS
    for name, _ in find_plugin_filters():
        yield name","def find_plugin_filters():
    for entrypoint in iter_entry_points(FILTER_ENTRY_POINT):
        yield entrypoint.name, entrypoint.load()","def discover_and_return_plugin_filters():
    for entrypoint in iter_entry_points(FILTER_ENTRY_POINT):
        yield entrypoint.name, entrypoint.load()","def generate_all_filter_names():
    """"""Return a generator of all filter names.""""""
    yield from FILTERS
    for name, _ in discover_and_return_plugin_filters():
        yield name","{""find_plugin_filters"": ""discover_and_return_plugin_filters""}","def 0():
    for entrypoint in iter_entry_points(FILTER_ENTRY_POINT):
        yield entrypoint.name, entrypoint.load()","def 1():
    """"""Return a generator of all filter names.""""""
    yield from FILTERS
    for name, _ in 0():
        yield name","{""find_plugin_filters"": ""0""}"
364,364,"def _load_lexers(module_name):
    """"""Load a lexer (and all others in the module too).""""""
    mod = __import__(module_name, None, None, ['__all__'])
    for lexer_name in mod.__all__:
        cls = getattr(mod, lexer_name)
        _lexer_cache[cls.name] = cls","def __import__(name, globals=None, locals=None, fromlist=(), level=0):
    """"""Import a module.

    The 'globals' argument is used to infer where the import is occurring from
    to handle relative imports. The 'locals' argument is ignored. The
    'fromlist' argument specifies what should exist as attributes on the module
    being imported (e.g. ``from module import <fromlist>``).  The 'level'
    argument represents the package location to import from in a relative
    import (e.g. ``from ..pkg import mod`` would have a 'level' of 2).

    """"""
    if level == 0:
        module = _gcd_import(name)
    else:
        globals_ = globals if globals is not None else {}
        package = _calc___package__(globals_)
        module = _gcd_import(name, package, level)
    if not fromlist:
        # Return up to the first dot in 'name'. This is complicated by the fact
        # that 'name' may be relative.
        if level == 0:
            return _gcd_import(name.partition('.')[0])
        elif not name:
            return module
        else:
            # Figure out where to slice the module's name up to the first dot
            # in 'name'.
            cut_off = len(name) - len(name.partition('.')[0])
            # Slice end needs to be positive to alleviate need to special-case
            # when ``'.' not in name``.
            return sys.modules[module.__name__[:len(module.__name__)-cut_off]]
    elif hasattr(module, '__path__'):
        return _handle_fromlist(module, fromlist, _gcd_import)
    else:
        return module","def import_module(name, globals=None, locals=None, fromlist=(), level=0):
    """"""Import a module.

    The 'globals' argument is used to infer where the import is occurring from
    to handle relative imports. The 'locals' argument is ignored. The
    'fromlist' argument specifies what should exist as attributes on the module
    being imported (e.g. ``from module import <fromlist>``).  The 'level'
    argument represents the package location to import from in a relative
    import (e.g. ``from ..pkg import mod`` would have a 'level' of 2).

    """"""
    if level == 0:
        module = _gcd_import(name)
    else:
        globals_ = globals if globals is not None else {}
        package = _calc___package__(globals_)
        module = _gcd_import(name, package, level)
    if not fromlist:
        # Return up to the first dot in 'name'. This is complicated by the fact
        # that 'name' may be relative.
        if level == 0:
            return _gcd_import(name.partition('.')[0])
        elif not name:
            return module
        else:
            # Figure out where to slice the module's name up to the first dot
            # in 'name'.
            cut_off = len(name) - len(name.partition('.')[0])
            # Slice end needs to be positive to alleviate need to special-case
            # when ``'.' not in name``.
            return sys.modules[module.__name__[:len(module.__name__)-cut_off]]
    elif hasattr(module, '__path__'):
        return _handle_fromlist(module, fromlist, _gcd_import)
    else:
        return module","def load_and_cache_lexers(module_name):
    """"""Load a lexer (and all others in the module too).""""""
    mod = import_module(module_name, None, None, ['__all__'])
    for lexer_name in mod.__all__:
        cls = getattr(mod, lexer_name)
        _lexer_cache[cls.name] = cls","{""__import__"": ""import_module""}","def 0(name, globals=None, locals=None, fromlist=(), level=0):
    """"""Import a module.

    The 'globals' argument is used to infer where the import is occurring from
    to handle relative imports. The 'locals' argument is ignored. The
    'fromlist' argument specifies what should exist as attributes on the module
    being imported (e.g. ``from module import <fromlist>``).  The 'level'
    argument represents the package location to import from in a relative
    import (e.g. ``from ..pkg import mod`` would have a 'level' of 2).

    """"""
    if level == 0:
        module = _gcd_import(name)
    else:
        globals_ = globals if globals is not None else {}
        package = _calc___package__(globals_)
        module = _gcd_import(name, package, level)
    if not fromlist:
        # Return up to the first dot in 'name'. This is complicated by the fact
        # that 'name' may be relative.
        if level == 0:
            return _gcd_import(name.partition('.')[0])
        elif not name:
            return module
        else:
            # Figure out where to slice the module's name up to the first dot
            # in 'name'.
            cut_off = len(name) - len(name.partition('.')[0])
            # Slice end needs to be positive to alleviate need to special-case
            # when ``'.' not in name``.
            return sys.modules[module.__name__[:len(module.__name__)-cut_off]]
    elif hasattr(module, '__path__'):
        return _handle_fromlist(module, fromlist, _gcd_import)
    else:
        return module","def 1(module_name):
    """"""Load a lexer (and all others in the module too).""""""
    mod = 0(module_name, None, None, ['__all__'])
    for lexer_name in mod.__all__:
        cls = getattr(mod, lexer_name)
        _lexer_cache[cls.name] = cls","{""__import__"": ""0""}"
365,365,"def get_all_lexers():
    """"""Return a generator of tuples in the form ``(name, aliases,
    filenames, mimetypes)`` of all know lexers.
    """"""
    for item in LEXERS.values():
        yield item[1:]
    for lexer in find_plugin_lexers():
        yield lexer.name, lexer.aliases, lexer.filenames, lexer.mimetypes","def find_plugin_lexers():
    for entrypoint in iter_entry_points(LEXER_ENTRY_POINT):
        yield entrypoint.load()","def search_and_retrieve_plugin_lexers():
    for entrypoint in iter_entry_points(LEXER_ENTRY_POINT):
        yield entrypoint.load()","def generate_all_lexer_info_tuples():
    """"""Return a generator of tuples in the form ``(name, aliases,
    filenames, mimetypes)`` of all know lexers.
    """"""
    for item in LEXERS.values():
        yield item[1:]
    for lexer in search_and_retrieve_plugin_lexers():
        yield lexer.name, lexer.aliases, lexer.filenames, lexer.mimetypes","{""find_plugin_lexers"": ""search_and_retrieve_plugin_lexers""}","def 0():
    for entrypoint in iter_entry_points(LEXER_ENTRY_POINT):
        yield entrypoint.load()","def 1():
    """"""Return a generator of tuples in the form ``(name, aliases,
    filenames, mimetypes)`` of all know lexers.
    """"""
    for item in LEXERS.values():
        yield item[1:]
    for lexer in 0():
        yield lexer.name, lexer.aliases, lexer.filenames, lexer.mimetypes","{""find_plugin_lexers"": ""0""}"
366,366,"def find_lexer_class(name):
    """"""Lookup a lexer class by name.

    Return None if not found.
    """"""
    if name in _lexer_cache:
        return _lexer_cache[name]
    # lookup builtin lexers
    for module_name, lname, aliases, _, _ in LEXERS.values():
        if name == lname:
            _load_lexers(module_name)
            return _lexer_cache[name]
    # continue with lexers from setuptools entrypoints
    for cls in find_plugin_lexers():
        if cls.name == name:
            return cls","def find_plugin_lexers():
    for entrypoint in iter_entry_points(LEXER_ENTRY_POINT):
        yield entrypoint.load()

def _load_lexers(module_name):
    """"""Load a lexer (and all others in the module too).""""""
    mod = __import__(module_name, None, None, ['__all__'])
    for lexer_name in mod.__all__:
        cls = getattr(mod, lexer_name)
        _lexer_cache[cls.name] = cls","def search_and_retrieve_plugin_lexers():
    for entrypoint in iter_entry_points(LEXER_ENTRY_POINT):
        yield entrypoint.load()

def load_and_cache_lexers(module_name):
    """"""Load a lexer (and all others in the module too).""""""
    mod = __import__(module_name, None, None, ['__all__'])
    for lexer_name in mod.__all__:
        cls = getattr(mod, lexer_name)
        _lexer_cache[cls.name] = cls","def lookup_lexer_class_by_name(name):
    """"""Lookup a lexer class by name.

    Return None if not found.
    """"""
    if name in _lexer_cache:
        return _lexer_cache[name]
    # lookup builtin lexers
    for module_name, lname, aliases, _, _ in LEXERS.values():
        if name == lname:
            load_and_cache_lexers(module_name)
            return _lexer_cache[name]
    # continue with lexers from setuptools entrypoints
    for cls in search_and_retrieve_plugin_lexers():
        if cls.name == name:
            return cls","{""find_plugin_lexers"": ""search_and_retrieve_plugin_lexers"", ""_load_lexers"": ""load_and_cache_lexers""}","def 0():
    for entrypoint in iter_entry_points(LEXER_ENTRY_POINT):
        yield entrypoint.load()

def 1(module_name):
    """"""Load a lexer (and all others in the module too).""""""
    mod = __import__(module_name, None, None, ['__all__'])
    for lexer_name in mod.__all__:
        cls = getattr(mod, lexer_name)
        _lexer_cache[cls.name] = cls","def 2(name):
    """"""Lookup a lexer class by name.

    Return None if not found.
    """"""
    if name in _lexer_cache:
        return _lexer_cache[name]
    # lookup builtin lexers
    for module_name, lname, aliases, _, _ in LEXERS.values():
        if name == lname:
            1(module_name)
            return _lexer_cache[name]
    # continue with lexers from setuptools entrypoints
    for cls in 0():
        if cls.name == name:
            return cls","{""find_plugin_lexers"": ""0"", ""_load_lexers"": ""1""}"
367,367,"def find_lexer_class_by_name(_alias):
    """"""Lookup a lexer class by alias.

    Like `get_lexer_by_name`, but does not instantiate the class.

    .. versionadded:: 2.2
    """"""
    if not _alias:
        raise ClassNotFound('no lexer for alias %r found' % _alias)
    # lookup builtin lexers
    for module_name, name, aliases, _, _ in LEXERS.values():
        if _alias.lower() in aliases:
            if name not in _lexer_cache:
                _load_lexers(module_name)
            return _lexer_cache[name]
    # continue with lexers from setuptools entrypoints
    for cls in find_plugin_lexers():
        if _alias.lower() in cls.aliases:
            return cls
    raise ClassNotFound('no lexer for alias %r found' % _alias)","def find_plugin_lexers():
    for entrypoint in iter_entry_points(LEXER_ENTRY_POINT):
        yield entrypoint.load()

def _load_lexers(module_name):
    """"""Load a lexer (and all others in the module too).""""""
    mod = __import__(module_name, None, None, ['__all__'])
    for lexer_name in mod.__all__:
        cls = getattr(mod, lexer_name)
        _lexer_cache[cls.name] = cls","def search_and_retrieve_plugin_lexers():
    for entrypoint in iter_entry_points(LEXER_ENTRY_POINT):
        yield entrypoint.load()

def load_and_cache_lexers(module_name):
    """"""Load a lexer (and all others in the module too).""""""
    mod = __import__(module_name, None, None, ['__all__'])
    for lexer_name in mod.__all__:
        cls = getattr(mod, lexer_name)
        _lexer_cache[cls.name] = cls","def lookup_lexer_class_by_alias(_alias):
    """"""Lookup a lexer class by alias.

    Like `get_lexer_by_name`, but does not instantiate the class.

    .. versionadded:: 2.2
    """"""
    if not _alias:
        raise ClassNotFound('no lexer for alias %r found' % _alias)
    # lookup builtin lexers
    for module_name, name, aliases, _, _ in LEXERS.values():
        if _alias.lower() in aliases:
            if name not in _lexer_cache:
                load_and_cache_lexers(module_name)
            return _lexer_cache[name]
    # continue with lexers from setuptools entrypoints
    for cls in search_and_retrieve_plugin_lexers():
        if _alias.lower() in cls.aliases:
            return cls
    raise ClassNotFound('no lexer for alias %r found' % _alias)","{""find_plugin_lexers"": ""search_and_retrieve_plugin_lexers"", ""_load_lexers"": ""load_and_cache_lexers""}","def 0():
    for entrypoint in iter_entry_points(LEXER_ENTRY_POINT):
        yield entrypoint.load()

def 1(module_name):
    """"""Load a lexer (and all others in the module too).""""""
    mod = __import__(module_name, None, None, ['__all__'])
    for lexer_name in mod.__all__:
        cls = getattr(mod, lexer_name)
        _lexer_cache[cls.name] = cls","def 2(_alias):
    """"""Lookup a lexer class by alias.

    Like `get_lexer_by_name`, but does not instantiate the class.

    .. versionadded:: 2.2
    """"""
    if not _alias:
        raise ClassNotFound('no lexer for alias %r found' % _alias)
    # lookup builtin lexers
    for module_name, name, aliases, _, _ in LEXERS.values():
        if _alias.lower() in aliases:
            if name not in _lexer_cache:
                1(module_name)
            return _lexer_cache[name]
    # continue with lexers from setuptools entrypoints
    for cls in 0():
        if _alias.lower() in cls.aliases:
            return cls
    raise ClassNotFound('no lexer for alias %r found' % _alias)","{""find_plugin_lexers"": ""0"", ""_load_lexers"": ""1""}"
368,368,"def get_lexer_by_name(_alias, **options):
    """"""Get a lexer by an alias.

    Raises ClassNotFound if not found.
    """"""
    if not _alias:
        raise ClassNotFound('no lexer for alias %r found' % _alias)

    # lookup builtin lexers
    for module_name, name, aliases, _, _ in LEXERS.values():
        if _alias.lower() in aliases:
            if name not in _lexer_cache:
                _load_lexers(module_name)
            return _lexer_cache[name](**options)
    # continue with lexers from setuptools entrypoints
    for cls in find_plugin_lexers():
        if _alias.lower() in cls.aliases:
            return cls(**options)
    raise ClassNotFound('no lexer for alias %r found' % _alias)","def find_plugin_lexers():
    for entrypoint in iter_entry_points(LEXER_ENTRY_POINT):
        yield entrypoint.load()

def _load_lexers(module_name):
    """"""Load a lexer (and all others in the module too).""""""
    mod = __import__(module_name, None, None, ['__all__'])
    for lexer_name in mod.__all__:
        cls = getattr(mod, lexer_name)
        _lexer_cache[cls.name] = cls","def search_and_retrieve_plugin_lexers():
    for entrypoint in iter_entry_points(LEXER_ENTRY_POINT):
        yield entrypoint.load()

def load_and_cache_lexers(module_name):
    """"""Load a lexer (and all others in the module too).""""""
    mod = __import__(module_name, None, None, ['__all__'])
    for lexer_name in mod.__all__:
        cls = getattr(mod, lexer_name)
        _lexer_cache[cls.name] = cls","def retrieve_lexer_by_alias(_alias, **options):
    """"""Get a lexer by an alias.

    Raises ClassNotFound if not found.
    """"""
    if not _alias:
        raise ClassNotFound('no lexer for alias %r found' % _alias)

    # lookup builtin lexers
    for module_name, name, aliases, _, _ in LEXERS.values():
        if _alias.lower() in aliases:
            if name not in _lexer_cache:
                load_and_cache_lexers(module_name)
            return _lexer_cache[name](**options)
    # continue with lexers from setuptools entrypoints
    for cls in search_and_retrieve_plugin_lexers():
        if _alias.lower() in cls.aliases:
            return cls(**options)
    raise ClassNotFound('no lexer for alias %r found' % _alias)","{""find_plugin_lexers"": ""search_and_retrieve_plugin_lexers"", ""_load_lexers"": ""load_and_cache_lexers""}","def 0():
    for entrypoint in iter_entry_points(LEXER_ENTRY_POINT):
        yield entrypoint.load()

def 1(module_name):
    """"""Load a lexer (and all others in the module too).""""""
    mod = __import__(module_name, None, None, ['__all__'])
    for lexer_name in mod.__all__:
        cls = getattr(mod, lexer_name)
        _lexer_cache[cls.name] = cls","def 2(_alias, **options):
    """"""Get a lexer by an alias.

    Raises ClassNotFound if not found.
    """"""
    if not _alias:
        raise ClassNotFound('no lexer for alias %r found' % _alias)

    # lookup builtin lexers
    for module_name, name, aliases, _, _ in LEXERS.values():
        if _alias.lower() in aliases:
            if name not in _lexer_cache:
                1(module_name)
            return _lexer_cache[name](**options)
    # continue with lexers from setuptools entrypoints
    for cls in 0():
        if _alias.lower() in cls.aliases:
            return cls(**options)
    raise ClassNotFound('no lexer for alias %r found' % _alias)","{""find_plugin_lexers"": ""0"", ""_load_lexers"": ""1""}"
369,369,"def get_lexer_for_filename(_fn, code=None, **options):
    """"""Get a lexer for a filename.

    If multiple lexers match the filename pattern, use ``analyse_text()`` to
    figure out which one is more appropriate.

    Raises ClassNotFound if not found.
    """"""
    res = find_lexer_class_for_filename(_fn, code)
    if not res:
        raise ClassNotFound('no lexer for filename %r found' % _fn)
    return res(**options)","def find_lexer_class_for_filename(_fn, code=None):
    """"""Get a lexer for a filename.

    If multiple lexers match the filename pattern, use ``analyse_text()`` to
    figure out which one is more appropriate.

    Returns None if not found.
    """"""
    matches = []
    fn = basename(_fn)
    for modname, name, _, filenames, _ in LEXERS.values():
        for filename in filenames:
            if _fn_matches(fn, filename):
                if name not in _lexer_cache:
                    _load_lexers(modname)
                matches.append((_lexer_cache[name], filename))
    for cls in find_plugin_lexers():
        for filename in cls.filenames:
            if _fn_matches(fn, filename):
                matches.append((cls, filename))

    if isinstance(code, bytes):
        # decode it, since all analyse_text functions expect unicode
        code = guess_decode(code)

    def get_rating(info):
        cls, filename = info
        # explicit patterns get a bonus
        bonus = '*' not in filename and 0.5 or 0
        # The class _always_ defines analyse_text because it's included in
        # the Lexer class.  The default implementation returns None which
        # gets turned into 0.0.  Run scripts/detect_missing_analyse_text.py
        # to find lexers which need it overridden.
        if code:
            return cls.analyse_text(code) + bonus, cls.__name__
        return cls.priority + bonus, cls.__name__

    if matches:
        matches.sort(key=get_rating)
        # print ""Possible lexers, after sort:"", matches
        return matches[-1][0]","def search_and_get_lexer_class_for_filename(_fn, code=None):
    """"""Get a lexer for a filename.

    If multiple lexers match the filename pattern, use ``analyse_text()`` to
    figure out which one is more appropriate.

    Returns None if not found.
    """"""
    matches = []
    fn = basename(_fn)
    for modname, name, _, filenames, _ in LEXERS.values():
        for filename in filenames:
            if _fn_matches(fn, filename):
                if name not in _lexer_cache:
                    _load_lexers(modname)
                matches.append((_lexer_cache[name], filename))
    for cls in find_plugin_lexers():
        for filename in cls.filenames:
            if _fn_matches(fn, filename):
                matches.append((cls, filename))

    if isinstance(code, bytes):
        # decode it, since all analyse_text functions expect unicode
        code = guess_decode(code)

    def get_rating(info):
        cls, filename = info
        # explicit patterns get a bonus
        bonus = '*' not in filename and 0.5 or 0
        # The class _always_ defines analyse_text because it's included in
        # the Lexer class.  The default implementation returns None which
        # gets turned into 0.0.  Run scripts/detect_missing_analyse_text.py
        # to find lexers which need it overridden.
        if code:
            return cls.analyse_text(code) + bonus, cls.__name__
        return cls.priority + bonus, cls.__name__

    if matches:
        matches.sort(key=get_rating)
        # print ""Possible lexers, after sort:"", matches
        return matches[-1][0]","def get_lexer_for_filename_and_code(_fn, code=None, **options):
    """"""Get a lexer for a filename.

    If multiple lexers match the filename pattern, use ``analyse_text()`` to
    figure out which one is more appropriate.

    Raises ClassNotFound if not found.
    """"""
    res = search_and_get_lexer_class_for_filename(_fn, code)
    if not res:
        raise ClassNotFound('no lexer for filename %r found' % _fn)
    return res(**options)","{""find_lexer_class_for_filename"": ""search_and_get_lexer_class_for_filename""}","def 0(_fn, code=None):
    """"""Get a lexer for a filename.

    If multiple lexers match the filename pattern, use ``analyse_text()`` to
    figure out which one is more appropriate.

    Returns None if not found.
    """"""
    matches = []
    fn = basename(_fn)
    for modname, name, _, filenames, _ in LEXERS.values():
        for filename in filenames:
            if _fn_matches(fn, filename):
                if name not in _lexer_cache:
                    _load_lexers(modname)
                matches.append((_lexer_cache[name], filename))
    for cls in find_plugin_lexers():
        for filename in cls.filenames:
            if _fn_matches(fn, filename):
                matches.append((cls, filename))

    if isinstance(code, bytes):
        # decode it, since all analyse_text functions expect unicode
        code = guess_decode(code)

    def get_rating(info):
        cls, filename = info
        # explicit patterns get a bonus
        bonus = '*' not in filename and 0.5 or 0
        # The class _always_ defines analyse_text because it's included in
        # the Lexer class.  The default implementation returns None which
        # gets turned into 0.0.  Run scripts/detect_missing_analyse_text.py
        # to find lexers which need it overridden.
        if code:
            return cls.analyse_text(code) + bonus, cls.__name__
        return cls.priority + bonus, cls.__name__

    if matches:
        matches.sort(key=get_rating)
        # print ""Possible lexers, after sort:"", matches
        return matches[-1][0]","def 2(_fn, code=None, **options):
    """"""Get a lexer for a filename.

    If multiple lexers match the filename pattern, use ``analyse_text()`` to
    figure out which one is more appropriate.

    Raises ClassNotFound if not found.
    """"""
    res = 0(_fn, code)
    if not res:
        raise ClassNotFound('no lexer for filename %r found' % _fn)
    return res(**options)","{""find_lexer_class_for_filename"": ""0""}"
370,370,"def get_lexer_for_mimetype(_mime, **options):
    """"""Get a lexer for a mimetype.

    Raises ClassNotFound if not found.
    """"""
    for modname, name, _, _, mimetypes in LEXERS.values():
        if _mime in mimetypes:
            if name not in _lexer_cache:
                _load_lexers(modname)
            return _lexer_cache[name](**options)
    for cls in find_plugin_lexers():
        if _mime in cls.mimetypes:
            return cls(**options)
    raise ClassNotFound('no lexer for mimetype %r found' % _mime)","def find_plugin_lexers():
    for entrypoint in iter_entry_points(LEXER_ENTRY_POINT):
        yield entrypoint.load()

def _load_lexers(module_name):
    """"""Load a lexer (and all others in the module too).""""""
    mod = __import__(module_name, None, None, ['__all__'])
    for lexer_name in mod.__all__:
        cls = getattr(mod, lexer_name)
        _lexer_cache[cls.name] = cls","def search_and_retrieve_plugin_lexers():
    for entrypoint in iter_entry_points(LEXER_ENTRY_POINT):
        yield entrypoint.load()

def load_and_cache_lexers(module_name):
    """"""Load a lexer (and all others in the module too).""""""
    mod = __import__(module_name, None, None, ['__all__'])
    for lexer_name in mod.__all__:
        cls = getattr(mod, lexer_name)
        _lexer_cache[cls.name] = cls","def get_lexer_for_mimetype_and_options(_mime, **options):
    """"""Get a lexer for a mimetype.

    Raises ClassNotFound if not found.
    """"""
    for modname, name, _, _, mimetypes in LEXERS.values():
        if _mime in mimetypes:
            if name not in _lexer_cache:
                load_and_cache_lexers(modname)
            return _lexer_cache[name](**options)
    for cls in search_and_retrieve_plugin_lexers():
        if _mime in cls.mimetypes:
            return cls(**options)
    raise ClassNotFound('no lexer for mimetype %r found' % _mime)","{""find_plugin_lexers"": ""search_and_retrieve_plugin_lexers"", ""_load_lexers"": ""load_and_cache_lexers""}","def 0():
    for entrypoint in iter_entry_points(LEXER_ENTRY_POINT):
        yield entrypoint.load()

def 1(module_name):
    """"""Load a lexer (and all others in the module too).""""""
    mod = __import__(module_name, None, None, ['__all__'])
    for lexer_name in mod.__all__:
        cls = getattr(mod, lexer_name)
        _lexer_cache[cls.name] = cls","def 2(_mime, **options):
    """"""Get a lexer for a mimetype.

    Raises ClassNotFound if not found.
    """"""
    for modname, name, _, _, mimetypes in LEXERS.values():
        if _mime in mimetypes:
            if name not in _lexer_cache:
                1(modname)
            return _lexer_cache[name](**options)
    for cls in 0():
        if _mime in cls.mimetypes:
            return cls(**options)
    raise ClassNotFound('no lexer for mimetype %r found' % _mime)","{""find_plugin_lexers"": ""0"", ""_load_lexers"": ""1""}"
371,371,"def _iter_lexerclasses(plugins=True):
    """"""Return an iterator over all lexer classes.""""""
    for key in sorted(LEXERS):
        module_name, name = LEXERS[key][:2]
        if name not in _lexer_cache:
            _load_lexers(module_name)
        yield _lexer_cache[name]
    if plugins:
        yield from find_plugin_lexers()","def find_plugin_lexers():
    for entrypoint in iter_entry_points(LEXER_ENTRY_POINT):
        yield entrypoint.load()

def _load_lexers(module_name):
    """"""Load a lexer (and all others in the module too).""""""
    mod = __import__(module_name, None, None, ['__all__'])
    for lexer_name in mod.__all__:
        cls = getattr(mod, lexer_name)
        _lexer_cache[cls.name] = cls","def search_and_retrieve_plugin_lexers():
    for entrypoint in iter_entry_points(LEXER_ENTRY_POINT):
        yield entrypoint.load()

def load_and_cache_lexers(module_name):
    """"""Load a lexer (and all others in the module too).""""""
    mod = __import__(module_name, None, None, ['__all__'])
    for lexer_name in mod.__all__:
        cls = getattr(mod, lexer_name)
        _lexer_cache[cls.name] = cls","def iterate_over_lexer_classes(plugins=True):
    """"""Return an iterator over all lexer classes.""""""
    for key in sorted(LEXERS):
        module_name, name = LEXERS[key][:2]
        if name not in _lexer_cache:
            load_and_cache_lexers(module_name)
        yield _lexer_cache[name]
    if plugins:
        yield from search_and_retrieve_plugin_lexers()","{""find_plugin_lexers"": ""search_and_retrieve_plugin_lexers"", ""_load_lexers"": ""load_and_cache_lexers""}","def 0():
    for entrypoint in iter_entry_points(LEXER_ENTRY_POINT):
        yield entrypoint.load()

def 1(module_name):
    """"""Load a lexer (and all others in the module too).""""""
    mod = __import__(module_name, None, None, ['__all__'])
    for lexer_name in mod.__all__:
        cls = getattr(mod, lexer_name)
        _lexer_cache[cls.name] = cls","def 2(plugins=True):
    """"""Return an iterator over all lexer classes.""""""
    for key in sorted(LEXERS):
        module_name, name = LEXERS[key][:2]
        if name not in _lexer_cache:
            1(module_name)
        yield _lexer_cache[name]
    if plugins:
        yield from 0()","{""find_plugin_lexers"": ""0"", ""_load_lexers"": ""1""}"
372,372,"def guess_lexer(_text, **options):
    """"""Guess a lexer by strong distinctions in the text (eg, shebang).""""""

    if not isinstance(_text, str):
        inencoding = options.get('inencoding', options.get('encoding'))
        if inencoding:
            _text = _text.decode(inencoding or 'utf8')
        else:
            _text, _ = guess_decode(_text)

    # try to get a vim modeline first
    ft = get_filetype_from_buffer(_text)

    if ft is not None:
        try:
            return get_lexer_by_name(ft, **options)
        except ClassNotFound:
            pass

    best_lexer = [0.0, None]
    for lexer in _iter_lexerclasses():
        rv = lexer.analyse_text(_text)
        if rv == 1.0:
            return lexer(**options)
        if rv > best_lexer[0]:
            best_lexer[:] = (rv, lexer)
    if not best_lexer[0] or best_lexer[1] is None:
        raise ClassNotFound('no lexer matching the text found')
    return best_lexer[1](**options)","def guess_decode(text):
    """"""Decode *text* with guessed encoding.

    First try UTF-8; this should fail for non-UTF-8 encodings.
    Then try the preferred locale encoding.
    Fall back to latin-1, which always works.
    """"""
    try:
        text = text.decode('utf-8')
        return text, 'utf-8'
    except UnicodeDecodeError:
        try:
            import locale
            prefencoding = locale.getpreferredencoding()
            text = text.decode()
            return text, prefencoding
        except (UnicodeDecodeError, LookupError):
            text = text.decode('latin1')
            return text, 'latin1'

def get_lexer_by_name(_alias, **options):
    """"""Get a lexer by an alias.

    Raises ClassNotFound if not found.
    """"""
    if not _alias:
        raise ClassNotFound('no lexer for alias %r found' % _alias)

    # lookup builtin lexers
    for module_name, name, aliases, _, _ in LEXERS.values():
        if _alias.lower() in aliases:
            if name not in _lexer_cache:
                _load_lexers(module_name)
            return _lexer_cache[name](**options)
    # continue with lexers from setuptools entrypoints
    for cls in find_plugin_lexers():
        if _alias.lower() in cls.aliases:
            return cls(**options)
    raise ClassNotFound('no lexer for alias %r found' % _alias)

def _iter_lexerclasses(plugins=True):
    """"""Return an iterator over all lexer classes.""""""
    for key in sorted(LEXERS):
        module_name, name = LEXERS[key][:2]
        if name not in _lexer_cache:
            _load_lexers(module_name)
        yield _lexer_cache[name]
    if plugins:
        yield from find_plugin_lexers()

def get_filetype_from_buffer(buf, max_lines=5):
    """"""
    Scan the buffer for modelines and return filetype if one is found.
    """"""
    lines = buf.splitlines()
    for l in lines[-1:-max_lines-1:-1]:
        ret = get_filetype_from_line(l)
        if ret:
            return ret
    for i in range(max_lines, -1, -1):
        if i < len(lines):
            ret = get_filetype_from_line(lines[i])
            if ret:
                return ret

    return None","def attempt_decoding_with_guess(text):
    """"""Decode *text* with guessed encoding.

    First try UTF-8; this should fail for non-UTF-8 encodings.
    Then try the preferred locale encoding.
    Fall back to latin-1, which always works.
    """"""
    try:
        text = text.decode('utf-8')
        return text, 'utf-8'
    except UnicodeDecodeError:
        try:
            import locale
            prefencoding = locale.getpreferredencoding()
            text = text.decode()
            return text, prefencoding
        except (UnicodeDecodeError, LookupError):
            text = text.decode('latin1')
            return text, 'latin1'

def retrieve_lexer_by_alias(_alias, **options):
    """"""Get a lexer by an alias.

    Raises ClassNotFound if not found.
    """"""
    if not _alias:
        raise ClassNotFound('no lexer for alias %r found' % _alias)

    # lookup builtin lexers
    for module_name, name, aliases, _, _ in LEXERS.values():
        if _alias.lower() in aliases:
            if name not in _lexer_cache:
                _load_lexers(module_name)
            return _lexer_cache[name](**options)
    # continue with lexers from setuptools entrypoints
    for cls in find_plugin_lexers():
        if _alias.lower() in cls.aliases:
            return cls(**options)
    raise ClassNotFound('no lexer for alias %r found' % _alias)

def iterate_over_lexer_classes(plugins=True):
    """"""Return an iterator over all lexer classes.""""""
    for key in sorted(LEXERS):
        module_name, name = LEXERS[key][:2]
        if name not in _lexer_cache:
            _load_lexers(module_name)
        yield _lexer_cache[name]
    if plugins:
        yield from find_plugin_lexers()

def determine_filetype_from_buffer(buf, max_lines=5):
    """"""
    Scan the buffer for modelines and return filetype if one is found.
    """"""
    lines = buf.splitlines()
    for l in lines[-1:-max_lines-1:-1]:
        ret = get_filetype_from_line(l)
        if ret:
            return ret
    for i in range(max_lines, -1, -1):
        if i < len(lines):
            ret = get_filetype_from_line(lines[i])
            if ret:
                return ret

    return None","def determine_lexer_by_text_with_options_and_handle_exceptions(_text, **options):
    """"""Guess a lexer by strong distinctions in the text (eg, shebang).""""""

    if not isinstance(_text, str):
        inencoding = options.get('inencoding', options.get('encoding'))
        if inencoding:
            _text = _text.decode(inencoding or 'utf8')
        else:
            _text, _ = attempt_decoding_with_guess(_text)

    # try to get a vim modeline first
    ft = determine_filetype_from_buffer(_text)

    if ft is not None:
        try:
            return retrieve_lexer_by_alias(ft, **options)
        except ClassNotFound:
            pass

    best_lexer = [0.0, None]
    for lexer in iterate_over_lexer_classes():
        rv = lexer.analyse_text(_text)
        if rv == 1.0:
            return lexer(**options)
        if rv > best_lexer[0]:
            best_lexer[:] = (rv, lexer)
    if not best_lexer[0] or best_lexer[1] is None:
        raise ClassNotFound('no lexer matching the text found')
    return best_lexer[1](**options)","{""guess_decode"": ""attempt_decoding_with_guess"", ""get_lexer_by_name"": ""retrieve_lexer_by_alias"", ""_iter_lexerclasses"": ""iterate_over_lexer_classes"", ""get_filetype_from_buffer"": ""determine_filetype_from_buffer""}","def 0(text):
    """"""Decode *text* with guessed encoding.

    First try UTF-8; this should fail for non-UTF-8 encodings.
    Then try the preferred locale encoding.
    Fall back to latin-1, which always works.
    """"""
    try:
        text = text.decode('utf-8')
        return text, 'utf-8'
    except UnicodeDecodeError:
        try:
            import locale
            prefencoding = locale.getpreferredencoding()
            text = text.decode()
            return text, prefencoding
        except (UnicodeDecodeError, LookupError):
            text = text.decode('latin1')
            return text, 'latin1'

def 1(_alias, **options):
    """"""Get a lexer by an alias.

    Raises ClassNotFound if not found.
    """"""
    if not _alias:
        raise ClassNotFound('no lexer for alias %r found' % _alias)

    # lookup builtin lexers
    for module_name, name, aliases, _, _ in LEXERS.values():
        if _alias.lower() in aliases:
            if name not in _lexer_cache:
                _load_lexers(module_name)
            return _lexer_cache[name](**options)
    # continue with lexers from setuptools entrypoints
    for cls in find_plugin_lexers():
        if _alias.lower() in cls.aliases:
            return cls(**options)
    raise ClassNotFound('no lexer for alias %r found' % _alias)

def 2(plugins=True):
    """"""Return an iterator over all lexer classes.""""""
    for key in sorted(LEXERS):
        module_name, name = LEXERS[key][:2]
        if name not in _lexer_cache:
            _load_lexers(module_name)
        yield _lexer_cache[name]
    if plugins:
        yield from find_plugin_lexers()

def 3(buf, max_lines=5):
    """"""
    Scan the buffer for modelines and return filetype if one is found.
    """"""
    lines = buf.splitlines()
    for l in lines[-1:-max_lines-1:-1]:
        ret = get_filetype_from_line(l)
        if ret:
            return ret
    for i in range(max_lines, -1, -1):
        if i < len(lines):
            ret = get_filetype_from_line(lines[i])
            if ret:
                return ret

    return None","def 4(_text, **options):
    """"""Guess a lexer by strong distinctions in the text (eg, shebang).""""""

    if not isinstance(_text, str):
        inencoding = options.get('inencoding', options.get('encoding'))
        if inencoding:
            _text = _text.decode(inencoding or 'utf8')
        else:
            _text, _ = 0(_text)

    # try to get a vim modeline first
    ft = 3(_text)

    if ft is not None:
        try:
            return 1(ft, **options)
        except ClassNotFound:
            pass

    best_lexer = [0.0, None]
    for lexer in 2():
        rv = lexer.analyse_text(_text)
        if rv == 1.0:
            return lexer(**options)
        if rv > best_lexer[0]:
            best_lexer[:] = (rv, lexer)
    if not best_lexer[0] or best_lexer[1] is None:
        raise ClassNotFound('no lexer matching the text found')
    return best_lexer[1](**options)","{""guess_decode"": ""0"", ""get_lexer_by_name"": ""1"", ""_iter_lexerclasses"": ""2"", ""get_filetype_from_buffer"": ""3""}"
373,373,"def _load_formatters(module_name):
    """"""Load a formatter (and all others in the module too).""""""
    mod = __import__(module_name, None, None, ['__all__'])
    for formatter_name in mod.__all__:
        cls = getattr(mod, formatter_name)
        _formatter_cache[cls.name] = cls","def __import__(name, globals=None, locals=None, fromlist=(), level=0):
    """"""Import a module.

    The 'globals' argument is used to infer where the import is occurring from
    to handle relative imports. The 'locals' argument is ignored. The
    'fromlist' argument specifies what should exist as attributes on the module
    being imported (e.g. ``from module import <fromlist>``).  The 'level'
    argument represents the package location to import from in a relative
    import (e.g. ``from ..pkg import mod`` would have a 'level' of 2).

    """"""
    if level == 0:
        module = _gcd_import(name)
    else:
        globals_ = globals if globals is not None else {}
        package = _calc___package__(globals_)
        module = _gcd_import(name, package, level)
    if not fromlist:
        # Return up to the first dot in 'name'. This is complicated by the fact
        # that 'name' may be relative.
        if level == 0:
            return _gcd_import(name.partition('.')[0])
        elif not name:
            return module
        else:
            # Figure out where to slice the module's name up to the first dot
            # in 'name'.
            cut_off = len(name) - len(name.partition('.')[0])
            # Slice end needs to be positive to alleviate need to special-case
            # when ``'.' not in name``.
            return sys.modules[module.__name__[:len(module.__name__)-cut_off]]
    elif hasattr(module, '__path__'):
        return _handle_fromlist(module, fromlist, _gcd_import)
    else:
        return module","def import_module(name, globals=None, locals=None, fromlist=(), level=0):
    """"""Import a module.

    The 'globals' argument is used to infer where the import is occurring from
    to handle relative imports. The 'locals' argument is ignored. The
    'fromlist' argument specifies what should exist as attributes on the module
    being imported (e.g. ``from module import <fromlist>``).  The 'level'
    argument represents the package location to import from in a relative
    import (e.g. ``from ..pkg import mod`` would have a 'level' of 2).

    """"""
    if level == 0:
        module = _gcd_import(name)
    else:
        globals_ = globals if globals is not None else {}
        package = _calc___package__(globals_)
        module = _gcd_import(name, package, level)
    if not fromlist:
        # Return up to the first dot in 'name'. This is complicated by the fact
        # that 'name' may be relative.
        if level == 0:
            return _gcd_import(name.partition('.')[0])
        elif not name:
            return module
        else:
            # Figure out where to slice the module's name up to the first dot
            # in 'name'.
            cut_off = len(name) - len(name.partition('.')[0])
            # Slice end needs to be positive to alleviate need to special-case
            # when ``'.' not in name``.
            return sys.modules[module.__name__[:len(module.__name__)-cut_off]]
    elif hasattr(module, '__path__'):
        return _handle_fromlist(module, fromlist, _gcd_import)
    else:
        return module","def load_and_cache_formatters(module_name):
    """"""Load a formatter (and all others in the module too).""""""
    mod = import_module(module_name, None, None, ['__all__'])
    for formatter_name in mod.__all__:
        cls = getattr(mod, formatter_name)
        _formatter_cache[cls.name] = cls","{""__import__"": ""import_module""}","def 0(name, globals=None, locals=None, fromlist=(), level=0):
    """"""Import a module.

    The 'globals' argument is used to infer where the import is occurring from
    to handle relative imports. The 'locals' argument is ignored. The
    'fromlist' argument specifies what should exist as attributes on the module
    being imported (e.g. ``from module import <fromlist>``).  The 'level'
    argument represents the package location to import from in a relative
    import (e.g. ``from ..pkg import mod`` would have a 'level' of 2).

    """"""
    if level == 0:
        module = _gcd_import(name)
    else:
        globals_ = globals if globals is not None else {}
        package = _calc___package__(globals_)
        module = _gcd_import(name, package, level)
    if not fromlist:
        # Return up to the first dot in 'name'. This is complicated by the fact
        # that 'name' may be relative.
        if level == 0:
            return _gcd_import(name.partition('.')[0])
        elif not name:
            return module
        else:
            # Figure out where to slice the module's name up to the first dot
            # in 'name'.
            cut_off = len(name) - len(name.partition('.')[0])
            # Slice end needs to be positive to alleviate need to special-case
            # when ``'.' not in name``.
            return sys.modules[module.__name__[:len(module.__name__)-cut_off]]
    elif hasattr(module, '__path__'):
        return _handle_fromlist(module, fromlist, _gcd_import)
    else:
        return module","def 1(module_name):
    """"""Load a formatter (and all others in the module too).""""""
    mod = 0(module_name, None, None, ['__all__'])
    for formatter_name in mod.__all__:
        cls = getattr(mod, formatter_name)
        _formatter_cache[cls.name] = cls","{""__import__"": ""0""}"
374,374,"def get_all_formatters():
    """"""Return a generator for all formatter classes.""""""
    # NB: this returns formatter classes, not info like get_all_lexers().
    for info in FORMATTERS.values():
        if info[1] not in _formatter_cache:
            _load_formatters(info[0])
        yield _formatter_cache[info[1]]
    for _, formatter in find_plugin_formatters():
        yield formatter","def _load_formatters(module_name):
    """"""Load a formatter (and all others in the module too).""""""
    mod = __import__(module_name, None, None, ['__all__'])
    for formatter_name in mod.__all__:
        cls = getattr(mod, formatter_name)
        _formatter_cache[cls.name] = cls

def find_plugin_formatters():
    for entrypoint in iter_entry_points(FORMATTER_ENTRY_POINT):
        yield entrypoint.name, entrypoint.load()","def load_and_cache_formatters(module_name):
    """"""Load a formatter (and all others in the module too).""""""
    mod = __import__(module_name, None, None, ['__all__'])
    for formatter_name in mod.__all__:
        cls = getattr(mod, formatter_name)
        _formatter_cache[cls.name] = cls

def discover_and_return_plugin_formatters():
    for entrypoint in iter_entry_points(FORMATTER_ENTRY_POINT):
        yield entrypoint.name, entrypoint.load()","def retrieve_all_formatter_classes_and_handle_cache():
    """"""Return a generator for all formatter classes.""""""
    # NB: this returns formatter classes, not info like get_all_lexers().
    for info in FORMATTERS.values():
        if info[1] not in _formatter_cache:
            load_and_cache_formatters(info[0])
        yield _formatter_cache[info[1]]
    for _, formatter in discover_and_return_plugin_formatters():
        yield formatter","{""_load_formatters"": ""load_and_cache_formatters"", ""find_plugin_formatters"": ""discover_and_return_plugin_formatters""}","def 0(module_name):
    """"""Load a formatter (and all others in the module too).""""""
    mod = __import__(module_name, None, None, ['__all__'])
    for formatter_name in mod.__all__:
        cls = getattr(mod, formatter_name)
        _formatter_cache[cls.name] = cls

def 1():
    for entrypoint in iter_entry_points(FORMATTER_ENTRY_POINT):
        yield entrypoint.name, entrypoint.load()","def 2():
    """"""Return a generator for all formatter classes.""""""
    # NB: this returns formatter classes, not info like get_all_lexers().
    for info in FORMATTERS.values():
        if info[1] not in _formatter_cache:
            0(info[0])
        yield _formatter_cache[info[1]]
    for _, formatter in 1():
        yield formatter","{""_load_formatters"": ""0"", ""find_plugin_formatters"": ""1""}"
375,375,"def find_formatter_class(alias):
    """"""Lookup a formatter by alias.

    Returns None if not found.
    """"""
    for module_name, name, aliases, _, _ in FORMATTERS.values():
        if alias in aliases:
            if name not in _formatter_cache:
                _load_formatters(module_name)
            return _formatter_cache[name]
    for _, cls in find_plugin_formatters():
        if alias in cls.aliases:
            return cls","def _load_formatters(module_name):
    """"""Load a formatter (and all others in the module too).""""""
    mod = __import__(module_name, None, None, ['__all__'])
    for formatter_name in mod.__all__:
        cls = getattr(mod, formatter_name)
        _formatter_cache[cls.name] = cls

def find_plugin_formatters():
    for entrypoint in iter_entry_points(FORMATTER_ENTRY_POINT):
        yield entrypoint.name, entrypoint.load()","def load_and_cache_formatters(module_name):
    """"""Load a formatter (and all others in the module too).""""""
    mod = __import__(module_name, None, None, ['__all__'])
    for formatter_name in mod.__all__:
        cls = getattr(mod, formatter_name)
        _formatter_cache[cls.name] = cls

def discover_and_return_plugin_formatters():
    for entrypoint in iter_entry_points(FORMATTER_ENTRY_POINT):
        yield entrypoint.name, entrypoint.load()","def look_up_formatter_class_by_alias_returning_none_if_not_found(alias):
    """"""Lookup a formatter by alias.

    Returns None if not found.
    """"""
    for module_name, name, aliases, _, _ in FORMATTERS.values():
        if alias in aliases:
            if name not in _formatter_cache:
                load_and_cache_formatters(module_name)
            return _formatter_cache[name]
    for _, cls in discover_and_return_plugin_formatters():
        if alias in cls.aliases:
            return cls","{""_load_formatters"": ""load_and_cache_formatters"", ""find_plugin_formatters"": ""discover_and_return_plugin_formatters""}","def 0(module_name):
    """"""Load a formatter (and all others in the module too).""""""
    mod = __import__(module_name, None, None, ['__all__'])
    for formatter_name in mod.__all__:
        cls = getattr(mod, formatter_name)
        _formatter_cache[cls.name] = cls

def 1():
    for entrypoint in iter_entry_points(FORMATTER_ENTRY_POINT):
        yield entrypoint.name, entrypoint.load()","def 2(alias):
    """"""Lookup a formatter by alias.

    Returns None if not found.
    """"""
    for module_name, name, aliases, _, _ in FORMATTERS.values():
        if alias in aliases:
            if name not in _formatter_cache:
                0(module_name)
            return _formatter_cache[name]
    for _, cls in 1():
        if alias in cls.aliases:
            return cls","{""_load_formatters"": ""0"", ""find_plugin_formatters"": ""1""}"
376,376,"def get_formatter_by_name(_alias, **options):
    """"""Lookup and instantiate a formatter by alias.

    Raises ClassNotFound if not found.
    """"""
    cls = find_formatter_class(_alias)
    if cls is None:
        raise ClassNotFound(""no formatter found for name %r"" % _alias)
    return cls(**options)","def find_formatter_class(alias):
    """"""Lookup a formatter by alias.

    Returns None if not found.
    """"""
    for module_name, name, aliases, _, _ in FORMATTERS.values():
        if alias in aliases:
            if name not in _formatter_cache:
                _load_formatters(module_name)
            return _formatter_cache[name]
    for _, cls in find_plugin_formatters():
        if alias in cls.aliases:
            return cls","def look_up_formatter_class_by_alias_returning_none_if_not_found(alias):
    """"""Lookup a formatter by alias.

    Returns None if not found.
    """"""
    for module_name, name, aliases, _, _ in FORMATTERS.values():
        if alias in aliases:
            if name not in _formatter_cache:
                _load_formatters(module_name)
            return _formatter_cache[name]
    for _, cls in find_plugin_formatters():
        if alias in cls.aliases:
            return cls","def lookup_and_instantiate_formatter_by_alias_with_exception(_alias, **options):
    """"""Lookup and instantiate a formatter by alias.

    Raises ClassNotFound if not found.
    """"""
    cls = look_up_formatter_class_by_alias_returning_none_if_not_found(_alias)
    if cls is None:
        raise ClassNotFound(""no formatter found for name %r"" % _alias)
    return cls(**options)","{""find_formatter_class"": ""look_up_formatter_class_by_alias_returning_none_if_not_found""}","def 0(alias):
    """"""Lookup a formatter by alias.

    Returns None if not found.
    """"""
    for module_name, name, aliases, _, _ in FORMATTERS.values():
        if alias in aliases:
            if name not in _formatter_cache:
                _load_formatters(module_name)
            return _formatter_cache[name]
    for _, cls in find_plugin_formatters():
        if alias in cls.aliases:
            return cls","def 1(_alias, **options):
    """"""Lookup and instantiate a formatter by alias.

    Raises ClassNotFound if not found.
    """"""
    cls = 0(_alias)
    if cls is None:
        raise ClassNotFound(""no formatter found for name %r"" % _alias)
    return cls(**options)","{""find_formatter_class"": ""0""}"
377,377,"def get_all_styles():
    """"""Return a generator for all styles by name,
    both builtin and plugin.""""""
    yield from STYLE_MAP
    for name, _ in find_plugin_styles():
        yield name","def find_plugin_styles():
    for entrypoint in iter_entry_points(STYLE_ENTRY_POINT):
        yield entrypoint.name, entrypoint.load()","def discover_and_return_plugin_styles():
    for entrypoint in iter_entry_points(STYLE_ENTRY_POINT):
        yield entrypoint.name, entrypoint.load()","def generate_generator_for_all_styles_both_builtin_and_plugin():
    """"""Return a generator for all styles by name,
    both builtin and plugin.""""""
    yield from STYLE_MAP
    for name, _ in discover_and_return_plugin_styles():
        yield name","{""find_plugin_styles"": ""discover_and_return_plugin_styles""}","def 0():
    for entrypoint in iter_entry_points(STYLE_ENTRY_POINT):
        yield entrypoint.name, entrypoint.load()","def 1():
    """"""Return a generator for all styles by name,
    both builtin and plugin.""""""
    yield from STYLE_MAP
    for name, _ in 0():
        yield name","{""find_plugin_styles"": ""0""}"
378,378,"def compat_system(source_dir):
    """"""
    Given a source dir, attempt to get a build system backend
    and requirements from pyproject.toml. Fallback to
    setuptools but only if the file was not found or a build
    system was not indicated.
    """"""
    try:
        system = load_system(source_dir)
    except (FileNotFoundError, KeyError):
        system = {}
    system.setdefault(
        'build-backend',
        'setuptools.build_meta:__legacy__',
    )
    system.setdefault('requires', ['setuptools', 'wheel'])
    return system","def load_system(source_dir):
    """"""
    Load the build system from a source dir (pyproject.toml).
    """"""
    pyproject = os.path.join(source_dir, 'pyproject.toml')
    with io.open(pyproject, 'rb') as f:
        pyproject_data = toml_load(f)
    return pyproject_data['build-system']","def load_build_system_from_source_directory_pyproject_toml(source_dir):
    """"""
    Load the build system from a source dir (pyproject.toml).
    """"""
    pyproject = os.path.join(source_dir, 'pyproject.toml')
    with io.open(pyproject, 'rb') as f:
        pyproject_data = toml_load(f)
    return pyproject_data['build-system']","def attempt_to_get_build_system_backend_and_requirements(source_dir):
    """"""
    Given a source dir, attempt to get a build system backend
    and requirements from pyproject.toml. Fallback to
    setuptools but only if the file was not found or a build
    system was not indicated.
    """"""
    try:
        system = load_build_system_from_source_directory_pyproject_toml(source_dir)
    except (FileNotFoundError, KeyError):
        system = {}
    system.setdefault(
        'build-backend',
        'setuptools.build_meta:__legacy__',
    )
    system.setdefault('requires', ['setuptools', 'wheel'])
    return system","{""load_system"": ""load_build_system_from_source_directory_pyproject_toml""}","def 0(source_dir):
    """"""
    Load the build system from a source dir (pyproject.toml).
    """"""
    pyproject = os.path.join(source_dir, 'pyproject.toml')
    with io.open(pyproject, 'rb') as f:
        pyproject_data = toml_load(f)
    return pyproject_data['build-system']","def 1(source_dir):
    """"""
    Given a source dir, attempt to get a build system backend
    and requirements from pyproject.toml. Fallback to
    setuptools but only if the file was not found or a build
    system was not indicated.
    """"""
    try:
        system = 0(source_dir)
    except (FileNotFoundError, KeyError):
        system = {}
    system.setdefault(
        'build-backend',
        'setuptools.build_meta:__legacy__',
    )
    system.setdefault('requires', ['setuptools', 'wheel'])
    return system","{""load_system"": ""0""}"
379,379,"def default_subprocess_runner(cmd, cwd=None, extra_environ=None):
    """"""The default method of calling the wrapper subprocess.""""""
    env = os.environ.copy()
    if extra_environ:
        env.update(extra_environ)

    check_call(cmd, cwd=cwd, env=env)","def check_call(*popenargs, **kwargs):
    """"""Run command with arguments.  Wait for command to complete.  If
    the exit code was zero then return, otherwise raise
    CalledProcessError.  The CalledProcessError object will have the
    return code in the returncode attribute.

    The arguments are the same as for the call function.  Example:

    check_call([""ls"", ""-l""])
    """"""
    retcode = call(*popenargs, **kwargs)
    if retcode:
        cmd = kwargs.get(""args"")
        if cmd is None:
            cmd = popenargs[0]
        raise CalledProcessError(retcode, cmd)
    return 0","def run_command_wait_for_completion_raise_exception_if_nonzero_exit_code(*popenargs, **kwargs):
    """"""Run command with arguments.  Wait for command to complete.  If
    the exit code was zero then return, otherwise raise
    CalledProcessError.  The CalledProcessError object will have the
    return code in the returncode attribute.

    The arguments are the same as for the call function.  Example:

    run_command_wait_for_completion_raise_exception_if_nonzero_exit_code([""ls"", ""-l""])
    """"""
    retcode = call(*popenargs, **kwargs)
    if retcode:
        cmd = kwargs.get(""args"")
        if cmd is None:
            cmd = popenargs[0]
        raise CalledProcessError(retcode, cmd)
    return 0","def execute_subprocess_with_default_settings(cmd, cwd=None, extra_environ=None):
    """"""The default method of calling the wrapper subprocess.""""""
    env = os.environ.copy()
    if extra_environ:
        env.update(extra_environ)

    run_command_wait_for_completion_raise_exception_if_nonzero_exit_code(cmd, cwd=cwd, env=env)","{""check_call"": ""run_command_wait_for_completion_raise_exception_if_nonzero_exit_code""}","def 0(*popenargs, **kwargs):
    """"""Run command with arguments.  Wait for command to complete.  If
    the exit code was zero then return, otherwise raise
    CalledProcessError.  The CalledProcessError object will have the
    return code in the returncode attribute.

    The arguments are the same as for the call function.  Example:

    0([""ls"", ""-l""])
    """"""
    retcode = call(*popenargs, **kwargs)
    if retcode:
        cmd = kwargs.get(""args"")
        if cmd is None:
            cmd = popenargs[0]
        raise CalledProcessError(retcode, cmd)
    return 0","def 1(cmd, cwd=None, extra_environ=None):
    """"""The default method of calling the wrapper subprocess.""""""
    env = os.environ.copy()
    if extra_environ:
        env.update(extra_environ)

    0(cmd, cwd=cwd, env=env)","{""check_call"": ""0""}"
380,380,"def _supported_features():
    """"""Return the list of options features supported by the backend.

    Returns a list of strings.
    The only possible value is 'build_editable'.
    """"""
    backend = _build_backend()
    features = []
    if hasattr(backend, ""build_editable""):
        features.append(""build_editable"")
    return features","def _build_backend():
    """"""Find and load the build backend""""""
    # Add in-tree backend directories to the front of sys.path.
    backend_path = os.environ.get('PEP517_BACKEND_PATH')
    if backend_path:
        extra_pathitems = backend_path.split(os.pathsep)
        sys.path[:0] = extra_pathitems

    ep = os.environ['PEP517_BUILD_BACKEND']
    mod_path, _, obj_path = ep.partition(':')
    try:
        obj = import_module(mod_path)
    except ImportError:
        raise BackendUnavailable(traceback.format_exc())

    if backend_path:
        if not any(
            contained_in(obj.__file__, path)
            for path in extra_pathitems
        ):
            raise BackendInvalid(""Backend was not loaded from backend-path"")

    if obj_path:
        for path_part in obj_path.split('.'):
            obj = getattr(obj, path_part)
    return obj","def find_and_load_build_backend_add_in_tree_backend_directories():
    """"""Find and load the build backend""""""
    # Add in-tree backend directories to the front of sys.path.
    backend_path = os.environ.get('PEP517_BACKEND_PATH')
    if backend_path:
        extra_pathitems = backend_path.split(os.pathsep)
        sys.path[:0] = extra_pathitems

    ep = os.environ['PEP517_BUILD_BACKEND']
    mod_path, _, obj_path = ep.partition(':')
    try:
        obj = import_module(mod_path)
    except ImportError:
        raise BackendUnavailable(traceback.format_exc())

    if backend_path:
        if not any(
            contained_in(obj.__file__, path)
            for path in extra_pathitems
        ):
            raise BackendInvalid(""Backend was not loaded from backend-path"")

    if obj_path:
        for path_part in obj_path.split('.'):
            obj = getattr(obj, path_part)
    return obj","def list_supported_options_features_supported_by_backend():
    """"""Return the list of options features supported by the backend.

    Returns a list of strings.
    The only possible value is 'build_editable'.
    """"""
    backend = find_and_load_build_backend_add_in_tree_backend_directories()
    features = []
    if hasattr(backend, ""build_editable""):
        features.append(""build_editable"")
    return features","{""_build_backend"": ""find_and_load_build_backend_add_in_tree_backend_directories""}","def 0():
    """"""Find and load the build backend""""""
    # Add in-tree backend directories to the front of sys.path.
    backend_path = os.environ.get('PEP517_BACKEND_PATH')
    if backend_path:
        extra_pathitems = backend_path.split(os.pathsep)
        sys.path[:0] = extra_pathitems

    ep = os.environ['PEP517_BUILD_BACKEND']
    mod_path, _, obj_path = ep.partition(':')
    try:
        obj = import_module(mod_path)
    except ImportError:
        raise BackendUnavailable(traceback.format_exc())

    if backend_path:
        if not any(
            contained_in(obj.__file__, path)
            for path in extra_pathitems
        ):
            raise BackendInvalid(""Backend was not loaded from backend-path"")

    if obj_path:
        for path_part in obj_path.split('.'):
            obj = getattr(obj, path_part)
    return obj","def 1():
    """"""Return the list of options features supported by the backend.

    Returns a list of strings.
    The only possible value is 'build_editable'.
    """"""
    backend = 0()
    features = []
    if hasattr(backend, ""build_editable""):
        features.append(""build_editable"")
    return features","{""_build_backend"": ""0""}"
381,381,"def get_requires_for_build_wheel(config_settings):
    """"""Invoke the optional get_requires_for_build_wheel hook

    Returns [] if the hook is not defined.
    """"""
    backend = _build_backend()
    try:
        hook = backend.get_requires_for_build_wheel
    except AttributeError:
        return []
    else:
        return hook(config_settings)","def _build_backend():
    """"""Find and load the build backend""""""
    # Add in-tree backend directories to the front of sys.path.
    backend_path = os.environ.get('PEP517_BACKEND_PATH')
    if backend_path:
        extra_pathitems = backend_path.split(os.pathsep)
        sys.path[:0] = extra_pathitems

    ep = os.environ['PEP517_BUILD_BACKEND']
    mod_path, _, obj_path = ep.partition(':')
    try:
        obj = import_module(mod_path)
    except ImportError:
        raise BackendUnavailable(traceback.format_exc())

    if backend_path:
        if not any(
            contained_in(obj.__file__, path)
            for path in extra_pathitems
        ):
            raise BackendInvalid(""Backend was not loaded from backend-path"")

    if obj_path:
        for path_part in obj_path.split('.'):
            obj = getattr(obj, path_part)
    return obj","def find_and_load_build_backend_add_in_tree_backend_directories():
    """"""Find and load the build backend""""""
    # Add in-tree backend directories to the front of sys.path.
    backend_path = os.environ.get('PEP517_BACKEND_PATH')
    if backend_path:
        extra_pathitems = backend_path.split(os.pathsep)
        sys.path[:0] = extra_pathitems

    ep = os.environ['PEP517_BUILD_BACKEND']
    mod_path, _, obj_path = ep.partition(':')
    try:
        obj = import_module(mod_path)
    except ImportError:
        raise BackendUnavailable(traceback.format_exc())

    if backend_path:
        if not any(
            contained_in(obj.__file__, path)
            for path in extra_pathitems
        ):
            raise BackendInvalid(""Backend was not loaded from backend-path"")

    if obj_path:
        for path_part in obj_path.split('.'):
            obj = getattr(obj, path_part)
    return obj","def invoke_optional_hook_get_requires_for_build_wheel(config_settings):
    """"""Invoke the optional invoke_optional_hook_get_requires_for_build_wheel hook

    Returns [] if the hook is not defined.
    """"""
    backend = find_and_load_build_backend_add_in_tree_backend_directories()
    try:
        hook = backend.invoke_optional_hook_get_requires_for_build_wheel
    except AttributeError:
        return []
    else:
        return hook(config_settings)","{""_build_backend"": ""find_and_load_build_backend_add_in_tree_backend_directories""}","def 0():
    """"""Find and load the build backend""""""
    # Add in-tree backend directories to the front of sys.path.
    backend_path = os.environ.get('PEP517_BACKEND_PATH')
    if backend_path:
        extra_pathitems = backend_path.split(os.pathsep)
        sys.path[:0] = extra_pathitems

    ep = os.environ['PEP517_BUILD_BACKEND']
    mod_path, _, obj_path = ep.partition(':')
    try:
        obj = import_module(mod_path)
    except ImportError:
        raise BackendUnavailable(traceback.format_exc())

    if backend_path:
        if not any(
            contained_in(obj.__file__, path)
            for path in extra_pathitems
        ):
            raise BackendInvalid(""Backend was not loaded from backend-path"")

    if obj_path:
        for path_part in obj_path.split('.'):
            obj = getattr(obj, path_part)
    return obj","def 1(config_settings):
    """"""Invoke the optional get_requires_for_build_wheel hook

    Returns [] if the hook is not defined.
    """"""
    backend = 0()
    try:
        hook = backend.get_requires_for_build_wheel
    except AttributeError:
        return []
    else:
        return hook(config_settings)","{""_build_backend"": ""0""}"
382,382,"def get_requires_for_build_editable(config_settings):
    """"""Invoke the optional get_requires_for_build_editable hook

    Returns [] if the hook is not defined.
    """"""
    backend = _build_backend()
    try:
        hook = backend.get_requires_for_build_editable
    except AttributeError:
        return []
    else:
        return hook(config_settings)","def _build_backend():
    """"""Find and load the build backend""""""
    # Add in-tree backend directories to the front of sys.path.
    backend_path = os.environ.get('PEP517_BACKEND_PATH')
    if backend_path:
        extra_pathitems = backend_path.split(os.pathsep)
        sys.path[:0] = extra_pathitems

    ep = os.environ['PEP517_BUILD_BACKEND']
    mod_path, _, obj_path = ep.partition(':')
    try:
        obj = import_module(mod_path)
    except ImportError:
        raise BackendUnavailable(traceback.format_exc())

    if backend_path:
        if not any(
            contained_in(obj.__file__, path)
            for path in extra_pathitems
        ):
            raise BackendInvalid(""Backend was not loaded from backend-path"")

    if obj_path:
        for path_part in obj_path.split('.'):
            obj = getattr(obj, path_part)
    return obj","def find_and_load_build_backend_add_in_tree_backend_directories():
    """"""Find and load the build backend""""""
    # Add in-tree backend directories to the front of sys.path.
    backend_path = os.environ.get('PEP517_BACKEND_PATH')
    if backend_path:
        extra_pathitems = backend_path.split(os.pathsep)
        sys.path[:0] = extra_pathitems

    ep = os.environ['PEP517_BUILD_BACKEND']
    mod_path, _, obj_path = ep.partition(':')
    try:
        obj = import_module(mod_path)
    except ImportError:
        raise BackendUnavailable(traceback.format_exc())

    if backend_path:
        if not any(
            contained_in(obj.__file__, path)
            for path in extra_pathitems
        ):
            raise BackendInvalid(""Backend was not loaded from backend-path"")

    if obj_path:
        for path_part in obj_path.split('.'):
            obj = getattr(obj, path_part)
    return obj","def invoke_optional_hook_get_requires_for_build_editable(config_settings):
    """"""Invoke the optional invoke_optional_hook_get_requires_for_build_editable hook

    Returns [] if the hook is not defined.
    """"""
    backend = find_and_load_build_backend_add_in_tree_backend_directories()
    try:
        hook = backend.invoke_optional_hook_get_requires_for_build_editable
    except AttributeError:
        return []
    else:
        return hook(config_settings)","{""_build_backend"": ""find_and_load_build_backend_add_in_tree_backend_directories""}","def 0():
    """"""Find and load the build backend""""""
    # Add in-tree backend directories to the front of sys.path.
    backend_path = os.environ.get('PEP517_BACKEND_PATH')
    if backend_path:
        extra_pathitems = backend_path.split(os.pathsep)
        sys.path[:0] = extra_pathitems

    ep = os.environ['PEP517_BUILD_BACKEND']
    mod_path, _, obj_path = ep.partition(':')
    try:
        obj = import_module(mod_path)
    except ImportError:
        raise BackendUnavailable(traceback.format_exc())

    if backend_path:
        if not any(
            contained_in(obj.__file__, path)
            for path in extra_pathitems
        ):
            raise BackendInvalid(""Backend was not loaded from backend-path"")

    if obj_path:
        for path_part in obj_path.split('.'):
            obj = getattr(obj, path_part)
    return obj","def 1(config_settings):
    """"""Invoke the optional get_requires_for_build_editable hook

    Returns [] if the hook is not defined.
    """"""
    backend = 0()
    try:
        hook = backend.get_requires_for_build_editable
    except AttributeError:
        return []
    else:
        return hook(config_settings)","{""_build_backend"": ""0""}"
383,383,"def prepare_metadata_for_build_wheel(
        metadata_directory, config_settings, _allow_fallback):
    """"""Invoke optional prepare_metadata_for_build_wheel

    Implements a fallback by building a wheel if the hook isn't defined,
    unless _allow_fallback is False in which case HookMissing is raised.
    """"""
    backend = _build_backend()
    try:
        hook = backend.prepare_metadata_for_build_wheel
    except AttributeError:
        if not _allow_fallback:
            raise HookMissing()
        whl_basename = backend.build_wheel(metadata_directory, config_settings)
        return _get_wheel_metadata_from_wheel(whl_basename, metadata_directory,
                                              config_settings)
    else:
        return hook(metadata_directory, config_settings)","def _get_wheel_metadata_from_wheel(
        whl_basename, metadata_directory, config_settings):
    """"""Extract the metadata from a wheel.

    Fallback for when the build backend does not
    define the 'get_wheel_metadata' hook.
    """"""
    from zipfile import ZipFile
    with open(os.path.join(metadata_directory, WHEEL_BUILT_MARKER), 'wb'):
        pass  # Touch marker file

    whl_file = os.path.join(metadata_directory, whl_basename)
    with ZipFile(whl_file) as zipf:
        dist_info = _dist_info_files(zipf)
        zipf.extractall(path=metadata_directory, members=dist_info)
    return dist_info[0].split('/')[0]

def _build_backend():
    """"""Find and load the build backend""""""
    # Add in-tree backend directories to the front of sys.path.
    backend_path = os.environ.get('PEP517_BACKEND_PATH')
    if backend_path:
        extra_pathitems = backend_path.split(os.pathsep)
        sys.path[:0] = extra_pathitems

    ep = os.environ['PEP517_BUILD_BACKEND']
    mod_path, _, obj_path = ep.partition(':')
    try:
        obj = import_module(mod_path)
    except ImportError:
        raise BackendUnavailable(traceback.format_exc())

    if backend_path:
        if not any(
            contained_in(obj.__file__, path)
            for path in extra_pathitems
        ):
            raise BackendInvalid(""Backend was not loaded from backend-path"")

    if obj_path:
        for path_part in obj_path.split('.'):
            obj = getattr(obj, path_part)
    return obj","def extract_metadata_from_wheel(
        whl_basename, metadata_directory, config_settings):
    """"""Extract the metadata from a wheel.

    Fallback for when the build backend does not
    define the 'get_wheel_metadata' hook.
    """"""
    from zipfile import ZipFile
    with open(os.path.join(metadata_directory, WHEEL_BUILT_MARKER), 'wb'):
        pass  # Touch marker file

    whl_file = os.path.join(metadata_directory, whl_basename)
    with ZipFile(whl_file) as zipf:
        dist_info = _dist_info_files(zipf)
        zipf.extractall(path=metadata_directory, members=dist_info)
    return dist_info[0].split('/')[0]

def find_and_load_build_backend_add_in_tree_backend_directories():
    """"""Find and load the build backend""""""
    # Add in-tree backend directories to the front of sys.path.
    backend_path = os.environ.get('PEP517_BACKEND_PATH')
    if backend_path:
        extra_pathitems = backend_path.split(os.pathsep)
        sys.path[:0] = extra_pathitems

    ep = os.environ['PEP517_BUILD_BACKEND']
    mod_path, _, obj_path = ep.partition(':')
    try:
        obj = import_module(mod_path)
    except ImportError:
        raise BackendUnavailable(traceback.format_exc())

    if backend_path:
        if not any(
            contained_in(obj.__file__, path)
            for path in extra_pathitems
        ):
            raise BackendInvalid(""Backend was not loaded from backend-path"")

    if obj_path:
        for path_part in obj_path.split('.'):
            obj = getattr(obj, path_part)
    return obj","def invoke_optional_hook_prepare_metadata_for_build_wheel(
        metadata_directory, config_settings, _allow_fallback):
    """"""Invoke optional invoke_optional_hook_prepare_metadata_for_build_wheel

    Implements a fallback by building a wheel if the hook isn't defined,
    unless _allow_fallback is False in which case HookMissing is raised.
    """"""
    backend = find_and_load_build_backend_add_in_tree_backend_directories()
    try:
        hook = backend.invoke_optional_hook_prepare_metadata_for_build_wheel
    except AttributeError:
        if not _allow_fallback:
            raise HookMissing()
        whl_basename = backend.build_wheel(metadata_directory, config_settings)
        return extract_metadata_from_wheel(whl_basename, metadata_directory,
                                              config_settings)
    else:
        return hook(metadata_directory, config_settings)","{""_get_wheel_metadata_from_wheel"": ""extract_metadata_from_wheel"", ""_build_backend"": ""find_and_load_build_backend_add_in_tree_backend_directories""}","def 0(
        whl_basename, metadata_directory, config_settings):
    """"""Extract the metadata from a wheel.

    Fallback for when the build backend does not
    define the 'get_wheel_metadata' hook.
    """"""
    from zipfile import ZipFile
    with open(os.path.join(metadata_directory, WHEEL_BUILT_MARKER), 'wb'):
        pass  # Touch marker file

    whl_file = os.path.join(metadata_directory, whl_basename)
    with ZipFile(whl_file) as zipf:
        dist_info = _dist_info_files(zipf)
        zipf.extractall(path=metadata_directory, members=dist_info)
    return dist_info[0].split('/')[0]

def 1():
    """"""Find and load the build backend""""""
    # Add in-tree backend directories to the front of sys.path.
    backend_path = os.environ.get('PEP517_BACKEND_PATH')
    if backend_path:
        extra_pathitems = backend_path.split(os.pathsep)
        sys.path[:0] = extra_pathitems

    ep = os.environ['PEP517_BUILD_BACKEND']
    mod_path, _, obj_path = ep.partition(':')
    try:
        obj = import_module(mod_path)
    except ImportError:
        raise BackendUnavailable(traceback.format_exc())

    if backend_path:
        if not any(
            contained_in(obj.__file__, path)
            for path in extra_pathitems
        ):
            raise BackendInvalid(""Backend was not loaded from backend-path"")

    if obj_path:
        for path_part in obj_path.split('.'):
            obj = getattr(obj, path_part)
    return obj","def 2(
        metadata_directory, config_settings, _allow_fallback):
    """"""Invoke optional prepare_metadata_for_build_wheel

    Implements a fallback by building a wheel if the hook isn't defined,
    unless _allow_fallback is False in which case HookMissing is raised.
    """"""
    backend = 1()
    try:
        hook = backend.prepare_metadata_for_build_wheel
    except AttributeError:
        if not _allow_fallback:
            raise HookMissing()
        whl_basename = backend.build_wheel(metadata_directory, config_settings)
        return 0(whl_basename, metadata_directory,
                                              config_settings)
    else:
        return hook(metadata_directory, config_settings)","{""_get_wheel_metadata_from_wheel"": ""0"", ""_build_backend"": ""1""}"
384,384,"def prepare_metadata_for_build_editable(
        metadata_directory, config_settings, _allow_fallback):
    """"""Invoke optional prepare_metadata_for_build_editable

    Implements a fallback by building an editable wheel if the hook isn't
    defined, unless _allow_fallback is False in which case HookMissing is
    raised.
    """"""
    backend = _build_backend()
    try:
        hook = backend.prepare_metadata_for_build_editable
    except AttributeError:
        if not _allow_fallback:
            raise HookMissing()
        try:
            build_hook = backend.build_editable
        except AttributeError:
            raise HookMissing(hook_name='build_editable')
        else:
            whl_basename = build_hook(metadata_directory, config_settings)
            return _get_wheel_metadata_from_wheel(whl_basename,
                                                  metadata_directory,
                                                  config_settings)
    else:
        return hook(metadata_directory, config_settings)","def _get_wheel_metadata_from_wheel(
        whl_basename, metadata_directory, config_settings):
    """"""Extract the metadata from a wheel.

    Fallback for when the build backend does not
    define the 'get_wheel_metadata' hook.
    """"""
    from zipfile import ZipFile
    with open(os.path.join(metadata_directory, WHEEL_BUILT_MARKER), 'wb'):
        pass  # Touch marker file

    whl_file = os.path.join(metadata_directory, whl_basename)
    with ZipFile(whl_file) as zipf:
        dist_info = _dist_info_files(zipf)
        zipf.extractall(path=metadata_directory, members=dist_info)
    return dist_info[0].split('/')[0]

def _build_backend():
    """"""Find and load the build backend""""""
    # Add in-tree backend directories to the front of sys.path.
    backend_path = os.environ.get('PEP517_BACKEND_PATH')
    if backend_path:
        extra_pathitems = backend_path.split(os.pathsep)
        sys.path[:0] = extra_pathitems

    ep = os.environ['PEP517_BUILD_BACKEND']
    mod_path, _, obj_path = ep.partition(':')
    try:
        obj = import_module(mod_path)
    except ImportError:
        raise BackendUnavailable(traceback.format_exc())

    if backend_path:
        if not any(
            contained_in(obj.__file__, path)
            for path in extra_pathitems
        ):
            raise BackendInvalid(""Backend was not loaded from backend-path"")

    if obj_path:
        for path_part in obj_path.split('.'):
            obj = getattr(obj, path_part)
    return obj","def extract_metadata_from_wheel(
        whl_basename, metadata_directory, config_settings):
    """"""Extract the metadata from a wheel.

    Fallback for when the build backend does not
    define the 'get_wheel_metadata' hook.
    """"""
    from zipfile import ZipFile
    with open(os.path.join(metadata_directory, WHEEL_BUILT_MARKER), 'wb'):
        pass  # Touch marker file

    whl_file = os.path.join(metadata_directory, whl_basename)
    with ZipFile(whl_file) as zipf:
        dist_info = _dist_info_files(zipf)
        zipf.extractall(path=metadata_directory, members=dist_info)
    return dist_info[0].split('/')[0]

def find_and_load_build_backend_add_in_tree_backend_directories():
    """"""Find and load the build backend""""""
    # Add in-tree backend directories to the front of sys.path.
    backend_path = os.environ.get('PEP517_BACKEND_PATH')
    if backend_path:
        extra_pathitems = backend_path.split(os.pathsep)
        sys.path[:0] = extra_pathitems

    ep = os.environ['PEP517_BUILD_BACKEND']
    mod_path, _, obj_path = ep.partition(':')
    try:
        obj = import_module(mod_path)
    except ImportError:
        raise BackendUnavailable(traceback.format_exc())

    if backend_path:
        if not any(
            contained_in(obj.__file__, path)
            for path in extra_pathitems
        ):
            raise BackendInvalid(""Backend was not loaded from backend-path"")

    if obj_path:
        for path_part in obj_path.split('.'):
            obj = getattr(obj, path_part)
    return obj","def invoke_optional_hook_prepare_metadata_for_build_editable(
        metadata_directory, config_settings, _allow_fallback):
    """"""Invoke optional invoke_optional_hook_prepare_metadata_for_build_editable

    Implements a fallback by building an editable wheel if the hook isn't
    defined, unless _allow_fallback is False in which case HookMissing is
    raised.
    """"""
    backend = find_and_load_build_backend_add_in_tree_backend_directories()
    try:
        hook = backend.invoke_optional_hook_prepare_metadata_for_build_editable
    except AttributeError:
        if not _allow_fallback:
            raise HookMissing()
        try:
            build_hook = backend.build_editable
        except AttributeError:
            raise HookMissing(hook_name='build_editable')
        else:
            whl_basename = build_hook(metadata_directory, config_settings)
            return extract_metadata_from_wheel(whl_basename,
                                                  metadata_directory,
                                                  config_settings)
    else:
        return hook(metadata_directory, config_settings)","{""_get_wheel_metadata_from_wheel"": ""extract_metadata_from_wheel"", ""_build_backend"": ""find_and_load_build_backend_add_in_tree_backend_directories""}","def 0(
        whl_basename, metadata_directory, config_settings):
    """"""Extract the metadata from a wheel.

    Fallback for when the build backend does not
    define the 'get_wheel_metadata' hook.
    """"""
    from zipfile import ZipFile
    with open(os.path.join(metadata_directory, WHEEL_BUILT_MARKER), 'wb'):
        pass  # Touch marker file

    whl_file = os.path.join(metadata_directory, whl_basename)
    with ZipFile(whl_file) as zipf:
        dist_info = _dist_info_files(zipf)
        zipf.extractall(path=metadata_directory, members=dist_info)
    return dist_info[0].split('/')[0]

def 1():
    """"""Find and load the build backend""""""
    # Add in-tree backend directories to the front of sys.path.
    backend_path = os.environ.get('PEP517_BACKEND_PATH')
    if backend_path:
        extra_pathitems = backend_path.split(os.pathsep)
        sys.path[:0] = extra_pathitems

    ep = os.environ['PEP517_BUILD_BACKEND']
    mod_path, _, obj_path = ep.partition(':')
    try:
        obj = import_module(mod_path)
    except ImportError:
        raise BackendUnavailable(traceback.format_exc())

    if backend_path:
        if not any(
            contained_in(obj.__file__, path)
            for path in extra_pathitems
        ):
            raise BackendInvalid(""Backend was not loaded from backend-path"")

    if obj_path:
        for path_part in obj_path.split('.'):
            obj = getattr(obj, path_part)
    return obj","def 2(
        metadata_directory, config_settings, _allow_fallback):
    """"""Invoke optional prepare_metadata_for_build_editable

    Implements a fallback by building an editable wheel if the hook isn't
    defined, unless _allow_fallback is False in which case HookMissing is
    raised.
    """"""
    backend = 1()
    try:
        hook = backend.prepare_metadata_for_build_editable
    except AttributeError:
        if not _allow_fallback:
            raise HookMissing()
        try:
            build_hook = backend.build_editable
        except AttributeError:
            raise HookMissing(hook_name='build_editable')
        else:
            whl_basename = build_hook(metadata_directory, config_settings)
            return 0(whl_basename,
                                                  metadata_directory,
                                                  config_settings)
    else:
        return hook(metadata_directory, config_settings)","{""_get_wheel_metadata_from_wheel"": ""0"", ""_build_backend"": ""1""}"
385,385,"def build_wheel(wheel_directory, config_settings, metadata_directory=None):
    """"""Invoke the mandatory build_wheel hook.

    If a wheel was already built in the
    prepare_metadata_for_build_wheel fallback, this
    will copy it rather than rebuilding the wheel.
    """"""
    prebuilt_whl = _find_already_built_wheel(metadata_directory)
    if prebuilt_whl:
        shutil.copy2(prebuilt_whl, wheel_directory)
        return os.path.basename(prebuilt_whl)

    return _build_backend().build_wheel(wheel_directory, config_settings,
                                        metadata_directory)","def _build_backend():
    """"""Find and load the build backend""""""
    # Add in-tree backend directories to the front of sys.path.
    backend_path = os.environ.get('PEP517_BACKEND_PATH')
    if backend_path:
        extra_pathitems = backend_path.split(os.pathsep)
        sys.path[:0] = extra_pathitems

    ep = os.environ['PEP517_BUILD_BACKEND']
    mod_path, _, obj_path = ep.partition(':')
    try:
        obj = import_module(mod_path)
    except ImportError:
        raise BackendUnavailable(traceback.format_exc())

    if backend_path:
        if not any(
            contained_in(obj.__file__, path)
            for path in extra_pathitems
        ):
            raise BackendInvalid(""Backend was not loaded from backend-path"")

    if obj_path:
        for path_part in obj_path.split('.'):
            obj = getattr(obj, path_part)
    return obj

def _find_already_built_wheel(metadata_directory):
    """"""Check for a wheel already built during the get_wheel_metadata hook.
    """"""
    if not metadata_directory:
        return None
    metadata_parent = os.path.dirname(metadata_directory)
    if not os.path.isfile(pjoin(metadata_parent, WHEEL_BUILT_MARKER)):
        return None

    whl_files = glob(os.path.join(metadata_parent, '*.whl'))
    if not whl_files:
        print('Found wheel built marker, but no .whl files')
        return None
    if len(whl_files) > 1:
        print('Found multiple .whl files; unspecified behaviour. '
              'Will call build_wheel.')
        return None

    # Exactly one .whl file
    return whl_files[0]","def find_and_load_build_backend_add_in_tree_backend_directories():
    """"""Find and load the build backend""""""
    # Add in-tree backend directories to the front of sys.path.
    backend_path = os.environ.get('PEP517_BACKEND_PATH')
    if backend_path:
        extra_pathitems = backend_path.split(os.pathsep)
        sys.path[:0] = extra_pathitems

    ep = os.environ['PEP517_BUILD_BACKEND']
    mod_path, _, obj_path = ep.partition(':')
    try:
        obj = import_module(mod_path)
    except ImportError:
        raise BackendUnavailable(traceback.format_exc())

    if backend_path:
        if not any(
            contained_in(obj.__file__, path)
            for path in extra_pathitems
        ):
            raise BackendInvalid(""Backend was not loaded from backend-path"")

    if obj_path:
        for path_part in obj_path.split('.'):
            obj = getattr(obj, path_part)
    return obj

def check_for_already_built_wheel_get_wheel_metadata_hook(metadata_directory):
    """"""Check for a wheel already built during the get_wheel_metadata hook.
    """"""
    if not metadata_directory:
        return None
    metadata_parent = os.path.dirname(metadata_directory)
    if not os.path.isfile(pjoin(metadata_parent, WHEEL_BUILT_MARKER)):
        return None

    whl_files = glob(os.path.join(metadata_parent, '*.whl'))
    if not whl_files:
        print('Found wheel built marker, but no .whl files')
        return None
    if len(whl_files) > 1:
        print('Found multiple .whl files; unspecified behaviour. '
              'Will call build_wheel.')
        return None

    # Exactly one .whl file
    return whl_files[0]","def invoke_mandatory_hook_build_wheel_with_copy_if_prebuilt(wheel_directory, config_settings, metadata_directory=None):
    """"""Invoke the mandatory invoke_mandatory_hook_build_wheel_with_copy_if_prebuilt hook.

    If a wheel was already built in the
    prepare_metadata_for_invoke_mandatory_hook_build_wheel_with_copy_if_prebuilt fallback, this
    will copy it rather than rebuilding the wheel.
    """"""
    prebuilt_whl = check_for_already_built_wheel_get_wheel_metadata_hook(metadata_directory)
    if prebuilt_whl:
        shutil.copy2(prebuilt_whl, wheel_directory)
        return os.path.basename(prebuilt_whl)

    return find_and_load_build_backend_add_in_tree_backend_directories().invoke_mandatory_hook_build_wheel_with_copy_if_prebuilt(wheel_directory, config_settings,
                                        metadata_directory)","{""_build_backend"": ""find_and_load_build_backend_add_in_tree_backend_directories"", ""_find_already_built_wheel"": ""check_for_already_built_wheel_get_wheel_metadata_hook""}","def 0():
    """"""Find and load the build backend""""""
    # Add in-tree backend directories to the front of sys.path.
    backend_path = os.environ.get('PEP517_BACKEND_PATH')
    if backend_path:
        extra_pathitems = backend_path.split(os.pathsep)
        sys.path[:0] = extra_pathitems

    ep = os.environ['PEP517_BUILD_BACKEND']
    mod_path, _, obj_path = ep.partition(':')
    try:
        obj = import_module(mod_path)
    except ImportError:
        raise BackendUnavailable(traceback.format_exc())

    if backend_path:
        if not any(
            contained_in(obj.__file__, path)
            for path in extra_pathitems
        ):
            raise BackendInvalid(""Backend was not loaded from backend-path"")

    if obj_path:
        for path_part in obj_path.split('.'):
            obj = getattr(obj, path_part)
    return obj

def 1(metadata_directory):
    """"""Check for a wheel already built during the get_wheel_metadata hook.
    """"""
    if not metadata_directory:
        return None
    metadata_parent = os.path.dirname(metadata_directory)
    if not os.path.isfile(pjoin(metadata_parent, WHEEL_BUILT_MARKER)):
        return None

    whl_files = glob(os.path.join(metadata_parent, '*.whl'))
    if not whl_files:
        print('Found wheel built marker, but no .whl files')
        return None
    if len(whl_files) > 1:
        print('Found multiple .whl files; unspecified behaviour. '
              'Will call build_wheel.')
        return None

    # Exactly one .whl file
    return whl_files[0]","def 2(wheel_directory, config_settings, metadata_directory=None):
    """"""Invoke the mandatory build_wheel hook.

    If a wheel was already built in the
    prepare_metadata_for_build_wheel fallback, this
    will copy it rather than rebuilding the wheel.
    """"""
    prebuilt_whl = 1(metadata_directory)
    if prebuilt_whl:
        shutil.copy2(prebuilt_whl, wheel_directory)
        return os.path.basename(prebuilt_whl)

    return 0().2(wheel_directory, config_settings,
                                        metadata_directory)","{""_build_backend"": ""0"", ""_find_already_built_wheel"": ""1""}"
386,386,"def build_editable(wheel_directory, config_settings, metadata_directory=None):
    """"""Invoke the optional build_editable hook.

    If a wheel was already built in the
    prepare_metadata_for_build_editable fallback, this
    will copy it rather than rebuilding the wheel.
    """"""
    backend = _build_backend()
    try:
        hook = backend.build_editable
    except AttributeError:
        raise HookMissing()
    else:
        prebuilt_whl = _find_already_built_wheel(metadata_directory)
        if prebuilt_whl:
            shutil.copy2(prebuilt_whl, wheel_directory)
            return os.path.basename(prebuilt_whl)

        return hook(wheel_directory, config_settings, metadata_directory)","def _build_backend():
    """"""Find and load the build backend""""""
    # Add in-tree backend directories to the front of sys.path.
    backend_path = os.environ.get('PEP517_BACKEND_PATH')
    if backend_path:
        extra_pathitems = backend_path.split(os.pathsep)
        sys.path[:0] = extra_pathitems

    ep = os.environ['PEP517_BUILD_BACKEND']
    mod_path, _, obj_path = ep.partition(':')
    try:
        obj = import_module(mod_path)
    except ImportError:
        raise BackendUnavailable(traceback.format_exc())

    if backend_path:
        if not any(
            contained_in(obj.__file__, path)
            for path in extra_pathitems
        ):
            raise BackendInvalid(""Backend was not loaded from backend-path"")

    if obj_path:
        for path_part in obj_path.split('.'):
            obj = getattr(obj, path_part)
    return obj

def _find_already_built_wheel(metadata_directory):
    """"""Check for a wheel already built during the get_wheel_metadata hook.
    """"""
    if not metadata_directory:
        return None
    metadata_parent = os.path.dirname(metadata_directory)
    if not os.path.isfile(pjoin(metadata_parent, WHEEL_BUILT_MARKER)):
        return None

    whl_files = glob(os.path.join(metadata_parent, '*.whl'))
    if not whl_files:
        print('Found wheel built marker, but no .whl files')
        return None
    if len(whl_files) > 1:
        print('Found multiple .whl files; unspecified behaviour. '
              'Will call build_wheel.')
        return None

    # Exactly one .whl file
    return whl_files[0]","def find_and_load_build_backend_add_in_tree_backend_directories():
    """"""Find and load the build backend""""""
    # Add in-tree backend directories to the front of sys.path.
    backend_path = os.environ.get('PEP517_BACKEND_PATH')
    if backend_path:
        extra_pathitems = backend_path.split(os.pathsep)
        sys.path[:0] = extra_pathitems

    ep = os.environ['PEP517_BUILD_BACKEND']
    mod_path, _, obj_path = ep.partition(':')
    try:
        obj = import_module(mod_path)
    except ImportError:
        raise BackendUnavailable(traceback.format_exc())

    if backend_path:
        if not any(
            contained_in(obj.__file__, path)
            for path in extra_pathitems
        ):
            raise BackendInvalid(""Backend was not loaded from backend-path"")

    if obj_path:
        for path_part in obj_path.split('.'):
            obj = getattr(obj, path_part)
    return obj

def check_for_already_built_wheel_get_wheel_metadata_hook(metadata_directory):
    """"""Check for a wheel already built during the get_wheel_metadata hook.
    """"""
    if not metadata_directory:
        return None
    metadata_parent = os.path.dirname(metadata_directory)
    if not os.path.isfile(pjoin(metadata_parent, WHEEL_BUILT_MARKER)):
        return None

    whl_files = glob(os.path.join(metadata_parent, '*.whl'))
    if not whl_files:
        print('Found wheel built marker, but no .whl files')
        return None
    if len(whl_files) > 1:
        print('Found multiple .whl files; unspecified behaviour. '
              'Will call build_wheel.')
        return None

    # Exactly one .whl file
    return whl_files[0]","def invoke_optional_hook_build_editable_with_copy_if_prebuilt(wheel_directory, config_settings, metadata_directory=None):
    """"""Invoke the optional invoke_optional_hook_build_editable_with_copy_if_prebuilt hook.

    If a wheel was already built in the
    prepare_metadata_for_invoke_optional_hook_build_editable_with_copy_if_prebuilt fallback, this
    will copy it rather than rebuilding the wheel.
    """"""
    backend = find_and_load_build_backend_add_in_tree_backend_directories()
    try:
        hook = backend.invoke_optional_hook_build_editable_with_copy_if_prebuilt
    except AttributeError:
        raise HookMissing()
    else:
        prebuilt_whl = check_for_already_built_wheel_get_wheel_metadata_hook(metadata_directory)
        if prebuilt_whl:
            shutil.copy2(prebuilt_whl, wheel_directory)
            return os.path.basename(prebuilt_whl)

        return hook(wheel_directory, config_settings, metadata_directory)","{""_build_backend"": ""find_and_load_build_backend_add_in_tree_backend_directories"", ""_find_already_built_wheel"": ""check_for_already_built_wheel_get_wheel_metadata_hook""}","def 0():
    """"""Find and load the build backend""""""
    # Add in-tree backend directories to the front of sys.path.
    backend_path = os.environ.get('PEP517_BACKEND_PATH')
    if backend_path:
        extra_pathitems = backend_path.split(os.pathsep)
        sys.path[:0] = extra_pathitems

    ep = os.environ['PEP517_BUILD_BACKEND']
    mod_path, _, obj_path = ep.partition(':')
    try:
        obj = import_module(mod_path)
    except ImportError:
        raise BackendUnavailable(traceback.format_exc())

    if backend_path:
        if not any(
            contained_in(obj.__file__, path)
            for path in extra_pathitems
        ):
            raise BackendInvalid(""Backend was not loaded from backend-path"")

    if obj_path:
        for path_part in obj_path.split('.'):
            obj = getattr(obj, path_part)
    return obj

def 1(metadata_directory):
    """"""Check for a wheel already built during the get_wheel_metadata hook.
    """"""
    if not metadata_directory:
        return None
    metadata_parent = os.path.dirname(metadata_directory)
    if not os.path.isfile(pjoin(metadata_parent, WHEEL_BUILT_MARKER)):
        return None

    whl_files = glob(os.path.join(metadata_parent, '*.whl'))
    if not whl_files:
        print('Found wheel built marker, but no .whl files')
        return None
    if len(whl_files) > 1:
        print('Found multiple .whl files; unspecified behaviour. '
              'Will call build_wheel.')
        return None

    # Exactly one .whl file
    return whl_files[0]","def 2(wheel_directory, config_settings, metadata_directory=None):
    """"""Invoke the optional build_editable hook.

    If a wheel was already built in the
    prepare_metadata_for_build_editable fallback, this
    will copy it rather than rebuilding the wheel.
    """"""
    backend = 0()
    try:
        hook = backend.build_editable
    except AttributeError:
        raise HookMissing()
    else:
        prebuilt_whl = 1(metadata_directory)
        if prebuilt_whl:
            shutil.copy2(prebuilt_whl, wheel_directory)
            return os.path.basename(prebuilt_whl)

        return hook(wheel_directory, config_settings, metadata_directory)","{""_build_backend"": ""0"", ""_find_already_built_wheel"": ""1""}"
387,387,"def get_requires_for_build_sdist(config_settings):
    """"""Invoke the optional get_requires_for_build_wheel hook

    Returns [] if the hook is not defined.
    """"""
    backend = _build_backend()
    try:
        hook = backend.get_requires_for_build_sdist
    except AttributeError:
        return []
    else:
        return hook(config_settings)","def _build_backend():
    """"""Find and load the build backend""""""
    # Add in-tree backend directories to the front of sys.path.
    backend_path = os.environ.get('PEP517_BACKEND_PATH')
    if backend_path:
        extra_pathitems = backend_path.split(os.pathsep)
        sys.path[:0] = extra_pathitems

    ep = os.environ['PEP517_BUILD_BACKEND']
    mod_path, _, obj_path = ep.partition(':')
    try:
        obj = import_module(mod_path)
    except ImportError:
        raise BackendUnavailable(traceback.format_exc())

    if backend_path:
        if not any(
            contained_in(obj.__file__, path)
            for path in extra_pathitems
        ):
            raise BackendInvalid(""Backend was not loaded from backend-path"")

    if obj_path:
        for path_part in obj_path.split('.'):
            obj = getattr(obj, path_part)
    return obj","def find_and_load_build_backend_add_in_tree_backend_directories():
    """"""Find and load the build backend""""""
    # Add in-tree backend directories to the front of sys.path.
    backend_path = os.environ.get('PEP517_BACKEND_PATH')
    if backend_path:
        extra_pathitems = backend_path.split(os.pathsep)
        sys.path[:0] = extra_pathitems

    ep = os.environ['PEP517_BUILD_BACKEND']
    mod_path, _, obj_path = ep.partition(':')
    try:
        obj = import_module(mod_path)
    except ImportError:
        raise BackendUnavailable(traceback.format_exc())

    if backend_path:
        if not any(
            contained_in(obj.__file__, path)
            for path in extra_pathitems
        ):
            raise BackendInvalid(""Backend was not loaded from backend-path"")

    if obj_path:
        for path_part in obj_path.split('.'):
            obj = getattr(obj, path_part)
    return obj","def invoke_optional_hook_get_requires_for_build_sdist(config_settings):
    """"""Invoke the optional get_requires_for_build_wheel hook

    Returns [] if the hook is not defined.
    """"""
    backend = find_and_load_build_backend_add_in_tree_backend_directories()
    try:
        hook = backend.invoke_optional_hook_get_requires_for_build_sdist
    except AttributeError:
        return []
    else:
        return hook(config_settings)","{""_build_backend"": ""find_and_load_build_backend_add_in_tree_backend_directories""}","def 0():
    """"""Find and load the build backend""""""
    # Add in-tree backend directories to the front of sys.path.
    backend_path = os.environ.get('PEP517_BACKEND_PATH')
    if backend_path:
        extra_pathitems = backend_path.split(os.pathsep)
        sys.path[:0] = extra_pathitems

    ep = os.environ['PEP517_BUILD_BACKEND']
    mod_path, _, obj_path = ep.partition(':')
    try:
        obj = import_module(mod_path)
    except ImportError:
        raise BackendUnavailable(traceback.format_exc())

    if backend_path:
        if not any(
            contained_in(obj.__file__, path)
            for path in extra_pathitems
        ):
            raise BackendInvalid(""Backend was not loaded from backend-path"")

    if obj_path:
        for path_part in obj_path.split('.'):
            obj = getattr(obj, path_part)
    return obj","def 1(config_settings):
    """"""Invoke the optional get_requires_for_build_wheel hook

    Returns [] if the hook is not defined.
    """"""
    backend = 0()
    try:
        hook = backend.get_requires_for_build_sdist
    except AttributeError:
        return []
    else:
        return hook(config_settings)","{""_build_backend"": ""0""}"
388,388,"def build_sdist(sdist_directory, config_settings):
    """"""Invoke the mandatory build_sdist hook.""""""
    backend = _build_backend()
    try:
        return backend.build_sdist(sdist_directory, config_settings)
    except getattr(backend, 'UnsupportedOperation', _DummyException):
        raise GotUnsupportedOperation(traceback.format_exc())","def _build_backend():
    """"""Find and load the build backend""""""
    # Add in-tree backend directories to the front of sys.path.
    backend_path = os.environ.get('PEP517_BACKEND_PATH')
    if backend_path:
        extra_pathitems = backend_path.split(os.pathsep)
        sys.path[:0] = extra_pathitems

    ep = os.environ['PEP517_BUILD_BACKEND']
    mod_path, _, obj_path = ep.partition(':')
    try:
        obj = import_module(mod_path)
    except ImportError:
        raise BackendUnavailable(traceback.format_exc())

    if backend_path:
        if not any(
            contained_in(obj.__file__, path)
            for path in extra_pathitems
        ):
            raise BackendInvalid(""Backend was not loaded from backend-path"")

    if obj_path:
        for path_part in obj_path.split('.'):
            obj = getattr(obj, path_part)
    return obj","def find_and_load_build_backend_add_in_tree_backend_directories():
    """"""Find and load the build backend""""""
    # Add in-tree backend directories to the front of sys.path.
    backend_path = os.environ.get('PEP517_BACKEND_PATH')
    if backend_path:
        extra_pathitems = backend_path.split(os.pathsep)
        sys.path[:0] = extra_pathitems

    ep = os.environ['PEP517_BUILD_BACKEND']
    mod_path, _, obj_path = ep.partition(':')
    try:
        obj = import_module(mod_path)
    except ImportError:
        raise BackendUnavailable(traceback.format_exc())

    if backend_path:
        if not any(
            contained_in(obj.__file__, path)
            for path in extra_pathitems
        ):
            raise BackendInvalid(""Backend was not loaded from backend-path"")

    if obj_path:
        for path_part in obj_path.split('.'):
            obj = getattr(obj, path_part)
    return obj","def invoke_mandatory_hook_build_sdist(sdist_directory, config_settings):
    """"""Invoke the mandatory invoke_mandatory_hook_build_sdist hook.""""""
    backend = find_and_load_build_backend_add_in_tree_backend_directories()
    try:
        return backend.invoke_mandatory_hook_build_sdist(sdist_directory, config_settings)
    except getattr(backend, 'UnsupportedOperation', _DummyException):
        raise GotUnsupportedOperation(traceback.format_exc())","{""_build_backend"": ""find_and_load_build_backend_add_in_tree_backend_directories""}","def 0():
    """"""Find and load the build backend""""""
    # Add in-tree backend directories to the front of sys.path.
    backend_path = os.environ.get('PEP517_BACKEND_PATH')
    if backend_path:
        extra_pathitems = backend_path.split(os.pathsep)
        sys.path[:0] = extra_pathitems

    ep = os.environ['PEP517_BUILD_BACKEND']
    mod_path, _, obj_path = ep.partition(':')
    try:
        obj = import_module(mod_path)
    except ImportError:
        raise BackendUnavailable(traceback.format_exc())

    if backend_path:
        if not any(
            contained_in(obj.__file__, path)
            for path in extra_pathitems
        ):
            raise BackendInvalid(""Backend was not loaded from backend-path"")

    if obj_path:
        for path_part in obj_path.split('.'):
            obj = getattr(obj, path_part)
    return obj","def 1(sdist_directory, config_settings):
    """"""Invoke the mandatory build_sdist hook.""""""
    backend = 0()
    try:
        return backend.1(sdist_directory, config_settings)
    except getattr(backend, 'UnsupportedOperation', _DummyException):
        raise GotUnsupportedOperation(traceback.format_exc())","{""_build_backend"": ""0""}"
389,389,"def get_dependent_dists(dists, dist):
    """"""Recursively generate a list of distributions from *dists* that are
    dependent on *dist*.

    :param dists: a list of distributions
    :param dist: a distribution, member of *dists* for which we are interested
    """"""
    if dist not in dists:
        raise DistlibException('given distribution %r is not a member '
                               'of the list' % dist.name)
    graph = make_graph(dists)

    dep = [dist]  # dependent distributions
    todo = graph.reverse_list[dist]  # list of nodes we should inspect

    while todo:
        d = todo.pop()
        dep.append(d)
        for succ in graph.reverse_list[d]:
            if succ not in dep:
                todo.append(succ)

    dep.pop(0)  # remove dist from dep, was there to prevent infinite loops
    return dep","def make_graph(dists, scheme='default'):
    """"""Makes a dependency graph from the given distributions.

    :parameter dists: a list of distributions
    :type dists: list of :class:`distutils2.database.InstalledDistribution` and
                 :class:`distutils2.database.EggInfoDistribution` instances
    :rtype: a :class:`DependencyGraph` instance
    """"""
    scheme = get_scheme(scheme)
    graph = DependencyGraph()
    provided = {}  # maps names to lists of (version, dist) tuples

    # first, build the graph and find out what's provided
    for dist in dists:
        graph.add_distribution(dist)

        for p in dist.provides:
            name, version = parse_name_and_version(p)
            logger.debug('Add to provided: %s, %s, %s', name, version, dist)
            provided.setdefault(name, []).append((version, dist))

    # now make the edges
    for dist in dists:
        requires = (dist.run_requires | dist.meta_requires |
                    dist.build_requires | dist.dev_requires)
        for req in requires:
            try:
                matcher = scheme.matcher(req)
            except UnsupportedVersionError:
                # XXX compat-mode if cannot read the version
                logger.warning('could not read version %r - using name only',
                               req)
                name = req.split()[0]
                matcher = scheme.matcher(name)

            name = matcher.key   # case-insensitive

            matched = False
            if name in provided:
                for version, provider in provided[name]:
                    try:
                        match = matcher.match(version)
                    except UnsupportedVersionError:
                        match = False

                    if match:
                        graph.add_edge(dist, provider, req)
                        matched = True
                        break
            if not matched:
                graph.add_missing(dist, req)
    return graph","def generate_dependency_graph(dists, scheme='default'):
    """"""Makes a dependency graph from the given distributions.

    :parameter dists: a list of distributions
    :type dists: list of :class:`distutils2.database.InstalledDistribution` and
                 :class:`distutils2.database.EggInfoDistribution` instances
    :rtype: a :class:`DependencyGraph` instance
    """"""
    scheme = get_scheme(scheme)
    graph = DependencyGraph()
    provided = {}  # maps names to lists of (version, dist) tuples

    # first, build the graph and find out what's provided
    for dist in dists:
        graph.add_distribution(dist)

        for p in dist.provides:
            name, version = parse_name_and_version(p)
            logger.debug('Add to provided: %s, %s, %s', name, version, dist)
            provided.setdefault(name, []).append((version, dist))

    # now make the edges
    for dist in dists:
        requires = (dist.run_requires | dist.meta_requires |
                    dist.build_requires | dist.dev_requires)
        for req in requires:
            try:
                matcher = scheme.matcher(req)
            except UnsupportedVersionError:
                # XXX compat-mode if cannot read the version
                logger.warning('could not read version %r - using name only',
                               req)
                name = req.split()[0]
                matcher = scheme.matcher(name)

            name = matcher.key   # case-insensitive

            matched = False
            if name in provided:
                for version, provider in provided[name]:
                    try:
                        match = matcher.match(version)
                    except UnsupportedVersionError:
                        match = False

                    if match:
                        graph.add_edge(dist, provider, req)
                        matched = True
                        break
            if not matched:
                graph.add_missing(dist, req)
    return graph","def generate_list_of_dependent_distributions_recursive(dists, dist):
    """"""Recursively generate a list of distributions from *dists* that are
    dependent on *dist*.

    :param dists: a list of distributions
    :param dist: a distribution, member of *dists* for which we are interested
    """"""
    if dist not in dists:
        raise DistlibException('given distribution %r is not a member '
                               'of the list' % dist.name)
    graph = generate_dependency_graph(dists)

    dep = [dist]  # dependent distributions
    todo = graph.reverse_list[dist]  # list of nodes we should inspect

    while todo:
        d = todo.pop()
        dep.append(d)
        for succ in graph.reverse_list[d]:
            if succ not in dep:
                todo.append(succ)

    dep.pop(0)  # remove dist from dep, was there to prevent infinite loops
    return dep","{""make_graph"": ""generate_dependency_graph""}","def 0(dists, scheme='default'):
    """"""Makes a dependency graph from the given distributions.

    :parameter dists: a list of distributions
    :type dists: list of :class:`distutils2.database.InstalledDistribution` and
                 :class:`distutils2.database.EggInfoDistribution` instances
    :rtype: a :class:`DependencyGraph` instance
    """"""
    scheme = get_scheme(scheme)
    graph = DependencyGraph()
    provided = {}  # maps names to lists of (version, dist) tuples

    # first, build the graph and find out what's provided
    for dist in dists:
        graph.add_distribution(dist)

        for p in dist.provides:
            name, version = parse_name_and_version(p)
            logger.debug('Add to provided: %s, %s, %s', name, version, dist)
            provided.setdefault(name, []).append((version, dist))

    # now make the edges
    for dist in dists:
        requires = (dist.run_requires | dist.meta_requires |
                    dist.build_requires | dist.dev_requires)
        for req in requires:
            try:
                matcher = scheme.matcher(req)
            except UnsupportedVersionError:
                # XXX compat-mode if cannot read the version
                logger.warning('could not read version %r - using name only',
                               req)
                name = req.split()[0]
                matcher = scheme.matcher(name)

            name = matcher.key   # case-insensitive

            matched = False
            if name in provided:
                for version, provider in provided[name]:
                    try:
                        match = matcher.match(version)
                    except UnsupportedVersionError:
                        match = False

                    if match:
                        graph.add_edge(dist, provider, req)
                        matched = True
                        break
            if not matched:
                graph.add_missing(dist, req)
    return graph","def 1(dists, dist):
    """"""Recursively generate a list of distributions from *dists* that are
    dependent on *dist*.

    :param dists: a list of distributions
    :param dist: a distribution, member of *dists* for which we are interested
    """"""
    if dist not in dists:
        raise DistlibException('given distribution %r is not a member '
                               'of the list' % dist.name)
    graph = 0(dists)

    dep = [dist]  # dependent distributions
    todo = graph.reverse_list[dist]  # list of nodes we should inspect

    while todo:
        d = todo.pop()
        dep.append(d)
        for succ in graph.reverse_list[d]:
            if succ not in dep:
                todo.append(succ)

    dep.pop(0)  # remove dist from dep, was there to prevent infinite loops
    return dep","{""make_graph"": ""0""}"
390,390,"def get_required_dists(dists, dist):
    """"""Recursively generate a list of distributions from *dists* that are
    required by *dist*.

    :param dists: a list of distributions
    :param dist: a distribution, member of *dists* for which we are interested
    """"""
    if dist not in dists:
        raise DistlibException('given distribution %r is not a member '
                               'of the list' % dist.name)
    graph = make_graph(dists)

    req = []  # required distributions
    todo = graph.adjacency_list[dist]  # list of nodes we should inspect

    while todo:
        d = todo.pop()[0]
        req.append(d)
        for pred in graph.adjacency_list[d]:
            if pred not in req:
                todo.append(pred)

    return req","def make_graph(dists, scheme='default'):
    """"""Makes a dependency graph from the given distributions.

    :parameter dists: a list of distributions
    :type dists: list of :class:`distutils2.database.InstalledDistribution` and
                 :class:`distutils2.database.EggInfoDistribution` instances
    :rtype: a :class:`DependencyGraph` instance
    """"""
    scheme = get_scheme(scheme)
    graph = DependencyGraph()
    provided = {}  # maps names to lists of (version, dist) tuples

    # first, build the graph and find out what's provided
    for dist in dists:
        graph.add_distribution(dist)

        for p in dist.provides:
            name, version = parse_name_and_version(p)
            logger.debug('Add to provided: %s, %s, %s', name, version, dist)
            provided.setdefault(name, []).append((version, dist))

    # now make the edges
    for dist in dists:
        requires = (dist.run_requires | dist.meta_requires |
                    dist.build_requires | dist.dev_requires)
        for req in requires:
            try:
                matcher = scheme.matcher(req)
            except UnsupportedVersionError:
                # XXX compat-mode if cannot read the version
                logger.warning('could not read version %r - using name only',
                               req)
                name = req.split()[0]
                matcher = scheme.matcher(name)

            name = matcher.key   # case-insensitive

            matched = False
            if name in provided:
                for version, provider in provided[name]:
                    try:
                        match = matcher.match(version)
                    except UnsupportedVersionError:
                        match = False

                    if match:
                        graph.add_edge(dist, provider, req)
                        matched = True
                        break
            if not matched:
                graph.add_missing(dist, req)
    return graph","def generate_dependency_graph(dists, scheme='default'):
    """"""Makes a dependency graph from the given distributions.

    :parameter dists: a list of distributions
    :type dists: list of :class:`distutils2.database.InstalledDistribution` and
                 :class:`distutils2.database.EggInfoDistribution` instances
    :rtype: a :class:`DependencyGraph` instance
    """"""
    scheme = get_scheme(scheme)
    graph = DependencyGraph()
    provided = {}  # maps names to lists of (version, dist) tuples

    # first, build the graph and find out what's provided
    for dist in dists:
        graph.add_distribution(dist)

        for p in dist.provides:
            name, version = parse_name_and_version(p)
            logger.debug('Add to provided: %s, %s, %s', name, version, dist)
            provided.setdefault(name, []).append((version, dist))

    # now make the edges
    for dist in dists:
        requires = (dist.run_requires | dist.meta_requires |
                    dist.build_requires | dist.dev_requires)
        for req in requires:
            try:
                matcher = scheme.matcher(req)
            except UnsupportedVersionError:
                # XXX compat-mode if cannot read the version
                logger.warning('could not read version %r - using name only',
                               req)
                name = req.split()[0]
                matcher = scheme.matcher(name)

            name = matcher.key   # case-insensitive

            matched = False
            if name in provided:
                for version, provider in provided[name]:
                    try:
                        match = matcher.match(version)
                    except UnsupportedVersionError:
                        match = False

                    if match:
                        graph.add_edge(dist, provider, req)
                        matched = True
                        break
            if not matched:
                graph.add_missing(dist, req)
    return graph","def generate_list_of_required_distributions_recursive(dists, dist):
    """"""Recursively generate a list of distributions from *dists* that are
    required by *dist*.

    :param dists: a list of distributions
    :param dist: a distribution, member of *dists* for which we are interested
    """"""
    if dist not in dists:
        raise DistlibException('given distribution %r is not a member '
                               'of the list' % dist.name)
    graph = generate_dependency_graph(dists)

    req = []  # required distributions
    todo = graph.adjacency_list[dist]  # list of nodes we should inspect

    while todo:
        d = todo.pop()[0]
        req.append(d)
        for pred in graph.adjacency_list[d]:
            if pred not in req:
                todo.append(pred)

    return req","{""make_graph"": ""generate_dependency_graph""}","def 0(dists, scheme='default'):
    """"""Makes a dependency graph from the given distributions.

    :parameter dists: a list of distributions
    :type dists: list of :class:`distutils2.database.InstalledDistribution` and
                 :class:`distutils2.database.EggInfoDistribution` instances
    :rtype: a :class:`DependencyGraph` instance
    """"""
    scheme = get_scheme(scheme)
    graph = DependencyGraph()
    provided = {}  # maps names to lists of (version, dist) tuples

    # first, build the graph and find out what's provided
    for dist in dists:
        graph.add_distribution(dist)

        for p in dist.provides:
            name, version = parse_name_and_version(p)
            logger.debug('Add to provided: %s, %s, %s', name, version, dist)
            provided.setdefault(name, []).append((version, dist))

    # now make the edges
    for dist in dists:
        requires = (dist.run_requires | dist.meta_requires |
                    dist.build_requires | dist.dev_requires)
        for req in requires:
            try:
                matcher = scheme.matcher(req)
            except UnsupportedVersionError:
                # XXX compat-mode if cannot read the version
                logger.warning('could not read version %r - using name only',
                               req)
                name = req.split()[0]
                matcher = scheme.matcher(name)

            name = matcher.key   # case-insensitive

            matched = False
            if name in provided:
                for version, provider in provided[name]:
                    try:
                        match = matcher.match(version)
                    except UnsupportedVersionError:
                        match = False

                    if match:
                        graph.add_edge(dist, provider, req)
                        matched = True
                        break
            if not matched:
                graph.add_missing(dist, req)
    return graph","def 1(dists, dist):
    """"""Recursively generate a list of distributions from *dists* that are
    required by *dist*.

    :param dists: a list of distributions
    :param dist: a distribution, member of *dists* for which we are interested
    """"""
    if dist not in dists:
        raise DistlibException('given distribution %r is not a member '
                               'of the list' % dist.name)
    graph = 0(dists)

    req = []  # required distributions
    todo = graph.adjacency_list[dist]  # list of nodes we should inspect

    while todo:
        d = todo.pop()[0]
        req.append(d)
        for pred in graph.adjacency_list[d]:
            if pred not in req:
                todo.append(pred)

    return req","{""make_graph"": ""0""}"
391,391,"def interpret(marker, execution_context=None):
    """"""
    Interpret a marker and return a result depending on environment.

    :param marker: The marker to interpret.
    :type marker: str
    :param execution_context: The context used for name lookup.
    :type execution_context: mapping
    """"""
    try:
        expr, rest = parse_marker(marker)
    except Exception as e:
        raise SyntaxError('Unable to interpret marker syntax: %s: %s' % (marker, e))
    if rest and rest[0] != '#':
        raise SyntaxError('unexpected trailing data in marker: %s: %s' % (marker, rest))
    context = dict(DEFAULT_CONTEXT)
    if execution_context:
        context.update(execution_context)
    return evaluator.evaluate(expr, context)","def parse_marker(marker_string):
    """"""
    Parse a marker string and return a dictionary containing a marker expression.

    The dictionary will contain keys ""op"", ""lhs"" and ""rhs"" for non-terminals in
    the expression grammar, or strings. A string contained in quotes is to be
    interpreted as a literal string, and a string not contained in quotes is a
    variable (such as os_name).
    """"""
    def marker_var(remaining):
        # either identifier, or literal string
        m = IDENTIFIER.match(remaining)
        if m:
            result = m.groups()[0]
            remaining = remaining[m.end():]
        elif not remaining:
            raise SyntaxError('unexpected end of input')
        else:
            q = remaining[0]
            if q not in '\'""':
                raise SyntaxError('invalid expression: %s' % remaining)
            oq = '\'""'.replace(q, '')
            remaining = remaining[1:]
            parts = [q]
            while remaining:
                # either a string chunk, or oq, or q to terminate
                if remaining[0] == q:
                    break
                elif remaining[0] == oq:
                    parts.append(oq)
                    remaining = remaining[1:]
                else:
                    m = STRING_CHUNK.match(remaining)
                    if not m:
                        raise SyntaxError('error in string literal: %s' % remaining)
                    parts.append(m.groups()[0])
                    remaining = remaining[m.end():]
            else:
                s = ''.join(parts)
                raise SyntaxError('unterminated string: %s' % s)
            parts.append(q)
            result = ''.join(parts)
            remaining = remaining[1:].lstrip() # skip past closing quote
        return result, remaining

    def marker_expr(remaining):
        if remaining and remaining[0] == '(':
            result, remaining = marker(remaining[1:].lstrip())
            if remaining[0] != ')':
                raise SyntaxError('unterminated parenthesis: %s' % remaining)
            remaining = remaining[1:].lstrip()
        else:
            lhs, remaining = marker_var(remaining)
            while remaining:
                m = MARKER_OP.match(remaining)
                if not m:
                    break
                op = m.groups()[0]
                remaining = remaining[m.end():]
                rhs, remaining = marker_var(remaining)
                lhs = {'op': op, 'lhs': lhs, 'rhs': rhs}
            result = lhs
        return result, remaining

    def marker_and(remaining):
        lhs, remaining = marker_expr(remaining)
        while remaining:
            m = AND.match(remaining)
            if not m:
                break
            remaining = remaining[m.end():]
            rhs, remaining = marker_expr(remaining)
            lhs = {'op': 'and', 'lhs': lhs, 'rhs': rhs}
        return lhs, remaining

    def marker(remaining):
        lhs, remaining = marker_and(remaining)
        while remaining:
            m = OR.match(remaining)
            if not m:
                break
            remaining = remaining[m.end():]
            rhs, remaining = marker_and(remaining)
            lhs = {'op': 'or', 'lhs': lhs, 'rhs': rhs}
        return lhs, remaining

    return marker(marker_string)","def parse_parse_marker_string_returning_dictionary_with_expression(parse_marker_string):
    """"""
    Parse a parse_marker string and return a dictionary containing a parse_marker expression.

    The dictionary will contain keys ""op"", ""lhs"" and ""rhs"" for non-terminals in
    the expression grammar, or strings. A string contained in quotes is to be
    interpreted as a literal string, and a string not contained in quotes is a
    variable (such as os_name).
    """"""
    def parse_marker_var(remaining):
        # either identifier, or literal string
        m = IDENTIFIER.match(remaining)
        if m:
            result = m.groups()[0]
            remaining = remaining[m.end():]
        elif not remaining:
            raise SyntaxError('unexpected end of input')
        else:
            q = remaining[0]
            if q not in '\'""':
                raise SyntaxError('invalid expression: %s' % remaining)
            oq = '\'""'.replace(q, '')
            remaining = remaining[1:]
            parts = [q]
            while remaining:
                # either a string chunk, or oq, or q to terminate
                if remaining[0] == q:
                    break
                elif remaining[0] == oq:
                    parts.append(oq)
                    remaining = remaining[1:]
                else:
                    m = STRING_CHUNK.match(remaining)
                    if not m:
                        raise SyntaxError('error in string literal: %s' % remaining)
                    parts.append(m.groups()[0])
                    remaining = remaining[m.end():]
            else:
                s = ''.join(parts)
                raise SyntaxError('unterminated string: %s' % s)
            parts.append(q)
            result = ''.join(parts)
            remaining = remaining[1:].lstrip() # skip past closing quote
        return result, remaining

    def parse_marker_expr(remaining):
        if remaining and remaining[0] == '(':
            result, remaining = parse_marker(remaining[1:].lstrip())
            if remaining[0] != ')':
                raise SyntaxError('unterminated parenthesis: %s' % remaining)
            remaining = remaining[1:].lstrip()
        else:
            lhs, remaining = parse_marker_var(remaining)
            while remaining:
                m = MARKER_OP.match(remaining)
                if not m:
                    break
                op = m.groups()[0]
                remaining = remaining[m.end():]
                rhs, remaining = parse_marker_var(remaining)
                lhs = {'op': op, 'lhs': lhs, 'rhs': rhs}
            result = lhs
        return result, remaining

    def parse_marker_and(remaining):
        lhs, remaining = parse_marker_expr(remaining)
        while remaining:
            m = AND.match(remaining)
            if not m:
                break
            remaining = remaining[m.end():]
            rhs, remaining = parse_marker_expr(remaining)
            lhs = {'op': 'and', 'lhs': lhs, 'rhs': rhs}
        return lhs, remaining

    def parse_marker(remaining):
        lhs, remaining = parse_marker_and(remaining)
        while remaining:
            m = OR.match(remaining)
            if not m:
                break
            remaining = remaining[m.end():]
            rhs, remaining = parse_marker_and(remaining)
            lhs = {'op': 'or', 'lhs': lhs, 'rhs': rhs}
        return lhs, remaining

    return parse_marker(parse_marker_string)","def evaluate_marker_and_return_result_with_context_handling(parse_marker, execution_context=None):
    """"""
    Interpret a parse_marker and return a result depending on environment.

    :param parse_marker: The parse_marker to evaluate_marker_and_return_result_with_context_handling.
    :type parse_marker: str
    :param execution_context: The context used for name lookup.
    :type execution_context: mapping
    """"""
    try:
        expr, rest = parse_parse_marker_string_returning_dictionary_with_expression(parse_marker)
    except Exception as e:
        raise SyntaxError('Unable to evaluate_marker_and_return_result_with_context_handling parse_marker syntax: %s: %s' % (parse_marker, e))
    if rest and rest[0] != '#':
        raise SyntaxError('unexpected trailing data in parse_marker: %s: %s' % (parse_marker, rest))
    context = dict(DEFAULT_CONTEXT)
    if execution_context:
        context.update(execution_context)
    return evaluator.evaluate(expr, context)","{""parse_marker"": ""parse_marker_string_returning_dictionary_with_expression"", ""marker"": ""parse_marker""}","def 0(marker_string):
    """"""
    Parse a marker string and return a dictionary containing a marker expression.

    The dictionary will contain keys ""op"", ""lhs"" and ""rhs"" for non-terminals in
    the expression grammar, or strings. A string contained in quotes is to be
    interpreted as a literal string, and a string not contained in quotes is a
    variable (such as os_name).
    """"""
    def marker_var(remaining):
        # either identifier, or literal string
        m = IDENTIFIER.match(remaining)
        if m:
            result = m.groups()[0]
            remaining = remaining[m.end():]
        elif not remaining:
            raise SyntaxError('unexpected end of input')
        else:
            q = remaining[0]
            if q not in '\'""':
                raise SyntaxError('invalid expression: %s' % remaining)
            oq = '\'""'.replace(q, '')
            remaining = remaining[1:]
            parts = [q]
            while remaining:
                # either a string chunk, or oq, or q to terminate
                if remaining[0] == q:
                    break
                elif remaining[0] == oq:
                    parts.append(oq)
                    remaining = remaining[1:]
                else:
                    m = STRING_CHUNK.match(remaining)
                    if not m:
                        raise SyntaxError('error in string literal: %s' % remaining)
                    parts.append(m.groups()[0])
                    remaining = remaining[m.end():]
            else:
                s = ''.join(parts)
                raise SyntaxError('unterminated string: %s' % s)
            parts.append(q)
            result = ''.join(parts)
            remaining = remaining[1:].lstrip() # skip past closing quote
        return result, remaining

    def marker_expr(remaining):
        if remaining and remaining[0] == '(':
            result, remaining = 4(remaining[1:].lstrip())
            if remaining[0] != ')':
                raise SyntaxError('unterminated parenthesis: %s' % remaining)
            remaining = remaining[1:].lstrip()
        else:
            lhs, remaining = marker_var(remaining)
            while remaining:
                m = MARKER_OP.match(remaining)
                if not m:
                    break
                op = m.groups()[0]
                remaining = remaining[m.end():]
                rhs, remaining = marker_var(remaining)
                lhs = {'op': op, 'lhs': lhs, 'rhs': rhs}
            result = lhs
        return result, remaining

    def marker_and(remaining):
        lhs, remaining = marker_expr(remaining)
        while remaining:
            m = AND.match(remaining)
            if not m:
                break
            remaining = remaining[m.end():]
            rhs, remaining = marker_expr(remaining)
            lhs = {'op': 'and', 'lhs': lhs, 'rhs': rhs}
        return lhs, remaining

    def 4(remaining):
        lhs, remaining = marker_and(remaining)
        while remaining:
            m = OR.match(remaining)
            if not m:
                break
            remaining = remaining[m.end():]
            rhs, remaining = marker_and(remaining)
            lhs = {'op': 'or', 'lhs': lhs, 'rhs': rhs}
        return lhs, remaining

    return 4(marker_string)","def 5(marker, execution_context=None):
    """"""
    Interpret a marker and return a result depending on environment.

    :param marker: The marker to interpret.
    :type marker: str
    :param execution_context: The context used for name lookup.
    :type execution_context: mapping
    """"""
    try:
        expr, rest = 0(marker)
    except Exception as e:
        raise SyntaxError('Unable to interpret marker syntax: %s: %s' % (marker, e))
    if rest and rest[0] != '#':
        raise SyntaxError('unexpected trailing data in marker: %s: %s' % (marker, rest))
    context = dict(DEFAULT_CONTEXT)
    if execution_context:
        context.update(execution_context)
    return evaluator.evaluate(expr, context)","{""parse_marker"": ""0"", ""marker"": ""4""}"
392,392,"def finder(package):
    """"""
    Return a resource finder for a package.
    :param package: The name of the package.
    :return: A :class:`ResourceFinder` instance for the package.
    """"""
    if package in _finder_cache:
        result = _finder_cache[package]
    else:
        if package not in sys.modules:
            __import__(package)
        module = sys.modules[package]
        path = getattr(module, '__path__', None)
        if path is None:
            raise DistlibException('You cannot get a finder for a module, '
                                   'only for a package')
        loader = getattr(module, '__loader__', None)
        finder_maker = _finder_registry.get(type(loader))
        if finder_maker is None:
            raise DistlibException('Unable to locate finder for %r' % package)
        result = finder_maker(module)
        _finder_cache[package] = result
    return result","def __import__(name, globals=None, locals=None, fromlist=(), level=0):
    """"""Import a module.

    The 'globals' argument is used to infer where the import is occurring from
    to handle relative imports. The 'locals' argument is ignored. The
    'fromlist' argument specifies what should exist as attributes on the module
    being imported (e.g. ``from module import <fromlist>``).  The 'level'
    argument represents the package location to import from in a relative
    import (e.g. ``from ..pkg import mod`` would have a 'level' of 2).

    """"""
    if level == 0:
        module = _gcd_import(name)
    else:
        globals_ = globals if globals is not None else {}
        package = _calc___package__(globals_)
        module = _gcd_import(name, package, level)
    if not fromlist:
        # Return up to the first dot in 'name'. This is complicated by the fact
        # that 'name' may be relative.
        if level == 0:
            return _gcd_import(name.partition('.')[0])
        elif not name:
            return module
        else:
            # Figure out where to slice the module's name up to the first dot
            # in 'name'.
            cut_off = len(name) - len(name.partition('.')[0])
            # Slice end needs to be positive to alleviate need to special-case
            # when ``'.' not in name``.
            return sys.modules[module.__name__[:len(module.__name__)-cut_off]]
    elif hasattr(module, '__path__'):
        return _handle_fromlist(module, fromlist, _gcd_import)
    else:
        return module","def import_module(name, globals=None, locals=None, fromlist=(), level=0):
    """"""Import a module.

    The 'globals' argument is used to infer where the import is occurring from
    to handle relative imports. The 'locals' argument is ignored. The
    'fromlist' argument specifies what should exist as attributes on the module
    being imported (e.g. ``from module import <fromlist>``).  The 'level'
    argument represents the package location to import from in a relative
    import (e.g. ``from ..pkg import mod`` would have a 'level' of 2).

    """"""
    if level == 0:
        module = _gcd_import(name)
    else:
        globals_ = globals if globals is not None else {}
        package = _calc___package__(globals_)
        module = _gcd_import(name, package, level)
    if not fromlist:
        # Return up to the first dot in 'name'. This is complicated by the fact
        # that 'name' may be relative.
        if level == 0:
            return _gcd_import(name.partition('.')[0])
        elif not name:
            return module
        else:
            # Figure out where to slice the module's name up to the first dot
            # in 'name'.
            cut_off = len(name) - len(name.partition('.')[0])
            # Slice end needs to be positive to alleviate need to special-case
            # when ``'.' not in name``.
            return sys.modules[module.__name__[:len(module.__name__)-cut_off]]
    elif hasattr(module, '__path__'):
        return _handle_fromlist(module, fromlist, _gcd_import)
    else:
        return module","def return_resource_finder_for_package_with_caching(package):
    """"""
    Return a resource return_resource_finder_for_package_with_caching for a package.
    :param package: The name of the package.
    :return: A :class:`ResourceFinder` instance for the package.
    """"""
    if package in _return_resource_finder_for_package_with_caching_cache:
        result = _return_resource_finder_for_package_with_caching_cache[package]
    else:
        if package not in sys.modules:
            import_module(package)
        module = sys.modules[package]
        path = getattr(module, '__path__', None)
        if path is None:
            raise DistlibException('You cannot get a return_resource_finder_for_package_with_caching for a module, '
                                   'only for a package')
        loader = getattr(module, '__loader__', None)
        return_resource_finder_for_package_with_caching_maker = _return_resource_finder_for_package_with_caching_registry.get(type(loader))
        if return_resource_finder_for_package_with_caching_maker is None:
            raise DistlibException('Unable to locate return_resource_finder_for_package_with_caching for %r' % package)
        result = return_resource_finder_for_package_with_caching_maker(module)
        _return_resource_finder_for_package_with_caching_cache[package] = result
    return result","{""__import__"": ""import_module""}","def 0(name, globals=None, locals=None, fromlist=(), level=0):
    """"""Import a module.

    The 'globals' argument is used to infer where the import is occurring from
    to handle relative imports. The 'locals' argument is ignored. The
    'fromlist' argument specifies what should exist as attributes on the module
    being imported (e.g. ``from module import <fromlist>``).  The 'level'
    argument represents the package location to import from in a relative
    import (e.g. ``from ..pkg import mod`` would have a 'level' of 2).

    """"""
    if level == 0:
        module = _gcd_import(name)
    else:
        globals_ = globals if globals is not None else {}
        package = _calc___package__(globals_)
        module = _gcd_import(name, package, level)
    if not fromlist:
        # Return up to the first dot in 'name'. This is complicated by the fact
        # that 'name' may be relative.
        if level == 0:
            return _gcd_import(name.partition('.')[0])
        elif not name:
            return module
        else:
            # Figure out where to slice the module's name up to the first dot
            # in 'name'.
            cut_off = len(name) - len(name.partition('.')[0])
            # Slice end needs to be positive to alleviate need to special-case
            # when ``'.' not in name``.
            return sys.modules[module.__name__[:len(module.__name__)-cut_off]]
    elif hasattr(module, '__path__'):
        return _handle_fromlist(module, fromlist, _gcd_import)
    else:
        return module","def 1(package):
    """"""
    Return a resource finder for a package.
    :param package: The name of the package.
    :return: A :class:`ResourceFinder` instance for the package.
    """"""
    if package in _finder_cache:
        result = _finder_cache[package]
    else:
        if package not in sys.modules:
            0(package)
        module = sys.modules[package]
        path = getattr(module, '__path__', None)
        if path is None:
            raise DistlibException('You cannot get a finder for a module, '
                                   'only for a package')
        loader = getattr(module, '__loader__', None)
        finder_maker = _finder_registry.get(type(loader))
        if finder_maker is None:
            raise DistlibException('Unable to locate finder for %r' % package)
        result = finder_maker(module)
        _finder_cache[package] = result
    return result","{""__import__"": ""0""}"
393,393,"def finder_for_path(path):
    """"""
    Return a resource finder for a path, which should represent a container.

    :param path: The path.
    :return: A :class:`ResourceFinder` instance for the path.
    """"""
    result = None
    # calls any path hooks, gets importer into cache
    pkgutil.get_importer(path)
    loader = sys.path_importer_cache.get(path)
    finder = _finder_registry.get(type(loader))
    if finder:
        module = _dummy_module
        module.__file__ = os.path.join(path, '')
        module.__loader__ = loader
        result = finder(module)
    return result","def finder(package):
    """"""
    Return a resource finder for a package.
    :param package: The name of the package.
    :return: A :class:`ResourceFinder` instance for the package.
    """"""
    if package in _finder_cache:
        result = _finder_cache[package]
    else:
        if package not in sys.modules:
            __import__(package)
        module = sys.modules[package]
        path = getattr(module, '__path__', None)
        if path is None:
            raise DistlibException('You cannot get a finder for a module, '
                                   'only for a package')
        loader = getattr(module, '__loader__', None)
        finder_maker = _finder_registry.get(type(loader))
        if finder_maker is None:
            raise DistlibException('Unable to locate finder for %r' % package)
        result = finder_maker(module)
        _finder_cache[package] = result
    return result","def return_resource_finder_for_package_with_caching(package):
    """"""
    Return a resource return_resource_finder_for_package_with_caching for a package.
    :param package: The name of the package.
    :return: A :class:`ResourceFinder` instance for the package.
    """"""
    if package in _return_resource_finder_for_package_with_caching_cache:
        result = _return_resource_finder_for_package_with_caching_cache[package]
    else:
        if package not in sys.modules:
            __import__(package)
        module = sys.modules[package]
        path = getattr(module, '__path__', None)
        if path is None:
            raise DistlibException('You cannot get a return_resource_finder_for_package_with_caching for a module, '
                                   'only for a package')
        loader = getattr(module, '__loader__', None)
        return_resource_finder_for_package_with_caching_maker = _return_resource_finder_for_package_with_caching_registry.get(type(loader))
        if return_resource_finder_for_package_with_caching_maker is None:
            raise DistlibException('Unable to locate return_resource_finder_for_package_with_caching for %r' % package)
        result = return_resource_finder_for_package_with_caching_maker(module)
        _return_resource_finder_for_package_with_caching_cache[package] = result
    return result","def return_resource_finder_for_package_with_caching_for_path(path):
    """"""
    Return a resource return_resource_finder_for_package_with_caching for a path, which should represent a container.

    :param path: The path.
    :return: A :class:`ResourceFinder` instance for the path.
    """"""
    result = None
    # calls any path hooks, gets importer into cache
    pkgutil.get_importer(path)
    loader = sys.path_importer_cache.get(path)
    return_resource_finder_for_package_with_caching = _return_resource_finder_for_package_with_caching_registry.get(type(loader))
    if return_resource_finder_for_package_with_caching:
        module = _dummy_module
        module.__file__ = os.path.join(path, '')
        module.__loader__ = loader
        result = return_resource_finder_for_package_with_caching(module)
    return result","{""finder"": ""return_resource_finder_for_package_with_caching""}","def 0(package):
    """"""
    Return a resource finder for a package.
    :param package: The name of the package.
    :return: A :class:`ResourceFinder` instance for the package.
    """"""
    if package in _finder_cache:
        result = _finder_cache[package]
    else:
        if package not in sys.modules:
            __import__(package)
        module = sys.modules[package]
        path = getattr(module, '__path__', None)
        if path is None:
            raise DistlibException('You cannot get a finder for a module, '
                                   'only for a package')
        loader = getattr(module, '__loader__', None)
        finder_maker = _finder_registry.get(type(loader))
        if finder_maker is None:
            raise DistlibException('Unable to locate finder for %r' % package)
        result = finder_maker(module)
        _finder_cache[package] = result
    return result","def 1(path):
    """"""
    Return a resource finder for a path, which should represent a container.

    :param path: The path.
    :return: A :class:`ResourceFinder` instance for the path.
    """"""
    result = None
    # calls any path hooks, gets importer into cache
    pkgutil.get_importer(path)
    loader = sys.path_importer_cache.get(path)
    finder = _finder_registry.get(type(loader))
    if finder:
        module = _dummy_module
        module.__file__ = os.path.join(path, '')
        module.__loader__ = loader
        result = 0(module)
    return result","{""finder"": ""0""}"
394,394,"def intranges_from_list(list_: List[int]) -> Tuple[int, ...]:
    """"""Represent a list of integers as a sequence of ranges:
    ((start_0, end_0), (start_1, end_1), ...), such that the original
    integers are exactly those x such that start_i <= x < end_i for some i.

    Ranges are encoded as single integers (start << 32 | end), not as tuples.
    """"""

    sorted_list = sorted(list_)
    ranges = []
    last_write = -1
    for i in range(len(sorted_list)):
        if i+1 < len(sorted_list):
            if sorted_list[i] == sorted_list[i+1]-1:
                continue
        current_range = sorted_list[last_write+1:i+1]
        ranges.append(_encode_range(current_range[0], current_range[-1] + 1))
        last_write = i

    return tuple(ranges)","def _encode_range(start: int, end: int) -> int:
    return (start << 32) | end","def _encode_range(start: int, end: int) -> int:
    return (start << 32) | end","def represent_list_of_integers_as_sequence_of_ranges(list_: List[int]) -> Tuple[int, ...]:
    """"""Represent a list of integers as a sequence of ranges:
    ((start_0, end_0), (start_1, end_1), ...), such that the original
    integers are exactly those x such that start_i <= x < end_i for some i.

    Ranges are encoded as single integers (start << 32 | end), not as tuples.
    """"""

    sorted_list = sorted(list_)
    ranges = []
    last_write = -1
    for i in range(len(sorted_list)):
        if i+1 < len(sorted_list):
            if sorted_list[i] == sorted_list[i+1]-1:
                continue
        current_range = sorted_list[last_write+1:i+1]
        ranges.append(_encode_range(current_range[0], current_range[-1] + 1))
        last_write = i

    return tuple(ranges)",{},"def _encode_range(start: int, end: int) -> int:
    return (start << 32) | end","def 1(list_: List[int]) -> Tuple[int, ...]:
    """"""Represent a list of integers as a sequence of ranges:
    ((start_0, end_0), (start_1, end_1), ...), such that the original
    integers are exactly those x such that start_i <= x < end_i for some i.

    Ranges are encoded as single integers (start << 32 | end), not as tuples.
    """"""

    sorted_list = sorted(list_)
    ranges = []
    last_write = -1
    for i in range(len(sorted_list)):
        if i+1 < len(sorted_list):
            if sorted_list[i] == sorted_list[i+1]-1:
                continue
        current_range = sorted_list[last_write+1:i+1]
        ranges.append(_encode_range(current_range[0], current_range[-1] + 1))
        last_write = i

    return tuple(ranges)",{}
395,395,"def intranges_contain(int_: int, ranges: Tuple[int, ...]) -> bool:
    """"""Determine if `int_` falls into one of the ranges in `ranges`.""""""
    tuple_ = _encode_range(int_, 0)
    pos = bisect.bisect_left(ranges, tuple_)
    # we could be immediately ahead of a tuple (start, end)
    # with start < int_ <= end
    if pos > 0:
        left, right = _decode_range(ranges[pos-1])
        if left <= int_ < right:
            return True
    # or we could be immediately behind a tuple (int_, end)
    if pos < len(ranges):
        left, _ = _decode_range(ranges[pos])
        if left == int_:
            return True
    return False","def _decode_range(r: int) -> Tuple[int, int]:
    return (r >> 32), (r & ((1 << 32) - 1))

def _encode_range(start: int, end: int) -> int:
    return (start << 32) | end","def _decode_range(r: int) -> Tuple[int, int]:
    return (r >> 32), (r & ((1 << 32) - 1))

def _encode_range(start: int, end: int) -> int:
    return (start << 32) | end","def determine_if_integer_falls_into_one_of_ranges(int_: int, ranges: Tuple[int, ...]) -> bool:
    """"""Determine if `int_` falls into one of the ranges in `ranges`.""""""
    tuple_ = _encode_range(int_, 0)
    pos = bisect.bisect_left(ranges, tuple_)
    # we could be immediately ahead of a tuple (start, end)
    # with start < int_ <= end
    if pos > 0:
        left, right = _decode_range(ranges[pos-1])
        if left <= int_ < right:
            return True
    # or we could be immediately behind a tuple (int_, end)
    if pos < len(ranges):
        left, _ = _decode_range(ranges[pos])
        if left == int_:
            return True
    return False",{},"def _decode_range(r: int) -> Tuple[int, int]:
    return (r >> 32), (r & ((1 << 32) - 1))

def _encode_range(start: int, end: int) -> int:
    return (start << 32) | end","def 1(int_: int, ranges: Tuple[int, ...]) -> bool:
    """"""Determine if `int_` falls into one of the ranges in `ranges`.""""""
    tuple_ = _encode_range(int_, 0)
    pos = bisect.bisect_left(ranges, tuple_)
    # we could be immediately ahead of a tuple (start, end)
    # with start < int_ <= end
    if pos > 0:
        left, right = _decode_range(ranges[pos-1])
        if left <= int_ < right:
            return True
    # or we could be immediately behind a tuple (int_, end)
    if pos < len(ranges):
        left, _ = _decode_range(ranges[pos])
        if left == int_:
            return True
    return False",{}
396,396,"def cookiejar_from_dict(cookie_dict, cookiejar=None, overwrite=True):
    """"""Returns a CookieJar from a key/value dictionary.

    :param cookie_dict: Dict of key/values to insert into CookieJar.
    :param cookiejar: (optional) A cookiejar to add the cookies to.
    :param overwrite: (optional) If False, will not replace cookies
        already in the jar with new ones.
    :rtype: CookieJar
    """"""
    if cookiejar is None:
        cookiejar = RequestsCookieJar()

    if cookie_dict is not None:
        names_from_jar = [cookie.name for cookie in cookiejar]
        for name in cookie_dict:
            if overwrite or (name not in names_from_jar):
                cookiejar.set_cookie(create_cookie(name, cookie_dict[name]))

    return cookiejar","def create_cookie(name, value, **kwargs):
    """"""Make a cookie from underspecified parameters.

    By default, the pair of `name` and `value` will be set for the domain ''
    and sent on every request (this is sometimes called a ""supercookie"").
    """"""
    result = {
        'version': 0,
        'name': name,
        'value': value,
        'port': None,
        'domain': '',
        'path': '/',
        'secure': False,
        'expires': None,
        'discard': True,
        'comment': None,
        'comment_url': None,
        'rest': {'HttpOnly': None},
        'rfc2109': False,
    }

    badargs = set(kwargs) - set(result)
    if badargs:
        err = 'create_cookie() got unexpected keyword arguments: %s'
        raise TypeError(err % list(badargs))

    result.update(kwargs)
    result['port_specified'] = bool(result['port'])
    result['domain_specified'] = bool(result['domain'])
    result['domain_initial_dot'] = result['domain'].startswith('.')
    result['path_specified'] = bool(result['path'])

    return cookielib.Cookie(**result)","def generate_cookie(name, value, **kwargs):
    """"""Make a cookie from underspecified parameters.

    By default, the pair of `name` and `value` will be set for the domain ''
    and sent on every request (this is sometimes called a ""supercookie"").
    """"""
    result = {
        'version': 0,
        'name': name,
        'value': value,
        'port': None,
        'domain': '',
        'path': '/',
        'secure': False,
        'expires': None,
        'discard': True,
        'comment': None,
        'comment_url': None,
        'rest': {'HttpOnly': None},
        'rfc2109': False,
    }

    badargs = set(kwargs) - set(result)
    if badargs:
        err = 'generate_cookie() got unexpected keyword arguments: %s'
        raise TypeError(err % list(badargs))

    result.update(kwargs)
    result['port_specified'] = bool(result['port'])
    result['domain_specified'] = bool(result['domain'])
    result['domain_initial_dot'] = result['domain'].startswith('.')
    result['path_specified'] = bool(result['path'])

    return cookielib.Cookie(**result)","def create_cookie_jar_from_key_value_dict_with_overwrite_option(cookie_dict, cookiejar=None, overwrite=True):
    """"""Returns a CookieJar from a key/value dictionary.

    :param cookie_dict: Dict of key/values to insert into CookieJar.
    :param cookiejar: (optional) A cookiejar to add the cookies to.
    :param overwrite: (optional) If False, will not replace cookies
        already in the jar with new ones.
    :rtype: CookieJar
    """"""
    if cookiejar is None:
        cookiejar = RequestsCookieJar()

    if cookie_dict is not None:
        names_from_jar = [cookie.name for cookie in cookiejar]
        for name in cookie_dict:
            if overwrite or (name not in names_from_jar):
                cookiejar.set_cookie(generate_cookie(name, cookie_dict[name]))

    return cookiejar","{""create_cookie"": ""generate_cookie""}","def 0(name, value, **kwargs):
    """"""Make a cookie from underspecified parameters.

    By default, the pair of `name` and `value` will be set for the domain ''
    and sent on every request (this is sometimes called a ""supercookie"").
    """"""
    result = {
        'version': 0,
        'name': name,
        'value': value,
        'port': None,
        'domain': '',
        'path': '/',
        'secure': False,
        'expires': None,
        'discard': True,
        'comment': None,
        'comment_url': None,
        'rest': {'HttpOnly': None},
        'rfc2109': False,
    }

    badargs = set(kwargs) - set(result)
    if badargs:
        err = '0() got unexpected keyword arguments: %s'
        raise TypeError(err % list(badargs))

    result.update(kwargs)
    result['port_specified'] = bool(result['port'])
    result['domain_specified'] = bool(result['domain'])
    result['domain_initial_dot'] = result['domain'].startswith('.')
    result['path_specified'] = bool(result['path'])

    return cookielib.Cookie(**result)","def 1(cookie_dict, cookiejar=None, overwrite=True):
    """"""Returns a CookieJar from a key/value dictionary.

    :param cookie_dict: Dict of key/values to insert into CookieJar.
    :param cookiejar: (optional) A cookiejar to add the cookies to.
    :param overwrite: (optional) If False, will not replace cookies
        already in the jar with new ones.
    :rtype: CookieJar
    """"""
    if cookiejar is None:
        cookiejar = RequestsCookieJar()

    if cookie_dict is not None:
        names_from_jar = [cookie.name for cookie in cookiejar]
        for name in cookie_dict:
            if overwrite or (name not in names_from_jar):
                cookiejar.set_cookie(0(name, cookie_dict[name]))

    return cookiejar","{""create_cookie"": ""0""}"
397,397,"def merge_cookies(cookiejar, cookies):
    """"""Add cookies to cookiejar and returns a merged CookieJar.

    :param cookiejar: CookieJar object to add the cookies to.
    :param cookies: Dictionary or CookieJar object to be added.
    :rtype: CookieJar
    """"""
    if not isinstance(cookiejar, cookielib.CookieJar):
        raise ValueError('You can only merge into CookieJar')

    if isinstance(cookies, dict):
        cookiejar = cookiejar_from_dict(
            cookies, cookiejar=cookiejar, overwrite=False)
    elif isinstance(cookies, cookielib.CookieJar):
        try:
            cookiejar.update(cookies)
        except AttributeError:
            for cookie_in_jar in cookies:
                cookiejar.set_cookie(cookie_in_jar)

    return cookiejar","def cookiejar_from_dict(cookie_dict, cookiejar=None, overwrite=True):
    """"""Returns a CookieJar from a key/value dictionary.

    :param cookie_dict: Dict of key/values to insert into CookieJar.
    :param cookiejar: (optional) A cookiejar to add the cookies to.
    :param overwrite: (optional) If False, will not replace cookies
        already in the jar with new ones.
    :rtype: CookieJar
    """"""
    if cookiejar is None:
        cookiejar = RequestsCookieJar()

    if cookie_dict is not None:
        names_from_jar = [cookie.name for cookie in cookiejar]
        for name in cookie_dict:
            if overwrite or (name not in names_from_jar):
                cookiejar.set_cookie(create_cookie(name, cookie_dict[name]))

    return cookiejar","def create_cookie_jar_from_key_value_dict_with_overwrite_option(cookie_dict, cookiejar=None, overwrite=True):
    """"""Returns a CookieJar from a key/value dictionary.

    :param cookie_dict: Dict of key/values to insert into CookieJar.
    :param cookiejar: (optional) A cookiejar to add the cookies to.
    :param overwrite: (optional) If False, will not replace cookies
        already in the jar with new ones.
    :rtype: CookieJar
    """"""
    if cookiejar is None:
        cookiejar = RequestsCookieJar()

    if cookie_dict is not None:
        names_from_jar = [cookie.name for cookie in cookiejar]
        for name in cookie_dict:
            if overwrite or (name not in names_from_jar):
                cookiejar.set_cookie(create_cookie(name, cookie_dict[name]))

    return cookiejar","def add_cookies_to_cookiejar_and_return_merged_cookiejar(cookiejar, cookies):
    """"""Add cookies to cookiejar and returns a merged CookieJar.

    :param cookiejar: CookieJar object to add the cookies to.
    :param cookies: Dictionary or CookieJar object to be added.
    :rtype: CookieJar
    """"""
    if not isinstance(cookiejar, cookielib.CookieJar):
        raise ValueError('You can only merge into CookieJar')

    if isinstance(cookies, dict):
        cookiejar = create_cookie_jar_from_key_value_dict_with_overwrite_option(
            cookies, cookiejar=cookiejar, overwrite=False)
    elif isinstance(cookies, cookielib.CookieJar):
        try:
            cookiejar.update(cookies)
        except AttributeError:
            for cookie_in_jar in cookies:
                cookiejar.set_cookie(cookie_in_jar)

    return cookiejar","{""cookiejar_from_dict"": ""create_cookie_jar_from_key_value_dict_with_overwrite_option""}","def 0(cookie_dict, cookiejar=None, overwrite=True):
    """"""Returns a CookieJar from a key/value dictionary.

    :param cookie_dict: Dict of key/values to insert into CookieJar.
    :param cookiejar: (optional) A cookiejar to add the cookies to.
    :param overwrite: (optional) If False, will not replace cookies
        already in the jar with new ones.
    :rtype: CookieJar
    """"""
    if cookiejar is None:
        cookiejar = RequestsCookieJar()

    if cookie_dict is not None:
        names_from_jar = [cookie.name for cookie in cookiejar]
        for name in cookie_dict:
            if overwrite or (name not in names_from_jar):
                cookiejar.set_cookie(create_cookie(name, cookie_dict[name]))

    return cookiejar","def 1(cookiejar, cookies):
    """"""Add cookies to cookiejar and returns a merged CookieJar.

    :param cookiejar: CookieJar object to add the cookies to.
    :param cookies: Dictionary or CookieJar object to be added.
    :rtype: CookieJar
    """"""
    if not isinstance(cookiejar, cookielib.CookieJar):
        raise ValueError('You can only merge into CookieJar')

    if isinstance(cookies, dict):
        cookiejar = 0(
            cookies, cookiejar=cookiejar, overwrite=False)
    elif isinstance(cookies, cookielib.CookieJar):
        try:
            cookiejar.update(cookies)
        except AttributeError:
            for cookie_in_jar in cookies:
                cookiejar.set_cookie(cookie_in_jar)

    return cookiejar","{""cookiejar_from_dict"": ""0""}"
398,398,"def merge_setting(request_setting, session_setting, dict_class=OrderedDict):
    """"""Determines appropriate setting for a given request, taking into account
    the explicit setting on that request, and the setting in the session. If a
    setting is a dictionary, they will be merged together using `dict_class`
    """"""

    if session_setting is None:
        return request_setting

    if request_setting is None:
        return session_setting

    # Bypass if not a dictionary (e.g. verify)
    if not (
            isinstance(session_setting, Mapping) and
            isinstance(request_setting, Mapping)
    ):
        return request_setting

    merged_setting = dict_class(to_key_val_list(session_setting))
    merged_setting.update(to_key_val_list(request_setting))

    # Remove keys that are set to None. Extract keys first to avoid altering
    # the dictionary during iteration.
    none_keys = [k for (k, v) in merged_setting.items() if v is None]
    for key in none_keys:
        del merged_setting[key]

    return merged_setting","def to_key_val_list(value):
    """"""Take an object and test to see if it can be represented as a
    dictionary. If it can be, return a list of tuples, e.g.,

    ::

        >>> to_key_val_list([('key', 'val')])
        [('key', 'val')]
        >>> to_key_val_list({'key': 'val'})
        [('key', 'val')]
        >>> to_key_val_list('string')
        Traceback (most recent call last):
        ...
        ValueError: cannot encode objects that are not 2-tuples

    :rtype: list
    """"""
    if value is None:
        return None

    if isinstance(value, (str, bytes, bool, int)):
        raise ValueError('cannot encode objects that are not 2-tuples')

    if isinstance(value, Mapping):
        value = value.items()

    return list(value)","def convert_object_to_list_of_tuples_if_possible(value):
    """"""Take an object and test to see if it can be represented as a
    dictionary. If it can be, return a list of tuples, e.g.,

    ::

        >>> convert_object_to_list_of_tuples_if_possible([('key', 'val')])
        [('key', 'val')]
        >>> convert_object_to_list_of_tuples_if_possible({'key': 'val'})
        [('key', 'val')]
        >>> convert_object_to_list_of_tuples_if_possible('string')
        Traceback (most recent call last):
        ...
        ValueError: cannot encode objects that are not 2-tuples

    :rtype: list
    """"""
    if value is None:
        return None

    if isinstance(value, (str, bytes, bool, int)):
        raise ValueError('cannot encode objects that are not 2-tuples')

    if isinstance(value, Mapping):
        value = value.items()

    return list(value)","def determine_appropriate_setting_for_request_and_session_with_dictionary_merging(request_setting, session_setting, dict_class=OrderedDict):
    """"""Determines appropriate setting for a given request, taking into account
    the explicit setting on that request, and the setting in the session. If a
    setting is a dictionary, they will be merged together using `dict_class`
    """"""

    if session_setting is None:
        return request_setting

    if request_setting is None:
        return session_setting

    # Bypass if not a dictionary (e.g. verify)
    if not (
            isinstance(session_setting, Mapping) and
            isinstance(request_setting, Mapping)
    ):
        return request_setting

    merged_setting = dict_class(convert_object_to_list_of_tuples_if_possible(session_setting))
    merged_setting.update(convert_object_to_list_of_tuples_if_possible(request_setting))

    # Remove keys that are set to None. Extract keys first to avoid altering
    # the dictionary during iteration.
    none_keys = [k for (k, v) in merged_setting.items() if v is None]
    for key in none_keys:
        del merged_setting[key]

    return merged_setting","{""to_key_val_list"": ""convert_object_to_list_of_tuples_if_possible""}","def 0(value):
    """"""Take an object and test to see if it can be represented as a
    dictionary. If it can be, return a list of tuples, e.g.,

    ::

        >>> 0([('key', 'val')])
        [('key', 'val')]
        >>> 0({'key': 'val'})
        [('key', 'val')]
        >>> 0('string')
        Traceback (most recent call last):
        ...
        ValueError: cannot encode objects that are not 2-tuples

    :rtype: list
    """"""
    if value is None:
        return None

    if isinstance(value, (str, bytes, bool, int)):
        raise ValueError('cannot encode objects that are not 2-tuples')

    if isinstance(value, Mapping):
        value = value.items()

    return list(value)","def 1(request_setting, session_setting, dict_class=OrderedDict):
    """"""Determines appropriate setting for a given request, taking into account
    the explicit setting on that request, and the setting in the session. If a
    setting is a dictionary, they will be merged together using `dict_class`
    """"""

    if session_setting is None:
        return request_setting

    if request_setting is None:
        return session_setting

    # Bypass if not a dictionary (e.g. verify)
    if not (
            isinstance(session_setting, Mapping) and
            isinstance(request_setting, Mapping)
    ):
        return request_setting

    merged_setting = dict_class(0(session_setting))
    merged_setting.update(0(request_setting))

    # Remove keys that are set to None. Extract keys first to avoid altering
    # the dictionary during iteration.
    none_keys = [k for (k, v) in merged_setting.items() if v is None]
    for key in none_keys:
        del merged_setting[key]

    return merged_setting","{""to_key_val_list"": ""0""}"
399,399,"def merge_hooks(request_hooks, session_hooks, dict_class=OrderedDict):
    """"""Properly merges both requests and session hooks.

    This is necessary because when request_hooks == {'response': []}, the
    merge breaks Session hooks entirely.
    """"""
    if session_hooks is None or session_hooks.get('response') == []:
        return request_hooks

    if request_hooks is None or request_hooks.get('response') == []:
        return session_hooks

    return merge_setting(request_hooks, session_hooks, dict_class)","def merge_setting(request_setting, session_setting, dict_class=OrderedDict):
    """"""Determines appropriate setting for a given request, taking into account
    the explicit setting on that request, and the setting in the session. If a
    setting is a dictionary, they will be merged together using `dict_class`
    """"""

    if session_setting is None:
        return request_setting

    if request_setting is None:
        return session_setting

    # Bypass if not a dictionary (e.g. verify)
    if not (
            isinstance(session_setting, Mapping) and
            isinstance(request_setting, Mapping)
    ):
        return request_setting

    merged_setting = dict_class(to_key_val_list(session_setting))
    merged_setting.update(to_key_val_list(request_setting))

    # Remove keys that are set to None. Extract keys first to avoid altering
    # the dictionary during iteration.
    none_keys = [k for (k, v) in merged_setting.items() if v is None]
    for key in none_keys:
        del merged_setting[key]

    return merged_setting","def determine_appropriate_setting_for_request_and_session_with_dictionary_merging(request_setting, session_setting, dict_class=OrderedDict):
    """"""Determines appropriate setting for a given request, taking into account
    the explicit setting on that request, and the setting in the session. If a
    setting is a dictionary, they will be merged together using `dict_class`
    """"""

    if session_setting is None:
        return request_setting

    if request_setting is None:
        return session_setting

    # Bypass if not a dictionary (e.g. verify)
    if not (
            isinstance(session_setting, Mapping) and
            isinstance(request_setting, Mapping)
    ):
        return request_setting

    merged_setting = dict_class(to_key_val_list(session_setting))
    merged_setting.update(to_key_val_list(request_setting))

    # Remove keys that are set to None. Extract keys first to avoid altering
    # the dictionary during iteration.
    none_keys = [k for (k, v) in merged_setting.items() if v is None]
    for key in none_keys:
        del merged_setting[key]

    return merged_setting","def properly_merge_request_and_session_hooks_with_handling(request_hooks, session_hooks, dict_class=OrderedDict):
    """"""Properly merges both requests and session hooks.

    This is necessary because when request_hooks == {'response': []}, the
    merge breaks Session hooks entirely.
    """"""
    if session_hooks is None or session_hooks.get('response') == []:
        return request_hooks

    if request_hooks is None or request_hooks.get('response') == []:
        return session_hooks

    return determine_appropriate_setting_for_request_and_session_with_dictionary_merging(request_hooks, session_hooks, dict_class)","{""merge_setting"": ""determine_appropriate_setting_for_request_and_session_with_dictionary_merging""}","def 0(request_setting, session_setting, dict_class=OrderedDict):
    """"""Determines appropriate setting for a given request, taking into account
    the explicit setting on that request, and the setting in the session. If a
    setting is a dictionary, they will be merged together using `dict_class`
    """"""

    if session_setting is None:
        return request_setting

    if request_setting is None:
        return session_setting

    # Bypass if not a dictionary (e.g. verify)
    if not (
            isinstance(session_setting, Mapping) and
            isinstance(request_setting, Mapping)
    ):
        return request_setting

    merged_setting = dict_class(to_key_val_list(session_setting))
    merged_setting.update(to_key_val_list(request_setting))

    # Remove keys that are set to None. Extract keys first to avoid altering
    # the dictionary during iteration.
    none_keys = [k for (k, v) in merged_setting.items() if v is None]
    for key in none_keys:
        del merged_setting[key]

    return merged_setting","def 1(request_hooks, session_hooks, dict_class=OrderedDict):
    """"""Properly merges both requests and session hooks.

    This is necessary because when request_hooks == {'response': []}, the
    merge breaks Session hooks entirely.
    """"""
    if session_hooks is None or session_hooks.get('response') == []:
        return request_hooks

    if request_hooks is None or request_hooks.get('response') == []:
        return session_hooks

    return 0(request_hooks, session_hooks, dict_class)","{""merge_setting"": ""0""}"
400,400,"def options(url, **kwargs):
    r""""""Sends an OPTIONS request.

    :param url: URL for the new :class:`Request` object.
    :param \*\*kwargs: Optional arguments that ``request`` takes.
    :return: :class:`Response <Response>` object
    :rtype: requests.Response
    """"""

    return request('options', url, **kwargs)","def request(method, url, **kwargs):
    """"""Constructs and sends a :class:`Request <Request>`.

    :param method: method for the new :class:`Request` object: ``GET``, ``OPTIONS``, ``HEAD``, ``POST``, ``PUT``, ``PATCH``, or ``DELETE``.
    :param url: URL for the new :class:`Request` object.
    :param params: (optional) Dictionary, list of tuples or bytes to send
        in the query string for the :class:`Request`.
    :param data: (optional) Dictionary, list of tuples, bytes, or file-like
        object to send in the body of the :class:`Request`.
    :param json: (optional) A JSON serializable Python object to send in the body of the :class:`Request`.
    :param headers: (optional) Dictionary of HTTP Headers to send with the :class:`Request`.
    :param cookies: (optional) Dict or CookieJar object to send with the :class:`Request`.
    :param files: (optional) Dictionary of ``'name': file-like-objects`` (or ``{'name': file-tuple}``) for multipart encoding upload.
        ``file-tuple`` can be a 2-tuple ``('filename', fileobj)``, 3-tuple ``('filename', fileobj, 'content_type')``
        or a 4-tuple ``('filename', fileobj, 'content_type', custom_headers)``, where ``'content-type'`` is a string
        defining the content type of the given file and ``custom_headers`` a dict-like object containing additional headers
        to add for the file.
    :param auth: (optional) Auth tuple to enable Basic/Digest/Custom HTTP Auth.
    :param timeout: (optional) How many seconds to wait for the server to send data
        before giving up, as a float, or a :ref:`(connect timeout, read
        timeout) <timeouts>` tuple.
    :type timeout: float or tuple
    :param allow_redirects: (optional) Boolean. Enable/disable GET/OPTIONS/POST/PUT/PATCH/DELETE/HEAD redirection. Defaults to ``True``.
    :type allow_redirects: bool
    :param proxies: (optional) Dictionary mapping protocol to the URL of the proxy.
    :param verify: (optional) Either a boolean, in which case it controls whether we verify
            the server's TLS certificate, or a string, in which case it must be a path
            to a CA bundle to use. Defaults to ``True``.
    :param stream: (optional) if ``False``, the response content will be immediately downloaded.
    :param cert: (optional) if String, path to ssl client cert file (.pem). If Tuple, ('cert', 'key') pair.
    :return: :class:`Response <Response>` object
    :rtype: requests.Response

    Usage::

      >>> import requests
      >>> req = requests.request('GET', 'https://httpbin.org/get')
      >>> req
      <Response [200]>
    """"""

    # By using the 'with' statement we are sure the session is closed, thus we
    # avoid leaving sockets open which can trigger a ResourceWarning in some
    # cases, and look like a memory leak in others.
    with sessions.Session() as session:
        return session.request(method=method, url=url, **kwargs)","def construct_and_send_request(method, url, **kwargs):
    """"""Constructs and sends a :class:`Request <Request>`.

    :param method: method for the new :class:`Request` object: ``GET``, ``OPTIONS``, ``HEAD``, ``POST``, ``PUT``, ``PATCH``, or ``DELETE``.
    :param url: URL for the new :class:`Request` object.
    :param params: (optional) Dictionary, list of tuples or bytes to send
        in the query string for the :class:`Request`.
    :param data: (optional) Dictionary, list of tuples, bytes, or file-like
        object to send in the body of the :class:`Request`.
    :param json: (optional) A JSON serializable Python object to send in the body of the :class:`Request`.
    :param headers: (optional) Dictionary of HTTP Headers to send with the :class:`Request`.
    :param cookies: (optional) Dict or CookieJar object to send with the :class:`Request`.
    :param files: (optional) Dictionary of ``'name': file-like-objects`` (or ``{'name': file-tuple}``) for multipart encoding upload.
        ``file-tuple`` can be a 2-tuple ``('filename', fileobj)``, 3-tuple ``('filename', fileobj, 'content_type')``
        or a 4-tuple ``('filename', fileobj, 'content_type', custom_headers)``, where ``'content-type'`` is a string
        defining the content type of the given file and ``custom_headers`` a dict-like object containing additional headers
        to add for the file.
    :param auth: (optional) Auth tuple to enable Basic/Digest/Custom HTTP Auth.
    :param timeout: (optional) How many seconds to wait for the server to send data
        before giving up, as a float, or a :ref:`(connect timeout, read
        timeout) <timeouts>` tuple.
    :type timeout: float or tuple
    :param allow_redirects: (optional) Boolean. Enable/disable GET/OPTIONS/POST/PUT/PATCH/DELETE/HEAD redirection. Defaults to ``True``.
    :type allow_redirects: bool
    :param proxies: (optional) Dictionary mapping protocol to the URL of the proxy.
    :param verify: (optional) Either a boolean, in which case it controls whether we verify
            the server's TLS certificate, or a string, in which case it must be a path
            to a CA bundle to use. Defaults to ``True``.
    :param stream: (optional) if ``False``, the response content will be immediately downloaded.
    :param cert: (optional) if String, path to ssl client cert file (.pem). If Tuple, ('cert', 'key') pair.
    :return: :class:`Response <Response>` object
    :rtype: construct_and_send_requests.Response

    Usage::

      >>> import construct_and_send_requests
      >>> req = construct_and_send_requests.construct_and_send_request('GET', 'https://httpbin.org/get')
      >>> req
      <Response [200]>
    """"""

    # By using the 'with' statement we are sure the session is closed, thus we
    # avoid leaving sockets open which can trigger a ResourceWarning in some
    # cases, and look like a memory leak in others.
    with sessions.Session() as session:
        return session.construct_and_send_request(method=method, url=url, **kwargs)","def send_options_request_with_optional_arguments_handling(url, **kwargs):
    r""""""Sends an OPTIONS construct_and_send_request.

    :param url: URL for the new :class:`Request` object.
    :param \*\*kwargs: Optional arguments that ``construct_and_send_request`` takes.
    :return: :class:`Response <Response>` object
    :rtype: construct_and_send_requests.Response
    """"""

    return construct_and_send_request('send_options_request_with_optional_arguments_handling', url, **kwargs)","{""request"": ""construct_and_send_request""}","def 0(method, url, **kwargs):
    """"""Constructs and sends a :class:`Request <Request>`.

    :param method: method for the new :class:`Request` object: ``GET``, ``OPTIONS``, ``HEAD``, ``POST``, ``PUT``, ``PATCH``, or ``DELETE``.
    :param url: URL for the new :class:`Request` object.
    :param params: (optional) Dictionary, list of tuples or bytes to send
        in the query string for the :class:`Request`.
    :param data: (optional) Dictionary, list of tuples, bytes, or file-like
        object to send in the body of the :class:`Request`.
    :param json: (optional) A JSON serializable Python object to send in the body of the :class:`Request`.
    :param headers: (optional) Dictionary of HTTP Headers to send with the :class:`Request`.
    :param cookies: (optional) Dict or CookieJar object to send with the :class:`Request`.
    :param files: (optional) Dictionary of ``'name': file-like-objects`` (or ``{'name': file-tuple}``) for multipart encoding upload.
        ``file-tuple`` can be a 2-tuple ``('filename', fileobj)``, 3-tuple ``('filename', fileobj, 'content_type')``
        or a 4-tuple ``('filename', fileobj, 'content_type', custom_headers)``, where ``'content-type'`` is a string
        defining the content type of the given file and ``custom_headers`` a dict-like object containing additional headers
        to add for the file.
    :param auth: (optional) Auth tuple to enable Basic/Digest/Custom HTTP Auth.
    :param timeout: (optional) How many seconds to wait for the server to send data
        before giving up, as a float, or a :ref:`(connect timeout, read
        timeout) <timeouts>` tuple.
    :type timeout: float or tuple
    :param allow_redirects: (optional) Boolean. Enable/disable GET/OPTIONS/POST/PUT/PATCH/DELETE/HEAD redirection. Defaults to ``True``.
    :type allow_redirects: bool
    :param proxies: (optional) Dictionary mapping protocol to the URL of the proxy.
    :param verify: (optional) Either a boolean, in which case it controls whether we verify
            the server's TLS certificate, or a string, in which case it must be a path
            to a CA bundle to use. Defaults to ``True``.
    :param stream: (optional) if ``False``, the response content will be immediately downloaded.
    :param cert: (optional) if String, path to ssl client cert file (.pem). If Tuple, ('cert', 'key') pair.
    :return: :class:`Response <Response>` object
    :rtype: requests.Response

    Usage::

      >>> import requests
      >>> req = requests.0('GET', 'https://httpbin.org/get')
      >>> req
      <Response [200]>
    """"""

    # By using the 'with' statement we are sure the session is closed, thus we
    # avoid leaving sockets open which can trigger a ResourceWarning in some
    # cases, and look like a memory leak in others.
    with sessions.Session() as session:
        return session.0(method=method, url=url, **kwargs)","def 1(url, **kwargs):
    r""""""Sends an OPTIONS request.

    :param url: URL for the new :class:`Request` object.
    :param \*\*kwargs: Optional arguments that ``request`` takes.
    :return: :class:`Response <Response>` object
    :rtype: requests.Response
    """"""

    return 0('options', url, **kwargs)","{""request"": ""0""}"
401,401,"def head(url, **kwargs):
    r""""""Sends a HEAD request.

    :param url: URL for the new :class:`Request` object.
    :param \*\*kwargs: Optional arguments that ``request`` takes. If
        `allow_redirects` is not provided, it will be set to `False` (as
        opposed to the default :meth:`request` behavior).
    :return: :class:`Response <Response>` object
    :rtype: requests.Response
    """"""

    kwargs.setdefault('allow_redirects', False)
    return request('head', url, **kwargs)","def request(method, url, **kwargs):
    """"""Constructs and sends a :class:`Request <Request>`.

    :param method: method for the new :class:`Request` object: ``GET``, ``OPTIONS``, ``HEAD``, ``POST``, ``PUT``, ``PATCH``, or ``DELETE``.
    :param url: URL for the new :class:`Request` object.
    :param params: (optional) Dictionary, list of tuples or bytes to send
        in the query string for the :class:`Request`.
    :param data: (optional) Dictionary, list of tuples, bytes, or file-like
        object to send in the body of the :class:`Request`.
    :param json: (optional) A JSON serializable Python object to send in the body of the :class:`Request`.
    :param headers: (optional) Dictionary of HTTP Headers to send with the :class:`Request`.
    :param cookies: (optional) Dict or CookieJar object to send with the :class:`Request`.
    :param files: (optional) Dictionary of ``'name': file-like-objects`` (or ``{'name': file-tuple}``) for multipart encoding upload.
        ``file-tuple`` can be a 2-tuple ``('filename', fileobj)``, 3-tuple ``('filename', fileobj, 'content_type')``
        or a 4-tuple ``('filename', fileobj, 'content_type', custom_headers)``, where ``'content-type'`` is a string
        defining the content type of the given file and ``custom_headers`` a dict-like object containing additional headers
        to add for the file.
    :param auth: (optional) Auth tuple to enable Basic/Digest/Custom HTTP Auth.
    :param timeout: (optional) How many seconds to wait for the server to send data
        before giving up, as a float, or a :ref:`(connect timeout, read
        timeout) <timeouts>` tuple.
    :type timeout: float or tuple
    :param allow_redirects: (optional) Boolean. Enable/disable GET/OPTIONS/POST/PUT/PATCH/DELETE/HEAD redirection. Defaults to ``True``.
    :type allow_redirects: bool
    :param proxies: (optional) Dictionary mapping protocol to the URL of the proxy.
    :param verify: (optional) Either a boolean, in which case it controls whether we verify
            the server's TLS certificate, or a string, in which case it must be a path
            to a CA bundle to use. Defaults to ``True``.
    :param stream: (optional) if ``False``, the response content will be immediately downloaded.
    :param cert: (optional) if String, path to ssl client cert file (.pem). If Tuple, ('cert', 'key') pair.
    :return: :class:`Response <Response>` object
    :rtype: requests.Response

    Usage::

      >>> import requests
      >>> req = requests.request('GET', 'https://httpbin.org/get')
      >>> req
      <Response [200]>
    """"""

    # By using the 'with' statement we are sure the session is closed, thus we
    # avoid leaving sockets open which can trigger a ResourceWarning in some
    # cases, and look like a memory leak in others.
    with sessions.Session() as session:
        return session.request(method=method, url=url, **kwargs)","def construct_and_send_request(method, url, **kwargs):
    """"""Constructs and sends a :class:`Request <Request>`.

    :param method: method for the new :class:`Request` object: ``GET``, ``OPTIONS``, ``HEAD``, ``POST``, ``PUT``, ``PATCH``, or ``DELETE``.
    :param url: URL for the new :class:`Request` object.
    :param params: (optional) Dictionary, list of tuples or bytes to send
        in the query string for the :class:`Request`.
    :param data: (optional) Dictionary, list of tuples, bytes, or file-like
        object to send in the body of the :class:`Request`.
    :param json: (optional) A JSON serializable Python object to send in the body of the :class:`Request`.
    :param headers: (optional) Dictionary of HTTP Headers to send with the :class:`Request`.
    :param cookies: (optional) Dict or CookieJar object to send with the :class:`Request`.
    :param files: (optional) Dictionary of ``'name': file-like-objects`` (or ``{'name': file-tuple}``) for multipart encoding upload.
        ``file-tuple`` can be a 2-tuple ``('filename', fileobj)``, 3-tuple ``('filename', fileobj, 'content_type')``
        or a 4-tuple ``('filename', fileobj, 'content_type', custom_headers)``, where ``'content-type'`` is a string
        defining the content type of the given file and ``custom_headers`` a dict-like object containing additional headers
        to add for the file.
    :param auth: (optional) Auth tuple to enable Basic/Digest/Custom HTTP Auth.
    :param timeout: (optional) How many seconds to wait for the server to send data
        before giving up, as a float, or a :ref:`(connect timeout, read
        timeout) <timeouts>` tuple.
    :type timeout: float or tuple
    :param allow_redirects: (optional) Boolean. Enable/disable GET/OPTIONS/POST/PUT/PATCH/DELETE/HEAD redirection. Defaults to ``True``.
    :type allow_redirects: bool
    :param proxies: (optional) Dictionary mapping protocol to the URL of the proxy.
    :param verify: (optional) Either a boolean, in which case it controls whether we verify
            the server's TLS certificate, or a string, in which case it must be a path
            to a CA bundle to use. Defaults to ``True``.
    :param stream: (optional) if ``False``, the response content will be immediately downloaded.
    :param cert: (optional) if String, path to ssl client cert file (.pem). If Tuple, ('cert', 'key') pair.
    :return: :class:`Response <Response>` object
    :rtype: construct_and_send_requests.Response

    Usage::

      >>> import construct_and_send_requests
      >>> req = construct_and_send_requests.construct_and_send_request('GET', 'https://httpbin.org/get')
      >>> req
      <Response [200]>
    """"""

    # By using the 'with' statement we are sure the session is closed, thus we
    # avoid leaving sockets open which can trigger a ResourceWarning in some
    # cases, and look like a memory leak in others.
    with sessions.Session() as session:
        return session.construct_and_send_request(method=method, url=url, **kwargs)","def send_head_request_with_optional_arguments_handling(url, **kwargs):
    r""""""Sends a HEAD construct_and_send_request.

    :param url: URL for the new :class:`Request` object.
    :param \*\*kwargs: Optional arguments that ``construct_and_send_request`` takes. If
        `allow_redirects` is not provided, it will be set to `False` (as
        opposed to the default :meth:`construct_and_send_request` behavior).
    :return: :class:`Response <Response>` object
    :rtype: construct_and_send_requests.Response
    """"""

    kwargs.setdefault('allow_redirects', False)
    return construct_and_send_request('send_head_request_with_optional_arguments_handling', url, **kwargs)","{""request"": ""construct_and_send_request""}","def 0(method, url, **kwargs):
    """"""Constructs and sends a :class:`Request <Request>`.

    :param method: method for the new :class:`Request` object: ``GET``, ``OPTIONS``, ``HEAD``, ``POST``, ``PUT``, ``PATCH``, or ``DELETE``.
    :param url: URL for the new :class:`Request` object.
    :param params: (optional) Dictionary, list of tuples or bytes to send
        in the query string for the :class:`Request`.
    :param data: (optional) Dictionary, list of tuples, bytes, or file-like
        object to send in the body of the :class:`Request`.
    :param json: (optional) A JSON serializable Python object to send in the body of the :class:`Request`.
    :param headers: (optional) Dictionary of HTTP Headers to send with the :class:`Request`.
    :param cookies: (optional) Dict or CookieJar object to send with the :class:`Request`.
    :param files: (optional) Dictionary of ``'name': file-like-objects`` (or ``{'name': file-tuple}``) for multipart encoding upload.
        ``file-tuple`` can be a 2-tuple ``('filename', fileobj)``, 3-tuple ``('filename', fileobj, 'content_type')``
        or a 4-tuple ``('filename', fileobj, 'content_type', custom_headers)``, where ``'content-type'`` is a string
        defining the content type of the given file and ``custom_headers`` a dict-like object containing additional headers
        to add for the file.
    :param auth: (optional) Auth tuple to enable Basic/Digest/Custom HTTP Auth.
    :param timeout: (optional) How many seconds to wait for the server to send data
        before giving up, as a float, or a :ref:`(connect timeout, read
        timeout) <timeouts>` tuple.
    :type timeout: float or tuple
    :param allow_redirects: (optional) Boolean. Enable/disable GET/OPTIONS/POST/PUT/PATCH/DELETE/HEAD redirection. Defaults to ``True``.
    :type allow_redirects: bool
    :param proxies: (optional) Dictionary mapping protocol to the URL of the proxy.
    :param verify: (optional) Either a boolean, in which case it controls whether we verify
            the server's TLS certificate, or a string, in which case it must be a path
            to a CA bundle to use. Defaults to ``True``.
    :param stream: (optional) if ``False``, the response content will be immediately downloaded.
    :param cert: (optional) if String, path to ssl client cert file (.pem). If Tuple, ('cert', 'key') pair.
    :return: :class:`Response <Response>` object
    :rtype: requests.Response

    Usage::

      >>> import requests
      >>> req = requests.0('GET', 'https://httpbin.org/get')
      >>> req
      <Response [200]>
    """"""

    # By using the 'with' statement we are sure the session is closed, thus we
    # avoid leaving sockets open which can trigger a ResourceWarning in some
    # cases, and look like a memory leak in others.
    with sessions.Session() as session:
        return session.0(method=method, url=url, **kwargs)","def 1(url, **kwargs):
    r""""""Sends a HEAD request.

    :param url: URL for the new :class:`Request` object.
    :param \*\*kwargs: Optional arguments that ``request`` takes. If
        `allow_redirects` is not provided, it will be set to `False` (as
        opposed to the default :meth:`request` behavior).
    :return: :class:`Response <Response>` object
    :rtype: requests.Response
    """"""

    kwargs.setdefault('allow_redirects', False)
    return 0('head', url, **kwargs)","{""request"": ""0""}"
402,402,"def post(url, data=None, json=None, **kwargs):
    r""""""Sends a POST request.

    :param url: URL for the new :class:`Request` object.
    :param data: (optional) Dictionary, list of tuples, bytes, or file-like
        object to send in the body of the :class:`Request`.
    :param json: (optional) json data to send in the body of the :class:`Request`.
    :param \*\*kwargs: Optional arguments that ``request`` takes.
    :return: :class:`Response <Response>` object
    :rtype: requests.Response
    """"""

    return request('post', url, data=data, json=json, **kwargs)","def request(method, url, **kwargs):
    """"""Constructs and sends a :class:`Request <Request>`.

    :param method: method for the new :class:`Request` object: ``GET``, ``OPTIONS``, ``HEAD``, ``POST``, ``PUT``, ``PATCH``, or ``DELETE``.
    :param url: URL for the new :class:`Request` object.
    :param params: (optional) Dictionary, list of tuples or bytes to send
        in the query string for the :class:`Request`.
    :param data: (optional) Dictionary, list of tuples, bytes, or file-like
        object to send in the body of the :class:`Request`.
    :param json: (optional) A JSON serializable Python object to send in the body of the :class:`Request`.
    :param headers: (optional) Dictionary of HTTP Headers to send with the :class:`Request`.
    :param cookies: (optional) Dict or CookieJar object to send with the :class:`Request`.
    :param files: (optional) Dictionary of ``'name': file-like-objects`` (or ``{'name': file-tuple}``) for multipart encoding upload.
        ``file-tuple`` can be a 2-tuple ``('filename', fileobj)``, 3-tuple ``('filename', fileobj, 'content_type')``
        or a 4-tuple ``('filename', fileobj, 'content_type', custom_headers)``, where ``'content-type'`` is a string
        defining the content type of the given file and ``custom_headers`` a dict-like object containing additional headers
        to add for the file.
    :param auth: (optional) Auth tuple to enable Basic/Digest/Custom HTTP Auth.
    :param timeout: (optional) How many seconds to wait for the server to send data
        before giving up, as a float, or a :ref:`(connect timeout, read
        timeout) <timeouts>` tuple.
    :type timeout: float or tuple
    :param allow_redirects: (optional) Boolean. Enable/disable GET/OPTIONS/POST/PUT/PATCH/DELETE/HEAD redirection. Defaults to ``True``.
    :type allow_redirects: bool
    :param proxies: (optional) Dictionary mapping protocol to the URL of the proxy.
    :param verify: (optional) Either a boolean, in which case it controls whether we verify
            the server's TLS certificate, or a string, in which case it must be a path
            to a CA bundle to use. Defaults to ``True``.
    :param stream: (optional) if ``False``, the response content will be immediately downloaded.
    :param cert: (optional) if String, path to ssl client cert file (.pem). If Tuple, ('cert', 'key') pair.
    :return: :class:`Response <Response>` object
    :rtype: requests.Response

    Usage::

      >>> import requests
      >>> req = requests.request('GET', 'https://httpbin.org/get')
      >>> req
      <Response [200]>
    """"""

    # By using the 'with' statement we are sure the session is closed, thus we
    # avoid leaving sockets open which can trigger a ResourceWarning in some
    # cases, and look like a memory leak in others.
    with sessions.Session() as session:
        return session.request(method=method, url=url, **kwargs)","def construct_and_send_request(method, url, **kwargs):
    """"""Constructs and sends a :class:`Request <Request>`.

    :param method: method for the new :class:`Request` object: ``GET``, ``OPTIONS``, ``HEAD``, ``POST``, ``PUT``, ``PATCH``, or ``DELETE``.
    :param url: URL for the new :class:`Request` object.
    :param params: (optional) Dictionary, list of tuples or bytes to send
        in the query string for the :class:`Request`.
    :param data: (optional) Dictionary, list of tuples, bytes, or file-like
        object to send in the body of the :class:`Request`.
    :param json: (optional) A JSON serializable Python object to send in the body of the :class:`Request`.
    :param headers: (optional) Dictionary of HTTP Headers to send with the :class:`Request`.
    :param cookies: (optional) Dict or CookieJar object to send with the :class:`Request`.
    :param files: (optional) Dictionary of ``'name': file-like-objects`` (or ``{'name': file-tuple}``) for multipart encoding upload.
        ``file-tuple`` can be a 2-tuple ``('filename', fileobj)``, 3-tuple ``('filename', fileobj, 'content_type')``
        or a 4-tuple ``('filename', fileobj, 'content_type', custom_headers)``, where ``'content-type'`` is a string
        defining the content type of the given file and ``custom_headers`` a dict-like object containing additional headers
        to add for the file.
    :param auth: (optional) Auth tuple to enable Basic/Digest/Custom HTTP Auth.
    :param timeout: (optional) How many seconds to wait for the server to send data
        before giving up, as a float, or a :ref:`(connect timeout, read
        timeout) <timeouts>` tuple.
    :type timeout: float or tuple
    :param allow_redirects: (optional) Boolean. Enable/disable GET/OPTIONS/POST/PUT/PATCH/DELETE/HEAD redirection. Defaults to ``True``.
    :type allow_redirects: bool
    :param proxies: (optional) Dictionary mapping protocol to the URL of the proxy.
    :param verify: (optional) Either a boolean, in which case it controls whether we verify
            the server's TLS certificate, or a string, in which case it must be a path
            to a CA bundle to use. Defaults to ``True``.
    :param stream: (optional) if ``False``, the response content will be immediately downloaded.
    :param cert: (optional) if String, path to ssl client cert file (.pem). If Tuple, ('cert', 'key') pair.
    :return: :class:`Response <Response>` object
    :rtype: construct_and_send_requests.Response

    Usage::

      >>> import construct_and_send_requests
      >>> req = construct_and_send_requests.construct_and_send_request('GET', 'https://httpbin.org/get')
      >>> req
      <Response [200]>
    """"""

    # By using the 'with' statement we are sure the session is closed, thus we
    # avoid leaving sockets open which can trigger a ResourceWarning in some
    # cases, and look like a memory leak in others.
    with sessions.Session() as session:
        return session.construct_and_send_request(method=method, url=url, **kwargs)","def send_post_request_with_data_and_json_handling(url, data=None, json=None, **kwargs):
    r""""""Sends a POST construct_and_send_request.

    :param url: URL for the new :class:`Request` object.
    :param data: (optional) Dictionary, list of tuples, bytes, or file-like
        object to send in the body of the :class:`Request`.
    :param json: (optional) json data to send in the body of the :class:`Request`.
    :param \*\*kwargs: Optional arguments that ``construct_and_send_request`` takes.
    :return: :class:`Response <Response>` object
    :rtype: construct_and_send_requests.Response
    """"""

    return construct_and_send_request('send_post_request_with_data_and_json_handling', url, data=data, json=json, **kwargs)","{""request"": ""construct_and_send_request""}","def 0(method, url, **kwargs):
    """"""Constructs and sends a :class:`Request <Request>`.

    :param method: method for the new :class:`Request` object: ``GET``, ``OPTIONS``, ``HEAD``, ``POST``, ``PUT``, ``PATCH``, or ``DELETE``.
    :param url: URL for the new :class:`Request` object.
    :param params: (optional) Dictionary, list of tuples or bytes to send
        in the query string for the :class:`Request`.
    :param data: (optional) Dictionary, list of tuples, bytes, or file-like
        object to send in the body of the :class:`Request`.
    :param json: (optional) A JSON serializable Python object to send in the body of the :class:`Request`.
    :param headers: (optional) Dictionary of HTTP Headers to send with the :class:`Request`.
    :param cookies: (optional) Dict or CookieJar object to send with the :class:`Request`.
    :param files: (optional) Dictionary of ``'name': file-like-objects`` (or ``{'name': file-tuple}``) for multipart encoding upload.
        ``file-tuple`` can be a 2-tuple ``('filename', fileobj)``, 3-tuple ``('filename', fileobj, 'content_type')``
        or a 4-tuple ``('filename', fileobj, 'content_type', custom_headers)``, where ``'content-type'`` is a string
        defining the content type of the given file and ``custom_headers`` a dict-like object containing additional headers
        to add for the file.
    :param auth: (optional) Auth tuple to enable Basic/Digest/Custom HTTP Auth.
    :param timeout: (optional) How many seconds to wait for the server to send data
        before giving up, as a float, or a :ref:`(connect timeout, read
        timeout) <timeouts>` tuple.
    :type timeout: float or tuple
    :param allow_redirects: (optional) Boolean. Enable/disable GET/OPTIONS/POST/PUT/PATCH/DELETE/HEAD redirection. Defaults to ``True``.
    :type allow_redirects: bool
    :param proxies: (optional) Dictionary mapping protocol to the URL of the proxy.
    :param verify: (optional) Either a boolean, in which case it controls whether we verify
            the server's TLS certificate, or a string, in which case it must be a path
            to a CA bundle to use. Defaults to ``True``.
    :param stream: (optional) if ``False``, the response content will be immediately downloaded.
    :param cert: (optional) if String, path to ssl client cert file (.pem). If Tuple, ('cert', 'key') pair.
    :return: :class:`Response <Response>` object
    :rtype: requests.Response

    Usage::

      >>> import requests
      >>> req = requests.0('GET', 'https://httpbin.org/get')
      >>> req
      <Response [200]>
    """"""

    # By using the 'with' statement we are sure the session is closed, thus we
    # avoid leaving sockets open which can trigger a ResourceWarning in some
    # cases, and look like a memory leak in others.
    with sessions.Session() as session:
        return session.0(method=method, url=url, **kwargs)","def 1(url, data=None, json=None, **kwargs):
    r""""""Sends a POST request.

    :param url: URL for the new :class:`Request` object.
    :param data: (optional) Dictionary, list of tuples, bytes, or file-like
        object to send in the body of the :class:`Request`.
    :param json: (optional) json data to send in the body of the :class:`Request`.
    :param \*\*kwargs: Optional arguments that ``request`` takes.
    :return: :class:`Response <Response>` object
    :rtype: requests.Response
    """"""

    return 0('post', url, data=data, json=json, **kwargs)","{""request"": ""0""}"
403,403,"def put(url, data=None, **kwargs):
    r""""""Sends a PUT request.

    :param url: URL for the new :class:`Request` object.
    :param data: (optional) Dictionary, list of tuples, bytes, or file-like
        object to send in the body of the :class:`Request`.
    :param json: (optional) json data to send in the body of the :class:`Request`.
    :param \*\*kwargs: Optional arguments that ``request`` takes.
    :return: :class:`Response <Response>` object
    :rtype: requests.Response
    """"""

    return request('put', url, data=data, **kwargs)","def request(method, url, **kwargs):
    """"""Constructs and sends a :class:`Request <Request>`.

    :param method: method for the new :class:`Request` object: ``GET``, ``OPTIONS``, ``HEAD``, ``POST``, ``PUT``, ``PATCH``, or ``DELETE``.
    :param url: URL for the new :class:`Request` object.
    :param params: (optional) Dictionary, list of tuples or bytes to send
        in the query string for the :class:`Request`.
    :param data: (optional) Dictionary, list of tuples, bytes, or file-like
        object to send in the body of the :class:`Request`.
    :param json: (optional) A JSON serializable Python object to send in the body of the :class:`Request`.
    :param headers: (optional) Dictionary of HTTP Headers to send with the :class:`Request`.
    :param cookies: (optional) Dict or CookieJar object to send with the :class:`Request`.
    :param files: (optional) Dictionary of ``'name': file-like-objects`` (or ``{'name': file-tuple}``) for multipart encoding upload.
        ``file-tuple`` can be a 2-tuple ``('filename', fileobj)``, 3-tuple ``('filename', fileobj, 'content_type')``
        or a 4-tuple ``('filename', fileobj, 'content_type', custom_headers)``, where ``'content-type'`` is a string
        defining the content type of the given file and ``custom_headers`` a dict-like object containing additional headers
        to add for the file.
    :param auth: (optional) Auth tuple to enable Basic/Digest/Custom HTTP Auth.
    :param timeout: (optional) How many seconds to wait for the server to send data
        before giving up, as a float, or a :ref:`(connect timeout, read
        timeout) <timeouts>` tuple.
    :type timeout: float or tuple
    :param allow_redirects: (optional) Boolean. Enable/disable GET/OPTIONS/POST/PUT/PATCH/DELETE/HEAD redirection. Defaults to ``True``.
    :type allow_redirects: bool
    :param proxies: (optional) Dictionary mapping protocol to the URL of the proxy.
    :param verify: (optional) Either a boolean, in which case it controls whether we verify
            the server's TLS certificate, or a string, in which case it must be a path
            to a CA bundle to use. Defaults to ``True``.
    :param stream: (optional) if ``False``, the response content will be immediately downloaded.
    :param cert: (optional) if String, path to ssl client cert file (.pem). If Tuple, ('cert', 'key') pair.
    :return: :class:`Response <Response>` object
    :rtype: requests.Response

    Usage::

      >>> import requests
      >>> req = requests.request('GET', 'https://httpbin.org/get')
      >>> req
      <Response [200]>
    """"""

    # By using the 'with' statement we are sure the session is closed, thus we
    # avoid leaving sockets open which can trigger a ResourceWarning in some
    # cases, and look like a memory leak in others.
    with sessions.Session() as session:
        return session.request(method=method, url=url, **kwargs)","def construct_and_send_request(method, url, **kwargs):
    """"""Constructs and sends a :class:`Request <Request>`.

    :param method: method for the new :class:`Request` object: ``GET``, ``OPTIONS``, ``HEAD``, ``POST``, ``PUT``, ``PATCH``, or ``DELETE``.
    :param url: URL for the new :class:`Request` object.
    :param params: (optional) Dictionary, list of tuples or bytes to send
        in the query string for the :class:`Request`.
    :param data: (optional) Dictionary, list of tuples, bytes, or file-like
        object to send in the body of the :class:`Request`.
    :param json: (optional) A JSON serializable Python object to send in the body of the :class:`Request`.
    :param headers: (optional) Dictionary of HTTP Headers to send with the :class:`Request`.
    :param cookies: (optional) Dict or CookieJar object to send with the :class:`Request`.
    :param files: (optional) Dictionary of ``'name': file-like-objects`` (or ``{'name': file-tuple}``) for multipart encoding upload.
        ``file-tuple`` can be a 2-tuple ``('filename', fileobj)``, 3-tuple ``('filename', fileobj, 'content_type')``
        or a 4-tuple ``('filename', fileobj, 'content_type', custom_headers)``, where ``'content-type'`` is a string
        defining the content type of the given file and ``custom_headers`` a dict-like object containing additional headers
        to add for the file.
    :param auth: (optional) Auth tuple to enable Basic/Digest/Custom HTTP Auth.
    :param timeout: (optional) How many seconds to wait for the server to send data
        before giving up, as a float, or a :ref:`(connect timeout, read
        timeout) <timeouts>` tuple.
    :type timeout: float or tuple
    :param allow_redirects: (optional) Boolean. Enable/disable GET/OPTIONS/POST/PUT/PATCH/DELETE/HEAD redirection. Defaults to ``True``.
    :type allow_redirects: bool
    :param proxies: (optional) Dictionary mapping protocol to the URL of the proxy.
    :param verify: (optional) Either a boolean, in which case it controls whether we verify
            the server's TLS certificate, or a string, in which case it must be a path
            to a CA bundle to use. Defaults to ``True``.
    :param stream: (optional) if ``False``, the response content will be immediately downloaded.
    :param cert: (optional) if String, path to ssl client cert file (.pem). If Tuple, ('cert', 'key') pair.
    :return: :class:`Response <Response>` object
    :rtype: construct_and_send_requests.Response

    Usage::

      >>> import construct_and_send_requests
      >>> req = construct_and_send_requests.construct_and_send_request('GET', 'https://httpbin.org/get')
      >>> req
      <Response [200]>
    """"""

    # By using the 'with' statement we are sure the session is closed, thus we
    # avoid leaving sockets open which can trigger a ResourceWarning in some
    # cases, and look like a memory leak in others.
    with sessions.Session() as session:
        return session.construct_and_send_request(method=method, url=url, **kwargs)","def send_put_request_with_data_handling(url, data=None, **kwargs):
    r""""""Sends a PUT construct_and_send_request.

    :param url: URL for the new :class:`Request` object.
    :param data: (optional) Dictionary, list of tuples, bytes, or file-like
        object to send in the body of the :class:`Request`.
    :param json: (optional) json data to send in the body of the :class:`Request`.
    :param \*\*kwargs: Optional arguments that ``construct_and_send_request`` takes.
    :return: :class:`Response <Response>` object
    :rtype: construct_and_send_requests.Response
    """"""

    return construct_and_send_request('send_put_request_with_data_handling', url, data=data, **kwargs)","{""request"": ""construct_and_send_request""}","def 0(method, url, **kwargs):
    """"""Constructs and sends a :class:`Request <Request>`.

    :param method: method for the new :class:`Request` object: ``GET``, ``OPTIONS``, ``HEAD``, ``POST``, ``PUT``, ``PATCH``, or ``DELETE``.
    :param url: URL for the new :class:`Request` object.
    :param params: (optional) Dictionary, list of tuples or bytes to send
        in the query string for the :class:`Request`.
    :param data: (optional) Dictionary, list of tuples, bytes, or file-like
        object to send in the body of the :class:`Request`.
    :param json: (optional) A JSON serializable Python object to send in the body of the :class:`Request`.
    :param headers: (optional) Dictionary of HTTP Headers to send with the :class:`Request`.
    :param cookies: (optional) Dict or CookieJar object to send with the :class:`Request`.
    :param files: (optional) Dictionary of ``'name': file-like-objects`` (or ``{'name': file-tuple}``) for multipart encoding upload.
        ``file-tuple`` can be a 2-tuple ``('filename', fileobj)``, 3-tuple ``('filename', fileobj, 'content_type')``
        or a 4-tuple ``('filename', fileobj, 'content_type', custom_headers)``, where ``'content-type'`` is a string
        defining the content type of the given file and ``custom_headers`` a dict-like object containing additional headers
        to add for the file.
    :param auth: (optional) Auth tuple to enable Basic/Digest/Custom HTTP Auth.
    :param timeout: (optional) How many seconds to wait for the server to send data
        before giving up, as a float, or a :ref:`(connect timeout, read
        timeout) <timeouts>` tuple.
    :type timeout: float or tuple
    :param allow_redirects: (optional) Boolean. Enable/disable GET/OPTIONS/POST/PUT/PATCH/DELETE/HEAD redirection. Defaults to ``True``.
    :type allow_redirects: bool
    :param proxies: (optional) Dictionary mapping protocol to the URL of the proxy.
    :param verify: (optional) Either a boolean, in which case it controls whether we verify
            the server's TLS certificate, or a string, in which case it must be a path
            to a CA bundle to use. Defaults to ``True``.
    :param stream: (optional) if ``False``, the response content will be immediately downloaded.
    :param cert: (optional) if String, path to ssl client cert file (.pem). If Tuple, ('cert', 'key') pair.
    :return: :class:`Response <Response>` object
    :rtype: requests.Response

    Usage::

      >>> import requests
      >>> req = requests.0('GET', 'https://httpbin.org/get')
      >>> req
      <Response [200]>
    """"""

    # By using the 'with' statement we are sure the session is closed, thus we
    # avoid leaving sockets open which can trigger a ResourceWarning in some
    # cases, and look like a memory leak in others.
    with sessions.Session() as session:
        return session.0(method=method, url=url, **kwargs)","def 1(url, data=None, **kwargs):
    r""""""Sends a PUT request.

    :param url: URL for the new :class:`Request` object.
    :param data: (optional) Dictionary, list of tuples, bytes, or file-like
        object to send in the body of the :class:`Request`.
    :param json: (optional) json data to send in the body of the :class:`Request`.
    :param \*\*kwargs: Optional arguments that ``request`` takes.
    :return: :class:`Response <Response>` object
    :rtype: requests.Response
    """"""

    return 0('put', url, data=data, **kwargs)","{""request"": ""0""}"
404,404,"def patch(url, data=None, **kwargs):
    r""""""Sends a PATCH request.

    :param url: URL for the new :class:`Request` object.
    :param data: (optional) Dictionary, list of tuples, bytes, or file-like
        object to send in the body of the :class:`Request`.
    :param json: (optional) json data to send in the body of the :class:`Request`.
    :param \*\*kwargs: Optional arguments that ``request`` takes.
    :return: :class:`Response <Response>` object
    :rtype: requests.Response
    """"""

    return request('patch', url, data=data, **kwargs)","def request(method, url, **kwargs):
    """"""Constructs and sends a :class:`Request <Request>`.

    :param method: method for the new :class:`Request` object: ``GET``, ``OPTIONS``, ``HEAD``, ``POST``, ``PUT``, ``PATCH``, or ``DELETE``.
    :param url: URL for the new :class:`Request` object.
    :param params: (optional) Dictionary, list of tuples or bytes to send
        in the query string for the :class:`Request`.
    :param data: (optional) Dictionary, list of tuples, bytes, or file-like
        object to send in the body of the :class:`Request`.
    :param json: (optional) A JSON serializable Python object to send in the body of the :class:`Request`.
    :param headers: (optional) Dictionary of HTTP Headers to send with the :class:`Request`.
    :param cookies: (optional) Dict or CookieJar object to send with the :class:`Request`.
    :param files: (optional) Dictionary of ``'name': file-like-objects`` (or ``{'name': file-tuple}``) for multipart encoding upload.
        ``file-tuple`` can be a 2-tuple ``('filename', fileobj)``, 3-tuple ``('filename', fileobj, 'content_type')``
        or a 4-tuple ``('filename', fileobj, 'content_type', custom_headers)``, where ``'content-type'`` is a string
        defining the content type of the given file and ``custom_headers`` a dict-like object containing additional headers
        to add for the file.
    :param auth: (optional) Auth tuple to enable Basic/Digest/Custom HTTP Auth.
    :param timeout: (optional) How many seconds to wait for the server to send data
        before giving up, as a float, or a :ref:`(connect timeout, read
        timeout) <timeouts>` tuple.
    :type timeout: float or tuple
    :param allow_redirects: (optional) Boolean. Enable/disable GET/OPTIONS/POST/PUT/PATCH/DELETE/HEAD redirection. Defaults to ``True``.
    :type allow_redirects: bool
    :param proxies: (optional) Dictionary mapping protocol to the URL of the proxy.
    :param verify: (optional) Either a boolean, in which case it controls whether we verify
            the server's TLS certificate, or a string, in which case it must be a path
            to a CA bundle to use. Defaults to ``True``.
    :param stream: (optional) if ``False``, the response content will be immediately downloaded.
    :param cert: (optional) if String, path to ssl client cert file (.pem). If Tuple, ('cert', 'key') pair.
    :return: :class:`Response <Response>` object
    :rtype: requests.Response

    Usage::

      >>> import requests
      >>> req = requests.request('GET', 'https://httpbin.org/get')
      >>> req
      <Response [200]>
    """"""

    # By using the 'with' statement we are sure the session is closed, thus we
    # avoid leaving sockets open which can trigger a ResourceWarning in some
    # cases, and look like a memory leak in others.
    with sessions.Session() as session:
        return session.request(method=method, url=url, **kwargs)","def construct_and_send_request(method, url, **kwargs):
    """"""Constructs and sends a :class:`Request <Request>`.

    :param method: method for the new :class:`Request` object: ``GET``, ``OPTIONS``, ``HEAD``, ``POST``, ``PUT``, ``PATCH``, or ``DELETE``.
    :param url: URL for the new :class:`Request` object.
    :param params: (optional) Dictionary, list of tuples or bytes to send
        in the query string for the :class:`Request`.
    :param data: (optional) Dictionary, list of tuples, bytes, or file-like
        object to send in the body of the :class:`Request`.
    :param json: (optional) A JSON serializable Python object to send in the body of the :class:`Request`.
    :param headers: (optional) Dictionary of HTTP Headers to send with the :class:`Request`.
    :param cookies: (optional) Dict or CookieJar object to send with the :class:`Request`.
    :param files: (optional) Dictionary of ``'name': file-like-objects`` (or ``{'name': file-tuple}``) for multipart encoding upload.
        ``file-tuple`` can be a 2-tuple ``('filename', fileobj)``, 3-tuple ``('filename', fileobj, 'content_type')``
        or a 4-tuple ``('filename', fileobj, 'content_type', custom_headers)``, where ``'content-type'`` is a string
        defining the content type of the given file and ``custom_headers`` a dict-like object containing additional headers
        to add for the file.
    :param auth: (optional) Auth tuple to enable Basic/Digest/Custom HTTP Auth.
    :param timeout: (optional) How many seconds to wait for the server to send data
        before giving up, as a float, or a :ref:`(connect timeout, read
        timeout) <timeouts>` tuple.
    :type timeout: float or tuple
    :param allow_redirects: (optional) Boolean. Enable/disable GET/OPTIONS/POST/PUT/PATCH/DELETE/HEAD redirection. Defaults to ``True``.
    :type allow_redirects: bool
    :param proxies: (optional) Dictionary mapping protocol to the URL of the proxy.
    :param verify: (optional) Either a boolean, in which case it controls whether we verify
            the server's TLS certificate, or a string, in which case it must be a path
            to a CA bundle to use. Defaults to ``True``.
    :param stream: (optional) if ``False``, the response content will be immediately downloaded.
    :param cert: (optional) if String, path to ssl client cert file (.pem). If Tuple, ('cert', 'key') pair.
    :return: :class:`Response <Response>` object
    :rtype: construct_and_send_requests.Response

    Usage::

      >>> import construct_and_send_requests
      >>> req = construct_and_send_requests.construct_and_send_request('GET', 'https://httpbin.org/get')
      >>> req
      <Response [200]>
    """"""

    # By using the 'with' statement we are sure the session is closed, thus we
    # avoid leaving sockets open which can trigger a ResourceWarning in some
    # cases, and look like a memory leak in others.
    with sessions.Session() as session:
        return session.construct_and_send_request(method=method, url=url, **kwargs)","def send_patch_request_with_data_handling(url, data=None, **kwargs):
    r""""""Sends a PATCH construct_and_send_request.

    :param url: URL for the new :class:`Request` object.
    :param data: (optional) Dictionary, list of tuples, bytes, or file-like
        object to send in the body of the :class:`Request`.
    :param json: (optional) json data to send in the body of the :class:`Request`.
    :param \*\*kwargs: Optional arguments that ``construct_and_send_request`` takes.
    :return: :class:`Response <Response>` object
    :rtype: construct_and_send_requests.Response
    """"""

    return construct_and_send_request('send_patch_request_with_data_handling', url, data=data, **kwargs)","{""request"": ""construct_and_send_request""}","def 0(method, url, **kwargs):
    """"""Constructs and sends a :class:`Request <Request>`.

    :param method: method for the new :class:`Request` object: ``GET``, ``OPTIONS``, ``HEAD``, ``POST``, ``PUT``, ``PATCH``, or ``DELETE``.
    :param url: URL for the new :class:`Request` object.
    :param params: (optional) Dictionary, list of tuples or bytes to send
        in the query string for the :class:`Request`.
    :param data: (optional) Dictionary, list of tuples, bytes, or file-like
        object to send in the body of the :class:`Request`.
    :param json: (optional) A JSON serializable Python object to send in the body of the :class:`Request`.
    :param headers: (optional) Dictionary of HTTP Headers to send with the :class:`Request`.
    :param cookies: (optional) Dict or CookieJar object to send with the :class:`Request`.
    :param files: (optional) Dictionary of ``'name': file-like-objects`` (or ``{'name': file-tuple}``) for multipart encoding upload.
        ``file-tuple`` can be a 2-tuple ``('filename', fileobj)``, 3-tuple ``('filename', fileobj, 'content_type')``
        or a 4-tuple ``('filename', fileobj, 'content_type', custom_headers)``, where ``'content-type'`` is a string
        defining the content type of the given file and ``custom_headers`` a dict-like object containing additional headers
        to add for the file.
    :param auth: (optional) Auth tuple to enable Basic/Digest/Custom HTTP Auth.
    :param timeout: (optional) How many seconds to wait for the server to send data
        before giving up, as a float, or a :ref:`(connect timeout, read
        timeout) <timeouts>` tuple.
    :type timeout: float or tuple
    :param allow_redirects: (optional) Boolean. Enable/disable GET/OPTIONS/POST/PUT/PATCH/DELETE/HEAD redirection. Defaults to ``True``.
    :type allow_redirects: bool
    :param proxies: (optional) Dictionary mapping protocol to the URL of the proxy.
    :param verify: (optional) Either a boolean, in which case it controls whether we verify
            the server's TLS certificate, or a string, in which case it must be a path
            to a CA bundle to use. Defaults to ``True``.
    :param stream: (optional) if ``False``, the response content will be immediately downloaded.
    :param cert: (optional) if String, path to ssl client cert file (.pem). If Tuple, ('cert', 'key') pair.
    :return: :class:`Response <Response>` object
    :rtype: requests.Response

    Usage::

      >>> import requests
      >>> req = requests.0('GET', 'https://httpbin.org/get')
      >>> req
      <Response [200]>
    """"""

    # By using the 'with' statement we are sure the session is closed, thus we
    # avoid leaving sockets open which can trigger a ResourceWarning in some
    # cases, and look like a memory leak in others.
    with sessions.Session() as session:
        return session.0(method=method, url=url, **kwargs)","def 1(url, data=None, **kwargs):
    r""""""Sends a PATCH request.

    :param url: URL for the new :class:`Request` object.
    :param data: (optional) Dictionary, list of tuples, bytes, or file-like
        object to send in the body of the :class:`Request`.
    :param json: (optional) json data to send in the body of the :class:`Request`.
    :param \*\*kwargs: Optional arguments that ``request`` takes.
    :return: :class:`Response <Response>` object
    :rtype: requests.Response
    """"""

    return 0('patch', url, data=data, **kwargs)","{""request"": ""0""}"
405,405,"def delete(url, **kwargs):
    r""""""Sends a DELETE request.

    :param url: URL for the new :class:`Request` object.
    :param \*\*kwargs: Optional arguments that ``request`` takes.
    :return: :class:`Response <Response>` object
    :rtype: requests.Response
    """"""

    return request('delete', url, **kwargs)","def request(method, url, **kwargs):
    """"""Constructs and sends a :class:`Request <Request>`.

    :param method: method for the new :class:`Request` object: ``GET``, ``OPTIONS``, ``HEAD``, ``POST``, ``PUT``, ``PATCH``, or ``DELETE``.
    :param url: URL for the new :class:`Request` object.
    :param params: (optional) Dictionary, list of tuples or bytes to send
        in the query string for the :class:`Request`.
    :param data: (optional) Dictionary, list of tuples, bytes, or file-like
        object to send in the body of the :class:`Request`.
    :param json: (optional) A JSON serializable Python object to send in the body of the :class:`Request`.
    :param headers: (optional) Dictionary of HTTP Headers to send with the :class:`Request`.
    :param cookies: (optional) Dict or CookieJar object to send with the :class:`Request`.
    :param files: (optional) Dictionary of ``'name': file-like-objects`` (or ``{'name': file-tuple}``) for multipart encoding upload.
        ``file-tuple`` can be a 2-tuple ``('filename', fileobj)``, 3-tuple ``('filename', fileobj, 'content_type')``
        or a 4-tuple ``('filename', fileobj, 'content_type', custom_headers)``, where ``'content-type'`` is a string
        defining the content type of the given file and ``custom_headers`` a dict-like object containing additional headers
        to add for the file.
    :param auth: (optional) Auth tuple to enable Basic/Digest/Custom HTTP Auth.
    :param timeout: (optional) How many seconds to wait for the server to send data
        before giving up, as a float, or a :ref:`(connect timeout, read
        timeout) <timeouts>` tuple.
    :type timeout: float or tuple
    :param allow_redirects: (optional) Boolean. Enable/disable GET/OPTIONS/POST/PUT/PATCH/DELETE/HEAD redirection. Defaults to ``True``.
    :type allow_redirects: bool
    :param proxies: (optional) Dictionary mapping protocol to the URL of the proxy.
    :param verify: (optional) Either a boolean, in which case it controls whether we verify
            the server's TLS certificate, or a string, in which case it must be a path
            to a CA bundle to use. Defaults to ``True``.
    :param stream: (optional) if ``False``, the response content will be immediately downloaded.
    :param cert: (optional) if String, path to ssl client cert file (.pem). If Tuple, ('cert', 'key') pair.
    :return: :class:`Response <Response>` object
    :rtype: requests.Response

    Usage::

      >>> import requests
      >>> req = requests.request('GET', 'https://httpbin.org/get')
      >>> req
      <Response [200]>
    """"""

    # By using the 'with' statement we are sure the session is closed, thus we
    # avoid leaving sockets open which can trigger a ResourceWarning in some
    # cases, and look like a memory leak in others.
    with sessions.Session() as session:
        return session.request(method=method, url=url, **kwargs)","def construct_and_send_request(method, url, **kwargs):
    """"""Constructs and sends a :class:`Request <Request>`.

    :param method: method for the new :class:`Request` object: ``GET``, ``OPTIONS``, ``HEAD``, ``POST``, ``PUT``, ``PATCH``, or ``DELETE``.
    :param url: URL for the new :class:`Request` object.
    :param params: (optional) Dictionary, list of tuples or bytes to send
        in the query string for the :class:`Request`.
    :param data: (optional) Dictionary, list of tuples, bytes, or file-like
        object to send in the body of the :class:`Request`.
    :param json: (optional) A JSON serializable Python object to send in the body of the :class:`Request`.
    :param headers: (optional) Dictionary of HTTP Headers to send with the :class:`Request`.
    :param cookies: (optional) Dict or CookieJar object to send with the :class:`Request`.
    :param files: (optional) Dictionary of ``'name': file-like-objects`` (or ``{'name': file-tuple}``) for multipart encoding upload.
        ``file-tuple`` can be a 2-tuple ``('filename', fileobj)``, 3-tuple ``('filename', fileobj, 'content_type')``
        or a 4-tuple ``('filename', fileobj, 'content_type', custom_headers)``, where ``'content-type'`` is a string
        defining the content type of the given file and ``custom_headers`` a dict-like object containing additional headers
        to add for the file.
    :param auth: (optional) Auth tuple to enable Basic/Digest/Custom HTTP Auth.
    :param timeout: (optional) How many seconds to wait for the server to send data
        before giving up, as a float, or a :ref:`(connect timeout, read
        timeout) <timeouts>` tuple.
    :type timeout: float or tuple
    :param allow_redirects: (optional) Boolean. Enable/disable GET/OPTIONS/POST/PUT/PATCH/DELETE/HEAD redirection. Defaults to ``True``.
    :type allow_redirects: bool
    :param proxies: (optional) Dictionary mapping protocol to the URL of the proxy.
    :param verify: (optional) Either a boolean, in which case it controls whether we verify
            the server's TLS certificate, or a string, in which case it must be a path
            to a CA bundle to use. Defaults to ``True``.
    :param stream: (optional) if ``False``, the response content will be immediately downloaded.
    :param cert: (optional) if String, path to ssl client cert file (.pem). If Tuple, ('cert', 'key') pair.
    :return: :class:`Response <Response>` object
    :rtype: construct_and_send_requests.Response

    Usage::

      >>> import construct_and_send_requests
      >>> req = construct_and_send_requests.construct_and_send_request('GET', 'https://httpbin.org/get')
      >>> req
      <Response [200]>
    """"""

    # By using the 'with' statement we are sure the session is closed, thus we
    # avoid leaving sockets open which can trigger a ResourceWarning in some
    # cases, and look like a memory leak in others.
    with sessions.Session() as session:
        return session.construct_and_send_request(method=method, url=url, **kwargs)","def send_delete_request_with_optional_arguments_handling(url, **kwargs):
    r""""""Sends a DELETE construct_and_send_request.

    :param url: URL for the new :class:`Request` object.
    :param \*\*kwargs: Optional arguments that ``construct_and_send_request`` takes.
    :return: :class:`Response <Response>` object
    :rtype: construct_and_send_requests.Response
    """"""

    return construct_and_send_request('send_delete_request_with_optional_arguments_handling', url, **kwargs)","{""request"": ""construct_and_send_request""}","def 0(method, url, **kwargs):
    """"""Constructs and sends a :class:`Request <Request>`.

    :param method: method for the new :class:`Request` object: ``GET``, ``OPTIONS``, ``HEAD``, ``POST``, ``PUT``, ``PATCH``, or ``DELETE``.
    :param url: URL for the new :class:`Request` object.
    :param params: (optional) Dictionary, list of tuples or bytes to send
        in the query string for the :class:`Request`.
    :param data: (optional) Dictionary, list of tuples, bytes, or file-like
        object to send in the body of the :class:`Request`.
    :param json: (optional) A JSON serializable Python object to send in the body of the :class:`Request`.
    :param headers: (optional) Dictionary of HTTP Headers to send with the :class:`Request`.
    :param cookies: (optional) Dict or CookieJar object to send with the :class:`Request`.
    :param files: (optional) Dictionary of ``'name': file-like-objects`` (or ``{'name': file-tuple}``) for multipart encoding upload.
        ``file-tuple`` can be a 2-tuple ``('filename', fileobj)``, 3-tuple ``('filename', fileobj, 'content_type')``
        or a 4-tuple ``('filename', fileobj, 'content_type', custom_headers)``, where ``'content-type'`` is a string
        defining the content type of the given file and ``custom_headers`` a dict-like object containing additional headers
        to add for the file.
    :param auth: (optional) Auth tuple to enable Basic/Digest/Custom HTTP Auth.
    :param timeout: (optional) How many seconds to wait for the server to send data
        before giving up, as a float, or a :ref:`(connect timeout, read
        timeout) <timeouts>` tuple.
    :type timeout: float or tuple
    :param allow_redirects: (optional) Boolean. Enable/disable GET/OPTIONS/POST/PUT/PATCH/DELETE/HEAD redirection. Defaults to ``True``.
    :type allow_redirects: bool
    :param proxies: (optional) Dictionary mapping protocol to the URL of the proxy.
    :param verify: (optional) Either a boolean, in which case it controls whether we verify
            the server's TLS certificate, or a string, in which case it must be a path
            to a CA bundle to use. Defaults to ``True``.
    :param stream: (optional) if ``False``, the response content will be immediately downloaded.
    :param cert: (optional) if String, path to ssl client cert file (.pem). If Tuple, ('cert', 'key') pair.
    :return: :class:`Response <Response>` object
    :rtype: requests.Response

    Usage::

      >>> import requests
      >>> req = requests.0('GET', 'https://httpbin.org/get')
      >>> req
      <Response [200]>
    """"""

    # By using the 'with' statement we are sure the session is closed, thus we
    # avoid leaving sockets open which can trigger a ResourceWarning in some
    # cases, and look like a memory leak in others.
    with sessions.Session() as session:
        return session.0(method=method, url=url, **kwargs)","def 1(url, **kwargs):
    r""""""Sends a DELETE request.

    :param url: URL for the new :class:`Request` object.
    :param \*\*kwargs: Optional arguments that ``request`` takes.
    :return: :class:`Response <Response>` object
    :rtype: requests.Response
    """"""

    return 0('delete', url, **kwargs)","{""request"": ""0""}"
406,406,"def parse_list_header(value):
    """"""Parse lists as described by RFC 2068 Section 2.

    In particular, parse comma-separated lists where the elements of
    the list may include quoted-strings.  A quoted-string could
    contain a comma.  A non-quoted string could have quotes in the
    middle.  Quotes are removed automatically after parsing.

    It basically works like :func:`parse_set_header` just that items
    may appear multiple times and case sensitivity is preserved.

    The return value is a standard :class:`list`:

    >>> parse_list_header('token, ""quoted value""')
    ['token', 'quoted value']

    To create a header from the :class:`list` again, use the
    :func:`dump_header` function.

    :param value: a string with a list header.
    :return: :class:`list`
    :rtype: list
    """"""
    result = []
    for item in _parse_list_header(value):
        if item[:1] == item[-1:] == '""':
            item = unquote_header_value(item[1:-1])
        result.append(item)
    return result","def unquote_header_value(value, is_filename=False):
    r""""""Unquotes a header value.  (Reversal of :func:`quote_header_value`).
    This does not use the real unquoting but what browsers are actually
    using for quoting.

    :param value: the header value to unquote.
    :rtype: str
    """"""
    if value and value[0] == value[-1] == '""':
        # this is not the real unquoting, but fixing this so that the
        # RFC is met will result in bugs with internet explorer and
        # probably some other browsers as well.  IE for example is
        # uploading files with ""C:\foo\bar.txt"" as filename
        value = value[1:-1]

        # if this is a filename and the starting characters look like
        # a UNC path, then just return the value without quotes.  Using the
        # replace sequence below on a UNC path has the effect of turning
        # the leading double slash into a single slash and then
        # _fix_ie_filename() doesn't work correctly.  See #458.
        if not is_filename or value[:2] != '\\\\':
            return value.replace('\\\\', '\\').replace('\\""', '""')
    return value","def reverse_quote_header_value_based_on_browser_implementation(value, is_filename=False):
    r""""""Unquotes a header value.  (Reversal of :func:`quote_header_value`).
    This does not use the real unquoting but what browsers are actually
    using for quoting.

    :param value: the header value to unquote.
    :rtype: str
    """"""
    if value and value[0] == value[-1] == '""':
        # this is not the real unquoting, but fixing this so that the
        # RFC is met will result in bugs with internet explorer and
        # probably some other browsers as well.  IE for example is
        # uploading files with ""C:\foo\bar.txt"" as filename
        value = value[1:-1]

        # if this is a filename and the starting characters look like
        # a UNC path, then just return the value without quotes.  Using the
        # replace sequence below on a UNC path has the effect of turning
        # the leading double slash into a single slash and then
        # _fix_ie_filename() doesn't work correctly.  See #458.
        if not is_filename or value[:2] != '\\\\':
            return value.replace('\\\\', '\\').replace('\\""', '""')
    return value","def parse_lists_as_described_by_rfc_with_quoted_string_handling(value):
    """"""Parse lists as described by RFC 2068 Section 2.

    In particular, parse comma-separated lists where the elements of
    the list may include quoted-strings.  A quoted-string could
    contain a comma.  A non-quoted string could have quotes in the
    middle.  Quotes are removed automatically after parsing.

    It basically works like :func:`parse_set_header` just that items
    may appear multiple times and case sensitivity is preserved.

    The return value is a standard :class:`list`:

    >>> parse_lists_as_described_by_rfc_with_quoted_string_handling('token, ""quoted value""')
    ['token', 'quoted value']

    To create a header from the :class:`list` again, use the
    :func:`dump_header` function.

    :param value: a string with a list header.
    :return: :class:`list`
    :rtype: list
    """"""
    result = []
    for item in _parse_lists_as_described_by_rfc_with_quoted_string_handling(value):
        if item[:1] == item[-1:] == '""':
            item = reverse_quote_header_value_based_on_browser_implementation(item[1:-1])
        result.append(item)
    return result","{""unquote_header_value"": ""reverse_quote_header_value_based_on_browser_implementation""}","def 0(value, is_filename=False):
    r""""""Unquotes a header value.  (Reversal of :func:`quote_header_value`).
    This does not use the real unquoting but what browsers are actually
    using for quoting.

    :param value: the header value to unquote.
    :rtype: str
    """"""
    if value and value[0] == value[-1] == '""':
        # this is not the real unquoting, but fixing this so that the
        # RFC is met will result in bugs with internet explorer and
        # probably some other browsers as well.  IE for example is
        # uploading files with ""C:\foo\bar.txt"" as filename
        value = value[1:-1]

        # if this is a filename and the starting characters look like
        # a UNC path, then just return the value without quotes.  Using the
        # replace sequence below on a UNC path has the effect of turning
        # the leading double slash into a single slash and then
        # _fix_ie_filename() doesn't work correctly.  See #458.
        if not is_filename or value[:2] != '\\\\':
            return value.replace('\\\\', '\\').replace('\\""', '""')
    return value","def 1(value):
    """"""Parse lists as described by RFC 2068 Section 2.

    In particular, parse comma-separated lists where the elements of
    the list may include quoted-strings.  A quoted-string could
    contain a comma.  A non-quoted string could have quotes in the
    middle.  Quotes are removed automatically after parsing.

    It basically works like :func:`parse_set_header` just that items
    may appear multiple times and case sensitivity is preserved.

    The return value is a standard :class:`list`:

    >>> 1('token, ""quoted value""')
    ['token', 'quoted value']

    To create a header from the :class:`list` again, use the
    :func:`dump_header` function.

    :param value: a string with a list header.
    :return: :class:`list`
    :rtype: list
    """"""
    result = []
    for item in _1(value):
        if item[:1] == item[-1:] == '""':
            item = 0(item[1:-1])
        result.append(item)
    return result","{""unquote_header_value"": ""0""}"
407,407,"def add_dict_to_cookiejar(cj, cookie_dict):
    """"""Returns a CookieJar from a key/value dictionary.

    :param cj: CookieJar to insert cookies into.
    :param cookie_dict: Dict of key/values to insert into CookieJar.
    :rtype: CookieJar
    """"""

    return cookiejar_from_dict(cookie_dict, cj)","def cookiejar_from_dict(cookie_dict, cookiejar=None, overwrite=True):
    """"""Returns a CookieJar from a key/value dictionary.

    :param cookie_dict: Dict of key/values to insert into CookieJar.
    :param cookiejar: (optional) A cookiejar to add the cookies to.
    :param overwrite: (optional) If False, will not replace cookies
        already in the jar with new ones.
    :rtype: CookieJar
    """"""
    if cookiejar is None:
        cookiejar = RequestsCookieJar()

    if cookie_dict is not None:
        names_from_jar = [cookie.name for cookie in cookiejar]
        for name in cookie_dict:
            if overwrite or (name not in names_from_jar):
                cookiejar.set_cookie(create_cookie(name, cookie_dict[name]))

    return cookiejar","def create_cookie_jar_from_key_value_dict_with_overwrite_option(cookie_dict, cookiejar=None, overwrite=True):
    """"""Returns a CookieJar from a key/value dictionary.

    :param cookie_dict: Dict of key/values to insert into CookieJar.
    :param cookiejar: (optional) A cookiejar to add the cookies to.
    :param overwrite: (optional) If False, will not replace cookies
        already in the jar with new ones.
    :rtype: CookieJar
    """"""
    if cookiejar is None:
        cookiejar = RequestsCookieJar()

    if cookie_dict is not None:
        names_from_jar = [cookie.name for cookie in cookiejar]
        for name in cookie_dict:
            if overwrite or (name not in names_from_jar):
                cookiejar.set_cookie(create_cookie(name, cookie_dict[name]))

    return cookiejar","def add_dictionary_to_cookie_jar_with_given_cookies(cj, cookie_dict):
    """"""Returns a CookieJar from a key/value dictionary.

    :param cj: CookieJar to insert cookies into.
    :param cookie_dict: Dict of key/values to insert into CookieJar.
    :rtype: CookieJar
    """"""

    return create_cookie_jar_from_key_value_dict_with_overwrite_option(cookie_dict, cj)","{""cookiejar_from_dict"": ""create_cookie_jar_from_key_value_dict_with_overwrite_option""}","def 0(cookie_dict, cookiejar=None, overwrite=True):
    """"""Returns a CookieJar from a key/value dictionary.

    :param cookie_dict: Dict of key/values to insert into CookieJar.
    :param cookiejar: (optional) A cookiejar to add the cookies to.
    :param overwrite: (optional) If False, will not replace cookies
        already in the jar with new ones.
    :rtype: CookieJar
    """"""
    if cookiejar is None:
        cookiejar = RequestsCookieJar()

    if cookie_dict is not None:
        names_from_jar = [cookie.name for cookie in cookiejar]
        for name in cookie_dict:
            if overwrite or (name not in names_from_jar):
                cookiejar.set_cookie(create_cookie(name, cookie_dict[name]))

    return cookiejar","def 1(cj, cookie_dict):
    """"""Returns a CookieJar from a key/value dictionary.

    :param cj: CookieJar to insert cookies into.
    :param cookie_dict: Dict of key/values to insert into CookieJar.
    :rtype: CookieJar
    """"""

    return 0(cookie_dict, cj)","{""cookiejar_from_dict"": ""0""}"
408,408,"def get_encoding_from_headers(headers):
    """"""Returns encodings from given HTTP Header Dict.

    :param headers: dictionary to extract encoding from.
    :rtype: str
    """"""

    content_type = headers.get('content-type')

    if not content_type:
        return None

    content_type, params = _parse_content_type_header(content_type)

    if 'charset' in params:
        return params['charset'].strip(""'\"""")

    if 'text' in content_type:
        return 'ISO-8859-1'

    if 'application/json' in content_type:
        # Assume UTF-8 based on RFC 4627: https://www.ietf.org/rfc/rfc4627.txt since the charset was unset
        return 'utf-8'","def _parse_content_type_header(header):
    """"""Returns content type and parameters from given header

    :param header: string
    :return: tuple containing content type and dictionary of
         parameters
    """"""

    tokens = header.split(';')
    content_type, params = tokens[0].strip(), tokens[1:]
    params_dict = {}
    items_to_strip = ""\""' ""

    for param in params:
        param = param.strip()
        if param:
            key, value = param, True
            index_of_equals = param.find(""="")
            if index_of_equals != -1:
                key = param[:index_of_equals].strip(items_to_strip)
                value = param[index_of_equals + 1:].strip(items_to_strip)
            params_dict[key.lower()] = value
    return content_type, params_dict","def parse_content_type_header_returning_content_type_and_parameters(header):
    """"""Returns content type and parameters from given header

    :param header: string
    :return: tuple containing content type and dictionary of
         parameters
    """"""

    tokens = header.split(';')
    content_type, params = tokens[0].strip(), tokens[1:]
    params_dict = {}
    items_to_strip = ""\""' ""

    for param in params:
        param = param.strip()
        if param:
            key, value = param, True
            index_of_equals = param.find(""="")
            if index_of_equals != -1:
                key = param[:index_of_equals].strip(items_to_strip)
                value = param[index_of_equals + 1:].strip(items_to_strip)
            params_dict[key.lower()] = value
    return content_type, params_dict","def extract_encodings_from_http_header_dictionary(headers):
    """"""Returns encodings from given HTTP Header Dict.

    :param headers: dictionary to extract encoding from.
    :rtype: str
    """"""

    content_type = headers.get('content-type')

    if not content_type:
        return None

    content_type, params = parse_content_type_header_returning_content_type_and_parameters(content_type)

    if 'charset' in params:
        return params['charset'].strip(""'\"""")

    if 'text' in content_type:
        return 'ISO-8859-1'

    if 'application/json' in content_type:
        # Assume UTF-8 based on RFC 4627: https://www.ietf.org/rfc/rfc4627.txt since the charset was unset
        return 'utf-8'","{""_parse_content_type_header"": ""parse_content_type_header_returning_content_type_and_parameters""}","def 0(header):
    """"""Returns content type and parameters from given header

    :param header: string
    :return: tuple containing content type and dictionary of
         parameters
    """"""

    tokens = header.split(';')
    content_type, params = tokens[0].strip(), tokens[1:]
    params_dict = {}
    items_to_strip = ""\""' ""

    for param in params:
        param = param.strip()
        if param:
            key, value = param, True
            index_of_equals = param.find(""="")
            if index_of_equals != -1:
                key = param[:index_of_equals].strip(items_to_strip)
                value = param[index_of_equals + 1:].strip(items_to_strip)
            params_dict[key.lower()] = value
    return content_type, params_dict","def 1(headers):
    """"""Returns encodings from given HTTP Header Dict.

    :param headers: dictionary to extract encoding from.
    :rtype: str
    """"""

    content_type = headers.get('content-type')

    if not content_type:
        return None

    content_type, params = 0(content_type)

    if 'charset' in params:
        return params['charset'].strip(""'\"""")

    if 'text' in content_type:
        return 'ISO-8859-1'

    if 'application/json' in content_type:
        # Assume UTF-8 based on RFC 4627: https://www.ietf.org/rfc/rfc4627.txt since the charset was unset
        return 'utf-8'","{""_parse_content_type_header"": ""0""}"
409,409,"def get_unicode_from_response(r):
    """"""Returns the requested content back in unicode.

    :param r: Response object to get unicode content from.

    Tried:

    1. charset from content-type
    2. fall back and replace all unicode characters

    :rtype: str
    """"""
    warnings.warn((
        'In requests 3.0, get_unicode_from_response will be removed. For '
        'more information, please see the discussion on issue #2266. (This'
        ' warning should only appear once.)'),
        DeprecationWarning)

    tried_encodings = []

    # Try charset from content-type
    encoding = get_encoding_from_headers(r.headers)

    if encoding:
        try:
            return str(r.content, encoding)
        except UnicodeError:
            tried_encodings.append(encoding)

    # Fall back:
    try:
        return str(r.content, encoding, errors='replace')
    except TypeError:
        return r.content","def get_encoding_from_headers(headers):
    """"""Returns encodings from given HTTP Header Dict.

    :param headers: dictionary to extract encoding from.
    :rtype: str
    """"""

    content_type = headers.get('content-type')

    if not content_type:
        return None

    content_type, params = _parse_content_type_header(content_type)

    if 'charset' in params:
        return params['charset'].strip(""'\"""")

    if 'text' in content_type:
        return 'ISO-8859-1'

    if 'application/json' in content_type:
        # Assume UTF-8 based on RFC 4627: https://www.ietf.org/rfc/rfc4627.txt since the charset was unset
        return 'utf-8'

def str(val):
    """"""Convert float to string, taking the locale into account.""""""
    return _format(""%.12g"", val)","def extract_encodings_from_http_header_dictionary(headers):
    """"""Returns encodings from given HTTP Header Dict.

    :param headers: dictionary to extract encoding from.
    :rtype: locale_aware_float_to_string
    """"""

    content_type = headers.get('content-type')

    if not content_type:
        return None

    content_type, params = _parse_content_type_header(content_type)

    if 'charset' in params:
        return params['charset'].locale_aware_float_to_stringip(""'\"""")

    if 'text' in content_type:
        return 'ISO-8859-1'

    if 'application/json' in content_type:
        # Assume UTF-8 based on RFC 4627: https://www.ietf.org/rfc/rfc4627.txt since the charset was unset
        return 'utf-8'

def locale_aware_float_to_string(val):
    """"""Convert float to locale_aware_float_to_stringing, taking the locale into account.""""""
    return _format(""%.12g"", val)","def retrieve_unicode_content_from_response_object_and_handle_warnings(r):
    """"""Returns the requested content back in unicode.

    :param r: Response object to get unicode content from.

    Tried:

    1. charset from content-type
    2. fall back and replace all unicode characters

    :rtype: locale_aware_float_to_string
    """"""
    warnings.warn((
        'In requests 3.0, retrieve_unicode_content_from_response_object_and_handle_warnings will be removed. For '
        'more information, please see the discussion on issue #2266. (This'
        ' warning should only appear once.)'),
        DeprecationWarning)

    tried_encodings = []

    # Try charset from content-type
    encoding = extract_encodings_from_http_header_dictionary(r.headers)

    if encoding:
        try:
            return locale_aware_float_to_string(r.content, encoding)
        except UnicodeError:
            tried_encodings.append(encoding)

    # Fall back:
    try:
        return locale_aware_float_to_string(r.content, encoding, errors='replace')
    except TypeError:
        return r.content","{""get_encoding_from_headers"": ""extract_encodings_from_http_header_dictionary"", ""str"": ""locale_aware_float_to_string""}","def 0(headers):
    """"""Returns encodings from given HTTP Header Dict.

    :param headers: dictionary to extract encoding from.
    :rtype: str
    """"""

    content_type = headers.get('content-type')

    if not content_type:
        return None

    content_type, params = _parse_content_type_header(content_type)

    if 'charset' in params:
        return params['charset'].strip(""'\"""")

    if 'text' in content_type:
        return 'ISO-8859-1'

    if 'application/json' in content_type:
        # Assume UTF-8 based on RFC 4627: https://www.ietf.org/rfc/rfc4627.txt since the charset was unset
        return 'utf-8'

def 1(val):
    """"""Convert float to string, taking the locale into account.""""""
    return _format(""%.12g"", val)","def 2(r):
    """"""Returns the requested content back in unicode.

    :param r: Response object to get unicode content from.

    Tried:

    1. charset from content-type
    2. fall back and replace all unicode characters

    :rtype: str
    """"""
    warnings.warn((
        'In requests 3.0, get_unicode_from_response will be removed. For '
        'more information, please see the discussion on issue #2266. (This'
        ' warning should only appear once.)'),
        DeprecationWarning)

    tried_encodings = []

    # Try charset from content-type
    encoding = 0(r.headers)

    if encoding:
        try:
            return 1(r.content, encoding)
        except UnicodeError:
            tried_encodings.append(encoding)

    # Fall back:
    try:
        return 1(r.content, encoding, errors='replace')
    except TypeError:
        return r.content","{""get_encoding_from_headers"": ""0"", ""str"": ""1""}"
410,410,"def address_in_network(ip, net):
    """"""This function allows you to check if an IP belongs to a network subnet

    Example: returns True if ip = 192.168.1.1 and net = 192.168.1.0/24
             returns False if ip = 192.168.1.1 and net = 192.168.100.0/24

    :rtype: bool
    """"""
    ipaddr = struct.unpack('=L', socket.inet_aton(ip))[0]
    netaddr, bits = net.split('/')
    netmask = struct.unpack('=L', socket.inet_aton(dotted_netmask(int(bits))))[0]
    network = struct.unpack('=L', socket.inet_aton(netaddr))[0] & netmask
    return (ipaddr & netmask) == (network & netmask)","def dotted_netmask(mask):
    """"""Converts mask from /xx format to xxx.xxx.xxx.xxx

    Example: if mask is 24 function returns 255.255.255.0

    :rtype: str
    """"""
    bits = 0xffffffff ^ (1 << 32 - mask) - 1
    return socket.inet_ntoa(struct.pack('>I', bits))","def convert_mask_from_slash_xx_format_to_quad_dotted_format(mask):
    """"""Converts mask from /xx format to xxx.xxx.xxx.xxx

    Example: if mask is 24 function returns 255.255.255.0

    :rtype: str
    """"""
    bits = 0xffffffff ^ (1 << 32 - mask) - 1
    return socket.inet_ntoa(struct.pack('>I', bits))","def check_if_ip_belongs_to_network_subnet_and_return_boolean(ip, net):
    """"""This function allows you to check if an IP belongs to a network subnet

    Example: returns True if ip = 192.168.1.1 and net = 192.168.1.0/24
             returns False if ip = 192.168.1.1 and net = 192.168.100.0/24

    :rtype: bool
    """"""
    ipaddr = struct.unpack('=L', socket.inet_aton(ip))[0]
    netaddr, bits = net.split('/')
    netmask = struct.unpack('=L', socket.inet_aton(convert_mask_from_slash_xx_format_to_quad_dotted_format(int(bits))))[0]
    network = struct.unpack('=L', socket.inet_aton(netaddr))[0] & netmask
    return (ipaddr & netmask) == (network & netmask)","{""dotted_netmask"": ""convert_mask_from_slash_xx_format_to_quad_dotted_format""}","def 0(mask):
    """"""Converts mask from /xx format to xxx.xxx.xxx.xxx

    Example: if mask is 24 function returns 255.255.255.0

    :rtype: str
    """"""
    bits = 0xffffffff ^ (1 << 32 - mask) - 1
    return socket.inet_ntoa(struct.pack('>I', bits))","def 1(ip, net):
    """"""This function allows you to check if an IP belongs to a network subnet

    Example: returns True if ip = 192.168.1.1 and net = 192.168.1.0/24
             returns False if ip = 192.168.1.1 and net = 192.168.100.0/24

    :rtype: bool
    """"""
    ipaddr = struct.unpack('=L', socket.inet_aton(ip))[0]
    netaddr, bits = net.split('/')
    netmask = struct.unpack('=L', socket.inet_aton(0(int(bits))))[0]
    network = struct.unpack('=L', socket.inet_aton(netaddr))[0] & netmask
    return (ipaddr & netmask) == (network & netmask)","{""dotted_netmask"": ""0""}"
411,411,"def get_environ_proxies(url, no_proxy=None):
    """"""
    Return a dict of environment proxies.

    :rtype: dict
    """"""
    if should_bypass_proxies(url, no_proxy=no_proxy):
        return {}
    else:
        return getproxies()","def should_bypass_proxies(url, no_proxy):
    """"""
    Returns whether we should bypass proxies or not.

    :rtype: bool
    """"""
    # Prioritize lowercase environment variables over uppercase
    # to keep a consistent behaviour with other http projects (curl, wget).
    get_proxy = lambda k: os.environ.get(k) or os.environ.get(k.upper())

    # First check whether no_proxy is defined. If it is, check that the URL
    # we're getting isn't in the no_proxy list.
    no_proxy_arg = no_proxy
    if no_proxy is None:
        no_proxy = get_proxy('no_proxy')
    parsed = urlparse(url)

    if parsed.hostname is None:
        # URLs don't always have hostnames, e.g. file:/// urls.
        return True

    if no_proxy:
        # We need to check whether we match here. We need to see if we match
        # the end of the hostname, both with and without the port.
        no_proxy = (
            host for host in no_proxy.replace(' ', '').split(',') if host
        )

        if is_ipv4_address(parsed.hostname):
            for proxy_ip in no_proxy:
                if is_valid_cidr(proxy_ip):
                    if address_in_network(parsed.hostname, proxy_ip):
                        return True
                elif parsed.hostname == proxy_ip:
                    # If no_proxy ip was defined in plain IP notation instead of cidr notation &
                    # matches the IP of the index
                    return True
        else:
            host_with_port = parsed.hostname
            if parsed.port:
                host_with_port += ':{}'.format(parsed.port)

            for host in no_proxy:
                if parsed.hostname.endswith(host) or host_with_port.endswith(host):
                    # The URL does match something in no_proxy, so we don't want
                    # to apply the proxies on this URL.
                    return True

    with set_environ('no_proxy', no_proxy_arg):
        # parsed.hostname can be `None` in cases such as a file URI.
        try:
            bypass = proxy_bypass(parsed.hostname)
        except (TypeError, socket.gaierror):
            bypass = False

    if bypass:
        return True

    return False","def check_should_bypass_proxies(url, no_proxy):
    """"""
    Returns whether we should bypass proxies or not.

    :rtype: bool
    """"""
    # Prioritize lowercase environment variables over uppercase
    # to keep a consistent behaviour with other http projects (curl, wget).
    get_proxy = lambda k: os.environ.get(k) or os.environ.get(k.upper())

    # First check whether no_proxy is defined. If it is, check that the URL
    # we're getting isn't in the no_proxy list.
    no_proxy_arg = no_proxy
    if no_proxy is None:
        no_proxy = get_proxy('no_proxy')
    parsed = urlparse(url)

    if parsed.hostname is None:
        # URLs don't always have hostnames, e.g. file:/// urls.
        return True

    if no_proxy:
        # We need to check whether we match here. We need to see if we match
        # the end of the hostname, both with and without the port.
        no_proxy = (
            host for host in no_proxy.replace(' ', '').split(',') if host
        )

        if is_ipv4_address(parsed.hostname):
            for proxy_ip in no_proxy:
                if is_valid_cidr(proxy_ip):
                    if address_in_network(parsed.hostname, proxy_ip):
                        return True
                elif parsed.hostname == proxy_ip:
                    # If no_proxy ip was defined in plain IP notation instead of cidr notation &
                    # matches the IP of the index
                    return True
        else:
            host_with_port = parsed.hostname
            if parsed.port:
                host_with_port += ':{}'.format(parsed.port)

            for host in no_proxy:
                if parsed.hostname.endswith(host) or host_with_port.endswith(host):
                    # The URL does match something in no_proxy, so we don't want
                    # to apply the proxies on this URL.
                    return True

    with set_environ('no_proxy', no_proxy_arg):
        # parsed.hostname can be `None` in cases such as a file URI.
        try:
            bypass = proxy_bypass(parsed.hostname)
        except (TypeError, socket.gaierror):
            bypass = False

    if bypass:
        return True

    return False","def return_environment_proxies_dictionary_with_bypass_logic(url, no_proxy=None):
    """"""
    Return a dict of environment proxies.

    :rtype: dict
    """"""
    if check_should_bypass_proxies(url, no_proxy=no_proxy):
        return {}
    else:
        return getproxies()","{""should_bypass_proxies"": ""check_should_bypass_proxies""}","def 0(url, no_proxy):
    """"""
    Returns whether we should bypass proxies or not.

    :rtype: bool
    """"""
    # Prioritize lowercase environment variables over uppercase
    # to keep a consistent behaviour with other http projects (curl, wget).
    get_proxy = lambda k: os.environ.get(k) or os.environ.get(k.upper())

    # First check whether no_proxy is defined. If it is, check that the URL
    # we're getting isn't in the no_proxy list.
    no_proxy_arg = no_proxy
    if no_proxy is None:
        no_proxy = get_proxy('no_proxy')
    parsed = urlparse(url)

    if parsed.hostname is None:
        # URLs don't always have hostnames, e.g. file:/// urls.
        return True

    if no_proxy:
        # We need to check whether we match here. We need to see if we match
        # the end of the hostname, both with and without the port.
        no_proxy = (
            host for host in no_proxy.replace(' ', '').split(',') if host
        )

        if is_ipv4_address(parsed.hostname):
            for proxy_ip in no_proxy:
                if is_valid_cidr(proxy_ip):
                    if address_in_network(parsed.hostname, proxy_ip):
                        return True
                elif parsed.hostname == proxy_ip:
                    # If no_proxy ip was defined in plain IP notation instead of cidr notation &
                    # matches the IP of the index
                    return True
        else:
            host_with_port = parsed.hostname
            if parsed.port:
                host_with_port += ':{}'.format(parsed.port)

            for host in no_proxy:
                if parsed.hostname.endswith(host) or host_with_port.endswith(host):
                    # The URL does match something in no_proxy, so we don't want
                    # to apply the proxies on this URL.
                    return True

    with set_environ('no_proxy', no_proxy_arg):
        # parsed.hostname can be `None` in cases such as a file URI.
        try:
            bypass = proxy_bypass(parsed.hostname)
        except (TypeError, socket.gaierror):
            bypass = False

    if bypass:
        return True

    return False","def 1(url, no_proxy=None):
    """"""
    Return a dict of environment proxies.

    :rtype: dict
    """"""
    if 0(url, no_proxy=no_proxy):
        return {}
    else:
        return getproxies()","{""should_bypass_proxies"": ""0""}"
412,412,"def select_proxy(url, proxies):
    """"""Select a proxy for the url, if applicable.

    :param url: The url being for the request
    :param proxies: A dictionary of schemes or schemes and hosts to proxy URLs
    """"""
    proxies = proxies or {}
    urlparts = urlparse(url)
    if urlparts.hostname is None:
        return proxies.get(urlparts.scheme, proxies.get('all'))

    proxy_keys = [
        urlparts.scheme + '://' + urlparts.hostname,
        urlparts.scheme,
        'all://' + urlparts.hostname,
        'all',
    ]
    proxy = None
    for proxy_key in proxy_keys:
        if proxy_key in proxies:
            proxy = proxies[proxy_key]
            break

    return proxy","def urlparse(url, scheme='', allow_fragments=True):
    """"""Parse a URL into 6 components:
    <scheme>://<netloc>/<path>;<params>?<query>#<fragment>

    The result is a named 6-tuple with fields corresponding to the
    above. It is either a ParseResult or ParseResultBytes object,
    depending on the type of the url parameter.

    The username, password, hostname, and port sub-components of netloc
    can also be accessed as attributes of the returned object.

    The scheme argument provides the default value of the scheme
    component when no scheme is found in url.

    If allow_fragments is False, no attempt is made to separate the
    fragment component from the previous component, which can be either
    path or query.

    Note that % escapes are not expanded.
    """"""
    url, scheme, _coerce_result = _coerce_args(url, scheme)
    splitresult = urlsplit(url, scheme, allow_fragments)
    scheme, netloc, url, query, fragment = splitresult
    if scheme in uses_params and ';' in url:
        url, params = _splitparams(url)
    else:
        params = ''
    result = ParseResult(scheme, netloc, url, params, query, fragment)
    return _coerce_result(result)","def parse_url(url, scheme='', allow_fragments=True):
    """"""Parse a URL into 6 components:
    <scheme>://<netloc>/<path>;<params>?<query>#<fragment>

    The result is a named 6-tuple with fields corresponding to the
    above. It is either a ParseResult or ParseResultBytes object,
    depending on the type of the url parameter.

    The username, password, hostname, and port sub-components of netloc
    can also be accessed as attributes of the returned object.

    The scheme argument provides the default value of the scheme
    component when no scheme is found in url.

    If allow_fragments is False, no attempt is made to separate the
    fragment component from the previous component, which can be either
    path or query.

    Note that % escapes are not expanded.
    """"""
    url, scheme, _coerce_result = _coerce_args(url, scheme)
    splitresult = urlsplit(url, scheme, allow_fragments)
    scheme, netloc, url, query, fragment = splitresult
    if scheme in uses_params and ';' in url:
        url, params = _splitparams(url)
    else:
        params = ''
    result = ParseResult(scheme, netloc, url, params, query, fragment)
    return _coerce_result(result)","def choose_proxy_for_given_url_from_provided_proxy_dictionary(url, proxies):
    """"""Select a proxy for the url, if applicable.

    :param url: The url being for the request
    :param proxies: A dictionary of schemes or schemes and hosts to proxy URLs
    """"""
    proxies = proxies or {}
    urlparts = parse_url(url)
    if urlparts.hostname is None:
        return proxies.get(urlparts.scheme, proxies.get('all'))

    proxy_keys = [
        urlparts.scheme + '://' + urlparts.hostname,
        urlparts.scheme,
        'all://' + urlparts.hostname,
        'all',
    ]
    proxy = None
    for proxy_key in proxy_keys:
        if proxy_key in proxies:
            proxy = proxies[proxy_key]
            break

    return proxy","{""urlparse"": ""parse_url""}","def 0(url, scheme='', allow_fragments=True):
    """"""Parse a URL into 6 components:
    <scheme>://<netloc>/<path>;<params>?<query>#<fragment>

    The result is a named 6-tuple with fields corresponding to the
    above. It is either a ParseResult or ParseResultBytes object,
    depending on the type of the url parameter.

    The username, password, hostname, and port sub-components of netloc
    can also be accessed as attributes of the returned object.

    The scheme argument provides the default value of the scheme
    component when no scheme is found in url.

    If allow_fragments is False, no attempt is made to separate the
    fragment component from the previous component, which can be either
    path or query.

    Note that % escapes are not expanded.
    """"""
    url, scheme, _coerce_result = _coerce_args(url, scheme)
    splitresult = urlsplit(url, scheme, allow_fragments)
    scheme, netloc, url, query, fragment = splitresult
    if scheme in uses_params and ';' in url:
        url, params = _splitparams(url)
    else:
        params = ''
    result = ParseResult(scheme, netloc, url, params, query, fragment)
    return _coerce_result(result)","def 1(url, proxies):
    """"""Select a proxy for the url, if applicable.

    :param url: The url being for the request
    :param proxies: A dictionary of schemes or schemes and hosts to proxy URLs
    """"""
    proxies = proxies or {}
    urlparts = 0(url)
    if urlparts.hostname is None:
        return proxies.get(urlparts.scheme, proxies.get('all'))

    proxy_keys = [
        urlparts.scheme + '://' + urlparts.hostname,
        urlparts.scheme,
        'all://' + urlparts.hostname,
        'all',
    ]
    proxy = None
    for proxy_key in proxy_keys:
        if proxy_key in proxies:
            proxy = proxies[proxy_key]
            break

    return proxy","{""urlparse"": ""0""}"
413,413,"def resolve_proxies(request, proxies, trust_env=True):
    """"""This method takes proxy information from a request and configuration
    input to resolve a mapping of target proxies. This will consider settings
    such a NO_PROXY to strip proxy configurations.

    :param request: Request or PreparedRequest
    :param proxies: A dictionary of schemes or schemes and hosts to proxy URLs
    :param trust_env: Boolean declaring whether to trust environment configs

    :rtype: dict
    """"""
    proxies = proxies if proxies is not None else {}
    url = request.url
    scheme = urlparse(url).scheme
    no_proxy = proxies.get('no_proxy')
    new_proxies = proxies.copy()

    if trust_env and not should_bypass_proxies(url, no_proxy=no_proxy):
        environ_proxies = get_environ_proxies(url, no_proxy=no_proxy)

        proxy = environ_proxies.get(scheme, environ_proxies.get('all'))

        if proxy:
            new_proxies.setdefault(scheme, proxy)
    return new_proxies","def should_bypass_proxies(url, no_proxy):
    """"""
    Returns whether we should bypass proxies or not.

    :rtype: bool
    """"""
    # Prioritize lowercase environment variables over uppercase
    # to keep a consistent behaviour with other http projects (curl, wget).
    get_proxy = lambda k: os.environ.get(k) or os.environ.get(k.upper())

    # First check whether no_proxy is defined. If it is, check that the URL
    # we're getting isn't in the no_proxy list.
    no_proxy_arg = no_proxy
    if no_proxy is None:
        no_proxy = get_proxy('no_proxy')
    parsed = urlparse(url)

    if parsed.hostname is None:
        # URLs don't always have hostnames, e.g. file:/// urls.
        return True

    if no_proxy:
        # We need to check whether we match here. We need to see if we match
        # the end of the hostname, both with and without the port.
        no_proxy = (
            host for host in no_proxy.replace(' ', '').split(',') if host
        )

        if is_ipv4_address(parsed.hostname):
            for proxy_ip in no_proxy:
                if is_valid_cidr(proxy_ip):
                    if address_in_network(parsed.hostname, proxy_ip):
                        return True
                elif parsed.hostname == proxy_ip:
                    # If no_proxy ip was defined in plain IP notation instead of cidr notation &
                    # matches the IP of the index
                    return True
        else:
            host_with_port = parsed.hostname
            if parsed.port:
                host_with_port += ':{}'.format(parsed.port)

            for host in no_proxy:
                if parsed.hostname.endswith(host) or host_with_port.endswith(host):
                    # The URL does match something in no_proxy, so we don't want
                    # to apply the proxies on this URL.
                    return True

    with set_environ('no_proxy', no_proxy_arg):
        # parsed.hostname can be `None` in cases such as a file URI.
        try:
            bypass = proxy_bypass(parsed.hostname)
        except (TypeError, socket.gaierror):
            bypass = False

    if bypass:
        return True

    return False

def urlparse(url, scheme='', allow_fragments=True):
    """"""Parse a URL into 6 components:
    <scheme>://<netloc>/<path>;<params>?<query>#<fragment>

    The result is a named 6-tuple with fields corresponding to the
    above. It is either a ParseResult or ParseResultBytes object,
    depending on the type of the url parameter.

    The username, password, hostname, and port sub-components of netloc
    can also be accessed as attributes of the returned object.

    The scheme argument provides the default value of the scheme
    component when no scheme is found in url.

    If allow_fragments is False, no attempt is made to separate the
    fragment component from the previous component, which can be either
    path or query.

    Note that % escapes are not expanded.
    """"""
    url, scheme, _coerce_result = _coerce_args(url, scheme)
    splitresult = urlsplit(url, scheme, allow_fragments)
    scheme, netloc, url, query, fragment = splitresult
    if scheme in uses_params and ';' in url:
        url, params = _splitparams(url)
    else:
        params = ''
    result = ParseResult(scheme, netloc, url, params, query, fragment)
    return _coerce_result(result)

def get_environ_proxies(url, no_proxy=None):
    """"""
    Return a dict of environment proxies.

    :rtype: dict
    """"""
    if should_bypass_proxies(url, no_proxy=no_proxy):
        return {}
    else:
        return getproxies()","def check_should_bypass_proxies(url, no_proxy):
    """"""
    Returns whether we should bypass proxies or not.

    :rtype: bool
    """"""
    # Prioritize lowercase environment variables over uppercase
    # to keep a consistent behaviour with other http projects (curl, wget).
    get_proxy = lambda k: os.environ.get(k) or os.environ.get(k.upper())

    # First check whether no_proxy is defined. If it is, check that the URL
    # we're getting isn't in the no_proxy list.
    no_proxy_arg = no_proxy
    if no_proxy is None:
        no_proxy = get_proxy('no_proxy')
    parsed = parse_url(url)

    if parsed.hostname is None:
        # URLs don't always have hostnames, e.g. file:/// urls.
        return True

    if no_proxy:
        # We need to check whether we match here. We need to see if we match
        # the end of the hostname, both with and without the port.
        no_proxy = (
            host for host in no_proxy.replace(' ', '').split(',') if host
        )

        if is_ipv4_address(parsed.hostname):
            for proxy_ip in no_proxy:
                if is_valid_cidr(proxy_ip):
                    if address_in_network(parsed.hostname, proxy_ip):
                        return True
                elif parsed.hostname == proxy_ip:
                    # If no_proxy ip was defined in plain IP notation instead of cidr notation &
                    # matches the IP of the index
                    return True
        else:
            host_with_port = parsed.hostname
            if parsed.port:
                host_with_port += ':{}'.format(parsed.port)

            for host in no_proxy:
                if parsed.hostname.endswith(host) or host_with_port.endswith(host):
                    # The URL does match something in no_proxy, so we don't want
                    # to apply the proxies on this URL.
                    return True

    with set_environ('no_proxy', no_proxy_arg):
        # parsed.hostname can be `None` in cases such as a file URI.
        try:
            bypass = proxy_bypass(parsed.hostname)
        except (TypeError, socket.gaierror):
            bypass = False

    if bypass:
        return True

    return False

def parse_url(url, scheme='', allow_fragments=True):
    """"""Parse a URL into 6 components:
    <scheme>://<netloc>/<path>;<params>?<query>#<fragment>

    The result is a named 6-tuple with fields corresponding to the
    above. It is either a ParseResult or ParseResultBytes object,
    depending on the type of the url parameter.

    The username, password, hostname, and port sub-components of netloc
    can also be accessed as attributes of the returned object.

    The scheme argument provides the default value of the scheme
    component when no scheme is found in url.

    If allow_fragments is False, no attempt is made to separate the
    fragment component from the previous component, which can be either
    path or query.

    Note that % escapes are not expanded.
    """"""
    url, scheme, _coerce_result = _coerce_args(url, scheme)
    splitresult = urlsplit(url, scheme, allow_fragments)
    scheme, netloc, url, query, fragment = splitresult
    if scheme in uses_params and ';' in url:
        url, params = _splitparams(url)
    else:
        params = ''
    result = ParseResult(scheme, netloc, url, params, query, fragment)
    return _coerce_result(result)

def return_environment_proxies_dictionary_with_bypass_logic(url, no_proxy=None):
    """"""
    Return a dict of environment proxies.

    :rtype: dict
    """"""
    if check_should_bypass_proxies(url, no_proxy=no_proxy):
        return {}
    else:
        return getproxies()","def resolve_proxies_based_on_request_and_configuration_input(request, proxies, trust_env=True):
    """"""This method takes proxy information from a request and configuration
    input to resolve a mapping of target proxies. This will consider settings
    such a NO_PROXY to strip proxy configurations.

    :param request: Request or PreparedRequest
    :param proxies: A dictionary of schemes or schemes and hosts to proxy URLs
    :param trust_env: Boolean declaring whether to trust environment configs

    :rtype: dict
    """"""
    proxies = proxies if proxies is not None else {}
    url = request.url
    scheme = parse_url(url).scheme
    no_proxy = proxies.get('no_proxy')
    new_proxies = proxies.copy()

    if trust_env and not check_should_bypass_proxies(url, no_proxy=no_proxy):
        environ_proxies = return_environment_proxies_dictionary_with_bypass_logic(url, no_proxy=no_proxy)

        proxy = environ_proxies.get(scheme, environ_proxies.get('all'))

        if proxy:
            new_proxies.setdefault(scheme, proxy)
    return new_proxies","{""should_bypass_proxies"": ""check_should_bypass_proxies"", ""urlparse"": ""parse_url"", ""get_environ_proxies"": ""return_environment_proxies_dictionary_with_bypass_logic""}","def 0(url, no_proxy):
    """"""
    Returns whether we should bypass proxies or not.

    :rtype: bool
    """"""
    # Prioritize lowercase environment variables over uppercase
    # to keep a consistent behaviour with other http projects (curl, wget).
    get_proxy = lambda k: os.environ.get(k) or os.environ.get(k.upper())

    # First check whether no_proxy is defined. If it is, check that the URL
    # we're getting isn't in the no_proxy list.
    no_proxy_arg = no_proxy
    if no_proxy is None:
        no_proxy = get_proxy('no_proxy')
    parsed = 1(url)

    if parsed.hostname is None:
        # URLs don't always have hostnames, e.g. file:/// urls.
        return True

    if no_proxy:
        # We need to check whether we match here. We need to see if we match
        # the end of the hostname, both with and without the port.
        no_proxy = (
            host for host in no_proxy.replace(' ', '').split(',') if host
        )

        if is_ipv4_address(parsed.hostname):
            for proxy_ip in no_proxy:
                if is_valid_cidr(proxy_ip):
                    if address_in_network(parsed.hostname, proxy_ip):
                        return True
                elif parsed.hostname == proxy_ip:
                    # If no_proxy ip was defined in plain IP notation instead of cidr notation &
                    # matches the IP of the index
                    return True
        else:
            host_with_port = parsed.hostname
            if parsed.port:
                host_with_port += ':{}'.format(parsed.port)

            for host in no_proxy:
                if parsed.hostname.endswith(host) or host_with_port.endswith(host):
                    # The URL does match something in no_proxy, so we don't want
                    # to apply the proxies on this URL.
                    return True

    with set_environ('no_proxy', no_proxy_arg):
        # parsed.hostname can be `None` in cases such as a file URI.
        try:
            bypass = proxy_bypass(parsed.hostname)
        except (TypeError, socket.gaierror):
            bypass = False

    if bypass:
        return True

    return False

def 1(url, scheme='', allow_fragments=True):
    """"""Parse a URL into 6 components:
    <scheme>://<netloc>/<path>;<params>?<query>#<fragment>

    The result is a named 6-tuple with fields corresponding to the
    above. It is either a ParseResult or ParseResultBytes object,
    depending on the type of the url parameter.

    The username, password, hostname, and port sub-components of netloc
    can also be accessed as attributes of the returned object.

    The scheme argument provides the default value of the scheme
    component when no scheme is found in url.

    If allow_fragments is False, no attempt is made to separate the
    fragment component from the previous component, which can be either
    path or query.

    Note that % escapes are not expanded.
    """"""
    url, scheme, _coerce_result = _coerce_args(url, scheme)
    splitresult = urlsplit(url, scheme, allow_fragments)
    scheme, netloc, url, query, fragment = splitresult
    if scheme in uses_params and ';' in url:
        url, params = _splitparams(url)
    else:
        params = ''
    result = ParseResult(scheme, netloc, url, params, query, fragment)
    return _coerce_result(result)

def 2(url, no_proxy=None):
    """"""
    Return a dict of environment proxies.

    :rtype: dict
    """"""
    if 0(url, no_proxy=no_proxy):
        return {}
    else:
        return getproxies()","def 3(request, proxies, trust_env=True):
    """"""This method takes proxy information from a request and configuration
    input to resolve a mapping of target proxies. This will consider settings
    such a NO_PROXY to strip proxy configurations.

    :param request: Request or PreparedRequest
    :param proxies: A dictionary of schemes or schemes and hosts to proxy URLs
    :param trust_env: Boolean declaring whether to trust environment configs

    :rtype: dict
    """"""
    proxies = proxies if proxies is not None else {}
    url = request.url
    scheme = 1(url).scheme
    no_proxy = proxies.get('no_proxy')
    new_proxies = proxies.copy()

    if trust_env and not 0(url, no_proxy=no_proxy):
        environ_proxies = 2(url, no_proxy=no_proxy)

        proxy = environ_proxies.get(scheme, environ_proxies.get('all'))

        if proxy:
            new_proxies.setdefault(scheme, proxy)
    return new_proxies","{""should_bypass_proxies"": ""0"", ""urlparse"": ""1"", ""get_environ_proxies"": ""2""}"
414,414,"def default_headers():
    """"""
    :rtype: requests.structures.CaseInsensitiveDict
    """"""
    return CaseInsensitiveDict({
        'User-Agent': default_user_agent(),
        'Accept-Encoding': DEFAULT_ACCEPT_ENCODING,
        'Accept': '*/*',
        'Connection': 'keep-alive',
    })","def default_user_agent(name=""python-requests""):
    """"""
    Return a string representing the default user agent.

    :rtype: str
    """"""
    return '%s/%s' % (name, __version__)","def get_default_user_agent(name=""python-requests""):
    """"""
    Return a string representing the default user agent.

    :rtype: str
    """"""
    return '%s/%s' % (name, __version__)","def generate_default_headers_as_case_insensitive_dictionary():
    """"""
    :rtype: requests.structures.CaseInsensitiveDict
    """"""
    return CaseInsensitiveDict({
        'User-Agent': get_default_user_agent(),
        'Accept-Encoding': DEFAULT_ACCEPT_ENCODING,
        'Accept': '*/*',
        'Connection': 'keep-alive',
    })","{""default_user_agent"": ""get_default_user_agent""}","def 0(name=""python-requests""):
    """"""
    Return a string representing the default user agent.

    :rtype: str
    """"""
    return '%s/%s' % (name, __version__)","def 1():
    """"""
    :rtype: requests.structures.CaseInsensitiveDict
    """"""
    return CaseInsensitiveDict({
        'User-Agent': 0(),
        'Accept-Encoding': DEFAULT_ACCEPT_ENCODING,
        'Accept': '*/*',
        'Connection': 'keep-alive',
    })","{""default_user_agent"": ""0""}"
415,415,"def prepend_scheme_if_needed(url, new_scheme):
    """"""Given a URL that may or may not have a scheme, prepend the given scheme.
    Does not replace a present scheme with the one provided as an argument.

    :rtype: str
    """"""
    parsed = parse_url(url)
    scheme, auth, host, port, path, query, fragment = parsed

    # A defect in urlparse determines that there isn't a netloc present in some
    # urls. We previously assumed parsing was overly cautious, and swapped the
    # netloc and path. Due to a lack of tests on the original defect, this is
    # maintained with parse_url for backwards compatibility.
    netloc = parsed.netloc
    if not netloc:
        netloc, path = path, netloc

    if auth:
        # parse_url doesn't provide the netloc with auth
        # so we'll add it ourselves.
        netloc = '@'.join([auth, netloc])
    if scheme is None:
        scheme = new_scheme
    if path is None:
        path = ''

    return urlunparse((scheme, netloc, path, '', query, fragment))","def parse_url(url):
    """"""
    Given a url, return a parsed :class:`.Url` namedtuple. Best-effort is
    performed to parse incomplete urls. Fields not provided will be None.
    This parser is RFC 3986 compliant.

    The parser logic and helper functions are based heavily on
    work done in the ``rfc3986`` module.

    :param str url: URL to parse into a :class:`.Url` namedtuple.

    Partly backwards-compatible with :mod:`urlparse`.

    Example::

        >>> parse_url('http://google.com/mail/')
        Url(scheme='http', host='google.com', port=None, path='/mail/', ...)
        >>> parse_url('google.com:80')
        Url(scheme=None, host='google.com', port=80, path=None, ...)
        >>> parse_url('/foo?bar')
        Url(scheme=None, host=None, port=None, path='/foo', query='bar', ...)
    """"""
    if not url:
        # Empty
        return Url()

    source_url = url
    if not SCHEME_RE.search(url):
        url = ""//"" + url

    try:
        scheme, authority, path, query, fragment = URI_RE.match(url).groups()
        normalize_uri = scheme is None or scheme.lower() in NORMALIZABLE_SCHEMES

        if scheme:
            scheme = scheme.lower()

        if authority:
            auth, _, host_port = authority.rpartition(""@"")
            auth = auth or None
            host, port = _HOST_PORT_RE.match(host_port).groups()
            if auth and normalize_uri:
                auth = _encode_invalid_chars(auth, USERINFO_CHARS)
            if port == """":
                port = None
        else:
            auth, host, port = None, None, None

        if port is not None:
            port = int(port)
            if not (0 <= port <= 65535):
                raise LocationParseError(url)

        host = _normalize_host(host, scheme)

        if normalize_uri and path:
            path = _remove_path_dot_segments(path)
            path = _encode_invalid_chars(path, PATH_CHARS)
        if normalize_uri and query:
            query = _encode_invalid_chars(query, QUERY_CHARS)
        if normalize_uri and fragment:
            fragment = _encode_invalid_chars(fragment, FRAGMENT_CHARS)

    except (ValueError, AttributeError):
        return six.raise_from(LocationParseError(source_url), None)

    # For the sake of backwards compatibility we put empty
    # string values for path if there are any defined values
    # beyond the path in the URL.
    # TODO: Remove this when we break backwards compatibility.
    if not path:
        if query is not None or fragment is not None:
            path = """"
        else:
            path = None

    # Ensure that each part of the URL is a `str` for
    # backwards compatibility.
    if isinstance(url, six.text_type):
        ensure_func = six.ensure_text
    else:
        ensure_func = six.ensure_str

    def ensure_type(x):
        return x if x is None else ensure_func(x)

    return Url(
        scheme=ensure_type(scheme),
        auth=ensure_type(auth),
        host=ensure_type(host),
        port=port,
        path=ensure_type(path),
        query=ensure_type(query),
        fragment=ensure_type(fragment),
    )

def urlunparse(components):
    """"""Put a parsed URL back together again.  This may result in a
    slightly different, but equivalent URL, if the URL that was parsed
    originally had redundant delimiters, e.g. a ? with an empty query
    (the draft states that these are equivalent).""""""
    scheme, netloc, url, params, query, fragment, _coerce_result = (
                                                  _coerce_args(*components))
    if params:
        url = ""%s;%s"" % (url, params)
    return _coerce_result(urlunsplit((scheme, netloc, url, query, fragment)))","def parse_url_into_namedtuple_with_rfc3986_compliance(url):
    """"""
    Given a url, return a parsed :class:`.Url` namedtuple. Best-effort is
    performed to parse incomplete urls. Fields not provided will be None.
    This parser is RFC 3986 compliant.

    The parser logic and helper functions are based heavily on
    work done in the ``rfc3986`` module.

    :param str url: URL to parse into a :class:`.Url` namedtuple.

    Partly backwards-compatible with :mod:`urlparse`.

    Example::

        >>> parse_url_into_namedtuple_with_rfc3986_compliance('http://google.com/mail/')
        Url(scheme='http', host='google.com', port=None, path='/mail/', ...)
        >>> parse_url_into_namedtuple_with_rfc3986_compliance('google.com:80')
        Url(scheme=None, host='google.com', port=80, path=None, ...)
        >>> parse_url_into_namedtuple_with_rfc3986_compliance('/foo?bar')
        Url(scheme=None, host=None, port=None, path='/foo', query='bar', ...)
    """"""
    if not url:
        # Empty
        return Url()

    source_url = url
    if not SCHEME_RE.search(url):
        url = ""//"" + url

    try:
        scheme, authority, path, query, fragment = URI_RE.match(url).groups()
        normalize_uri = scheme is None or scheme.lower() in NORMALIZABLE_SCHEMES

        if scheme:
            scheme = scheme.lower()

        if authority:
            auth, _, host_port = authority.rpartition(""@"")
            auth = auth or None
            host, port = _HOST_PORT_RE.match(host_port).groups()
            if auth and normalize_uri:
                auth = _encode_invalid_chars(auth, USERINFO_CHARS)
            if port == """":
                port = None
        else:
            auth, host, port = None, None, None

        if port is not None:
            port = int(port)
            if not (0 <= port <= 65535):
                raise LocationParseError(url)

        host = _normalize_host(host, scheme)

        if normalize_uri and path:
            path = _remove_path_dot_segments(path)
            path = _encode_invalid_chars(path, PATH_CHARS)
        if normalize_uri and query:
            query = _encode_invalid_chars(query, QUERY_CHARS)
        if normalize_uri and fragment:
            fragment = _encode_invalid_chars(fragment, FRAGMENT_CHARS)

    except (ValueError, AttributeError):
        return six.raise_from(LocationParseError(source_url), None)

    # For the sake of backwards compatibility we put empty
    # string values for path if there are any defined values
    # beyond the path in the URL.
    # TODO: Remove this when we break backwards compatibility.
    if not path:
        if query is not None or fragment is not None:
            path = """"
        else:
            path = None

    # Ensure that each part of the URL is a `str` for
    # backwards compatibility.
    if isinstance(url, six.text_type):
        ensure_func = six.ensure_text
    else:
        ensure_func = six.ensure_str

    def ensure_type(x):
        return x if x is None else ensure_func(x)

    return Url(
        scheme=ensure_type(scheme),
        auth=ensure_type(auth),
        host=ensure_type(host),
        port=port,
        path=ensure_type(path),
        query=ensure_type(query),
        fragment=ensure_type(fragment),
    )

def put_together_parsed_url_with_redundant_delimiters_handling(components):
    """"""Put a parsed URL back together again.  This may result in a
    slightly different, but equivalent URL, if the URL that was parsed
    originally had redundant delimiters, e.g. a ? with an empty query
    (the draft states that these are equivalent).""""""
    scheme, netloc, url, params, query, fragment, _coerce_result = (
                                                  _coerce_args(*components))
    if params:
        url = ""%s;%s"" % (url, params)
    return _coerce_result(urlunsplit((scheme, netloc, url, query, fragment)))","def prepend_given_scheme_to_url_if_not_present(url, new_scheme):
    """"""Given a URL that may or may not have a scheme, prepend the given scheme.
    Does not replace a present scheme with the one provided as an argument.

    :rtype: str
    """"""
    parsed = parse_url_into_namedtuple_with_rfc3986_compliance(url)
    scheme, auth, host, port, path, query, fragment = parsed

    # A defect in urlparse determines that there isn't a netloc present in some
    # urls. We previously assumed parsing was overly cautious, and swapped the
    # netloc and path. Due to a lack of tests on the original defect, this is
    # maintained with parse_url_into_namedtuple_with_rfc3986_compliance for backwards compatibility.
    netloc = parsed.netloc
    if not netloc:
        netloc, path = path, netloc

    if auth:
        # parse_url_into_namedtuple_with_rfc3986_compliance doesn't provide the netloc with auth
        # so we'll add it ourselves.
        netloc = '@'.join([auth, netloc])
    if scheme is None:
        scheme = new_scheme
    if path is None:
        path = ''

    return put_together_parsed_url_with_redundant_delimiters_handling((scheme, netloc, path, '', query, fragment))","{""parse_url"": ""parse_url_into_namedtuple_with_rfc3986_compliance"", ""urlunparse"": ""put_together_parsed_url_with_redundant_delimiters_handling""}","def 0(url):
    """"""
    Given a url, return a parsed :class:`.Url` namedtuple. Best-effort is
    performed to parse incomplete urls. Fields not provided will be None.
    This parser is RFC 3986 compliant.

    The parser logic and helper functions are based heavily on
    work done in the ``rfc3986`` module.

    :param str url: URL to parse into a :class:`.Url` namedtuple.

    Partly backwards-compatible with :mod:`urlparse`.

    Example::

        >>> 0('http://google.com/mail/')
        Url(scheme='http', host='google.com', port=None, path='/mail/', ...)
        >>> 0('google.com:80')
        Url(scheme=None, host='google.com', port=80, path=None, ...)
        >>> 0('/foo?bar')
        Url(scheme=None, host=None, port=None, path='/foo', query='bar', ...)
    """"""
    if not url:
        # Empty
        return Url()

    source_url = url
    if not SCHEME_RE.search(url):
        url = ""//"" + url

    try:
        scheme, authority, path, query, fragment = URI_RE.match(url).groups()
        normalize_uri = scheme is None or scheme.lower() in NORMALIZABLE_SCHEMES

        if scheme:
            scheme = scheme.lower()

        if authority:
            auth, _, host_port = authority.rpartition(""@"")
            auth = auth or None
            host, port = _HOST_PORT_RE.match(host_port).groups()
            if auth and normalize_uri:
                auth = _encode_invalid_chars(auth, USERINFO_CHARS)
            if port == """":
                port = None
        else:
            auth, host, port = None, None, None

        if port is not None:
            port = int(port)
            if not (0 <= port <= 65535):
                raise LocationParseError(url)

        host = _normalize_host(host, scheme)

        if normalize_uri and path:
            path = _remove_path_dot_segments(path)
            path = _encode_invalid_chars(path, PATH_CHARS)
        if normalize_uri and query:
            query = _encode_invalid_chars(query, QUERY_CHARS)
        if normalize_uri and fragment:
            fragment = _encode_invalid_chars(fragment, FRAGMENT_CHARS)

    except (ValueError, AttributeError):
        return six.raise_from(LocationParseError(source_url), None)

    # For the sake of backwards compatibility we put empty
    # string values for path if there are any defined values
    # beyond the path in the URL.
    # TODO: Remove this when we break backwards compatibility.
    if not path:
        if query is not None or fragment is not None:
            path = """"
        else:
            path = None

    # Ensure that each part of the URL is a `str` for
    # backwards compatibility.
    if isinstance(url, six.text_type):
        ensure_func = six.ensure_text
    else:
        ensure_func = six.ensure_str

    def ensure_type(x):
        return x if x is None else ensure_func(x)

    return Url(
        scheme=ensure_type(scheme),
        auth=ensure_type(auth),
        host=ensure_type(host),
        port=port,
        path=ensure_type(path),
        query=ensure_type(query),
        fragment=ensure_type(fragment),
    )

def 2(components):
    """"""Put a parsed URL back together again.  This may result in a
    slightly different, but equivalent URL, if the URL that was parsed
    originally had redundant delimiters, e.g. a ? with an empty query
    (the draft states that these are equivalent).""""""
    scheme, netloc, url, params, query, fragment, _coerce_result = (
                                                  _coerce_args(*components))
    if params:
        url = ""%s;%s"" % (url, params)
    return _coerce_result(urlunsplit((scheme, netloc, url, query, fragment)))","def 3(url, new_scheme):
    """"""Given a URL that may or may not have a scheme, prepend the given scheme.
    Does not replace a present scheme with the one provided as an argument.

    :rtype: str
    """"""
    parsed = 0(url)
    scheme, auth, host, port, path, query, fragment = parsed

    # A defect in urlparse determines that there isn't a netloc present in some
    # urls. We previously assumed parsing was overly cautious, and swapped the
    # netloc and path. Due to a lack of tests on the original defect, this is
    # maintained with parse_url for backwards compatibility.
    netloc = parsed.netloc
    if not netloc:
        netloc, path = path, netloc

    if auth:
        # parse_url doesn't provide the netloc with auth
        # so we'll add it ourselves.
        netloc = '@'.join([auth, netloc])
    if scheme is None:
        scheme = new_scheme
    if path is None:
        path = ''

    return 2((scheme, netloc, path, '', query, fragment))","{""parse_url"": ""0"", ""urlunparse"": ""2""}"
416,416,"def urldefragauth(url):
    """"""
    Given a url remove the fragment and the authentication part.

    :rtype: str
    """"""
    scheme, netloc, path, params, query, fragment = urlparse(url)

    # see func:`prepend_scheme_if_needed`
    if not netloc:
        netloc, path = path, netloc

    netloc = netloc.rsplit('@', 1)[-1]

    return urlunparse((scheme, netloc, path, params, query, ''))","def urlunparse(components):
    """"""Put a parsed URL back together again.  This may result in a
    slightly different, but equivalent URL, if the URL that was parsed
    originally had redundant delimiters, e.g. a ? with an empty query
    (the draft states that these are equivalent).""""""
    scheme, netloc, url, params, query, fragment, _coerce_result = (
                                                  _coerce_args(*components))
    if params:
        url = ""%s;%s"" % (url, params)
    return _coerce_result(urlunsplit((scheme, netloc, url, query, fragment)))

def urlparse(url, scheme='', allow_fragments=True):
    """"""Parse a URL into 6 components:
    <scheme>://<netloc>/<path>;<params>?<query>#<fragment>

    The result is a named 6-tuple with fields corresponding to the
    above. It is either a ParseResult or ParseResultBytes object,
    depending on the type of the url parameter.

    The username, password, hostname, and port sub-components of netloc
    can also be accessed as attributes of the returned object.

    The scheme argument provides the default value of the scheme
    component when no scheme is found in url.

    If allow_fragments is False, no attempt is made to separate the
    fragment component from the previous component, which can be either
    path or query.

    Note that % escapes are not expanded.
    """"""
    url, scheme, _coerce_result = _coerce_args(url, scheme)
    splitresult = urlsplit(url, scheme, allow_fragments)
    scheme, netloc, url, query, fragment = splitresult
    if scheme in uses_params and ';' in url:
        url, params = _splitparams(url)
    else:
        params = ''
    result = ParseResult(scheme, netloc, url, params, query, fragment)
    return _coerce_result(result)","def put_together_parsed_url_with_redundant_delimiters_handling(components):
    """"""Put a parsed URL back together again.  This may result in a
    slightly different, but equivalent URL, if the URL that was parsed
    originally had redundant delimiters, e.g. a ? with an empty query
    (the draft states that these are equivalent).""""""
    scheme, netloc, url, params, query, fragment, _coerce_result = (
                                                  _coerce_args(*components))
    if params:
        url = ""%s;%s"" % (url, params)
    return _coerce_result(urlunsplit((scheme, netloc, url, query, fragment)))

def parse_url(url, scheme='', allow_fragments=True):
    """"""Parse a URL into 6 components:
    <scheme>://<netloc>/<path>;<params>?<query>#<fragment>

    The result is a named 6-tuple with fields corresponding to the
    above. It is either a ParseResult or ParseResultBytes object,
    depending on the type of the url parameter.

    The username, password, hostname, and port sub-components of netloc
    can also be accessed as attributes of the returned object.

    The scheme argument provides the default value of the scheme
    component when no scheme is found in url.

    If allow_fragments is False, no attempt is made to separate the
    fragment component from the previous component, which can be either
    path or query.

    Note that % escapes are not expanded.
    """"""
    url, scheme, _coerce_result = _coerce_args(url, scheme)
    splitresult = urlsplit(url, scheme, allow_fragments)
    scheme, netloc, url, query, fragment = splitresult
    if scheme in uses_params and ';' in url:
        url, params = _splitparams(url)
    else:
        params = ''
    result = ParseResult(scheme, netloc, url, params, query, fragment)
    return _coerce_result(result)","def remove_fragment_and_authentication_from_url(url):
    """"""
    Given a url remove the fragment and the authentication part.

    :rtype: str
    """"""
    scheme, netloc, path, params, query, fragment = parse_url(url)

    # see func:`prepend_scheme_if_needed`
    if not netloc:
        netloc, path = path, netloc

    netloc = netloc.rsplit('@', 1)[-1]

    return put_together_parsed_url_with_redundant_delimiters_handling((scheme, netloc, path, params, query, ''))","{""urlunparse"": ""put_together_parsed_url_with_redundant_delimiters_handling"", ""urlparse"": ""parse_url""}","def 0(components):
    """"""Put a parsed URL back together again.  This may result in a
    slightly different, but equivalent URL, if the URL that was parsed
    originally had redundant delimiters, e.g. a ? with an empty query
    (the draft states that these are equivalent).""""""
    scheme, netloc, url, params, query, fragment, _coerce_result = (
                                                  _coerce_args(*components))
    if params:
        url = ""%s;%s"" % (url, params)
    return _coerce_result(urlunsplit((scheme, netloc, url, query, fragment)))

def 1(url, scheme='', allow_fragments=True):
    """"""Parse a URL into 6 components:
    <scheme>://<netloc>/<path>;<params>?<query>#<fragment>

    The result is a named 6-tuple with fields corresponding to the
    above. It is either a ParseResult or ParseResultBytes object,
    depending on the type of the url parameter.

    The username, password, hostname, and port sub-components of netloc
    can also be accessed as attributes of the returned object.

    The scheme argument provides the default value of the scheme
    component when no scheme is found in url.

    If allow_fragments is False, no attempt is made to separate the
    fragment component from the previous component, which can be either
    path or query.

    Note that % escapes are not expanded.
    """"""
    url, scheme, _coerce_result = _coerce_args(url, scheme)
    splitresult = urlsplit(url, scheme, allow_fragments)
    scheme, netloc, url, query, fragment = splitresult
    if scheme in uses_params and ';' in url:
        url, params = _splitparams(url)
    else:
        params = ''
    result = ParseResult(scheme, netloc, url, params, query, fragment)
    return _coerce_result(result)","def 2(url):
    """"""
    Given a url remove the fragment and the authentication part.

    :rtype: str
    """"""
    scheme, netloc, path, params, query, fragment = 1(url)

    # see func:`prepend_scheme_if_needed`
    if not netloc:
        netloc, path = path, netloc

    netloc = netloc.rsplit('@', 1)[-1]

    return 0((scheme, netloc, path, params, query, ''))","{""urlunparse"": ""0"", ""urlparse"": ""1""}"
417,417,"def _is_jupyter() -> bool:  # pragma: no cover
    """"""Check if we're running in a Jupyter notebook.""""""
    try:
        get_ipython  # type: ignore[name-defined]
    except NameError:
        return False
    ipython = get_ipython()  # type: ignore[name-defined]
    shell = ipython.__class__.__name__
    if ""google.colab"" in str(ipython.__class__) or shell == ""ZMQInteractiveShell"":
        return True  # Jupyter notebook or qtconsole
    elif shell == ""TerminalInteractiveShell"":
        return False  # Terminal running IPython
    else:
        return False  # Other type (?)","def str(val):
    """"""Convert float to string, taking the locale into account.""""""
    return _format(""%.12g"", val)","def locale_aware_float_to_string(val):
    """"""Convert float to locale_aware_float_to_stringing, taking the locale into account.""""""
    return _format(""%.12g"", val)","def _is_jupyter() -> bool:  # pragma: no cover
    """"""Check if we're running in a Jupyter notebook.""""""
    try:
        get_ipython  # type: ignore[name-defined]
    except NameError:
        return False
    ipython = get_ipython()  # type: ignore[name-defined]
    shell = ipython.__class__.__name__
    if ""google.colab"" in locale_aware_float_to_string(ipython.__class__) or shell == ""ZMQInteractiveShell"":
        return True  # Jupyter notebook or qtconsole
    elif shell == ""TerminalInteractiveShell"":
        return False  # Terminal running IPython
    else:
        return False  # Other type (?)","{""str"": ""locale_aware_float_to_string""}","def 0(val):
    """"""Convert float to string, taking the locale into account.""""""
    return _format(""%.12g"", val)","def _is_jupyter() -> bool:  # pragma: no cover
    """"""Check if we're running in a Jupyter notebook.""""""
    try:
        get_ipython  # type: ignore[name-defined]
    except NameError:
        return False
    ipython = get_ipython()  # type: ignore[name-defined]
    shell = ipython.__class__.__name__
    if ""google.colab"" in 0(ipython.__class__) or shell == ""ZMQInteractiveShell"":
        return True  # Jupyter notebook or qtconsole
    elif shell == ""TerminalInteractiveShell"":
        return False  # Terminal running IPython
    else:
        return False  # Other type (?)","{""str"": ""0""}"
418,418,"def detect_legacy_windows() -> bool:
    """"""Detect legacy Windows.""""""
    return WINDOWS and not get_windows_console_features().vt","def get_windows_console_features() -> ""WindowsConsoleFeatures"":  # pragma: no cover
    global _windows_console_features
    if _windows_console_features is not None:
        return _windows_console_features
    from ._windows import get_windows_console_features

    _windows_console_features = get_windows_console_features()
    return _windows_console_features","def get_windows_console_features() -> ""WindowsConsoleFeatures"":  # pragma: no cover
    global _windows_console_features
    if _windows_console_features is not None:
        return _windows_console_features
    from ._windows import get_windows_console_features

    _windows_console_features = get_windows_console_features()
    return _windows_console_features","def detect_legacy_windows() -> bool:
    """"""Detect legacy Windows.""""""
    return WINDOWS and not get_windows_console_features().vt",{},"def get_windows_console_features() -> ""WindowsConsoleFeatures"":  # pragma: no cover
    global _windows_console_features
    if _windows_console_features is not None:
        return _windows_console_features
    from ._windows import get_windows_console_features

    _windows_console_features = get_windows_console_features()
    return _windows_console_features","def detect_legacy_windows() -> bool:
    """"""Detect legacy Windows.""""""
    return WINDOWS and not get_windows_console_features().vt",{}
419,419,"def pick_unit_and_suffix(size: int, suffixes: List[str], base: int) -> Tuple[int, str]:
    """"""Pick a suffix and base for the given size.""""""
    for i, suffix in enumerate(suffixes):
        unit = base**i
        if size < unit * base:
            break
    return unit, suffix","def enumerate():
    """"""Return a list of all Thread objects currently alive.

    The list includes daemonic threads, dummy thread objects created by
    current_thread(), and the main thread. It excludes terminated threads and
    threads that have not yet been started.

    """"""
    with _active_limbo_lock:
        return list(_active.values()) + list(_limbo.values())","def get_alive_threads():
    """"""Return a list of all Thread objects currently alive.

    The list includes daemonic threads, dummy thread objects created by
    current_thread(), and the main thread. It excludes terminated threads and
    threads that have not yet been started.

    """"""
    with _active_limbo_lock:
        return list(_active.values()) + list(_limbo.values())","def choose_unit_and_suffix_for_given_size_and_suffix_list(size: int, suffixes: List[str], base: int) -> Tuple[int, str]:
    """"""Pick a suffix and base for the given size.""""""
    for i, suffix in get_alive_threads(suffixes):
        unit = base**i
        if size < unit * base:
            break
    return unit, suffix","{""enumerate"": ""get_alive_threads""}","def 0():
    """"""Return a list of all Thread objects currently alive.

    The list includes daemonic threads, dummy thread objects created by
    current_thread(), and the main thread. It excludes terminated threads and
    threads that have not yet been started.

    """"""
    with _active_limbo_lock:
        return list(_active.values()) + list(_limbo.values())","def 1(size: int, suffixes: List[str], base: int) -> Tuple[int, str]:
    """"""Pick a suffix and base for the given size.""""""
    for i, suffix in 0(suffixes):
        unit = base**i
        if size < unit * base:
            break
    return unit, suffix","{""enumerate"": ""0""}"
420,420,"def reconfigure(*args: Any, **kwargs: Any) -> None:
    """"""Reconfigures the global console by replacing it with another.

    Args:
        console (Console): Replacement console instance.
    """"""
    from pip._vendor.rich.console import Console

    new_console = Console(*args, **kwargs)
    _console = get_console()
    _console.__dict__ = new_console.__dict__","def get_console() -> ""Console"":
    """"""Get a global :class:`~rich.console.Console` instance. This function is used when Rich requires a Console,
    and hasn't been explicitly given one.

    Returns:
        Console: A console instance.
    """"""
    global _console
    if _console is None:
        from .console import Console

        _console = Console()

    return _console","def get_console() -> ""Console"":
    """"""Get a global :class:`~rich.console.Console` instance. This function is used when Rich requires a Console,
    and hasn't been explicitly given one.

    Returns:
        Console: A console instance.
    """"""
    global _console
    if _console is None:
        from .console import Console

        _console = Console()

    return _console","def replace_global_console_with_another_console_instance(*args: Any, **kwargs: Any) -> None:
    """"""Reconfigures the global console by replacing it with another.

    Args:
        console (Console): Replacement console instance.
    """"""
    from pip._vendor.rich.console import Console

    new_console = Console(*args, **kwargs)
    _console = get_console()
    _console.__dict__ = new_console.__dict__",{},"def get_console() -> ""Console"":
    """"""Get a global :class:`~rich.console.Console` instance. This function is used when Rich requires a Console,
    and hasn't been explicitly given one.

    Returns:
        Console: A console instance.
    """"""
    global _console
    if _console is None:
        from .console import Console

        _console = Console()

    return _console","def 1(*args: Any, **kwargs: Any) -> None:
    """"""Reconfigures the global console by replacing it with another.

    Args:
        console (Console): Replacement console instance.
    """"""
    from pip._vendor.rich.console import Console

    new_console = Console(*args, **kwargs)
    _console = get_console()
    _console.__dict__ = new_console.__dict__",{}
421,421,"def print(*args: Any, **kwargs: Any) -> None:
    """"""Proxy for Console print.""""""
    console = get_console()
    return console.print(*args, **kwargs)","def get_console() -> ""Console"":
    """"""Get a global :class:`~rich.console.Console` instance. This function is used when Rich requires a Console,
    and hasn't been explicitly given one.

    Returns:
        Console: A console instance.
    """"""
    global _console
    if _console is None:
        from .console import Console

        _console = Console()

    return _console","def get_console() -> ""Console"":
    """"""Get a global :class:`~rich.console.Console` instance. This function is used when Rich requires a Console,
    and hasn't been explicitly given one.

    Returns:
        Console: A console instance.
    """"""
    global _console
    if _console is None:
        from .console import Console

        _console = Console()

    return _console","def print(*args: Any, **kwargs: Any) -> None:
    """"""Proxy for Console print.""""""
    console = get_console()
    return console.print(*args, **kwargs)",{},"def get_console() -> ""Console"":
    """"""Get a global :class:`~rich.console.Console` instance. This function is used when Rich requires a Console,
    and hasn't been explicitly given one.

    Returns:
        Console: A console instance.
    """"""
    global _console
    if _console is None:
        from .console import Console

        _console = Console()

    return _console","def print(*args: Any, **kwargs: Any) -> None:
    """"""Proxy for Console print.""""""
    console = get_console()
    return console.print(*args, **kwargs)",{}
422,422,"def is_expandable(obj: Any) -> bool:
    """"""Check if an object may be expanded by pretty print.""""""
    return (
        _safe_isinstance(obj, _CONTAINERS)
        or (is_dataclass(obj))
        or (hasattr(obj, ""__rich_repr__""))
        or _is_attr_object(obj)
    ) and not isclass(obj)","def isclass(object):
    """"""Return true if the object is a class.

    Class objects provide these attributes:
        __doc__         documentation string
        __module__      name of module in which this class was defined""""""
    return isinstance(object, type)

def is_dataclass(obj):
    """"""Returns True if obj is a dataclass or an instance of a
    dataclass.""""""
    cls = obj if isinstance(obj, type) and not isinstance(obj, GenericAlias) else type(obj)
    return hasattr(cls, _FIELDS)

def _safe_isinstance(
    obj: object, class_or_tuple: Union[type, Tuple[type, ...]]
) -> bool:
    """"""isinstance can fail in rare cases, for example types with no __class__""""""
    try:
        return isinstance(obj, class_or_tuple)
    except Exception:
        return False

def _is_attr_object(obj: Any) -> bool:
    """"""Check if an object was created with attrs module.""""""
    return _attr_module is not None and _attr_module.has(type(obj))","def check_if_object_is_class(object):
    """"""Return true if the object is a class.

    Class objects provide these attributes:
        __doc__         documentation string
        __module__      name of module in which this class was defined""""""
    return isinstance(object, type)

def check_if_object_is_dataclass_or_instance(obj):
    """"""Returns True if obj is a dataclass or an instance of a
    dataclass.""""""
    cls = obj if isinstance(obj, type) and not isinstance(obj, GenericAlias) else type(obj)
    return hasattr(cls, _FIELDS)

def _safe_isinstance(
    obj: object, class_or_tuple: Union[type, Tuple[type, ...]]
) -> bool:
    """"""isinstance can fail in rare cases, for example types with no __class__""""""
    try:
        return isinstance(obj, class_or_tuple)
    except Exception:
        return False

def _is_attr_object(obj: Any) -> bool:
    """"""Check if an object was created with attrs module.""""""
    return _attr_module is not None and _attr_module.has(type(obj))","def is_expandable(obj: Any) -> bool:
    """"""Check if an object may be expanded by pretty print.""""""
    return (
        _safe_isinstance(obj, _CONTAINERS)
        or (check_if_object_is_dataclass_or_instance(obj))
        or (hasattr(obj, ""__rich_repr__""))
        or _is_attr_object(obj)
    ) and not check_if_object_is_class(obj)","{""isclass"": ""check_if_object_is_class"", ""is_dataclass"": ""check_if_object_is_dataclass_or_instance""}","def 0(object):
    """"""Return true if the object is a class.

    Class objects provide these attributes:
        __doc__         documentation string
        __module__      name of module in which this class was defined""""""
    return isinstance(object, type)

def 1(obj):
    """"""Returns True if obj is a dataclass or an instance of a
    dataclass.""""""
    cls = obj if isinstance(obj, type) and not isinstance(obj, GenericAlias) else type(obj)
    return hasattr(cls, _FIELDS)

def _safe_isinstance(
    obj: object, class_or_tuple: Union[type, Tuple[type, ...]]
) -> bool:
    """"""isinstance can fail in rare cases, for example types with no __class__""""""
    try:
        return isinstance(obj, class_or_tuple)
    except Exception:
        return False

def _is_attr_object(obj: Any) -> bool:
    """"""Check if an object was created with attrs module.""""""
    return _attr_module is not None and _attr_module.has(type(obj))","def is_expandable(obj: Any) -> bool:
    """"""Check if an object may be expanded by pretty print.""""""
    return (
        _safe_isinstance(obj, _CONTAINERS)
        or (1(obj))
        or (hasattr(obj, ""__rich_repr__""))
        or _is_attr_object(obj)
    ) and not 0(obj)","{""isclass"": ""0"", ""is_dataclass"": ""1""}"
423,423,"def report() -> None:  # pragma: no cover
    """"""Print a report to the terminal with debugging information""""""
    console = Console()
    inspect(console)
    features = get_windows_console_features()
    inspect(features)

    env_names = (
        ""TERM"",
        ""COLORTERM"",
        ""CLICOLOR"",
        ""NO_COLOR"",
        ""TERM_PROGRAM"",
        ""COLUMNS"",
        ""LINES"",
        ""JPY_PARENT_PID"",
        ""VSCODE_VERBOSE_LOGGING"",
    )
    env = {name: os.getenv(name) for name in env_names}
    console.print(Panel.fit((Pretty(env)), title=""[b]Environment Variables""))

    console.print(f'platform=""{platform.system()}""')","def inspect(
    obj: Any,
    *,
    console: Optional[""Console""] = None,
    title: Optional[str] = None,
    help: bool = False,
    methods: bool = False,
    docs: bool = True,
    private: bool = False,
    dunder: bool = False,
    sort: bool = True,
    all: bool = False,
    value: bool = True,
) -> None:
    """"""Inspect any Python object.

    * inspect(<OBJECT>) to see summarized info.
    * inspect(<OBJECT>, methods=True) to see methods.
    * inspect(<OBJECT>, help=True) to see full (non-abbreviated) help.
    * inspect(<OBJECT>, private=True) to see private attributes (single underscore).
    * inspect(<OBJECT>, dunder=True) to see attributes beginning with double underscore.
    * inspect(<OBJECT>, all=True) to see all attributes.

    Args:
        obj (Any): An object to inspect.
        title (str, optional): Title to display over inspect result, or None use type. Defaults to None.
        help (bool, optional): Show full help text rather than just first paragraph. Defaults to False.
        methods (bool, optional): Enable inspection of callables. Defaults to False.
        docs (bool, optional): Also render doc strings. Defaults to True.
        private (bool, optional): Show private attributes (beginning with underscore). Defaults to False.
        dunder (bool, optional): Show attributes starting with double underscore. Defaults to False.
        sort (bool, optional): Sort attributes alphabetically. Defaults to True.
        all (bool, optional): Show all attributes. Defaults to False.
        value (bool, optional): Pretty print value. Defaults to True.
    """"""
    _console = console or get_console()
    from pip._vendor.rich._inspect import Inspect

    # Special case for inspect(inspect)
    is_inspect = obj is inspect

    _inspect = Inspect(
        obj,
        title=title,
        help=is_inspect or help,
        methods=is_inspect or methods,
        docs=is_inspect or docs,
        private=private,
        dunder=dunder,
        sort=sort,
        all=all,
        value=value,
    )
    _console.print(_inspect)

def get_windows_console_features() -> ""WindowsConsoleFeatures"":  # pragma: no cover
    global _windows_console_features
    if _windows_console_features is not None:
        return _windows_console_features
    from ._windows import get_windows_console_features

    _windows_console_features = get_windows_console_features()
    return _windows_console_features","def inspect_python_object(
    obj: Any,
    *,
    console: Optional[""Console""] = None,
    title: Optional[str] = None,
    help: bool = False,
    methods: bool = False,
    docs: bool = True,
    private: bool = False,
    dunder: bool = False,
    sort: bool = True,
    all: bool = False,
    value: bool = True,
) -> None:
    """"""Inspect any Python object.

    * inspect_python_object(<OBJECT>) to see summarized info.
    * inspect_python_object(<OBJECT>, methods=True) to see methods.
    * inspect_python_object(<OBJECT>, help=True) to see full (non-abbreviated) help.
    * inspect_python_object(<OBJECT>, private=True) to see private attributes (single underscore).
    * inspect_python_object(<OBJECT>, dunder=True) to see attributes beginning with double underscore.
    * inspect_python_object(<OBJECT>, all=True) to see all attributes.

    Args:
        obj (Any): An object to inspect_python_object.
        title (str, optional): Title to display over inspect_python_object result, or None use type. Defaults to None.
        help (bool, optional): Show full help text rather than just first paragraph. Defaults to False.
        methods (bool, optional): Enable inspect_python_objection of callables. Defaults to False.
        docs (bool, optional): Also render doc strings. Defaults to True.
        private (bool, optional): Show private attributes (beginning with underscore). Defaults to False.
        dunder (bool, optional): Show attributes starting with double underscore. Defaults to False.
        sort (bool, optional): Sort attributes alphabetically. Defaults to True.
        all (bool, optional): Show all attributes. Defaults to False.
        value (bool, optional): Pretty print value. Defaults to True.
    """"""
    _console = console or get_console()
    from pip._vendor.rich._inspect_python_object import Inspect

    # Special case for inspect_python_object(inspect_python_object)
    is_inspect_python_object = obj is inspect_python_object

    _inspect_python_object = Inspect(
        obj,
        title=title,
        help=is_inspect_python_object or help,
        methods=is_inspect_python_object or methods,
        docs=is_inspect_python_object or docs,
        private=private,
        dunder=dunder,
        sort=sort,
        all=all,
        value=value,
    )
    _console.print(_inspect_python_object)

def get_windows_console_features() -> ""WindowsConsoleFeatures"":  # pragma: no cover
    global _windows_console_features
    if _windows_console_features is not None:
        return _windows_console_features
    from ._windows import get_windows_console_features

    _windows_console_features = get_windows_console_features()
    return _windows_console_features","def report() -> None:  # pragma: no cover
    """"""Print a report to the terminal with debugging information""""""
    console = Console()
    inspect_python_object(console)
    features = get_windows_console_features()
    inspect_python_object(features)

    env_names = (
        ""TERM"",
        ""COLORTERM"",
        ""CLICOLOR"",
        ""NO_COLOR"",
        ""TERM_PROGRAM"",
        ""COLUMNS"",
        ""LINES"",
        ""JPY_PARENT_PID"",
        ""VSCODE_VERBOSE_LOGGING"",
    )
    env = {name: os.getenv(name) for name in env_names}
    console.print(Panel.fit((Pretty(env)), title=""[b]Environment Variables""))

    console.print(f'platform=""{platform.system()}""')","{""inspect"": ""inspect_python_object""}","def 0(
    obj: Any,
    *,
    console: Optional[""Console""] = None,
    title: Optional[str] = None,
    help: bool = False,
    methods: bool = False,
    docs: bool = True,
    private: bool = False,
    dunder: bool = False,
    sort: bool = True,
    all: bool = False,
    value: bool = True,
) -> None:
    """"""Inspect any Python object.

    * 0(<OBJECT>) to see summarized info.
    * 0(<OBJECT>, methods=True) to see methods.
    * 0(<OBJECT>, help=True) to see full (non-abbreviated) help.
    * 0(<OBJECT>, private=True) to see private attributes (single underscore).
    * 0(<OBJECT>, dunder=True) to see attributes beginning with double underscore.
    * 0(<OBJECT>, all=True) to see all attributes.

    Args:
        obj (Any): An object to inspect.
        title (str, optional): Title to display over inspect result, or None use type. Defaults to None.
        help (bool, optional): Show full help text rather than just first paragraph. Defaults to False.
        methods (bool, optional): Enable inspection of callables. Defaults to False.
        docs (bool, optional): Also render doc strings. Defaults to True.
        private (bool, optional): Show private attributes (beginning with underscore). Defaults to False.
        dunder (bool, optional): Show attributes starting with double underscore. Defaults to False.
        sort (bool, optional): Sort attributes alphabetically. Defaults to True.
        all (bool, optional): Show all attributes. Defaults to False.
        value (bool, optional): Pretty print value. Defaults to True.
    """"""
    _console = console or get_console()
    from pip._vendor.rich._inspect import Inspect

    # Special case for 0(inspect)
    is_inspect = obj is inspect

    _inspect = Inspect(
        obj,
        title=title,
        help=is_inspect or help,
        methods=is_inspect or methods,
        docs=is_inspect or docs,
        private=private,
        dunder=dunder,
        sort=sort,
        all=all,
        value=value,
    )
    _console.print(_inspect)

def get_windows_console_features() -> ""WindowsConsoleFeatures"":  # pragma: no cover
    global _windows_console_features
    if _windows_console_features is not None:
        return _windows_console_features
    from ._windows import get_windows_console_features

    _windows_console_features = get_windows_console_features()
    return _windows_console_features","def report() -> None:  # pragma: no cover
    """"""Print a report to the terminal with debugging information""""""
    console = Console()
    0(console)
    features = get_windows_console_features()
    0(features)

    env_names = (
        ""TERM"",
        ""COLORTERM"",
        ""CLICOLOR"",
        ""NO_COLOR"",
        ""TERM_PROGRAM"",
        ""COLUMNS"",
        ""LINES"",
        ""JPY_PARENT_PID"",
        ""VSCODE_VERBOSE_LOGGING"",
    )
    env = {name: os.getenv(name) for name in env_names}
    console.print(Panel.fit((Pretty(env)), title=""[b]Environment Variables""))

    console.print(f'platform=""{platform.system()}""')","{""inspect"": ""0""}"
424,424,"def _reformat_doc(doc: str) -> str:
    """"""Reformat docstring.""""""
    doc = cleandoc(doc).strip()
    return doc","def cleandoc(doc):
    """"""Clean up indentation from docstrings.

    Any whitespace that can be uniformly removed from the second line
    onwards is removed.""""""
    try:
        lines = doc.expandtabs().split('\n')
    except UnicodeError:
        return None
    else:
        # Find minimum indentation of any non-blank lines after first line.
        margin = sys.maxsize
        for line in lines[1:]:
            content = len(line.lstrip())
            if content:
                indent = len(line) - content
                margin = min(margin, indent)
        # Remove indentation.
        if lines:
            lines[0] = lines[0].lstrip()
        if margin < sys.maxsize:
            for i in range(1, len(lines)): lines[i] = lines[i][margin:]
        # Remove any trailing or leading blank lines.
        while lines and not lines[-1]:
            lines.pop()
        while lines and not lines[0]:
            lines.pop(0)
        return '\n'.join(lines)","def clean_up_indentation_from_docstrings(doc):
    """"""Clean up indentation from docstrings.

    Any whitespace that can be uniformly removed from the second line
    onwards is removed.""""""
    try:
        lines = doc.expandtabs().split('\n')
    except UnicodeError:
        return None
    else:
        # Find minimum indentation of any non-blank lines after first line.
        margin = sys.maxsize
        for line in lines[1:]:
            content = len(line.lstrip())
            if content:
                indent = len(line) - content
                margin = min(margin, indent)
        # Remove indentation.
        if lines:
            lines[0] = lines[0].lstrip()
        if margin < sys.maxsize:
            for i in range(1, len(lines)): lines[i] = lines[i][margin:]
        # Remove any trailing or leading blank lines.
        while lines and not lines[-1]:
            lines.pop()
        while lines and not lines[0]:
            lines.pop(0)
        return '\n'.join(lines)","def _reformat_doc(doc: str) -> str:
    """"""Reformat docstring.""""""
    doc = clean_up_indentation_from_docstrings(doc).strip()
    return doc","{""cleandoc"": ""clean_up_indentation_from_docstrings""}","def 0(doc):
    """"""Clean up indentation from docstrings.

    Any whitespace that can be uniformly removed from the second line
    onwards is removed.""""""
    try:
        lines = doc.expandtabs().split('\n')
    except UnicodeError:
        return None
    else:
        # Find minimum indentation of any non-blank lines after first line.
        margin = sys.maxsize
        for line in lines[1:]:
            content = len(line.lstrip())
            if content:
                indent = len(line) - content
                margin = min(margin, indent)
        # Remove indentation.
        if lines:
            lines[0] = lines[0].lstrip()
        if margin < sys.maxsize:
            for i in range(1, len(lines)): lines[i] = lines[i][margin:]
        # Remove any trailing or leading blank lines.
        while lines and not lines[-1]:
            lines.pop()
        while lines and not lines[0]:
            lines.pop(0)
        return '\n'.join(lines)","def _reformat_doc(doc: str) -> str:
    """"""Reformat docstring.""""""
    doc = 0(doc).strip()
    return doc","{""cleandoc"": ""0""}"
425,425,"def set_cell_size(text: str, total: int) -> str:
    """"""Set the length of a string to fit within given number of cells.""""""

    if _is_single_cell_widths(text):
        size = len(text)
        if size < total:
            return text + "" "" * (total - size)
        return text[:total]

    if not total:
        return """"
    cell_size = cell_len(text)
    if cell_size == total:
        return text
    if cell_size < total:
        return text + "" "" * (total - cell_size)

    start = 0
    end = len(text)

    # Binary search until we find the right size
    while True:
        pos = (start + end) // 2
        before = text[: pos + 1]
        before_len = cell_len(before)
        if before_len == total + 1 and cell_len(before[-1]) == 2:
            return before[:-1] + "" ""
        if before_len == total:
            return before
        if before_len > total:
            end = pos
        else:
            start = pos","def cell_len(text: str, _cache: Dict[str, int] = LRUCache(1024 * 4)) -> int:
    """"""Get the number of cells required to display text.

    Args:
        text (str): Text to display.

    Returns:
        int: Get the number of cells required to display text.
    """"""
    cached_result = _cache.get(text, None)
    if cached_result is not None:
        return cached_result

    _get_size = get_character_cell_size
    total_size = sum(_get_size(character) for character in text)
    if len(text) <= 512:
        _cache[text] = total_size
    return total_size","def get_number_of_cells_required_to_display_text(text: str, _cache: Dict[str, int] = LRUCache(1024 * 4)) -> int:
    """"""Get the number of cells required to display text.

    Args:
        text (str): Text to display.

    Returns:
        int: Get the number of cells required to display text.
    """"""
    cached_result = _cache.get(text, None)
    if cached_result is not None:
        return cached_result

    _get_size = get_character_cell_size
    total_size = sum(_get_size(character) for character in text)
    if len(text) <= 512:
        _cache[text] = total_size
    return total_size","def adjust_string_length_to_fit_within_given_number_of_cells(text: str, total: int) -> str:
    """"""Set the length of a string to fit within given number of cells.""""""

    if _is_single_cell_widths(text):
        size = len(text)
        if size < total:
            return text + "" "" * (total - size)
        return text[:total]

    if not total:
        return """"
    cell_size = get_number_of_cells_required_to_display_text(text)
    if cell_size == total:
        return text
    if cell_size < total:
        return text + "" "" * (total - cell_size)

    start = 0
    end = len(text)

    # Binary search until we find the right size
    while True:
        pos = (start + end) // 2
        before = text[: pos + 1]
        before_len = get_number_of_cells_required_to_display_text(before)
        if before_len == total + 1 and get_number_of_cells_required_to_display_text(before[-1]) == 2:
            return before[:-1] + "" ""
        if before_len == total:
            return before
        if before_len > total:
            end = pos
        else:
            start = pos","{""cell_len"": ""get_number_of_cells_required_to_display_text""}","def 0(text: str, _cache: Dict[str, int] = LRUCache(1024 * 4)) -> int:
    """"""Get the number of cells required to display text.

    Args:
        text (str): Text to display.

    Returns:
        int: Get the number of cells required to display text.
    """"""
    cached_result = _cache.get(text, None)
    if cached_result is not None:
        return cached_result

    _get_size = get_character_cell_size
    total_size = sum(_get_size(character) for character in text)
    if len(text) <= 512:
        _cache[text] = total_size
    return total_size","def 1(text: str, total: int) -> str:
    """"""Set the length of a string to fit within given number of cells.""""""

    if _is_single_cell_widths(text):
        size = len(text)
        if size < total:
            return text + "" "" * (total - size)
        return text[:total]

    if not total:
        return """"
    cell_size = 0(text)
    if cell_size == total:
        return text
    if cell_size < total:
        return text + "" "" * (total - cell_size)

    start = 0
    end = len(text)

    # Binary search until we find the right size
    while True:
        pos = (start + end) // 2
        before = text[: pos + 1]
        before_len = 0(before)
        if before_len == total + 1 and 0(before[-1]) == 2:
            return before[:-1] + "" ""
        if before_len == total:
            return before
        if before_len > total:
            end = pos
        else:
            start = pos","{""cell_len"": ""0""}"
426,426,"def print(*args: Any, **kwargs: Any) -> None:
    """"""Proxy for Console print.""""""
    console = get_console()
    return console.print(*args, **kwargs)","def get_console() -> ""Console"":
    """"""Get a global :class:`~rich.console.Console` instance. This function is used when Rich requires a Console,
    and hasn't been explicitly given one.

    Returns:
        Console: A console instance.
    """"""
    global _console
    if _console is None:
        from .console import Console

        _console = Console()

    return _console","def get_console() -> ""Console"":
    """"""Get a global :class:`~rich.console.Console` instance. This function is used when Rich requires a Console,
    and hasn't been explicitly given one.

    Returns:
        Console: A console instance.
    """"""
    global _console
    if _console is None:
        from .console import Console

        _console = Console()

    return _console","def print(*args: Any, **kwargs: Any) -> None:
    """"""Proxy for Console print.""""""
    console = get_console()
    return console.print(*args, **kwargs)",{},"def get_console() -> ""Console"":
    """"""Get a global :class:`~rich.console.Console` instance. This function is used when Rich requires a Console,
    and hasn't been explicitly given one.

    Returns:
        Console: A console instance.
    """"""
    global _console
    if _console is None:
        from .console import Console

        _console = Console()

    return _console","def print(*args: Any, **kwargs: Any) -> None:
    """"""Proxy for Console print.""""""
    console = get_console()
    return console.print(*args, **kwargs)",{}
427,427,"def format_header_param_html5(name, value):
    """"""
    Helper function to format and quote a single header parameter using the
    HTML5 strategy.

    Particularly useful for header parameters which might contain
    non-ASCII values, like file names. This follows the `HTML5 Working Draft
    Section 4.10.22.7`_ and matches the behavior of curl and modern browsers.

    .. _HTML5 Working Draft Section 4.10.22.7:
        https://w3c.github.io/html/sec-forms.html#multipart-form-data

    :param name:
        The name of the parameter, a string expected to be ASCII only.
    :param value:
        The value of the parameter, provided as ``bytes`` or `str``.
    :ret:
        A unicode string, stripped of troublesome characters.
    """"""
    if isinstance(value, six.binary_type):
        value = value.decode(""utf-8"")

    value = _replace_multiple(value, _HTML5_REPLACEMENTS)

    return u'%s=""%s""' % (name, value)","def _replace_multiple(value, needles_and_replacements):
    def replacer(match):
        return needles_and_replacements[match.group(0)]

    pattern = re.compile(
        r""|"".join([re.escape(needle) for needle in needles_and_replacements.keys()])
    )

    result = pattern.sub(replacer, value)

    return result","def replace_multiple_occurrences(value, needles_and_replacements):
    def replacer(match):
        return needles_and_replacements[match.group(0)]

    pattern = re.compile(
        r""|"".join([re.escape(needle) for needle in needles_and_replacements.keys()])
    )

    result = pattern.sub(replacer, value)

    return result","def format_and_quote_single_header_param_using_html5_strategy(name, value):
    """"""
    Helper function to format and quote a single header parameter using the
    HTML5 strategy.

    Particularly useful for header parameters which might contain
    non-ASCII values, like file names. This follows the `HTML5 Working Draft
    Section 4.10.22.7`_ and matches the behavior of curl and modern browsers.

    .. _HTML5 Working Draft Section 4.10.22.7:
        https://w3c.github.io/html/sec-forms.html#multipart-form-data

    :param name:
        The name of the parameter, a string expected to be ASCII only.
    :param value:
        The value of the parameter, provided as ``bytes`` or `str``.
    :ret:
        A unicode string, stripped of troublesome characters.
    """"""
    if isinstance(value, six.binary_type):
        value = value.decode(""utf-8"")

    value = replace_multiple_occurrences(value, _HTML5_REPLACEMENTS)

    return u'%s=""%s""' % (name, value)","{""_replace_multiple"": ""replace_multiple_occurrences""}","def 0(value, needles_and_replacements):
    def replacer(match):
        return needles_and_replacements[match.group(0)]

    pattern = re.compile(
        r""|"".join([re.escape(needle) for needle in needles_and_replacements.keys()])
    )

    result = pattern.sub(replacer, value)

    return result","def 2(name, value):
    """"""
    Helper function to format and quote a single header parameter using the
    HTML5 strategy.

    Particularly useful for header parameters which might contain
    non-ASCII values, like file names. This follows the `HTML5 Working Draft
    Section 4.10.22.7`_ and matches the behavior of curl and modern browsers.

    .. _HTML5 Working Draft Section 4.10.22.7:
        https://w3c.github.io/html/sec-forms.html#multipart-form-data

    :param name:
        The name of the parameter, a string expected to be ASCII only.
    :param value:
        The value of the parameter, provided as ``bytes`` or `str``.
    :ret:
        A unicode string, stripped of troublesome characters.
    """"""
    if isinstance(value, six.binary_type):
        value = value.decode(""utf-8"")

    value = 0(value, _HTML5_REPLACEMENTS)

    return u'%s=""%s""' % (name, value)","{""_replace_multiple"": ""0""}"
428,428,"def connection_from_url(url, **kw):
    """"""
    Given a url, return an :class:`.ConnectionPool` instance of its host.

    This is a shortcut for not having to parse out the scheme, host, and port
    of the url before creating an :class:`.ConnectionPool` instance.

    :param url:
        Absolute URL string that must include the scheme. Port is optional.

    :param \\**kw:
        Passes additional parameters to the constructor of the appropriate
        :class:`.ConnectionPool`. Useful for specifying things like
        timeout, maxsize, headers, etc.

    Example::

        >>> conn = connection_from_url('http://google.com/')
        >>> r = conn.request('GET', '/')
    """"""
    scheme, host, port = get_host(url)
    port = port or port_by_scheme.get(scheme, 80)
    if scheme == ""https"":
        return HTTPSConnectionPool(host, port=port, **kw)
    else:
        return HTTPConnectionPool(host, port=port, **kw)","def get_host(url):
    """"""
    Deprecated. Use :func:`parse_url` instead.
    """"""
    p = parse_url(url)
    return p.scheme or ""http"", p.hostname, p.port","def deprecated_use_parse_url_instead_get_scheme_host_and_port(url):
    """"""
    Deprecated. Use :func:`parse_url` instead.
    """"""
    p = parse_url(url)
    return p.scheme or ""http"", p.hostname, p.port","def create_connection_pool_instance_for_given_url(url, **kw):
    """"""
    Given a url, return an :class:`.ConnectionPool` instance of its host.

    This is a shortcut for not having to parse out the scheme, host, and port
    of the url before creating an :class:`.ConnectionPool` instance.

    :param url:
        Absolute URL string that must include the scheme. Port is optional.

    :param \\**kw:
        Passes additional parameters to the constructor of the appropriate
        :class:`.ConnectionPool`. Useful for specifying things like
        timeout, maxsize, headers, etc.

    Example::

        >>> conn = create_connection_pool_instance_for_given_url('http://google.com/')
        >>> r = conn.request('GET', '/')
    """"""
    scheme, host, port = deprecated_use_parse_url_instead_get_scheme_host_and_port(url)
    port = port or port_by_scheme.get(scheme, 80)
    if scheme == ""https"":
        return HTTPSConnectionPool(host, port=port, **kw)
    else:
        return HTTPConnectionPool(host, port=port, **kw)","{""get_host"": ""deprecated_use_parse_url_instead_get_scheme_host_and_port""}","def 0(url):
    """"""
    Deprecated. Use :func:`parse_url` instead.
    """"""
    p = parse_url(url)
    return p.scheme or ""http"", p.hostname, p.port","def 1(url, **kw):
    """"""
    Given a url, return an :class:`.ConnectionPool` instance of its host.

    This is a shortcut for not having to parse out the scheme, host, and port
    of the url before creating an :class:`.ConnectionPool` instance.

    :param url:
        Absolute URL string that must include the scheme. Port is optional.

    :param \\**kw:
        Passes additional parameters to the constructor of the appropriate
        :class:`.ConnectionPool`. Useful for specifying things like
        timeout, maxsize, headers, etc.

    Example::

        >>> conn = 1('http://google.com/')
        >>> r = conn.request('GET', '/')
    """"""
    scheme, host, port = 0(url)
    port = port or port_by_scheme.get(scheme, 80)
    if scheme == ""https"":
        return HTTPSConnectionPool(host, port=port, **kw)
    else:
        return HTTPConnectionPool(host, port=port, **kw)","{""get_host"": ""0""}"
429,429,"def create_proxy_ssl_context(
    ssl_version, cert_reqs, ca_certs=None, ca_cert_dir=None, ca_cert_data=None
):
    """"""
    Generates a default proxy ssl context if one hasn't been provided by the
    user.
    """"""
    ssl_context = create_urllib3_context(
        ssl_version=resolve_ssl_version(ssl_version),
        cert_reqs=resolve_cert_reqs(cert_reqs),
    )

    if (
        not ca_certs
        and not ca_cert_dir
        and not ca_cert_data
        and hasattr(ssl_context, ""load_default_certs"")
    ):
        ssl_context.load_default_certs()

    return ssl_context","def resolve_cert_reqs(candidate):
    """"""
    Resolves the argument to a numeric constant, which can be passed to
    the wrap_socket function/method from the ssl module.
    Defaults to :data:`ssl.CERT_REQUIRED`.
    If given a string it is assumed to be the name of the constant in the
    :mod:`ssl` module or its abbreviation.
    (So you can specify `REQUIRED` instead of `CERT_REQUIRED`.
    If it's neither `None` nor a string we assume it is already the numeric
    constant which can directly be passed to wrap_socket.
    """"""
    if candidate is None:
        return CERT_REQUIRED

    if isinstance(candidate, str):
        res = getattr(ssl, candidate, None)
        if res is None:
            res = getattr(ssl, ""CERT_"" + candidate)
        return res

    return candidate

def resolve_ssl_version(candidate):
    """"""
    like resolve_cert_reqs
    """"""
    if candidate is None:
        return PROTOCOL_TLS

    if isinstance(candidate, str):
        res = getattr(ssl, candidate, None)
        if res is None:
            res = getattr(ssl, ""PROTOCOL_"" + candidate)
        return res

    return candidate

def create_urllib3_context(
    ssl_version=None, cert_reqs=None, options=None, ciphers=None
):
    """"""All arguments have the same meaning as ``ssl_wrap_socket``.

    By default, this function does a lot of the same work that
    ``ssl.create_default_context`` does on Python 3.4+. It:

    - Disables SSLv2, SSLv3, and compression
    - Sets a restricted set of server ciphers

    If you wish to enable SSLv3, you can do::

        from pip._vendor.urllib3.util import ssl_
        context = ssl_.create_urllib3_context()
        context.options &= ~ssl_.OP_NO_SSLv3

    You can do the same to enable compression (substituting ``COMPRESSION``
    for ``SSLv3`` in the last line above).

    :param ssl_version:
        The desired protocol version to use. This will default to
        PROTOCOL_SSLv23 which will negotiate the highest protocol that both
        the server and your installation of OpenSSL support.
    :param cert_reqs:
        Whether to require the certificate verification. This defaults to
        ``ssl.CERT_REQUIRED``.
    :param options:
        Specific OpenSSL options. These default to ``ssl.OP_NO_SSLv2``,
        ``ssl.OP_NO_SSLv3``, ``ssl.OP_NO_COMPRESSION``, and ``ssl.OP_NO_TICKET``.
    :param ciphers:
        Which cipher suites to allow the server to select.
    :returns:
        Constructed SSLContext object with specified options
    :rtype: SSLContext
    """"""
    # PROTOCOL_TLS is deprecated in Python 3.10
    if not ssl_version or ssl_version == PROTOCOL_TLS:
        ssl_version = PROTOCOL_TLS_CLIENT

    context = SSLContext(ssl_version)

    context.set_ciphers(ciphers or DEFAULT_CIPHERS)

    # Setting the default here, as we may have no ssl module on import
    cert_reqs = ssl.CERT_REQUIRED if cert_reqs is None else cert_reqs

    if options is None:
        options = 0
        # SSLv2 is easily broken and is considered harmful and dangerous
        options |= OP_NO_SSLv2
        # SSLv3 has several problems and is now dangerous
        options |= OP_NO_SSLv3
        # Disable compression to prevent CRIME attacks for OpenSSL 1.0+
        # (issue #309)
        options |= OP_NO_COMPRESSION
        # TLSv1.2 only. Unless set explicitly, do not request tickets.
        # This may save some bandwidth on wire, and although the ticket is encrypted,
        # there is a risk associated with it being on wire,
        # if the server is not rotating its ticketing keys properly.
        options |= OP_NO_TICKET

    context.options |= options

    # Enable post-handshake authentication for TLS 1.3, see GH #1634. PHA is
    # necessary for conditional client cert authentication with TLS 1.3.
    # The attribute is None for OpenSSL <= 1.1.0 or does not exist in older
    # versions of Python.  We only enable on Python 3.7.4+ or if certificate
    # verification is enabled to work around Python issue #37428
    # See: https://bugs.python.org/issue37428
    if (cert_reqs == ssl.CERT_REQUIRED or sys.version_info >= (3, 7, 4)) and getattr(
        context, ""post_handshake_auth"", None
    ) is not None:
        context.post_handshake_auth = True

    def disable_check_hostname():
        if (
            getattr(context, ""check_hostname"", None) is not None
        ):  # Platform-specific: Python 3.2
            # We do our own verification, including fingerprints and alternative
            # hostnames. So disable it here
            context.check_hostname = False

    # The order of the below lines setting verify_mode and check_hostname
    # matter due to safe-guards SSLContext has to prevent an SSLContext with
    # check_hostname=True, verify_mode=NONE/OPTIONAL. This is made even more
    # complex because we don't know whether PROTOCOL_TLS_CLIENT will be used
    # or not so we don't know the initial state of the freshly created SSLContext.
    if cert_reqs == ssl.CERT_REQUIRED:
        context.verify_mode = cert_reqs
        disable_check_hostname()
    else:
        disable_check_hostname()
        context.verify_mode = cert_reqs

    # Enable logging of TLS session keys via defacto standard environment variable
    # 'SSLKEYLOGFILE', if the feature is available (Python 3.8+). Skip empty values.
    if hasattr(context, ""keylog_filename""):
        sslkeylogfile = os.environ.get(""SSLKEYLOGFILE"")
        if sslkeylogfile:
            context.keylog_filename = sslkeylogfile

    return context","def resolve_argument_to_numeric_constant_for_cert_reqs(candidate):
    """"""
    Resolves the argument to a numeric constant, which can be passed to
    the wrap_socket function/method from the ssl module.
    Defaults to :data:`ssl.CERT_REQUIRED`.
    If given a string it is assumed to be the name of the constant in the
    :mod:`ssl` module or its abbreviation.
    (So you can specify `REQUIRED` instead of `CERT_REQUIRED`.
    If it's neither `None` nor a string we assume it is already the numeric
    constant which can directly be passed to wrap_socket.
    """"""
    if candidate is None:
        return CERT_REQUIRED

    if isinstance(candidate, str):
        res = getattr(ssl, candidate, None)
        if res is None:
            res = getattr(ssl, ""CERT_"" + candidate)
        return res

    return candidate

def resolve_argument_to_numeric_constant_for_ssl_version(candidate):
    """"""
    like resolve_argument_to_numeric_constant_for_cert_reqs
    """"""
    if candidate is None:
        return PROTOCOL_TLS

    if isinstance(candidate, str):
        res = getattr(ssl, candidate, None)
        if res is None:
            res = getattr(ssl, ""PROTOCOL_"" + candidate)
        return res

    return candidate

def generate_urllib3_context(
    ssl_version=None, cert_reqs=None, options=None, ciphers=None
):
    """"""All arguments have the same meaning as ``ssl_wrap_socket``.

    By default, this function does a lot of the same work that
    ``ssl.create_default_context`` does on Python 3.4+. It:

    - Disables SSLv2, SSLv3, and compression
    - Sets a restricted set of server ciphers

    If you wish to enable SSLv3, you can do::

        from pip._vendor.urllib3.util import ssl_
        context = ssl_.generate_urllib3_context()
        context.options &= ~ssl_.OP_NO_SSLv3

    You can do the same to enable compression (substituting ``COMPRESSION``
    for ``SSLv3`` in the last line above).

    :param ssl_version:
        The desired protocol version to use. This will default to
        PROTOCOL_SSLv23 which will negotiate the highest protocol that both
        the server and your installation of OpenSSL support.
    :param cert_reqs:
        Whether to require the certificate verification. This defaults to
        ``ssl.CERT_REQUIRED``.
    :param options:
        Specific OpenSSL options. These default to ``ssl.OP_NO_SSLv2``,
        ``ssl.OP_NO_SSLv3``, ``ssl.OP_NO_COMPRESSION``, and ``ssl.OP_NO_TICKET``.
    :param ciphers:
        Which cipher suites to allow the server to select.
    :returns:
        Constructed SSLContext object with specified options
    :rtype: SSLContext
    """"""
    # PROTOCOL_TLS is deprecated in Python 3.10
    if not ssl_version or ssl_version == PROTOCOL_TLS:
        ssl_version = PROTOCOL_TLS_CLIENT

    context = SSLContext(ssl_version)

    context.set_ciphers(ciphers or DEFAULT_CIPHERS)

    # Setting the default here, as we may have no ssl module on import
    cert_reqs = ssl.CERT_REQUIRED if cert_reqs is None else cert_reqs

    if options is None:
        options = 0
        # SSLv2 is easily broken and is considered harmful and dangerous
        options |= OP_NO_SSLv2
        # SSLv3 has several problems and is now dangerous
        options |= OP_NO_SSLv3
        # Disable compression to prevent CRIME attacks for OpenSSL 1.0+
        # (issue #309)
        options |= OP_NO_COMPRESSION
        # TLSv1.2 only. Unless set explicitly, do not request tickets.
        # This may save some bandwidth on wire, and although the ticket is encrypted,
        # there is a risk associated with it being on wire,
        # if the server is not rotating its ticketing keys properly.
        options |= OP_NO_TICKET

    context.options |= options

    # Enable post-handshake authentication for TLS 1.3, see GH #1634. PHA is
    # necessary for conditional client cert authentication with TLS 1.3.
    # The attribute is None for OpenSSL <= 1.1.0 or does not exist in older
    # versions of Python.  We only enable on Python 3.7.4+ or if certificate
    # verification is enabled to work around Python issue #37428
    # See: https://bugs.python.org/issue37428
    if (cert_reqs == ssl.CERT_REQUIRED or sys.version_info >= (3, 7, 4)) and getattr(
        context, ""post_handshake_auth"", None
    ) is not None:
        context.post_handshake_auth = True

    def disable_check_hostname():
        if (
            getattr(context, ""check_hostname"", None) is not None
        ):  # Platform-specific: Python 3.2
            # We do our own verification, including fingerprints and alternative
            # hostnames. So disable it here
            context.check_hostname = False

    # The order of the below lines setting verify_mode and check_hostname
    # matter due to safe-guards SSLContext has to prevent an SSLContext with
    # check_hostname=True, verify_mode=NONE/OPTIONAL. This is made even more
    # complex because we don't know whether PROTOCOL_TLS_CLIENT will be used
    # or not so we don't know the initial state of the freshly created SSLContext.
    if cert_reqs == ssl.CERT_REQUIRED:
        context.verify_mode = cert_reqs
        disable_check_hostname()
    else:
        disable_check_hostname()
        context.verify_mode = cert_reqs

    # Enable logging of TLS session keys via defacto standard environment variable
    # 'SSLKEYLOGFILE', if the feature is available (Python 3.8+). Skip empty values.
    if hasattr(context, ""keylog_filename""):
        sslkeylogfile = os.environ.get(""SSLKEYLOGFILE"")
        if sslkeylogfile:
            context.keylog_filename = sslkeylogfile

    return context","def generate_ssl_context_for_given_ssl_version_and_cert_requirements(
    ssl_version, cert_reqs, ca_certs=None, ca_cert_dir=None, ca_cert_data=None
):
    """"""
    Generates a default proxy ssl context if one hasn't been provided by the
    user.
    """"""
    ssl_context = generate_urllib3_context(
        ssl_version=resolve_argument_to_numeric_constant_for_ssl_version(ssl_version),
        cert_reqs=resolve_argument_to_numeric_constant_for_cert_reqs(cert_reqs),
    )

    if (
        not ca_certs
        and not ca_cert_dir
        and not ca_cert_data
        and hasattr(ssl_context, ""load_default_certs"")
    ):
        ssl_context.load_default_certs()

    return ssl_context","{""resolve_cert_reqs"": ""resolve_argument_to_numeric_constant_for_cert_reqs"", ""resolve_ssl_version"": ""resolve_argument_to_numeric_constant_for_ssl_version"", ""create_urllib3_context"": ""generate_urllib3_context""}","def 0(candidate):
    """"""
    Resolves the argument to a numeric constant, which can be passed to
    the wrap_socket function/method from the ssl module.
    Defaults to :data:`ssl.CERT_REQUIRED`.
    If given a string it is assumed to be the name of the constant in the
    :mod:`ssl` module or its abbreviation.
    (So you can specify `REQUIRED` instead of `CERT_REQUIRED`.
    If it's neither `None` nor a string we assume it is already the numeric
    constant which can directly be passed to wrap_socket.
    """"""
    if candidate is None:
        return CERT_REQUIRED

    if isinstance(candidate, str):
        res = getattr(ssl, candidate, None)
        if res is None:
            res = getattr(ssl, ""CERT_"" + candidate)
        return res

    return candidate

def 1(candidate):
    """"""
    like resolve_cert_reqs
    """"""
    if candidate is None:
        return PROTOCOL_TLS

    if isinstance(candidate, str):
        res = getattr(ssl, candidate, None)
        if res is None:
            res = getattr(ssl, ""PROTOCOL_"" + candidate)
        return res

    return candidate

def 2(
    ssl_version=None, cert_reqs=None, options=None, ciphers=None
):
    """"""All arguments have the same meaning as ``ssl_wrap_socket``.

    By default, this function does a lot of the same work that
    ``ssl.create_default_context`` does on Python 3.4+. It:

    - Disables SSLv2, SSLv3, and compression
    - Sets a restricted set of server ciphers

    If you wish to enable SSLv3, you can do::

        from pip._vendor.urllib3.util import ssl_
        context = ssl_.2()
        context.options &= ~ssl_.OP_NO_SSLv3

    You can do the same to enable compression (substituting ``COMPRESSION``
    for ``SSLv3`` in the last line above).

    :param ssl_version:
        The desired protocol version to use. This will default to
        PROTOCOL_SSLv23 which will negotiate the highest protocol that both
        the server and your installation of OpenSSL support.
    :param cert_reqs:
        Whether to require the certificate verification. This defaults to
        ``ssl.CERT_REQUIRED``.
    :param options:
        Specific OpenSSL options. These default to ``ssl.OP_NO_SSLv2``,
        ``ssl.OP_NO_SSLv3``, ``ssl.OP_NO_COMPRESSION``, and ``ssl.OP_NO_TICKET``.
    :param ciphers:
        Which cipher suites to allow the server to select.
    :returns:
        Constructed SSLContext object with specified options
    :rtype: SSLContext
    """"""
    # PROTOCOL_TLS is deprecated in Python 3.10
    if not ssl_version or ssl_version == PROTOCOL_TLS:
        ssl_version = PROTOCOL_TLS_CLIENT

    context = SSLContext(ssl_version)

    context.set_ciphers(ciphers or DEFAULT_CIPHERS)

    # Setting the default here, as we may have no ssl module on import
    cert_reqs = ssl.CERT_REQUIRED if cert_reqs is None else cert_reqs

    if options is None:
        options = 0
        # SSLv2 is easily broken and is considered harmful and dangerous
        options |= OP_NO_SSLv2
        # SSLv3 has several problems and is now dangerous
        options |= OP_NO_SSLv3
        # Disable compression to prevent CRIME attacks for OpenSSL 1.0+
        # (issue #309)
        options |= OP_NO_COMPRESSION
        # TLSv1.2 only. Unless set explicitly, do not request tickets.
        # This may save some bandwidth on wire, and although the ticket is encrypted,
        # there is a risk associated with it being on wire,
        # if the server is not rotating its ticketing keys properly.
        options |= OP_NO_TICKET

    context.options |= options

    # Enable post-handshake authentication for TLS 1.3, see GH #1634. PHA is
    # necessary for conditional client cert authentication with TLS 1.3.
    # The attribute is None for OpenSSL <= 1.1.0 or does not exist in older
    # versions of Python.  We only enable on Python 3.7.4+ or if certificate
    # verification is enabled to work around Python issue #37428
    # See: https://bugs.python.org/issue37428
    if (cert_reqs == ssl.CERT_REQUIRED or sys.version_info >= (3, 7, 4)) and getattr(
        context, ""post_handshake_auth"", None
    ) is not None:
        context.post_handshake_auth = True

    def disable_check_hostname():
        if (
            getattr(context, ""check_hostname"", None) is not None
        ):  # Platform-specific: Python 3.2
            # We do our own verification, including fingerprints and alternative
            # hostnames. So disable it here
            context.check_hostname = False

    # The order of the below lines setting verify_mode and check_hostname
    # matter due to safe-guards SSLContext has to prevent an SSLContext with
    # check_hostname=True, verify_mode=NONE/OPTIONAL. This is made even more
    # complex because we don't know whether PROTOCOL_TLS_CLIENT will be used
    # or not so we don't know the initial state of the freshly created SSLContext.
    if cert_reqs == ssl.CERT_REQUIRED:
        context.verify_mode = cert_reqs
        disable_check_hostname()
    else:
        disable_check_hostname()
        context.verify_mode = cert_reqs

    # Enable logging of TLS session keys via defacto standard environment variable
    # 'SSLKEYLOGFILE', if the feature is available (Python 3.8+). Skip empty values.
    if hasattr(context, ""keylog_filename""):
        sslkeylogfile = os.environ.get(""SSLKEYLOGFILE"")
        if sslkeylogfile:
            context.keylog_filename = sslkeylogfile

    return context","def 4(
    ssl_version, cert_reqs, ca_certs=None, ca_cert_dir=None, ca_cert_data=None
):
    """"""
    Generates a default proxy ssl context if one hasn't been provided by the
    user.
    """"""
    ssl_context = 2(
        ssl_version=1(ssl_version),
        cert_reqs=0(cert_reqs),
    )

    if (
        not ca_certs
        and not ca_cert_dir
        and not ca_cert_data
        and hasattr(ssl_context, ""load_default_certs"")
    ):
        ssl_context.load_default_certs()

    return ssl_context","{""resolve_cert_reqs"": ""0"", ""resolve_ssl_version"": ""1"", ""create_urllib3_context"": ""2""}"
430,430,"def wait_for_read(sock, timeout=None):
    """"""Waits for reading to be available on a given socket.
    Returns True if the socket is readable, or False if the timeout expired.
    """"""
    return wait_for_socket(sock, read=True, timeout=timeout)","def wait_for_socket(*args, **kwargs):
    # We delay choosing which implementation to use until the first time we're
    # called. We could do it at import time, but then we might make the wrong
    # decision if someone goes wild with monkeypatching select.poll after
    # we're imported.
    global wait_for_socket
    if _have_working_poll():
        wait_for_socket = poll_wait_for_socket
    elif hasattr(select, ""select""):
        wait_for_socket = select_wait_for_socket
    else:  # Platform-specific: Appengine.
        wait_for_socket = null_wait_for_socket
    return wait_for_socket(*args, **kwargs)","def choose_wait_for_socket_implementation_delayed(*args, **kwargs):
    # We delay choosing which implementation to use until the first time we're
    # called. We could do it at import time, but then we might make the wrong
    # decision if someone goes wild with monkeypatching select.poll after
    # we're imported.
    global choose_wait_for_socket_implementation_delayed
    if _have_working_poll():
        choose_wait_for_socket_implementation_delayed = poll_choose_wait_for_socket_implementation_delayed
    elif hasattr(select, ""select""):
        choose_wait_for_socket_implementation_delayed = select_choose_wait_for_socket_implementation_delayed
    else:  # Platform-specific: Appengine.
        choose_wait_for_socket_implementation_delayed = null_choose_wait_for_socket_implementation_delayed
    return choose_wait_for_socket_implementation_delayed(*args, **kwargs)","def wait_for_socket_to_become_readable(sock, timeout=None):
    """"""Waits for reading to be available on a given socket.
    Returns True if the socket is readable, or False if the timeout expired.
    """"""
    return choose_wait_for_socket_implementation_delayed(sock, read=True, timeout=timeout)","{""wait_for_socket"": ""choose_wait_for_socket_implementation_delayed""}","def 0(*args, **kwargs):
    # We delay choosing which implementation to use until the first time we're
    # called. We could do it at import time, but then we might make the wrong
    # decision if someone goes wild with monkeypatching select.poll after
    # we're imported.
    global wait_for_socket
    if _have_working_poll():
        wait_for_socket = poll_wait_for_socket
    elif hasattr(select, ""select""):
        wait_for_socket = select_wait_for_socket
    else:  # Platform-specific: Appengine.
        wait_for_socket = null_wait_for_socket
    return 0(*args, **kwargs)","def 1(sock, timeout=None):
    """"""Waits for reading to be available on a given socket.
    Returns True if the socket is readable, or False if the timeout expired.
    """"""
    return 0(sock, read=True, timeout=timeout)","{""wait_for_socket"": ""0""}"
431,431,"def wait_for_write(sock, timeout=None):
    """"""Waits for writing to be available on a given socket.
    Returns True if the socket is readable, or False if the timeout expired.
    """"""
    return wait_for_socket(sock, write=True, timeout=timeout)","def wait_for_socket(*args, **kwargs):
    # We delay choosing which implementation to use until the first time we're
    # called. We could do it at import time, but then we might make the wrong
    # decision if someone goes wild with monkeypatching select.poll after
    # we're imported.
    global wait_for_socket
    if _have_working_poll():
        wait_for_socket = poll_wait_for_socket
    elif hasattr(select, ""select""):
        wait_for_socket = select_wait_for_socket
    else:  # Platform-specific: Appengine.
        wait_for_socket = null_wait_for_socket
    return wait_for_socket(*args, **kwargs)","def choose_wait_for_socket_implementation_delayed(*args, **kwargs):
    # We delay choosing which implementation to use until the first time we're
    # called. We could do it at import time, but then we might make the wrong
    # decision if someone goes wild with monkeypatching select.poll after
    # we're imported.
    global choose_wait_for_socket_implementation_delayed
    if _have_working_poll():
        choose_wait_for_socket_implementation_delayed = poll_choose_wait_for_socket_implementation_delayed
    elif hasattr(select, ""select""):
        choose_wait_for_socket_implementation_delayed = select_choose_wait_for_socket_implementation_delayed
    else:  # Platform-specific: Appengine.
        choose_wait_for_socket_implementation_delayed = null_choose_wait_for_socket_implementation_delayed
    return choose_wait_for_socket_implementation_delayed(*args, **kwargs)","def wait_for_writing_to_be_available_on_given_socket(sock, timeout=None):
    """"""Waits for writing to be available on a given socket.
    Returns True if the socket is readable, or False if the timeout expired.
    """"""
    return choose_wait_for_socket_implementation_delayed(sock, write=True, timeout=timeout)","{""wait_for_socket"": ""choose_wait_for_socket_implementation_delayed""}","def 0(*args, **kwargs):
    # We delay choosing which implementation to use until the first time we're
    # called. We could do it at import time, but then we might make the wrong
    # decision if someone goes wild with monkeypatching select.poll after
    # we're imported.
    global wait_for_socket
    if _have_working_poll():
        wait_for_socket = poll_wait_for_socket
    elif hasattr(select, ""select""):
        wait_for_socket = select_wait_for_socket
    else:  # Platform-specific: Appengine.
        wait_for_socket = null_wait_for_socket
    return 0(*args, **kwargs)","def 1(sock, timeout=None):
    """"""Waits for writing to be available on a given socket.
    Returns True if the socket is readable, or False if the timeout expired.
    """"""
    return 0(sock, write=True, timeout=timeout)","{""wait_for_socket"": ""0""}"
432,432,"def _const_compare_digest_backport(a, b):
    """"""
    Compare two digests of equal length in constant time.

    The digests must be of type str/bytes.
    Returns True if the digests match, and False otherwise.
    """"""
    result = abs(len(a) - len(b))
    for left, right in zip(bytearray(a), bytearray(b)):
        result |= left ^ right
    return result == 0","def abs(a):
    ""Same as abs(a).""
    return _abs(a)","def calculate_absolute_value(a):
    ""Same as calculate_absolute_value(a).""
    return _calculate_absolute_value(a)","def compare_two_digests_of_equal_length_in_constant_time(a, b):
    """"""
    Compare two digests of equal length in constant time.

    The digests must be of type str/bytes.
    Returns True if the digests match, and False otherwise.
    """"""
    result = calculate_absolute_value(len(a) - len(b))
    for left, right in zip(bytearray(a), bytearray(b)):
        result |= left ^ right
    return result == 0","{""abs"": ""calculate_absolute_value""}","def 0(a):
    ""Same as 0(a).""
    return _0(a)","def 1(a, b):
    """"""
    Compare two digests of equal length in constant time.

    The digests must be of type str/bytes.
    Returns True if the digests match, and False otherwise.
    """"""
    result = 0(len(a) - len(b))
    for left, right in zip(bytearray(a), bytearray(b)):
        result |= left ^ right
    return result == 0","{""abs"": ""0""}"
433,433,"def _encode_target(target):
    """"""Percent-encodes a request target so that there are no invalid characters""""""
    path, query = TARGET_RE.match(target).groups()
    target = _encode_invalid_chars(path, PATH_CHARS)
    query = _encode_invalid_chars(query, QUERY_CHARS)
    if query is not None:
        target += ""?"" + query
    return target","def _encode_invalid_chars(component, allowed_chars, encoding=""utf-8""):
    """"""Percent-encodes a URI component without reapplying
    onto an already percent-encoded component.
    """"""
    if component is None:
        return component

    component = six.ensure_text(component)

    # Normalize existing percent-encoded bytes.
    # Try to see if the component we're encoding is already percent-encoded
    # so we can skip all '%' characters but still encode all others.
    component, percent_encodings = PERCENT_RE.subn(
        lambda match: match.group(0).upper(), component
    )

    uri_bytes = component.encode(""utf-8"", ""surrogatepass"")
    is_percent_encoded = percent_encodings == uri_bytes.count(b""%"")
    encoded_component = bytearray()

    for i in range(0, len(uri_bytes)):
        # Will return a single character bytestring on both Python 2 & 3
        byte = uri_bytes[i : i + 1]
        byte_ord = ord(byte)
        if (is_percent_encoded and byte == b""%"") or (
            byte_ord < 128 and byte.decode() in allowed_chars
        ):
            encoded_component += byte
            continue
        encoded_component.extend(b""%"" + (hex(byte_ord)[2:].encode().zfill(2).upper()))

    return encoded_component.decode(encoding)","def _percent_encode_uri_component_without_reapplying_on_existing_percent_encoding(component, allowed_chars, encoding=""utf-8""):
    """"""Percent-encodes a URI component without reapplying
    onto an already percent-encoded component.
    """"""
    if component is None:
        return component

    component = six.ensure_text(component)

    # Normalize existing percent-encoded bytes.
    # Try to see if the component we're encoding is already percent-encoded
    # so we can skip all '%' characters but still encode all others.
    component, percent_encodings = PERCENT_RE.subn(
        lambda match: match.group(0).upper(), component
    )

    uri_bytes = component.encode(""utf-8"", ""surrogatepass"")
    is_percent_encoded = percent_encodings == uri_bytes.count(b""%"")
    encoded_component = bytearray()

    for i in range(0, len(uri_bytes)):
        # Will return a single character bytestring on both Python 2 & 3
        byte = uri_bytes[i : i + 1]
        byte_ord = ord(byte)
        if (is_percent_encoded and byte == b""%"") or (
            byte_ord < 128 and byte.decode() in allowed_chars
        ):
            encoded_component += byte
            continue
        encoded_component.extend(b""%"" + (hex(byte_ord)[2:].encode().zfill(2).upper()))

    return encoded_component.decode(encoding)","def percent_encode_request_target_to_avoid_invalid_characters(target):
    """"""Percent-encodes a request target so that there are no invalid characters""""""
    path, query = TARGET_RE.match(target).groups()
    target = _percent_encode_uri_component_without_reapplying_on_existing_percent_encoding(path, PATH_CHARS)
    query = _percent_encode_uri_component_without_reapplying_on_existing_percent_encoding(query, QUERY_CHARS)
    if query is not None:
        target += ""?"" + query
    return target","{""_encode_invalid_chars"": ""_percent_encode_uri_component_without_reapplying_on_existing_percent_encoding""}","def 0(component, allowed_chars, encoding=""utf-8""):
    """"""Percent-encodes a URI component without reapplying
    onto an already percent-encoded component.
    """"""
    if component is None:
        return component

    component = six.ensure_text(component)

    # Normalize existing percent-encoded bytes.
    # Try to see if the component we're encoding is already percent-encoded
    # so we can skip all '%' characters but still encode all others.
    component, percent_encodings = PERCENT_RE.subn(
        lambda match: match.group(0).upper(), component
    )

    uri_bytes = component.encode(""utf-8"", ""surrogatepass"")
    is_percent_encoded = percent_encodings == uri_bytes.count(b""%"")
    encoded_component = bytearray()

    for i in range(0, len(uri_bytes)):
        # Will return a single character bytestring on both Python 2 & 3
        byte = uri_bytes[i : i + 1]
        byte_ord = ord(byte)
        if (is_percent_encoded and byte == b""%"") or (
            byte_ord < 128 and byte.decode() in allowed_chars
        ):
            encoded_component += byte
            continue
        encoded_component.extend(b""%"" + (hex(byte_ord)[2:].encode().zfill(2).upper()))

    return encoded_component.decode(encoding)","def 1(target):
    """"""Percent-encodes a request target so that there are no invalid characters""""""
    path, query = TARGET_RE.match(target).groups()
    target = 0(path, PATH_CHARS)
    query = 0(query, QUERY_CHARS)
    if query is not None:
        target += ""?"" + query
    return target","{""_encode_invalid_chars"": ""0""}"
434,434,"def get_host(url):
    """"""
    Deprecated. Use :func:`parse_url` instead.
    """"""
    p = parse_url(url)
    return p.scheme or ""http"", p.hostname, p.port","def parse_url(url):
    """"""
    Given a url, return a parsed :class:`.Url` namedtuple. Best-effort is
    performed to parse incomplete urls. Fields not provided will be None.
    This parser is RFC 3986 compliant.

    The parser logic and helper functions are based heavily on
    work done in the ``rfc3986`` module.

    :param str url: URL to parse into a :class:`.Url` namedtuple.

    Partly backwards-compatible with :mod:`urlparse`.

    Example::

        >>> parse_url('http://google.com/mail/')
        Url(scheme='http', host='google.com', port=None, path='/mail/', ...)
        >>> parse_url('google.com:80')
        Url(scheme=None, host='google.com', port=80, path=None, ...)
        >>> parse_url('/foo?bar')
        Url(scheme=None, host=None, port=None, path='/foo', query='bar', ...)
    """"""
    if not url:
        # Empty
        return Url()

    source_url = url
    if not SCHEME_RE.search(url):
        url = ""//"" + url

    try:
        scheme, authority, path, query, fragment = URI_RE.match(url).groups()
        normalize_uri = scheme is None or scheme.lower() in NORMALIZABLE_SCHEMES

        if scheme:
            scheme = scheme.lower()

        if authority:
            auth, _, host_port = authority.rpartition(""@"")
            auth = auth or None
            host, port = _HOST_PORT_RE.match(host_port).groups()
            if auth and normalize_uri:
                auth = _encode_invalid_chars(auth, USERINFO_CHARS)
            if port == """":
                port = None
        else:
            auth, host, port = None, None, None

        if port is not None:
            port = int(port)
            if not (0 <= port <= 65535):
                raise LocationParseError(url)

        host = _normalize_host(host, scheme)

        if normalize_uri and path:
            path = _remove_path_dot_segments(path)
            path = _encode_invalid_chars(path, PATH_CHARS)
        if normalize_uri and query:
            query = _encode_invalid_chars(query, QUERY_CHARS)
        if normalize_uri and fragment:
            fragment = _encode_invalid_chars(fragment, FRAGMENT_CHARS)

    except (ValueError, AttributeError):
        return six.raise_from(LocationParseError(source_url), None)

    # For the sake of backwards compatibility we put empty
    # string values for path if there are any defined values
    # beyond the path in the URL.
    # TODO: Remove this when we break backwards compatibility.
    if not path:
        if query is not None or fragment is not None:
            path = """"
        else:
            path = None

    # Ensure that each part of the URL is a `str` for
    # backwards compatibility.
    if isinstance(url, six.text_type):
        ensure_func = six.ensure_text
    else:
        ensure_func = six.ensure_str

    def ensure_type(x):
        return x if x is None else ensure_func(x)

    return Url(
        scheme=ensure_type(scheme),
        auth=ensure_type(auth),
        host=ensure_type(host),
        port=port,
        path=ensure_type(path),
        query=ensure_type(query),
        fragment=ensure_type(fragment),
    )","def parse_url_into_namedtuple_with_rfc3986_compliance(url):
    """"""
    Given a url, return a parsed :class:`.Url` namedtuple. Best-effort is
    performed to parse incomplete urls. Fields not provided will be None.
    This parser is RFC 3986 compliant.

    The parser logic and helper functions are based heavily on
    work done in the ``rfc3986`` module.

    :param str url: URL to parse into a :class:`.Url` namedtuple.

    Partly backwards-compatible with :mod:`urlparse`.

    Example::

        >>> parse_url_into_namedtuple_with_rfc3986_compliance('http://google.com/mail/')
        Url(scheme='http', host='google.com', port=None, path='/mail/', ...)
        >>> parse_url_into_namedtuple_with_rfc3986_compliance('google.com:80')
        Url(scheme=None, host='google.com', port=80, path=None, ...)
        >>> parse_url_into_namedtuple_with_rfc3986_compliance('/foo?bar')
        Url(scheme=None, host=None, port=None, path='/foo', query='bar', ...)
    """"""
    if not url:
        # Empty
        return Url()

    source_url = url
    if not SCHEME_RE.search(url):
        url = ""//"" + url

    try:
        scheme, authority, path, query, fragment = URI_RE.match(url).groups()
        normalize_uri = scheme is None or scheme.lower() in NORMALIZABLE_SCHEMES

        if scheme:
            scheme = scheme.lower()

        if authority:
            auth, _, host_port = authority.rpartition(""@"")
            auth = auth or None
            host, port = _HOST_PORT_RE.match(host_port).groups()
            if auth and normalize_uri:
                auth = _encode_invalid_chars(auth, USERINFO_CHARS)
            if port == """":
                port = None
        else:
            auth, host, port = None, None, None

        if port is not None:
            port = int(port)
            if not (0 <= port <= 65535):
                raise LocationParseError(url)

        host = _normalize_host(host, scheme)

        if normalize_uri and path:
            path = _remove_path_dot_segments(path)
            path = _encode_invalid_chars(path, PATH_CHARS)
        if normalize_uri and query:
            query = _encode_invalid_chars(query, QUERY_CHARS)
        if normalize_uri and fragment:
            fragment = _encode_invalid_chars(fragment, FRAGMENT_CHARS)

    except (ValueError, AttributeError):
        return six.raise_from(LocationParseError(source_url), None)

    # For the sake of backwards compatibility we put empty
    # string values for path if there are any defined values
    # beyond the path in the URL.
    # TODO: Remove this when we break backwards compatibility.
    if not path:
        if query is not None or fragment is not None:
            path = """"
        else:
            path = None

    # Ensure that each part of the URL is a `str` for
    # backwards compatibility.
    if isinstance(url, six.text_type):
        ensure_func = six.ensure_text
    else:
        ensure_func = six.ensure_str

    def ensure_type(x):
        return x if x is None else ensure_func(x)

    return Url(
        scheme=ensure_type(scheme),
        auth=ensure_type(auth),
        host=ensure_type(host),
        port=port,
        path=ensure_type(path),
        query=ensure_type(query),
        fragment=ensure_type(fragment),
    )","def deprecated_use_parse_url_instead_get_scheme_host_and_port(url):
    """"""
    Deprecated. Use :func:`parse_url_into_namedtuple_with_rfc3986_compliance` instead.
    """"""
    p = parse_url_into_namedtuple_with_rfc3986_compliance(url)
    return p.scheme or ""http"", p.hostname, p.port","{""parse_url"": ""parse_url_into_namedtuple_with_rfc3986_compliance""}","def 0(url):
    """"""
    Given a url, return a parsed :class:`.Url` namedtuple. Best-effort is
    performed to parse incomplete urls. Fields not provided will be None.
    This parser is RFC 3986 compliant.

    The parser logic and helper functions are based heavily on
    work done in the ``rfc3986`` module.

    :param str url: URL to parse into a :class:`.Url` namedtuple.

    Partly backwards-compatible with :mod:`urlparse`.

    Example::

        >>> 0('http://google.com/mail/')
        Url(scheme='http', host='google.com', port=None, path='/mail/', ...)
        >>> 0('google.com:80')
        Url(scheme=None, host='google.com', port=80, path=None, ...)
        >>> 0('/foo?bar')
        Url(scheme=None, host=None, port=None, path='/foo', query='bar', ...)
    """"""
    if not url:
        # Empty
        return Url()

    source_url = url
    if not SCHEME_RE.search(url):
        url = ""//"" + url

    try:
        scheme, authority, path, query, fragment = URI_RE.match(url).groups()
        normalize_uri = scheme is None or scheme.lower() in NORMALIZABLE_SCHEMES

        if scheme:
            scheme = scheme.lower()

        if authority:
            auth, _, host_port = authority.rpartition(""@"")
            auth = auth or None
            host, port = _HOST_PORT_RE.match(host_port).groups()
            if auth and normalize_uri:
                auth = _encode_invalid_chars(auth, USERINFO_CHARS)
            if port == """":
                port = None
        else:
            auth, host, port = None, None, None

        if port is not None:
            port = int(port)
            if not (0 <= port <= 65535):
                raise LocationParseError(url)

        host = _normalize_host(host, scheme)

        if normalize_uri and path:
            path = _remove_path_dot_segments(path)
            path = _encode_invalid_chars(path, PATH_CHARS)
        if normalize_uri and query:
            query = _encode_invalid_chars(query, QUERY_CHARS)
        if normalize_uri and fragment:
            fragment = _encode_invalid_chars(fragment, FRAGMENT_CHARS)

    except (ValueError, AttributeError):
        return six.raise_from(LocationParseError(source_url), None)

    # For the sake of backwards compatibility we put empty
    # string values for path if there are any defined values
    # beyond the path in the URL.
    # TODO: Remove this when we break backwards compatibility.
    if not path:
        if query is not None or fragment is not None:
            path = """"
        else:
            path = None

    # Ensure that each part of the URL is a `str` for
    # backwards compatibility.
    if isinstance(url, six.text_type):
        ensure_func = six.ensure_text
    else:
        ensure_func = six.ensure_str

    def ensure_type(x):
        return x if x is None else ensure_func(x)

    return Url(
        scheme=ensure_type(scheme),
        auth=ensure_type(auth),
        host=ensure_type(host),
        port=port,
        path=ensure_type(path),
        query=ensure_type(query),
        fragment=ensure_type(fragment),
    )","def 2(url):
    """"""
    Deprecated. Use :func:`parse_url` instead.
    """"""
    p = 0(url)
    return p.scheme or ""http"", p.hostname, p.port","{""parse_url"": ""0""}"
435,435,"def is_connection_dropped(conn):  # Platform-specific
    """"""
    Returns True if the connection is dropped and should be closed.

    :param conn:
        :class:`http.client.HTTPConnection` object.

    Note: For platforms like AppEngine, this will always return ``False`` to
    let the platform handle connection recycling transparently for us.
    """"""
    sock = getattr(conn, ""sock"", False)
    if sock is False:  # Platform-specific: AppEngine
        return False
    if sock is None:  # Connection already closed (such as by httplib).
        return True
    try:
        # Returns True if readable, which here means it's been dropped
        return wait_for_read(sock, timeout=0.0)
    except NoWayToWaitForSocketError:  # Platform-specific: AppEngine
        return False","def wait_for_read(sock, timeout=None):
    """"""Waits for reading to be available on a given socket.
    Returns True if the socket is readable, or False if the timeout expired.
    """"""
    return wait_for_socket(sock, read=True, timeout=timeout)","def wait_for_socket_to_become_readable(sock, timeout=None):
    """"""Waits for reading to be available on a given socket.
    Returns True if the socket is readable, or False if the timeout expired.
    """"""
    return wait_for_socket(sock, read=True, timeout=timeout)","def check_if_connection_dropped_and_should_be_closed(conn):  # Platform-specific
    """"""
    Returns True if the connection is dropped and should be closed.

    :param conn:
        :class:`http.client.HTTPConnection` object.

    Note: For platforms like AppEngine, this will always return ``False`` to
    let the platform handle connection recycling transparently for us.
    """"""
    sock = getattr(conn, ""sock"", False)
    if sock is False:  # Platform-specific: AppEngine
        return False
    if sock is None:  # Connection already closed (such as by httplib).
        return True
    try:
        # Returns True if readable, which here means it's been dropped
        return wait_for_socket_to_become_readable(sock, timeout=0.0)
    except NoWayToWaitForSocketError:  # Platform-specific: AppEngine
        return False","{""wait_for_read"": ""wait_for_socket_to_become_readable""}","def 0(sock, timeout=None):
    """"""Waits for reading to be available on a given socket.
    Returns True if the socket is readable, or False if the timeout expired.
    """"""
    return wait_for_socket(sock, read=True, timeout=timeout)","def 1(conn):  # Platform-specific
    """"""
    Returns True if the connection is dropped and should be closed.

    :param conn:
        :class:`http.client.HTTPConnection` object.

    Note: For platforms like AppEngine, this will always return ``False`` to
    let the platform handle connection recycling transparently for us.
    """"""
    sock = getattr(conn, ""sock"", False)
    if sock is False:  # Platform-specific: AppEngine
        return False
    if sock is None:  # Connection already closed (such as by httplib).
        return True
    try:
        # Returns True if readable, which here means it's been dropped
        return 0(sock, timeout=0.0)
    except NoWayToWaitForSocketError:  # Platform-specific: AppEngine
        return False","{""wait_for_read"": ""0""}"
436,436,"def _ipaddress_match(ipname, host_ip):
    """"""Exact matching of IP addresses.

    RFC 6125 explicitly doesn't define an algorithm for this
    (section 1.7.2 - ""Out of Scope"").
    """"""
    # OpenSSL may add a trailing newline to a subjectAltName's IP address
    # Divergence from upstream: ipaddress can't handle byte str
    ip = ipaddress.ip_address(_to_unicode(ipname).rstrip())
    return ip == host_ip","def _to_unicode(obj):
    if isinstance(obj, str) and sys.version_info < (3,):
        # ignored flake8 # F821 to support python 2.7 function
        obj = unicode(obj, encoding=""ascii"", errors=""strict"")  # noqa: F821
    return obj","def convert_object_to_unicode_for_python2(obj):
    if isinstance(obj, str) and sys.version_info < (3,):
        # ignored flake8 # F821 to support python 2.7 function
        obj = unicode(obj, encoding=""ascii"", errors=""strict"")  # noqa: F821
    return obj","def perform_exact_matching_of_ip_addresses(ipname, host_ip):
    """"""Exact matching of IP addresses.

    RFC 6125 explicitly doesn't define an algorithm for this
    (section 1.7.2 - ""Out of Scope"").
    """"""
    # OpenSSL may add a trailing newline to a subjectAltName's IP address
    # Divergence from upstream: ipaddress can't handle byte str
    ip = ipaddress.ip_address(convert_object_to_unicode_for_python2(ipname).rstrip())
    return ip == host_ip","{""_to_unicode"": ""convert_object_to_unicode_for_python2""}","def 0(obj):
    if isinstance(obj, str) and sys.version_info < (3,):
        # ignored flake8 # F821 to support python 2.7 function
        obj = unicode(obj, encoding=""ascii"", errors=""strict"")  # noqa: F821
    return obj","def 1(ipname, host_ip):
    """"""Exact matching of IP addresses.

    RFC 6125 explicitly doesn't define an algorithm for this
    (section 1.7.2 - ""Out of Scope"").
    """"""
    # OpenSSL may add a trailing newline to a subjectAltName's IP address
    # Divergence from upstream: ipaddress can't handle byte str
    ip = ipaddress.ip_address(0(ipname).rstrip())
    return ip == host_ip","{""_to_unicode"": ""0""}"
437,437,"def is_appengine_sandbox():
    """"""Reports if the app is running in the first generation sandbox.

    The second generation runtimes are technically still in a sandbox, but it
    is much less restrictive, so generally you shouldn't need to check for it.
    see https://cloud.google.com/appengine/docs/standard/runtimes
    """"""
    return is_appengine() and os.environ[""APPENGINE_RUNTIME""] == ""python27""","def is_appengine():
    return is_local_appengine() or is_prod_appengine()","def check_if_running_on_appengine():
    return is_local_appengine() or is_prod_appengine()","def check_if_running_on_appengine_sandbox():
    """"""Reports if the app is running in the first generation sandbox.

    The second generation runtimes are technically still in a sandbox, but it
    is much less restrictive, so generally you shouldn't need to check for it.
    see https://cloud.google.com/appengine/docs/standard/runtimes
    """"""
    return check_if_running_on_appengine() and os.environ[""APPENGINE_RUNTIME""] == ""python27""","{""is_appengine"": ""check_if_running_on_appengine""}","def 0():
    return is_local_appengine() or is_prod_appengine()","def 1():
    """"""Reports if the app is running in the first generation sandbox.

    The second generation runtimes are technically still in a sandbox, but it
    is much less restrictive, so generally you shouldn't need to check for it.
    see https://cloud.google.com/appengine/docs/standard/runtimes
    """"""
    return 0() and os.environ[""APPENGINE_RUNTIME""] == ""python27""","{""is_appengine"": ""0""}"
438,438,"def inject_into_urllib3():
    ""Monkey-patch urllib3 with PyOpenSSL-backed SSL-support.""

    _validate_dependencies_met()

    util.SSLContext = PyOpenSSLContext
    util.ssl_.SSLContext = PyOpenSSLContext
    util.HAS_SNI = HAS_SNI
    util.ssl_.HAS_SNI = HAS_SNI
    util.IS_PYOPENSSL = True
    util.ssl_.IS_PYOPENSSL = True","def _validate_dependencies_met():
    """"""
    Verifies that PyOpenSSL's package-level dependencies have been met.
    Throws `ImportError` if they are not met.
    """"""
    # Method added in `cryptography==1.1`; not available in older versions
    from cryptography.x509.extensions import Extensions

    if getattr(Extensions, ""get_extension_for_class"", None) is None:
        raise ImportError(
            ""'cryptography' module missing required functionality.  ""
            ""Try upgrading to v1.3.4 or newer.""
        )

    # pyOpenSSL 0.14 and above use cryptography for OpenSSL bindings. The _x509
    # attribute is only present on those versions.
    from OpenSSL.crypto import X509

    x509 = X509()
    if getattr(x509, ""_x509"", None) is None:
        raise ImportError(
            ""'pyOpenSSL' module missing required functionality. ""
            ""Try upgrading to v0.14 or newer.""
        )","def validate_pyopenssl_dependencies():
    """"""
    Verifies that PyOpenSSL's package-level dependencies have been met.
    Throws `ImportError` if they are not met.
    """"""
    # Method added in `cryptography==1.1`; not available in older versions
    from cryptography.x509.extensions import Extensions

    if getattr(Extensions, ""get_extension_for_class"", None) is None:
        raise ImportError(
            ""'cryptography' module missing required functionality.  ""
            ""Try upgrading to v1.3.4 or newer.""
        )

    # pyOpenSSL 0.14 and above use cryptography for OpenSSL bindings. The _x509
    # attribute is only present on those versions.
    from OpenSSL.crypto import X509

    x509 = X509()
    if getattr(x509, ""_x509"", None) is None:
        raise ImportError(
            ""'pyOpenSSL' module missing required functionality. ""
            ""Try upgrading to v0.14 or newer.""
        )","def monkey_patch_urllib3_with_pyopenssl_ssl_support():
    ""Monkey-patch urllib3 with PyOpenSSL-backed SSL-support.""

    validate_pyopenssl_dependencies()

    util.SSLContext = PyOpenSSLContext
    util.ssl_.SSLContext = PyOpenSSLContext
    util.HAS_SNI = HAS_SNI
    util.ssl_.HAS_SNI = HAS_SNI
    util.IS_PYOPENSSL = True
    util.ssl_.IS_PYOPENSSL = True","{""_validate_dependencies_met"": ""validate_pyopenssl_dependencies""}","def 0():
    """"""
    Verifies that PyOpenSSL's package-level dependencies have been met.
    Throws `ImportError` if they are not met.
    """"""
    # Method added in `cryptography==1.1`; not available in older versions
    from cryptography.x509.extensions import Extensions

    if getattr(Extensions, ""get_extension_for_class"", None) is None:
        raise ImportError(
            ""'cryptography' module missing required functionality.  ""
            ""Try upgrading to v1.3.4 or newer.""
        )

    # pyOpenSSL 0.14 and above use cryptography for OpenSSL bindings. The _x509
    # attribute is only present on those versions.
    from OpenSSL.crypto import X509

    x509 = X509()
    if getattr(x509, ""_x509"", None) is None:
        raise ImportError(
            ""'pyOpenSSL' module missing required functionality. ""
            ""Try upgrading to v0.14 or newer.""
        )","def 1():
    ""Monkey-patch urllib3 with PyOpenSSL-backed SSL-support.""

    0()

    util.SSLContext = PyOpenSSLContext
    util.ssl_.SSLContext = PyOpenSSLContext
    util.HAS_SNI = HAS_SNI
    util.ssl_.HAS_SNI = HAS_SNI
    util.IS_PYOPENSSL = True
    util.ssl_.IS_PYOPENSSL = True","{""_validate_dependencies_met"": ""0""}"
439,439,"def _create_cfstring_array(lst):
    """"""
    Given a list of Python binary data, create an associated CFMutableArray.
    The array must be CFReleased by the caller.

    Raises an ssl.SSLError on failure.
    """"""
    cf_arr = None
    try:
        cf_arr = CoreFoundation.CFArrayCreateMutable(
            CoreFoundation.kCFAllocatorDefault,
            0,
            ctypes.byref(CoreFoundation.kCFTypeArrayCallBacks),
        )
        if not cf_arr:
            raise MemoryError(""Unable to allocate memory!"")
        for item in lst:
            cf_str = _cfstr(item)
            if not cf_str:
                raise MemoryError(""Unable to allocate memory!"")
            try:
                CoreFoundation.CFArrayAppendValue(cf_arr, cf_str)
            finally:
                CoreFoundation.CFRelease(cf_str)
    except BaseException as e:
        if cf_arr:
            CoreFoundation.CFRelease(cf_arr)
        raise ssl.SSLError(""Unable to allocate array: %s"" % (e,))
    return cf_arr","def _cfstr(py_bstr):
    """"""
    Given a Python binary data, create a CFString.
    The string must be CFReleased by the caller.
    """"""
    c_str = ctypes.c_char_p(py_bstr)
    cf_str = CoreFoundation.CFStringCreateWithCString(
        CoreFoundation.kCFAllocatorDefault,
        c_str,
        CFConst.kCFStringEncodingUTF8,
    )
    return cf_str","def create_cfstring_from_python_binary_data(py_bstr):
    """"""
    Given a Python binary data, create a CFString.
    The string must be CFReleased by the caller.
    """"""
    c_str = ctypes.c_char_p(py_bstr)
    cf_str = CoreFoundation.CFStringCreateWithCString(
        CoreFoundation.kCFAllocatorDefault,
        c_str,
        CFConst.kCFStringEncodingUTF8,
    )
    return cf_str","def _createcreate_cfstring_from_python_binary_dataing_array(lst):
    """"""
    Given a list of Python binary data, create an associated CFMutableArray.
    The array must be CFReleased by the caller.

    Raises an ssl.SSLError on failure.
    """"""
    cf_arr = None
    try:
        cf_arr = CoreFoundation.CFArrayCreateMutable(
            CoreFoundation.kCFAllocatorDefault,
            0,
            ctypes.byref(CoreFoundation.kCFTypeArrayCallBacks),
        )
        if not cf_arr:
            raise MemoryError(""Unable to allocate memory!"")
        for item in lst:
            cf_str = create_cfstring_from_python_binary_data(item)
            if not cf_str:
                raise MemoryError(""Unable to allocate memory!"")
            try:
                CoreFoundation.CFArrayAppendValue(cf_arr, cf_str)
            finally:
                CoreFoundation.CFRelease(cf_str)
    except BaseException as e:
        if cf_arr:
            CoreFoundation.CFRelease(cf_arr)
        raise ssl.SSLError(""Unable to allocate array: %s"" % (e,))
    return cf_arr","{""_cfstr"": ""create_cfstring_from_python_binary_data""}","def 0(py_bstr):
    """"""
    Given a Python binary data, create a CFString.
    The string must be CFReleased by the caller.
    """"""
    c_str = ctypes.c_char_p(py_bstr)
    cf_str = CoreFoundation.CFStringCreateWithCString(
        CoreFoundation.kCFAllocatorDefault,
        c_str,
        CFConst.kCFStringEncodingUTF8,
    )
    return cf_str","def 1(lst):
    """"""
    Given a list of Python binary data, create an associated CFMutableArray.
    The array must be CFReleased by the caller.

    Raises an ssl.SSLError on failure.
    """"""
    cf_arr = None
    try:
        cf_arr = CoreFoundation.CFArrayCreateMutable(
            CoreFoundation.kCFAllocatorDefault,
            0,
            ctypes.byref(CoreFoundation.kCFTypeArrayCallBacks),
        )
        if not cf_arr:
            raise MemoryError(""Unable to allocate memory!"")
        for item in lst:
            cf_str = 0(item)
            if not cf_str:
                raise MemoryError(""Unable to allocate memory!"")
            try:
                CoreFoundation.CFArrayAppendValue(cf_arr, cf_str)
            finally:
                CoreFoundation.CFRelease(cf_str)
    except BaseException as e:
        if cf_arr:
            CoreFoundation.CFRelease(cf_arr)
        raise ssl.SSLError(""Unable to allocate array: %s"" % (e,))
    return cf_arr","{""_cfstr"": ""0""}"
440,440,"def _assert_no_error(error, exception_class=None):
    """"""
    Checks the return code and throws an exception if there is an error to
    report
    """"""
    if error == 0:
        return

    cf_error_string = Security.SecCopyErrorMessageString(error, None)
    output = _cf_string_to_unicode(cf_error_string)
    CoreFoundation.CFRelease(cf_error_string)

    if output is None or output == u"""":
        output = u""OSStatus %s"" % error

    if exception_class is None:
        exception_class = ssl.SSLError

    raise exception_class(output)","def _cf_string_to_unicode(value):
    """"""
    Creates a Unicode string from a CFString object. Used entirely for error
    reporting.

    Yes, it annoys me quite a lot that this function is this complex.
    """"""
    value_as_void_p = ctypes.cast(value, ctypes.POINTER(ctypes.c_void_p))

    string = CoreFoundation.CFStringGetCStringPtr(
        value_as_void_p, CFConst.kCFStringEncodingUTF8
    )
    if string is None:
        buffer = ctypes.create_string_buffer(1024)
        result = CoreFoundation.CFStringGetCString(
            value_as_void_p, buffer, 1024, CFConst.kCFStringEncodingUTF8
        )
        if not result:
            raise OSError(""Error copying C string from CFStringRef"")
        string = buffer.value
    if string is not None:
        string = string.decode(""utf-8"")
    return string","def convert_cfstring_to_unicode_string(value):
    """"""
    Creates a Unicode string from a CFString object. Used entirely for error
    reporting.

    Yes, it annoys me quite a lot that this function is this complex.
    """"""
    value_as_void_p = ctypes.cast(value, ctypes.POINTER(ctypes.c_void_p))

    string = CoreFoundation.CFStringGetCStringPtr(
        value_as_void_p, CFConst.kCFStringEncodingUTF8
    )
    if string is None:
        buffer = ctypes.create_string_buffer(1024)
        result = CoreFoundation.CFStringGetCString(
            value_as_void_p, buffer, 1024, CFConst.kCFStringEncodingUTF8
        )
        if not result:
            raise OSError(""Error copying C string from CFStringRef"")
        string = buffer.value
    if string is not None:
        string = string.decode(""utf-8"")
    return string","def check_return_code_and_throw_exception_on_error(error, exception_class=None):
    """"""
    Checks the return code and throws an exception if there is an error to
    report
    """"""
    if error == 0:
        return

    cf_error_string = Security.SecCopyErrorMessageString(error, None)
    output = convert_cfstring_to_unicode_string(cf_error_string)
    CoreFoundation.CFRelease(cf_error_string)

    if output is None or output == u"""":
        output = u""OSStatus %s"" % error

    if exception_class is None:
        exception_class = ssl.SSLError

    raise exception_class(output)","{""_cf_string_to_unicode"": ""convert_cfstring_to_unicode_string""}","def 0(value):
    """"""
    Creates a Unicode string from a CFString object. Used entirely for error
    reporting.

    Yes, it annoys me quite a lot that this function is this complex.
    """"""
    value_as_void_p = ctypes.cast(value, ctypes.POINTER(ctypes.c_void_p))

    string = CoreFoundation.CFStringGetCStringPtr(
        value_as_void_p, CFConst.kCFStringEncodingUTF8
    )
    if string is None:
        buffer = ctypes.create_string_buffer(1024)
        result = CoreFoundation.CFStringGetCString(
            value_as_void_p, buffer, 1024, CFConst.kCFStringEncodingUTF8
        )
        if not result:
            raise OSError(""Error copying C string from CFStringRef"")
        string = buffer.value
    if string is not None:
        string = string.decode(""utf-8"")
    return string","def 1(error, exception_class=None):
    """"""
    Checks the return code and throws an exception if there is an error to
    report
    """"""
    if error == 0:
        return

    cf_error_string = Security.SecCopyErrorMessageString(error, None)
    output = 0(cf_error_string)
    CoreFoundation.CFRelease(cf_error_string)

    if output is None or output == u"""":
        output = u""OSStatus %s"" % error

    if exception_class is None:
        exception_class = ssl.SSLError

    raise exception_class(output)","{""_cf_string_to_unicode"": ""0""}"
441,441,"def load_cdll(name, macos10_16_path):
    """"""Loads a CDLL by name, falling back to known path on 10.16+""""""
    try:
        # Big Sur is technically 11 but we use 10.16 due to the Big Sur
        # beta being labeled as 10.16.
        if version_info >= (10, 16):
            path = macos10_16_path
        else:
            path = find_library(name)
        if not path:
            raise OSError  # Caught and reraised as 'ImportError'
        return CDLL(path, use_errno=True)
    except OSError:
        raise_from(ImportError(""The library %s failed to load"" % name), None)","def find_library(name):
    """"""AIX implementation of ctypes.util.find_library()
    Find an archive member that will dlopen(). If not available,
    also search for a file (or link) with a .so suffix.

    AIX supports two types of schemes that can be used with dlopen().
    The so-called SystemV Release4 (svr4) format is commonly suffixed
    with .so while the (default) AIX scheme has the library (archive)
    ending with the suffix .a
    As an archive has multiple members (e.g., 32-bit and 64-bit) in one file
    the argument passed to dlopen must include both the library and
    the member names in a single string.

    find_library() looks first for an archive (.a) with a suitable member.
    If no archive+member pair is found, look for a .so file.
    """"""

    libpaths = get_libpaths()
    (base, member) = find_shared(libpaths, name)
    if base is not None:
        return f""{base}({member})""

    # To get here, a member in an archive has not been found
    # In other words, either:
    # a) a .a file was not found
    # b) a .a file did not have a suitable member
    # So, look for a .so file
    # Check libpaths for .so file
    # Note, the installation must prepare a link from a .so
    # to a versioned file
    # This is common practice by GNU libtool on other platforms
    soname = f""lib{name}.so""
    for dir in libpaths:
        # /lib is a symbolic link to /usr/lib, skip it
        if dir == ""/lib"":
            continue
        shlib = path.join(dir, soname)
        if path.exists(shlib):
            return soname
    # if we are here, we have not found anything plausible
    return None","def search_library_on_aix(name):
    """"""AIX implementation of ctypes.util.search_library_on_aix()
    Find an archive member that will dlopen(). If not available,
    also search for a file (or link) with a .so suffix.

    AIX supports two types of schemes that can be used with dlopen().
    The so-called SystemV Release4 (svr4) format is commonly suffixed
    with .so while the (default) AIX scheme has the library (archive)
    ending with the suffix .a
    As an archive has multiple members (e.g., 32-bit and 64-bit) in one file
    the argument passed to dlopen must include both the library and
    the member names in a single string.

    search_library_on_aix() looks first for an archive (.a) with a suitable member.
    If no archive+member pair is found, look for a .so file.
    """"""

    libpaths = get_libpaths()
    (base, member) = find_shared(libpaths, name)
    if base is not None:
        return f""{base}({member})""

    # To get here, a member in an archive has not been found
    # In other words, either:
    # a) a .a file was not found
    # b) a .a file did not have a suitable member
    # So, look for a .so file
    # Check libpaths for .so file
    # Note, the installation must prepare a link from a .so
    # to a versioned file
    # This is common practice by GNU libtool on other platforms
    soname = f""lib{name}.so""
    for dir in libpaths:
        # /lib is a symbolic link to /usr/lib, skip it
        if dir == ""/lib"":
            continue
        shlib = path.join(dir, soname)
        if path.exists(shlib):
            return soname
    # if we are here, we have not found anything plausible
    return None","def load_dynamic_link_library_by_name_with_fallback_path(name, macos10_16_path):
    """"""Loads a CDLL by name, falling back to known path on 10.16+""""""
    try:
        # Big Sur is technically 11 but we use 10.16 due to the Big Sur
        # beta being labeled as 10.16.
        if version_info >= (10, 16):
            path = macos10_16_path
        else:
            path = search_library_on_aix(name)
        if not path:
            raise OSError  # Caught and reraised as 'ImportError'
        return CDLL(path, use_errno=True)
    except OSError:
        raise_from(ImportError(""The library %s failed to load"" % name), None)","{""find_library"": ""search_library_on_aix""}","def 0(name):
    """"""AIX implementation of ctypes.util.0()
    Find an archive member that will dlopen(). If not available,
    also search for a file (or link) with a .so suffix.

    AIX supports two types of schemes that can be used with dlopen().
    The so-called SystemV Release4 (svr4) format is commonly suffixed
    with .so while the (default) AIX scheme has the library (archive)
    ending with the suffix .a
    As an archive has multiple members (e.g., 32-bit and 64-bit) in one file
    the argument passed to dlopen must include both the library and
    the member names in a single string.

    0() looks first for an archive (.a) with a suitable member.
    If no archive+member pair is found, look for a .so file.
    """"""

    libpaths = get_libpaths()
    (base, member) = find_shared(libpaths, name)
    if base is not None:
        return f""{base}({member})""

    # To get here, a member in an archive has not been found
    # In other words, either:
    # a) a .a file was not found
    # b) a .a file did not have a suitable member
    # So, look for a .so file
    # Check libpaths for .so file
    # Note, the installation must prepare a link from a .so
    # to a versioned file
    # This is common practice by GNU libtool on other platforms
    soname = f""lib{name}.so""
    for dir in libpaths:
        # /lib is a symbolic link to /usr/lib, skip it
        if dir == ""/lib"":
            continue
        shlib = path.join(dir, soname)
        if path.exists(shlib):
            return soname
    # if we are here, we have not found anything plausible
    return None","def 1(name, macos10_16_path):
    """"""Loads a CDLL by name, falling back to known path on 10.16+""""""
    try:
        # Big Sur is technically 11 but we use 10.16 due to the Big Sur
        # beta being labeled as 10.16.
        if version_info >= (10, 16):
            path = macos10_16_path
        else:
            path = 0(name)
        if not path:
            raise OSError  # Caught and reraised as 'ImportError'
        return CDLL(path, use_errno=True)
    except OSError:
        raise_from(ImportError(""The library %s failed to load"" % name), None)","{""find_library"": ""0""}"
442,442,"def _import_module(name):
    """"""Import module, returning the module after the last dot.""""""
    __import__(name)
    return sys.modules[name]","def __import__(name, globals=None, locals=None, fromlist=(), level=0):
    """"""Import a module.

    The 'globals' argument is used to infer where the import is occurring from
    to handle relative imports. The 'locals' argument is ignored. The
    'fromlist' argument specifies what should exist as attributes on the module
    being imported (e.g. ``from module import <fromlist>``).  The 'level'
    argument represents the package location to import from in a relative
    import (e.g. ``from ..pkg import mod`` would have a 'level' of 2).

    """"""
    if level == 0:
        module = _gcd_import(name)
    else:
        globals_ = globals if globals is not None else {}
        package = _calc___package__(globals_)
        module = _gcd_import(name, package, level)
    if not fromlist:
        # Return up to the first dot in 'name'. This is complicated by the fact
        # that 'name' may be relative.
        if level == 0:
            return _gcd_import(name.partition('.')[0])
        elif not name:
            return module
        else:
            # Figure out where to slice the module's name up to the first dot
            # in 'name'.
            cut_off = len(name) - len(name.partition('.')[0])
            # Slice end needs to be positive to alleviate need to special-case
            # when ``'.' not in name``.
            return sys.modules[module.__name__[:len(module.__name__)-cut_off]]
    elif hasattr(module, '__path__'):
        return _handle_fromlist(module, fromlist, _gcd_import)
    else:
        return module","def import_module(name, globals=None, locals=None, fromlist=(), level=0):
    """"""Import a module.

    The 'globals' argument is used to infer where the import is occurring from
    to handle relative imports. The 'locals' argument is ignored. The
    'fromlist' argument specifies what should exist as attributes on the module
    being imported (e.g. ``from module import <fromlist>``).  The 'level'
    argument represents the package location to import from in a relative
    import (e.g. ``from ..pkg import mod`` would have a 'level' of 2).

    """"""
    if level == 0:
        module = _gcd_import(name)
    else:
        globals_ = globals if globals is not None else {}
        package = _calc___package__(globals_)
        module = _gcd_import(name, package, level)
    if not fromlist:
        # Return up to the first dot in 'name'. This is complicated by the fact
        # that 'name' may be relative.
        if level == 0:
            return _gcd_import(name.partition('.')[0])
        elif not name:
            return module
        else:
            # Figure out where to slice the module's name up to the first dot
            # in 'name'.
            cut_off = len(name) - len(name.partition('.')[0])
            # Slice end needs to be positive to alleviate need to special-case
            # when ``'.' not in name``.
            return sys.modules[module.__name__[:len(module.__name__)-cut_off]]
    elif hasattr(module, '__path__'):
        return _handle_fromlist(module, fromlist, _gcd_import)
    else:
        return module","def import_module_and_return_last_module(name):
    """"""Import module, returning the module after the last dot.""""""
    import_module(name)
    return sys.modules[name]","{""__import__"": ""import_module""}","def 0(name, globals=None, locals=None, fromlist=(), level=0):
    """"""Import a module.

    The 'globals' argument is used to infer where the import is occurring from
    to handle relative imports. The 'locals' argument is ignored. The
    'fromlist' argument specifies what should exist as attributes on the module
    being imported (e.g. ``from module import <fromlist>``).  The 'level'
    argument represents the package location to import from in a relative
    import (e.g. ``from ..pkg import mod`` would have a 'level' of 2).

    """"""
    if level == 0:
        module = _gcd_import(name)
    else:
        globals_ = globals if globals is not None else {}
        package = _calc___package__(globals_)
        module = _gcd_import(name, package, level)
    if not fromlist:
        # Return up to the first dot in 'name'. This is complicated by the fact
        # that 'name' may be relative.
        if level == 0:
            return _gcd_import(name.partition('.')[0])
        elif not name:
            return module
        else:
            # Figure out where to slice the module's name up to the first dot
            # in 'name'.
            cut_off = len(name) - len(name.partition('.')[0])
            # Slice end needs to be positive to alleviate need to special-case
            # when ``'.' not in name``.
            return sys.modules[module.__name__[:len(module.__name__)-cut_off]]
    elif hasattr(module, '__path__'):
        return _handle_fromlist(module, fromlist, _gcd_import)
    else:
        return module","def 1(name):
    """"""Import module, returning the module after the last dot.""""""
    0(name)
    return sys.modules[name]","{""__import__"": ""0""}"
443,443,"def safe_version(version):
    """"""
    Convert an arbitrary string to a standard version string
    """"""
    try:
        # normalize the version
        return str(packaging.version.Version(version))
    except packaging.version.InvalidVersion:
        version = version.replace(' ', '.')
        return re.sub('[^A-Za-z0-9.]+', '-', version)","def str(val):
    """"""Convert float to string, taking the locale into account.""""""
    return _format(""%.12g"", val)","def locale_aware_float_to_string(val):
    """"""Convert float to locale_aware_float_to_stringing, taking the locale into account.""""""
    return _format(""%.12g"", val)","def convert_arbitrary_string_to_standard_version_string(version):
    """"""
    Convert an arbitrary locale_aware_float_to_stringing to a standard version locale_aware_float_to_stringing
    """"""
    try:
        # normalize the version
        return locale_aware_float_to_string(packaging.version.Version(version))
    except packaging.version.InvalidVersion:
        version = version.replace(' ', '.')
        return re.sub('[^A-Za-z0-9.]+', '-', version)","{""str"": ""locale_aware_float_to_string""}","def 0(val):
    """"""Convert float to string, taking the locale into account.""""""
    return _format(""%.12g"", val)","def 1(version):
    """"""
    Convert an arbitrary string to a standard version string
    """"""
    try:
        # normalize the version
        return 0(packaging.version.Version(version))
    except packaging.version.InvalidVersion:
        version = version.replace(' ', '.')
        return re.sub('[^A-Za-z0-9.]+', '-', version)","{""str"": ""0""}"
444,444,"def yield_lines(strs):
    """"""Yield non-empty/non-comment lines of a string or sequence""""""
    if isinstance(strs, six.string_types):
        for s in strs.splitlines():
            s = s.strip()
            # skip blank lines/comments
            if s and not s.startswith('#'):
                yield s
    else:
        for ss in strs:
            for s in yield_lines(ss):
                yield s","def yield_lines(strs):
    """"""Yield non-empty/non-comment lines of a string or sequence""""""
    if isinstance(strs, six.string_types):
        for s in strs.splitlines():
            s = s.strip()
            # skip blank lines/comments
            if s and not s.startswith('#'):
                yield s
    else:
        for ss in strs:
            for s in yield_lines(ss):
                yield s","def generate_non_empty_lines(strs):
    """"""Yield non-empty/non-comment lines of a string or sequence""""""
    if isinstance(strs, six.string_types):
        for s in strs.splitlines():
            s = s.strip()
            # skip blank lines/comments
            if s and not s.startswith('#'):
                yield s
    else:
        for ss in strs:
            for s in generate_non_empty_lines(ss):
                yield s","def generate_non_empty_lines(strs):
    """"""Yield non-empty/non-comment lines of a string or sequence""""""
    if isinstance(strs, six.string_types):
        for s in strs.splitlines():
            s = s.strip()
            # skip blank lines/comments
            if s and not s.startswith('#'):
                yield s
    else:
        for ss in strs:
            for s in generate_non_empty_lines(ss):
                yield s","{""yield_lines"": ""generate_non_empty_lines""}","def 0(strs):
    """"""Yield non-empty/non-comment lines of a string or sequence""""""
    if isinstance(strs, six.string_types):
        for s in strs.splitlines():
            s = s.strip()
            # skip blank lines/comments
            if s and not s.startswith('#'):
                yield s
    else:
        for ss in strs:
            for s in 0(ss):
                yield s","def 0(strs):
    """"""Yield non-empty/non-comment lines of a string or sequence""""""
    if isinstance(strs, six.string_types):
        for s in strs.splitlines():
            s = s.strip()
            # skip blank lines/comments
            if s and not s.startswith('#'):
                yield s
    else:
        for ss in strs:
            for s in 0(ss):
                yield s","{""yield_lines"": ""0""}"
445,445,"def parse_requirements(strs):
    """"""
    Yield ``Requirement`` objects for each specification in `strs`.

    `strs` must be a string, or a (possibly-nested) iterable thereof.
    """"""
    return map(Requirement, join_continuation(map(drop_comment, yield_lines(strs))))","def yield_lines(strs):
    """"""Yield non-empty/non-comment lines of a string or sequence""""""
    if isinstance(strs, six.string_types):
        for s in strs.splitlines():
            s = s.strip()
            # skip blank lines/comments
            if s and not s.startswith('#'):
                yield s
    else:
        for ss in strs:
            for s in yield_lines(ss):
                yield s","def generate_non_empty_lines(strs):
    """"""Yield non-empty/non-comment lines of a string or sequence""""""
    if isinstance(strs, six.string_types):
        for s in strs.splitlines():
            s = s.strip()
            # skip blank lines/comments
            if s and not s.startswith('#'):
                yield s
    else:
        for ss in strs:
            for s in generate_non_empty_lines(ss):
                yield s","def yield_requirement_objects_for_each_specification_in_strings(strs):
    """"""
    Yield ``Requirement`` objects for each specification in `strs`.

    `strs` must be a string, or a (possibly-nested) iterable thereof.
    """"""
    return map(Requirement, join_continuation(map(drop_comment, generate_non_empty_lines(strs))))","{""yield_lines"": ""generate_non_empty_lines""}","def 0(strs):
    """"""Yield non-empty/non-comment lines of a string or sequence""""""
    if isinstance(strs, six.string_types):
        for s in strs.splitlines():
            s = s.strip()
            # skip blank lines/comments
            if s and not s.startswith('#'):
                yield s
    else:
        for ss in strs:
            for s in 0(ss):
                yield s","def 1(strs):
    """"""
    Yield ``Requirement`` objects for each specification in `strs`.

    `strs` must be a string, or a (possibly-nested) iterable thereof.
    """"""
    return map(Requirement, join_continuation(map(drop_comment, 0(strs))))","{""yield_lines"": ""0""}"
446,446,"def split_sections(s):
    """"""Split a string or iterable thereof into (section, content) pairs

    Each ``section`` is a stripped version of the section header (""[section]"")
    and each ``content`` is a list of stripped lines excluding blank lines and
    comment-only lines.  If there are any such lines before the first section
    header, they're returned in a first ``section`` of ``None``.
    """"""
    section = None
    content = []
    for line in yield_lines(s):
        if line.startswith(""[""):
            if line.endswith(""]""):
                if section or content:
                    yield section, content
                section = line[1:-1].strip()
                content = []
            else:
                raise ValueError(""Invalid section heading"", line)
        else:
            content.append(line)

    # wrap up last segment
    yield section, content","def yield_lines(strs):
    """"""Yield non-empty/non-comment lines of a string or sequence""""""
    if isinstance(strs, six.string_types):
        for s in strs.splitlines():
            s = s.strip()
            # skip blank lines/comments
            if s and not s.startswith('#'):
                yield s
    else:
        for ss in strs:
            for s in yield_lines(ss):
                yield s","def generate_non_empty_lines(strs):
    """"""Yield non-empty/non-comment lines of a string or sequence""""""
    if isinstance(strs, six.string_types):
        for s in strs.splitlines():
            s = s.strip()
            # skip blank lines/comments
            if s and not s.startswith('#'):
                yield s
    else:
        for ss in strs:
            for s in generate_non_empty_lines(ss):
                yield s","def split_string_or_iterable_into_section_content_pairs(s):
    """"""Split a string or iterable thereof into (section, content) pairs

    Each ``section`` is a stripped version of the section header (""[section]"")
    and each ``content`` is a list of stripped lines excluding blank lines and
    comment-only lines.  If there are any such lines before the first section
    header, they're returned in a first ``section`` of ``None``.
    """"""
    section = None
    content = []
    for line in generate_non_empty_lines(s):
        if line.startswith(""[""):
            if line.endswith(""]""):
                if section or content:
                    yield section, content
                section = line[1:-1].strip()
                content = []
            else:
                raise ValueError(""Invalid section heading"", line)
        else:
            content.append(line)

    # wrap up last segment
    yield section, content","{""yield_lines"": ""generate_non_empty_lines""}","def 0(strs):
    """"""Yield non-empty/non-comment lines of a string or sequence""""""
    if isinstance(strs, six.string_types):
        for s in strs.splitlines():
            s = s.strip()
            # skip blank lines/comments
            if s and not s.startswith('#'):
                yield s
    else:
        for ss in strs:
            for s in 0(ss):
                yield s","def 1(s):
    """"""Split a string or iterable thereof into (section, content) pairs

    Each ``section`` is a stripped version of the section header (""[section]"")
    and each ``content`` is a list of stripped lines excluding blank lines and
    comment-only lines.  If there are any such lines before the first section
    header, they're returned in a first ``section`` of ``None``.
    """"""
    section = None
    content = []
    for line in 0(s):
        if line.startswith(""[""):
            if line.endswith(""]""):
                if section or content:
                    yield section, content
                section = line[1:-1].strip()
                content = []
            else:
                raise ValueError(""Invalid section heading"", line)
        else:
            content.append(line)

    # wrap up last segment
    yield section, content","{""yield_lines"": ""0""}"
447,447,"def get_win_folder_from_registry(csidl_name: str) -> str:
    """"""Get folder from the registry.

    This is a fallback technique at best. I'm not sure if using the
    registry for this guarantees us the correct answer for all CSIDL_*
    names.
    """"""
    shell_folder_name = {
        ""CSIDL_APPDATA"": ""AppData"",
        ""CSIDL_COMMON_APPDATA"": ""Common AppData"",
        ""CSIDL_LOCAL_APPDATA"": ""Local AppData"",
        ""CSIDL_PERSONAL"": ""Personal"",
    }.get(csidl_name)
    if shell_folder_name is None:
        raise ValueError(f""Unknown CSIDL name: {csidl_name}"")

    import winreg

    key = winreg.OpenKey(winreg.HKEY_CURRENT_USER, r""Software\Microsoft\Windows\CurrentVersion\Explorer\Shell Folders"")
    directory, _ = winreg.QueryValueEx(key, shell_folder_name)
    return str(directory)","def str(val):
    """"""Convert float to string, taking the locale into account.""""""
    return _format(""%.12g"", val)","def locale_aware_float_to_string(val):
    """"""Convert float to locale_aware_float_to_stringing, taking the locale into account.""""""
    return _format(""%.12g"", val)","def get_win_folder_from_regilocale_aware_float_to_stringy(csidl_name: locale_aware_float_to_string) -> locale_aware_float_to_string:
    """"""Get folder from the regilocale_aware_float_to_stringy.

    This is a fallback technique at best. I'm not sure if using the
    regilocale_aware_float_to_stringy for this guarantees us the correct answer for all CSIDL_*
    names.
    """"""
    shell_folder_name = {
        ""CSIDL_APPDATA"": ""AppData"",
        ""CSIDL_COMMON_APPDATA"": ""Common AppData"",
        ""CSIDL_LOCAL_APPDATA"": ""Local AppData"",
        ""CSIDL_PERSONAL"": ""Personal"",
    }.get(csidl_name)
    if shell_folder_name is None:
        raise ValueError(f""Unknown CSIDL name: {csidl_name}"")

    import winreg

    key = winreg.OpenKey(winreg.HKEY_CURRENT_USER, r""Software\Microsoft\Windows\CurrentVersion\Explorer\Shell Folders"")
    directory, _ = winreg.QueryValueEx(key, shell_folder_name)
    return locale_aware_float_to_string(directory)","{""str"": ""locale_aware_float_to_string""}","def 0(val):
    """"""Convert float to string, taking the locale into account.""""""
    return _format(""%.12g"", val)","def get_win_folder_from_registry(csidl_name: str) -> str:
    """"""Get folder from the registry.

    This is a fallback technique at best. I'm not sure if using the
    registry for this guarantees us the correct answer for all CSIDL_*
    names.
    """"""
    shell_folder_name = {
        ""CSIDL_APPDATA"": ""AppData"",
        ""CSIDL_COMMON_APPDATA"": ""Common AppData"",
        ""CSIDL_LOCAL_APPDATA"": ""Local AppData"",
        ""CSIDL_PERSONAL"": ""Personal"",
    }.get(csidl_name)
    if shell_folder_name is None:
        raise ValueError(f""Unknown CSIDL name: {csidl_name}"")

    import winreg

    key = winreg.OpenKey(winreg.HKEY_CURRENT_USER, r""Software\Microsoft\Windows\CurrentVersion\Explorer\Shell Folders"")
    directory, _ = winreg.QueryValueEx(key, shell_folder_name)
    return 0(directory)","{""str"": ""0""}"
448,448,"def distros_for_url(url, metadata=None):
    """"""Yield egg or source distribution objects that might be found at a URL""""""
    base, fragment = egg_info_for_url(url)
    for dist in distros_for_location(url, base, metadata):
        yield dist
    if fragment:
        match = EGG_FRAGMENT.match(fragment)
        if match:
            for dist in interpret_distro_name(
                url, match.group(1), metadata, precedence=CHECKOUT_DIST
            ):
                yield dist","def interpret_distro_name(
        location, basename, metadata, py_version=None, precedence=SOURCE_DIST,
        platform=None
):
    """"""Generate alternative interpretations of a source distro name

    Note: if `location` is a filesystem filename, you should call
    ``pkg_resources.normalize_path()`` on it before passing it to this
    routine!
    """"""
    # Generate alternative interpretations of a source distro name
    # Because some packages are ambiguous as to name/versions split
    # e.g. ""adns-python-1.1.0"", ""egenix-mx-commercial"", etc.
    # So, we generate each possible interpretation (e.g. ""adns, python-1.1.0""
    # ""adns-python, 1.1.0"", and ""adns-python-1.1.0, no version"").  In practice,
    # the spurious interpretations should be ignored, because in the event
    # there's also an ""adns"" package, the spurious ""python-1.1.0"" version will
    # compare lower than any numeric version number, and is therefore unlikely
    # to match a request for it.  It's still a potential problem, though, and
    # in the long run PyPI and the distutils should go for ""safe"" names and
    # versions in distribution archive names (sdist and bdist).

    parts = basename.split('-')
    if not py_version and any(re.match(r'py\d\.\d$', p) for p in parts[2:]):
        # it is a bdist_dumb, not an sdist -- bail out
        return

    for p in range(1, len(parts) + 1):
        yield Distribution(
            location, metadata, '-'.join(parts[:p]), '-'.join(parts[p:]),
            py_version=py_version, precedence=precedence,
            platform=platform
        )

def egg_info_for_url(url):
    parts = urllib.parse.urlparse(url)
    scheme, server, path, parameters, query, fragment = parts
    base = urllib.parse.unquote(path.split('/')[-1])
    if server == 'sourceforge.net' and base == 'download':  # XXX Yuck
        base = urllib.parse.unquote(path.split('/')[-2])
    if '#' in base:
        base, fragment = base.split('#', 1)
    return base, fragment

def distros_for_location(location, basename, metadata=None):
    """"""Yield egg or source distribution objects based on basename""""""
    if basename.endswith('.egg.zip'):
        basename = basename[:-4]  # strip the .zip
    if basename.endswith('.egg') and '-' in basename:
        # only one, unambiguous interpretation
        return [Distribution.from_location(location, basename, metadata)]
    if basename.endswith('.whl') and '-' in basename:
        wheel = Wheel(basename)
        if not wheel.is_compatible():
            return []
        return [Distribution(
            location=location,
            project_name=wheel.project_name,
            version=wheel.version,
            # Increase priority over eggs.
            precedence=EGG_DIST + 1,
        )]
    if basename.endswith('.exe'):
        win_base, py_ver, platform = parse_bdist_wininst(basename)
        if win_base is not None:
            return interpret_distro_name(
                location, win_base, metadata, py_ver, BINARY_DIST, platform
            )
    # Try source distro extensions (.zip, .tgz, etc.)
    #
    for ext in EXTENSIONS:
        if basename.endswith(ext):
            basename = basename[:-len(ext)]
            return interpret_distro_name(location, basename, metadata)
    return []  # no extension matched","def generate_alternative_interpretations_of_source_distro_name(
        location, basename, metadata, py_version=None, precedence=SOURCE_DIST,
        platform=None
):
    """"""Generate alternative interpretations of a source distro name

    Note: if `location` is a filesystem filename, you should call
    ``pkg_resources.normalize_path()`` on it before passing it to this
    routine!
    """"""
    # Generate alternative interpretations of a source distro name
    # Because some packages are ambiguous as to name/versions split
    # e.g. ""adns-python-1.1.0"", ""egenix-mx-commercial"", etc.
    # So, we generate each possible interpretation (e.g. ""adns, python-1.1.0""
    # ""adns-python, 1.1.0"", and ""adns-python-1.1.0, no version"").  In practice,
    # the spurious interpretations should be ignored, because in the event
    # there's also an ""adns"" package, the spurious ""python-1.1.0"" version will
    # compare lower than any numeric version number, and is therefore unlikely
    # to match a request for it.  It's still a potential problem, though, and
    # in the long run PyPI and the distutils should go for ""safe"" names and
    # versions in distribution archive names (sdist and bdist).

    parts = basename.split('-')
    if not py_version and any(re.match(r'py\d\.\d$', p) for p in parts[2:]):
        # it is a bdist_dumb, not an sdist -- bail out
        return

    for p in range(1, len(parts) + 1):
        yield Distribution(
            location, metadata, '-'.join(parts[:p]), '-'.join(parts[p:]),
            py_version=py_version, precedence=precedence,
            platform=platform
        )

def extract_egg_info_from_url(url):
    parts = urllib.parse.urlparse(url)
    scheme, server, path, parameters, query, fragment = parts
    base = urllib.parse.unquote(path.split('/')[-1])
    if server == 'sourceforge.net' and base == 'download':  # XXX Yuck
        base = urllib.parse.unquote(path.split('/')[-2])
    if '#' in base:
        base, fragment = base.split('#', 1)
    return base, fragment

def yield_egg_or_source_distributions_based_on_basename(location, basename, metadata=None):
    """"""Yield egg or source distribution objects based on basename""""""
    if basename.endswith('.egg.zip'):
        basename = basename[:-4]  # strip the .zip
    if basename.endswith('.egg') and '-' in basename:
        # only one, unambiguous interpretation
        return [Distribution.from_location(location, basename, metadata)]
    if basename.endswith('.whl') and '-' in basename:
        wheel = Wheel(basename)
        if not wheel.is_compatible():
            return []
        return [Distribution(
            location=location,
            project_name=wheel.project_name,
            version=wheel.version,
            # Increase priority over eggs.
            precedence=EGG_DIST + 1,
        )]
    if basename.endswith('.exe'):
        win_base, py_ver, platform = parse_bdist_wininst(basename)
        if win_base is not None:
            return generate_alternative_interpretations_of_source_distro_name(
                location, win_base, metadata, py_ver, BINARY_DIST, platform
            )
    # Try source distro extensions (.zip, .tgz, etc.)
    #
    for ext in EXTENSIONS:
        if basename.endswith(ext):
            basename = basename[:-len(ext)]
            return generate_alternative_interpretations_of_source_distro_name(location, basename, metadata)
    return []  # no extension matched","def yield_egg_or_source_distribution_objects_for_given_url(url, metadata=None):
    """"""Yield egg or source distribution objects that might be found at a URL""""""
    base, fragment = extract_egg_info_from_url(url)
    for dist in yield_egg_or_source_distributions_based_on_basename(url, base, metadata):
        yield dist
    if fragment:
        match = EGG_FRAGMENT.match(fragment)
        if match:
            for dist in generate_alternative_interpretations_of_source_distro_name(
                url, match.group(1), metadata, precedence=CHECKOUT_DIST
            ):
                yield dist","{""interpret_distro_name"": ""generate_alternative_interpretations_of_source_distro_name"", ""egg_info_for_url"": ""extract_egg_info_from_url"", ""distros_for_location"": ""yield_egg_or_source_distributions_based_on_basename""}","def 0(
        location, basename, metadata, py_version=None, precedence=SOURCE_DIST,
        platform=None
):
    """"""Generate alternative interpretations of a source distro name

    Note: if `location` is a filesystem filename, you should call
    ``pkg_resources.normalize_path()`` on it before passing it to this
    routine!
    """"""
    # Generate alternative interpretations of a source distro name
    # Because some packages are ambiguous as to name/versions split
    # e.g. ""adns-python-1.1.0"", ""egenix-mx-commercial"", etc.
    # So, we generate each possible interpretation (e.g. ""adns, python-1.1.0""
    # ""adns-python, 1.1.0"", and ""adns-python-1.1.0, no version"").  In practice,
    # the spurious interpretations should be ignored, because in the event
    # there's also an ""adns"" package, the spurious ""python-1.1.0"" version will
    # compare lower than any numeric version number, and is therefore unlikely
    # to match a request for it.  It's still a potential problem, though, and
    # in the long run PyPI and the distutils should go for ""safe"" names and
    # versions in distribution archive names (sdist and bdist).

    parts = basename.split('-')
    if not py_version and any(re.match(r'py\d\.\d$', p) for p in parts[2:]):
        # it is a bdist_dumb, not an sdist -- bail out
        return

    for p in range(1, len(parts) + 1):
        yield Distribution(
            location, metadata, '-'.join(parts[:p]), '-'.join(parts[p:]),
            py_version=py_version, precedence=precedence,
            platform=platform
        )

def 1(url):
    parts = urllib.parse.urlparse(url)
    scheme, server, path, parameters, query, fragment = parts
    base = urllib.parse.unquote(path.split('/')[-1])
    if server == 'sourceforge.net' and base == 'download':  # XXX Yuck
        base = urllib.parse.unquote(path.split('/')[-2])
    if '#' in base:
        base, fragment = base.split('#', 1)
    return base, fragment

def 2(location, basename, metadata=None):
    """"""Yield egg or source distribution objects based on basename""""""
    if basename.endswith('.egg.zip'):
        basename = basename[:-4]  # strip the .zip
    if basename.endswith('.egg') and '-' in basename:
        # only one, unambiguous interpretation
        return [Distribution.from_location(location, basename, metadata)]
    if basename.endswith('.whl') and '-' in basename:
        wheel = Wheel(basename)
        if not wheel.is_compatible():
            return []
        return [Distribution(
            location=location,
            project_name=wheel.project_name,
            version=wheel.version,
            # Increase priority over eggs.
            precedence=EGG_DIST + 1,
        )]
    if basename.endswith('.exe'):
        win_base, py_ver, platform = parse_bdist_wininst(basename)
        if win_base is not None:
            return 0(
                location, win_base, metadata, py_ver, BINARY_DIST, platform
            )
    # Try source distro extensions (.zip, .tgz, etc.)
    #
    for ext in EXTENSIONS:
        if basename.endswith(ext):
            basename = basename[:-len(ext)]
            return 0(location, basename, metadata)
    return []  # no extension matched","def 3(url, metadata=None):
    """"""Yield egg or source distribution objects that might be found at a URL""""""
    base, fragment = 1(url)
    for dist in 2(url, base, metadata):
        yield dist
    if fragment:
        match = EGG_FRAGMENT.match(fragment)
        if match:
            for dist in 0(
                url, match.group(1), metadata, precedence=CHECKOUT_DIST
            ):
                yield dist","{""interpret_distro_name"": ""0"", ""egg_info_for_url"": ""1"", ""distros_for_location"": ""2""}"
449,449,"def unpack_zipfile(filename, extract_dir, progress_filter=default_filter):
    """"""Unpack zip `filename` to `extract_dir`

    Raises ``UnrecognizedFormat`` if `filename` is not a zipfile (as determined
    by ``zipfile.is_zipfile()``).  See ``unpack_archive()`` for an explanation
    of the `progress_filter` argument.
    """"""

    if not zipfile.is_zipfile(filename):
        raise UnrecognizedFormat(""%s is not a zip file"" % (filename,))

    with zipfile.ZipFile(filename) as z:
        _unpack_zipfile_obj(z, extract_dir, progress_filter)","def _unpack_zipfile_obj(zipfile_obj, extract_dir, progress_filter=default_filter):
    """"""Internal/private API used by other parts of setuptools.
    Similar to ``unpack_zipfile``, but receives an already opened :obj:`zipfile.ZipFile`
    object instead of a filename.
    """"""
    for info in zipfile_obj.infolist():
        name = info.filename

        # don't extract absolute paths or ones with .. in them
        if name.startswith('/') or '..' in name.split('/'):
            continue

        target = os.path.join(extract_dir, *name.split('/'))
        target = progress_filter(name, target)
        if not target:
            continue
        if name.endswith('/'):
            # directory
            ensure_directory(target)
        else:
            # file
            ensure_directory(target)
            data = zipfile_obj.read(info.filename)
            with open(target, 'wb') as f:
                f.write(data)
        unix_attributes = info.external_attr >> 16
        if unix_attributes:
            os.chmod(target, unix_attributes)","def extract_from_zipfile_object(zipfile_obj, extract_dir, progress_filter=default_filter):
    """"""Internal/private API used by other parts of setuptools.
    Similar to ``unpack_zipfile``, but receives an already opened :obj:`zipfile.ZipFile`
    object instead of a filename.
    """"""
    for info in zipfile_obj.infolist():
        name = info.filename

        # don't extract absolute paths or ones with .. in them
        if name.startswith('/') or '..' in name.split('/'):
            continue

        target = os.path.join(extract_dir, *name.split('/'))
        target = progress_filter(name, target)
        if not target:
            continue
        if name.endswith('/'):
            # directory
            ensure_directory(target)
        else:
            # file
            ensure_directory(target)
            data = zipfile_obj.read(info.filename)
            with open(target, 'wb') as f:
                f.write(data)
        unix_attributes = info.external_attr >> 16
        if unix_attributes:
            os.chmod(target, unix_attributes)","def unpack_zip_file_to_given_extract_directory(filename, extract_dir, progress_filter=default_filter):
    """"""Unpack zip `filename` to `extract_dir`

    Raises ``UnrecognizedFormat`` if `filename` is not a zipfile (as determined
    by ``zipfile.is_zipfile()``).  See ``unpack_archive()`` for an explanation
    of the `progress_filter` argument.
    """"""

    if not zipfile.is_zipfile(filename):
        raise UnrecognizedFormat(""%s is not a zip file"" % (filename,))

    with zipfile.ZipFile(filename) as z:
        extract_from_zipfile_object(z, extract_dir, progress_filter)","{""_unpack_zipfile_obj"": ""extract_from_zipfile_object""}","def 0(zipfile_obj, extract_dir, progress_filter=default_filter):
    """"""Internal/private API used by other parts of setuptools.
    Similar to ``unpack_zipfile``, but receives an already opened :obj:`zipfile.ZipFile`
    object instead of a filename.
    """"""
    for info in zipfile_obj.infolist():
        name = info.filename

        # don't extract absolute paths or ones with .. in them
        if name.startswith('/') or '..' in name.split('/'):
            continue

        target = os.path.join(extract_dir, *name.split('/'))
        target = progress_filter(name, target)
        if not target:
            continue
        if name.endswith('/'):
            # directory
            ensure_directory(target)
        else:
            # file
            ensure_directory(target)
            data = zipfile_obj.read(info.filename)
            with open(target, 'wb') as f:
                f.write(data)
        unix_attributes = info.external_attr >> 16
        if unix_attributes:
            os.chmod(target, unix_attributes)","def 1(filename, extract_dir, progress_filter=default_filter):
    """"""Unpack zip `filename` to `extract_dir`

    Raises ``UnrecognizedFormat`` if `filename` is not a zipfile (as determined
    by ``zipfile.is_zipfile()``).  See ``unpack_archive()`` for an explanation
    of the `progress_filter` argument.
    """"""

    if not zipfile.is_zipfile(filename):
        raise UnrecognizedFormat(""%s is not a zip file"" % (filename,))

    with zipfile.ZipFile(filename) as z:
        0(z, extract_dir, progress_filter)","{""_unpack_zipfile_obj"": ""0""}"
450,450,"def _iter_open_tar(tar_obj, extract_dir, progress_filter):
    """"""Emit member-destination pairs from a tar archive.""""""
    # don't do any chowning!
    tar_obj.chown = lambda *args: None

    with contextlib.closing(tar_obj):
        for member in tar_obj:
            name = member.name
            # don't extract absolute paths or ones with .. in them
            if name.startswith('/') or '..' in name.split('/'):
                continue

            prelim_dst = os.path.join(extract_dir, *name.split('/'))

            try:
                member = _resolve_tar_file_or_dir(tar_obj, member)
            except LookupError:
                continue

            final_dst = progress_filter(name, prelim_dst)
            if not final_dst:
                continue

            if final_dst.endswith(os.sep):
                final_dst = final_dst[:-1]

            yield member, final_dst","def _resolve_tar_file_or_dir(tar_obj, tar_member_obj):
    """"""Resolve any links and extract link targets as normal files.""""""
    while tar_member_obj is not None and (
            tar_member_obj.islnk() or tar_member_obj.issym()):
        linkpath = tar_member_obj.linkname
        if tar_member_obj.issym():
            base = posixpath.dirname(tar_member_obj.name)
            linkpath = posixpath.join(base, linkpath)
            linkpath = posixpath.normpath(linkpath)
        tar_member_obj = tar_obj._getmember(linkpath)

    is_file_or_dir = (
        tar_member_obj is not None and
        (tar_member_obj.isfile() or tar_member_obj.isdir())
    )
    if is_file_or_dir:
        return tar_member_obj

    raise LookupError('Got unknown file type')","def resolve_links_and_extract_link_targets_as_normal_files(tar_obj, tar_member_obj):
    """"""Resolve any links and extract link targets as normal files.""""""
    while tar_member_obj is not None and (
            tar_member_obj.islnk() or tar_member_obj.issym()):
        linkpath = tar_member_obj.linkname
        if tar_member_obj.issym():
            base = posixpath.dirname(tar_member_obj.name)
            linkpath = posixpath.join(base, linkpath)
            linkpath = posixpath.normpath(linkpath)
        tar_member_obj = tar_obj._getmember(linkpath)

    is_file_or_dir = (
        tar_member_obj is not None and
        (tar_member_obj.isfile() or tar_member_obj.isdir())
    )
    if is_file_or_dir:
        return tar_member_obj

    raise LookupError('Got unknown file type')","def iterate_and_open_tar(tar_obj, extract_dir, progress_filter):
    """"""Emit member-destination pairs from a tar archive.""""""
    # don't do any chowning!
    tar_obj.chown = lambda *args: None

    with contextlib.closing(tar_obj):
        for member in tar_obj:
            name = member.name
            # don't extract absolute paths or ones with .. in them
            if name.startswith('/') or '..' in name.split('/'):
                continue

            prelim_dst = os.path.join(extract_dir, *name.split('/'))

            try:
                member = resolve_links_and_extract_link_targets_as_normal_files(tar_obj, member)
            except LookupError:
                continue

            final_dst = progress_filter(name, prelim_dst)
            if not final_dst:
                continue

            if final_dst.endswith(os.sep):
                final_dst = final_dst[:-1]

            yield member, final_dst","{""_resolve_tar_file_or_dir"": ""resolve_links_and_extract_link_targets_as_normal_files""}","def 0(tar_obj, tar_member_obj):
    """"""Resolve any links and extract link targets as normal files.""""""
    while tar_member_obj is not None and (
            tar_member_obj.islnk() or tar_member_obj.issym()):
        linkpath = tar_member_obj.linkname
        if tar_member_obj.issym():
            base = posixpath.dirname(tar_member_obj.name)
            linkpath = posixpath.join(base, linkpath)
            linkpath = posixpath.normpath(linkpath)
        tar_member_obj = tar_obj._getmember(linkpath)

    is_file_or_dir = (
        tar_member_obj is not None and
        (tar_member_obj.isfile() or tar_member_obj.isdir())
    )
    if is_file_or_dir:
        return tar_member_obj

    raise LookupError('Got unknown file type')","def 1(tar_obj, extract_dir, progress_filter):
    """"""Emit member-destination pairs from a tar archive.""""""
    # don't do any chowning!
    tar_obj.chown = lambda *args: None

    with contextlib.closing(tar_obj):
        for member in tar_obj:
            name = member.name
            # don't extract absolute paths or ones with .. in them
            if name.startswith('/') or '..' in name.split('/'):
                continue

            prelim_dst = os.path.join(extract_dir, *name.split('/'))

            try:
                member = 0(tar_obj, member)
            except LookupError:
                continue

            final_dst = progress_filter(name, prelim_dst)
            if not final_dst:
                continue

            if final_dst.endswith(os.sep):
                final_dst = final_dst[:-1]

            yield member, final_dst","{""_resolve_tar_file_or_dir"": ""0""}"
451,451,"def unpack_tarfile(filename, extract_dir, progress_filter=default_filter):
    """"""Unpack tar/tar.gz/tar.bz2 `filename` to `extract_dir`

    Raises ``UnrecognizedFormat`` if `filename` is not a tarfile (as determined
    by ``tarfile.open()``).  See ``unpack_archive()`` for an explanation
    of the `progress_filter` argument.
    """"""
    try:
        tarobj = tarfile.open(filename)
    except tarfile.TarError as e:
        raise UnrecognizedFormat(
            ""%s is not a compressed or uncompressed tar file"" % (filename,)
        ) from e

    for member, final_dst in _iter_open_tar(
            tarobj, extract_dir, progress_filter,
    ):
        try:
            # XXX Ugh
            tarobj._extract_member(member, final_dst)
        except tarfile.ExtractError:
            # chown/chmod/mkfifo/mknode/makedev failed
            pass

    return True","def _iter_open_tar(tar_obj, extract_dir, progress_filter):
    """"""Emit member-destination pairs from a tar archive.""""""
    # don't do any chowning!
    tar_obj.chown = lambda *args: None

    with contextlib.closing(tar_obj):
        for member in tar_obj:
            name = member.name
            # don't extract absolute paths or ones with .. in them
            if name.startswith('/') or '..' in name.split('/'):
                continue

            prelim_dst = os.path.join(extract_dir, *name.split('/'))

            try:
                member = _resolve_tar_file_or_dir(tar_obj, member)
            except LookupError:
                continue

            final_dst = progress_filter(name, prelim_dst)
            if not final_dst:
                continue

            if final_dst.endswith(os.sep):
                final_dst = final_dst[:-1]

            yield member, final_dst","def iterate_and_open_tar(tar_obj, extract_dir, progress_filter):
    """"""Emit member-destination pairs from a tar archive.""""""
    # don't do any chowning!
    tar_obj.chown = lambda *args: None

    with contextlib.closing(tar_obj):
        for member in tar_obj:
            name = member.name
            # don't extract absolute paths or ones with .. in them
            if name.startswith('/') or '..' in name.split('/'):
                continue

            prelim_dst = os.path.join(extract_dir, *name.split('/'))

            try:
                member = _resolve_tar_file_or_dir(tar_obj, member)
            except LookupError:
                continue

            final_dst = progress_filter(name, prelim_dst)
            if not final_dst:
                continue

            if final_dst.endswith(os.sep):
                final_dst = final_dst[:-1]

            yield member, final_dst","def unpack_tar_or_targz_or_tarbz2_file_to_given_extract_directory(filename, extract_dir, progress_filter=default_filter):
    """"""Unpack tar/tar.gz/tar.bz2 `filename` to `extract_dir`

    Raises ``UnrecognizedFormat`` if `filename` is not a tarfile (as determined
    by ``tarfile.open()``).  See ``unpack_archive()`` for an explanation
    of the `progress_filter` argument.
    """"""
    try:
        tarobj = tarfile.open(filename)
    except tarfile.TarError as e:
        raise UnrecognizedFormat(
            ""%s is not a compressed or uncompressed tar file"" % (filename,)
        ) from e

    for member, final_dst in iterate_and_open_tar(
            tarobj, extract_dir, progress_filter,
    ):
        try:
            # XXX Ugh
            tarobj._extract_member(member, final_dst)
        except tarfile.ExtractError:
            # chown/chmod/mkfifo/mknode/makedev failed
            pass

    return True","{""_iter_open_tar"": ""iterate_and_open_tar""}","def 0(tar_obj, extract_dir, progress_filter):
    """"""Emit member-destination pairs from a tar archive.""""""
    # don't do any chowning!
    tar_obj.chown = lambda *args: None

    with contextlib.closing(tar_obj):
        for member in tar_obj:
            name = member.name
            # don't extract absolute paths or ones with .. in them
            if name.startswith('/') or '..' in name.split('/'):
                continue

            prelim_dst = os.path.join(extract_dir, *name.split('/'))

            try:
                member = _resolve_tar_file_or_dir(tar_obj, member)
            except LookupError:
                continue

            final_dst = progress_filter(name, prelim_dst)
            if not final_dst:
                continue

            if final_dst.endswith(os.sep):
                final_dst = final_dst[:-1]

            yield member, final_dst","def 1(filename, extract_dir, progress_filter=default_filter):
    """"""Unpack tar/tar.gz/tar.bz2 `filename` to `extract_dir`

    Raises ``UnrecognizedFormat`` if `filename` is not a tarfile (as determined
    by ``tarfile.open()``).  See ``unpack_archive()`` for an explanation
    of the `progress_filter` argument.
    """"""
    try:
        tarobj = tarfile.open(filename)
    except tarfile.TarError as e:
        raise UnrecognizedFormat(
            ""%s is not a compressed or uncompressed tar file"" % (filename,)
        ) from e

    for member, final_dst in 0(
            tarobj, extract_dir, progress_filter,
    ):
        try:
            # XXX Ugh
            tarobj._extract_member(member, final_dst)
        except tarfile.ExtractError:
            # chown/chmod/mkfifo/mknode/makedev failed
            pass

    return True","{""_iter_open_tar"": ""0""}"
452,452,"def find_parent_package(
    packages: List[str], package_dir: Dict[str, str], root_dir: _Path
) -> Optional[str]:
    """"""Find the parent package that is not a namespace.""""""
    packages = sorted(packages, key=len)
    common_ancestors = []
    for i, name in enumerate(packages):
        if not all(n.startswith(f""{name}."") for n in packages[i+1:]):
            # Since packages are sorted by length, this condition is able
            # to find a list of all common ancestors.
            # When there is divergence (e.g. multiple root packages)
            # the list will be empty
            break
        common_ancestors.append(name)

    for name in common_ancestors:
        pkg_path = find_package_path(name, package_dir, root_dir)
        init = os.path.join(pkg_path, ""__init__.py"")
        if os.path.isfile(init):
            return name

    return None","def enumerate():
    """"""Return a list of all Thread objects currently alive.

    The list includes daemonic threads, dummy thread objects created by
    current_thread(), and the main thread. It excludes terminated threads and
    threads that have not yet been started.

    """"""
    with _active_limbo_lock:
        return list(_active.values()) + list(_limbo.values())

def find_package_path(name: str, package_dir: Dict[str, str], root_dir: _Path) -> str:
    """"""Given a package name, return the path where it should be found on
    disk, considering the ``package_dir`` option.

    >>> path = find_package_path(""my.pkg"", {"""": ""root/is/nested""}, ""."")
    >>> path.replace(os.sep, ""/"")
    './root/is/nested/my/pkg'

    >>> path = find_package_path(""my.pkg"", {""my"": ""root/is/nested""}, ""."")
    >>> path.replace(os.sep, ""/"")
    './root/is/nested/pkg'

    >>> path = find_package_path(""my.pkg"", {""my.pkg"": ""root/is/nested""}, ""."")
    >>> path.replace(os.sep, ""/"")
    './root/is/nested'

    >>> path = find_package_path(""other.pkg"", {""my.pkg"": ""root/is/nested""}, ""."")
    >>> path.replace(os.sep, ""/"")
    './other/pkg'
    """"""
    parts = name.split(""."")
    for i in range(len(parts), 0, -1):
        # Look backwards, the most specific package_dir first
        partial_name = ""."".join(parts[:i])
        if partial_name in package_dir:
            parent = package_dir[partial_name]
            return os.path.join(root_dir, parent, *parts[i:])

    parent = package_dir.get("""") or """"
    return os.path.join(root_dir, *parent.split(""/""), *parts)","def get_alive_threads():
    """"""Return a list of all Thread objects currently alive.

    The list includes daemonic threads, dummy thread objects created by
    current_thread(), and the main thread. It excludes terminated threads and
    threads that have not yet been started.

    """"""
    with _active_limbo_lock:
        return list(_active.values()) + list(_limbo.values())

def resolve_package_path_with_package_dir(name: str, package_dir: Dict[str, str], root_dir: _Path) -> str:
    """"""Given a package name, return the path where it should be found on
    disk, considering the ``package_dir`` option.

    >>> path = resolve_package_path_with_package_dir(""my.pkg"", {"""": ""root/is/nested""}, ""."")
    >>> path.replace(os.sep, ""/"")
    './root/is/nested/my/pkg'

    >>> path = resolve_package_path_with_package_dir(""my.pkg"", {""my"": ""root/is/nested""}, ""."")
    >>> path.replace(os.sep, ""/"")
    './root/is/nested/pkg'

    >>> path = resolve_package_path_with_package_dir(""my.pkg"", {""my.pkg"": ""root/is/nested""}, ""."")
    >>> path.replace(os.sep, ""/"")
    './root/is/nested'

    >>> path = resolve_package_path_with_package_dir(""other.pkg"", {""my.pkg"": ""root/is/nested""}, ""."")
    >>> path.replace(os.sep, ""/"")
    './other/pkg'
    """"""
    parts = name.split(""."")
    for i in range(len(parts), 0, -1):
        # Look backwards, the most specific package_dir first
        partial_name = ""."".join(parts[:i])
        if partial_name in package_dir:
            parent = package_dir[partial_name]
            return os.path.join(root_dir, parent, *parts[i:])

    parent = package_dir.get("""") or """"
    return os.path.join(root_dir, *parent.split(""/""), *parts)","def find_parent_package_with_sorted_and_common_ancestors(
    packages: List[str], package_dir: Dict[str, str], root_dir: _Path
) -> Optional[str]:
    """"""Find the parent package that is not a namespace.""""""
    packages = sorted(packages, key=len)
    common_ancestors = []
    for i, name in get_alive_threads(packages):
        if not all(n.startswith(f""{name}."") for n in packages[i+1:]):
            # Since packages are sorted by length, this condition is able
            # to find a list of all common ancestors.
            # When there is divergence (e.g. multiple root packages)
            # the list will be empty
            break
        common_ancestors.append(name)

    for name in common_ancestors:
        pkg_path = resolve_package_path_with_package_dir(name, package_dir, root_dir)
        init = os.path.join(pkg_path, ""__init__.py"")
        if os.path.isfile(init):
            return name

    return None","{""enumerate"": ""get_alive_threads"", ""find_package_path"": ""resolve_package_path_with_package_dir""}","def 0():
    """"""Return a list of all Thread objects currently alive.

    The list includes daemonic threads, dummy thread objects created by
    current_thread(), and the main thread. It excludes terminated threads and
    threads that have not yet been started.

    """"""
    with _active_limbo_lock:
        return list(_active.values()) + list(_limbo.values())

def 1(name: str, package_dir: Dict[str, str], root_dir: _Path) -> str:
    """"""Given a package name, return the path where it should be found on
    disk, considering the ``package_dir`` option.

    >>> path = 1(""my.pkg"", {"""": ""root/is/nested""}, ""."")
    >>> path.replace(os.sep, ""/"")
    './root/is/nested/my/pkg'

    >>> path = 1(""my.pkg"", {""my"": ""root/is/nested""}, ""."")
    >>> path.replace(os.sep, ""/"")
    './root/is/nested/pkg'

    >>> path = 1(""my.pkg"", {""my.pkg"": ""root/is/nested""}, ""."")
    >>> path.replace(os.sep, ""/"")
    './root/is/nested'

    >>> path = 1(""other.pkg"", {""my.pkg"": ""root/is/nested""}, ""."")
    >>> path.replace(os.sep, ""/"")
    './other/pkg'
    """"""
    parts = name.split(""."")
    for i in range(len(parts), 0, -1):
        # Look backwards, the most specific package_dir first
        partial_name = ""."".join(parts[:i])
        if partial_name in package_dir:
            parent = package_dir[partial_name]
            return os.path.join(root_dir, parent, *parts[i:])

    parent = package_dir.get("""") or """"
    return os.path.join(root_dir, *parent.split(""/""), *parts)","def 2(
    packages: List[str], package_dir: Dict[str, str], root_dir: _Path
) -> Optional[str]:
    """"""Find the parent package that is not a namespace.""""""
    packages = sorted(packages, key=len)
    common_ancestors = []
    for i, name in 0(packages):
        if not all(n.startswith(f""{name}."") for n in packages[i+1:]):
            # Since packages are sorted by length, this condition is able
            # to find a list of all common ancestors.
            # When there is divergence (e.g. multiple root packages)
            # the list will be empty
            break
        common_ancestors.append(name)

    for name in common_ancestors:
        pkg_path = 1(name, package_dir, root_dir)
        init = os.path.join(pkg_path, ""__init__.py"")
        if os.path.isfile(init):
            return name

    return None","{""enumerate"": ""0"", ""find_package_path"": ""1""}"
453,453,"def _find_all_simple(path):
    """"""
    Find all files under 'path'
    """"""
    all_unique = _UniqueDirs.filter(os.walk(path, followlinks=True))
    results = (
        os.path.join(base, file) for base, dirs, files in all_unique for file in files
    )
    return filter(os.path.isfile, results)","def filter(names, pat):
    """"""Construct a list from those elements of the iterable NAMES that match PAT.""""""
    result = []
    pat = os.path.normcase(pat)
    match = _compile_pattern(pat)
    if os.path is posixpath:
        # normcase on posix is NOP. Optimize it away from the loop.
        for name in names:
            if match(name):
                result.append(name)
    else:
        for name in names:
            if match(os.path.normcase(name)):
                result.append(name)
    return result","def filter_names(names, pat):
    """"""Construct a list from those elements of the iterable NAMES that match PAT.""""""
    result = []
    pat = os.path.normcase(pat)
    match = _compile_pattern(pat)
    if os.path is posixpath:
        # normcase on posix is NOP. Optimize it away from the loop.
        for name in names:
            if match(name):
                result.append(name)
    else:
        for name in names:
            if match(os.path.normcase(name)):
                result.append(name)
    return result","def find_all_files_under_path(path):
    """"""
    Find all files under 'path'
    """"""
    all_unique = _UniqueDirs.filter_names(os.walk(path, followlinks=True))
    results = (
        os.path.join(base, file) for base, dirs, files in all_unique for file in files
    )
    return filter_names(os.path.isfile, results)","{""filter"": ""filter_names""}","def 0(names, pat):
    """"""Construct a list from those elements of the iterable NAMES that match PAT.""""""
    result = []
    pat = os.path.normcase(pat)
    match = _compile_pattern(pat)
    if os.path is posixpath:
        # normcase on posix is NOP. Optimize it away from the loop.
        for name in names:
            if match(name):
                result.append(name)
    else:
        for name in names:
            if match(os.path.normcase(name)):
                result.append(name)
    return result","def 1(path):
    """"""
    Find all files under 'path'
    """"""
    all_unique = _UniqueDirs.0(os.walk(path, followlinks=True))
    results = (
        os.path.join(base, file) for base, dirs, files in all_unique for file in files
    )
    return 0(os.path.isfile, results)","{""filter"": ""0""}"
454,454,"def strip_marker(req):
    """"""
    Return a new requirement without the environment marker to avoid
    calling pip with something like `babel; extra == ""i18n""`, which
    would always be ignored.
    """"""
    # create a copy to avoid mutating the input
    req = pkg_resources.Requirement.parse(str(req))
    req.marker = None
    return req","def str(val):
    """"""Convert float to string, taking the locale into account.""""""
    return _format(""%.12g"", val)","def locale_aware_float_to_string(val):
    """"""Convert float to locale_aware_float_to_stringing, taking the locale into account.""""""
    return _format(""%.12g"", val)","def locale_aware_float_to_stringip_marker(req):
    """"""
    Return a new requirement without the environment marker to avoid
    calling pip with something like `babel; extra == ""i18n""`, which
    would always be ignored.
    """"""
    # create a copy to avoid mutating the input
    req = pkg_resources.Requirement.parse(locale_aware_float_to_string(req))
    req.marker = None
    return req","{""str"": ""locale_aware_float_to_string""}","def 0(val):
    """"""Convert float to string, taking the locale into account.""""""
    return _format(""%.12g"", val)","def 1(req):
    """"""
    Return a new requirement without the environment marker to avoid
    calling pip with something like `babel; extra == ""i18n""`, which
    would always be ignored.
    """"""
    # create a copy to avoid mutating the input
    req = pkg_resources.Requirement.parse(0(req))
    req.marker = None
    return req","{""str"": ""0""}"
455,455,"def hide_setuptools():
    """"""
    Remove references to setuptools' modules from sys.modules to allow the
    invocation to import the most appropriate setuptools. This technique is
    necessary to avoid issues such as #315 where setuptools upgrading itself
    would fail to find a function declared in the metadata.
    """"""
    _distutils_hack = sys.modules.get('_distutils_hack', None)
    if _distutils_hack is not None:
        _distutils_hack.remove_shim()

    modules = filter(_needs_hiding, sys.modules)
    _clear_modules(modules)","def filter(names, pat):
    """"""Construct a list from those elements of the iterable NAMES that match PAT.""""""
    result = []
    pat = os.path.normcase(pat)
    match = _compile_pattern(pat)
    if os.path is posixpath:
        # normcase on posix is NOP. Optimize it away from the loop.
        for name in names:
            if match(name):
                result.append(name)
    else:
        for name in names:
            if match(os.path.normcase(name)):
                result.append(name)
    return result

def _clear_modules(module_names):
    for mod_name in list(module_names):
        del sys.modules[mod_name]","def filter_names(names, pat):
    """"""Construct a list from those elements of the iterable NAMES that match PAT.""""""
    result = []
    pat = os.path.normcase(pat)
    match = _compile_pattern(pat)
    if os.path is posixpath:
        # normcase on posix is NOP. Optimize it away from the loop.
        for name in names:
            if match(name):
                result.append(name)
    else:
        for name in names:
            if match(os.path.normcase(name)):
                result.append(name)
    return result

def clear_modules_list(module_names):
    for mod_name in list(module_names):
        del sys.modules[mod_name]","def remove_references_to_setuptools_modules_and_allow_appropriate_import():
    """"""
    Remove references to setuptools' modules from sys.modules to allow the
    invocation to import the most appropriate setuptools. This technique is
    necessary to avoid issues such as #315 where setuptools upgrading itself
    would fail to find a function declared in the metadata.
    """"""
    _distutils_hack = sys.modules.get('_distutils_hack', None)
    if _distutils_hack is not None:
        _distutils_hack.remove_shim()

    modules = filter_names(_needs_hiding, sys.modules)
    clear_modules_list(modules)","{""filter"": ""filter_names"", ""_clear_modules"": ""clear_modules_list""}","def 0(names, pat):
    """"""Construct a list from those elements of the iterable NAMES that match PAT.""""""
    result = []
    pat = os.path.normcase(pat)
    match = _compile_pattern(pat)
    if os.path is posixpath:
        # normcase on posix is NOP. Optimize it away from the loop.
        for name in names:
            if match(name):
                result.append(name)
    else:
        for name in names:
            if match(os.path.normcase(name)):
                result.append(name)
    return result

def 1(module_names):
    for mod_name in list(module_names):
        del sys.modules[mod_name]","def 2():
    """"""
    Remove references to setuptools' modules from sys.modules to allow the
    invocation to import the most appropriate setuptools. This technique is
    necessary to avoid issues such as #315 where setuptools upgrading itself
    would fail to find a function declared in the metadata.
    """"""
    _distutils_hack = sys.modules.get('_distutils_hack', None)
    if _distutils_hack is not None:
        _distutils_hack.remove_shim()

    modules = 0(_needs_hiding, sys.modules)
    1(modules)","{""filter"": ""0"", ""_clear_modules"": ""1""}"
456,456,"def _have_cython():
    """"""
    Return True if Cython can be imported.
    """"""
    cython_impl = 'Cython.Distutils.build_ext'
    try:
        # from (cython_impl) import build_ext
        __import__(cython_impl, fromlist=['build_ext']).build_ext
        return True
    except Exception:
        pass
    return False","def __import__(name, globals=None, locals=None, fromlist=(), level=0):
    """"""Import a module.

    The 'globals' argument is used to infer where the import is occurring from
    to handle relative imports. The 'locals' argument is ignored. The
    'fromlist' argument specifies what should exist as attributes on the module
    being imported (e.g. ``from module import <fromlist>``).  The 'level'
    argument represents the package location to import from in a relative
    import (e.g. ``from ..pkg import mod`` would have a 'level' of 2).

    """"""
    if level == 0:
        module = _gcd_import(name)
    else:
        globals_ = globals if globals is not None else {}
        package = _calc___package__(globals_)
        module = _gcd_import(name, package, level)
    if not fromlist:
        # Return up to the first dot in 'name'. This is complicated by the fact
        # that 'name' may be relative.
        if level == 0:
            return _gcd_import(name.partition('.')[0])
        elif not name:
            return module
        else:
            # Figure out where to slice the module's name up to the first dot
            # in 'name'.
            cut_off = len(name) - len(name.partition('.')[0])
            # Slice end needs to be positive to alleviate need to special-case
            # when ``'.' not in name``.
            return sys.modules[module.__name__[:len(module.__name__)-cut_off]]
    elif hasattr(module, '__path__'):
        return _handle_fromlist(module, fromlist, _gcd_import)
    else:
        return module","def import_module(name, globals=None, locals=None, fromlist=(), level=0):
    """"""Import a module.

    The 'globals' argument is used to infer where the import is occurring from
    to handle relative imports. The 'locals' argument is ignored. The
    'fromlist' argument specifies what should exist as attributes on the module
    being imported (e.g. ``from module import <fromlist>``).  The 'level'
    argument represents the package location to import from in a relative
    import (e.g. ``from ..pkg import mod`` would have a 'level' of 2).

    """"""
    if level == 0:
        module = _gcd_import(name)
    else:
        globals_ = globals if globals is not None else {}
        package = _calc___package__(globals_)
        module = _gcd_import(name, package, level)
    if not fromlist:
        # Return up to the first dot in 'name'. This is complicated by the fact
        # that 'name' may be relative.
        if level == 0:
            return _gcd_import(name.partition('.')[0])
        elif not name:
            return module
        else:
            # Figure out where to slice the module's name up to the first dot
            # in 'name'.
            cut_off = len(name) - len(name.partition('.')[0])
            # Slice end needs to be positive to alleviate need to special-case
            # when ``'.' not in name``.
            return sys.modules[module.__name__[:len(module.__name__)-cut_off]]
    elif hasattr(module, '__path__'):
        return _handle_fromlist(module, fromlist, _gcd_import)
    else:
        return module","def check_if_cython_can_be_imported_and_return_true_or_false():
    """"""
    Return True if Cython can be imported.
    """"""
    cython_impl = 'Cython.Distutils.build_ext'
    try:
        # from (cython_impl) import build_ext
        import_module(cython_impl, fromlist=['build_ext']).build_ext
        return True
    except Exception:
        pass
    return False","{""__import__"": ""import_module""}","def 0(name, globals=None, locals=None, fromlist=(), level=0):
    """"""Import a module.

    The 'globals' argument is used to infer where the import is occurring from
    to handle relative imports. The 'locals' argument is ignored. The
    'fromlist' argument specifies what should exist as attributes on the module
    being imported (e.g. ``from module import <fromlist>``).  The 'level'
    argument represents the package location to import from in a relative
    import (e.g. ``from ..pkg import mod`` would have a 'level' of 2).

    """"""
    if level == 0:
        module = _gcd_import(name)
    else:
        globals_ = globals if globals is not None else {}
        package = _calc___package__(globals_)
        module = _gcd_import(name, package, level)
    if not fromlist:
        # Return up to the first dot in 'name'. This is complicated by the fact
        # that 'name' may be relative.
        if level == 0:
            return _gcd_import(name.partition('.')[0])
        elif not name:
            return module
        else:
            # Figure out where to slice the module's name up to the first dot
            # in 'name'.
            cut_off = len(name) - len(name.partition('.')[0])
            # Slice end needs to be positive to alleviate need to special-case
            # when ``'.' not in name``.
            return sys.modules[module.__name__[:len(module.__name__)-cut_off]]
    elif hasattr(module, '__path__'):
        return _handle_fromlist(module, fromlist, _gcd_import)
    else:
        return module","def 1():
    """"""
    Return True if Cython can be imported.
    """"""
    cython_impl = 'Cython.Distutils.build_ext'
    try:
        # from (cython_impl) import build_ext
        0(cython_impl, fromlist=['build_ext']).build_ext
        return True
    except Exception:
        pass
    return False","{""__import__"": ""0""}"
457,457,"def get_unpatched_class(cls):
    """"""Protect against re-patching the distutils if reloaded

    Also ensures that no other distutils extension monkeypatched the distutils
    first.
    """"""
    external_bases = (
        cls
        for cls in _get_mro(cls)
        if not cls.__module__.startswith('setuptools')
    )
    base = next(external_bases)
    if not base.__module__.startswith('distutils'):
        msg = ""distutils has already been patched by %r"" % cls
        raise AssertionError(msg)
    return base","def _get_mro(cls):
    """"""
    Returns the bases classes for cls sorted by the MRO.

    Works around an issue on Jython where inspect.getmro will not return all
    base classes if multiple classes share the same name. Instead, this
    function will return a tuple containing the class itself, and the contents
    of cls.__bases__. See https://github.com/pypa/setuptools/issues/1024.
    """"""
    if platform.python_implementation() == ""Jython"":
        return (cls,) + cls.__bases__
    return inspect.getmro(cls)","def get_sorted_mro_of_bases(cls):
    """"""
    Returns the bases classes for cls sorted by the MRO.

    Works around an issue on Jython where inspect.getmro will not return all
    base classes if multiple classes share the same name. Instead, this
    function will return a tuple containing the class itself, and the contents
    of cls.__bases__. See https://github.com/pypa/setuptools/issues/1024.
    """"""
    if platform.python_implementation() == ""Jython"":
        return (cls,) + cls.__bases__
    return inspect.getmro(cls)","def protect_against_repatching_distutils_and_ensure_no_prior_patch(cls):
    """"""Protect against re-patching the distutils if reloaded

    Also ensures that no other distutils extension monkeypatched the distutils
    first.
    """"""
    external_bases = (
        cls
        for cls in get_sorted_mro_of_bases(cls)
        if not cls.__module__.startswith('setuptools')
    )
    base = next(external_bases)
    if not base.__module__.startswith('distutils'):
        msg = ""distutils has already been patched by %r"" % cls
        raise AssertionError(msg)
    return base","{""_get_mro"": ""get_sorted_mro_of_bases""}","def 0(cls):
    """"""
    Returns the bases classes for cls sorted by the MRO.

    Works around an issue on Jython where inspect.getmro will not return all
    base classes if multiple classes share the same name. Instead, this
    function will return a tuple containing the class itself, and the contents
    of cls.__bases__. See https://github.com/pypa/setuptools/issues/1024.
    """"""
    if platform.python_implementation() == ""Jython"":
        return (cls,) + cls.__bases__
    return inspect.getmro(cls)","def 1(cls):
    """"""Protect against re-patching the distutils if reloaded

    Also ensures that no other distutils extension monkeypatched the distutils
    first.
    """"""
    external_bases = (
        cls
        for cls in 0(cls)
        if not cls.__module__.startswith('setuptools')
    )
    base = next(external_bases)
    if not base.__module__.startswith('distutils'):
        msg = ""distutils has already been patched by %r"" % cls
        raise AssertionError(msg)
    return base","{""_get_mro"": ""0""}"
458,458,"def _msvc14_find_vc2015():
    """"""Python 3.8 ""distutils/_msvccompiler.py"" backport""""""
    try:
        key = winreg.OpenKey(
            winreg.HKEY_LOCAL_MACHINE,
            r""Software\Microsoft\VisualStudio\SxS\VC7"",
            0,
            winreg.KEY_READ | winreg.KEY_WOW64_32KEY
        )
    except OSError:
        return None, None

    best_version = 0
    best_dir = None
    with key:
        for i in itertools.count():
            try:
                v, vc_dir, vt = winreg.EnumValue(key, i)
            except OSError:
                break
            if v and vt == winreg.REG_SZ and isdir(vc_dir):
                try:
                    version = int(float(v))
                except (ValueError, TypeError):
                    continue
                if version >= 14 and version > best_version:
                    best_version, best_dir = version, vc_dir
    return best_version, best_dir","def isdir(s):
    """"""Return true if the pathname refers to an existing directory.""""""
    try:
        st = os.stat(s)
    except (OSError, ValueError):
        return False
    return stat.S_ISDIR(st.st_mode)","def check_if_path_is_existing_directory(s):
    """"""Return true if the pathname refers to an existing directory.""""""
    try:
        st = os.stat(s)
    except (OSError, ValueError):
        return False
    return stat.S_ISDIR(st.st_mode)","def find_vc2015_for_msvc14_and_return_best_version_and_directory():
    """"""Python 3.8 ""distutils/_msvccompiler.py"" backport""""""
    try:
        key = winreg.OpenKey(
            winreg.HKEY_LOCAL_MACHINE,
            r""Software\Microsoft\VisualStudio\SxS\VC7"",
            0,
            winreg.KEY_READ | winreg.KEY_WOW64_32KEY
        )
    except OSError:
        return None, None

    best_version = 0
    best_dir = None
    with key:
        for i in itertools.count():
            try:
                v, vc_dir, vt = winreg.EnumValue(key, i)
            except OSError:
                break
            if v and vt == winreg.REG_SZ and check_if_path_is_existing_directory(vc_dir):
                try:
                    version = int(float(v))
                except (ValueError, TypeError):
                    continue
                if version >= 14 and version > best_version:
                    best_version, best_dir = version, vc_dir
    return best_version, best_dir","{""isdir"": ""check_if_path_is_existing_directory""}","def 0(s):
    """"""Return true if the pathname refers to an existing directory.""""""
    try:
        st = os.stat(s)
    except (OSError, ValueError):
        return False
    return stat.S_ISDIR(st.st_mode)","def 1():
    """"""Python 3.8 ""distutils/_msvccompiler.py"" backport""""""
    try:
        key = winreg.OpenKey(
            winreg.HKEY_LOCAL_MACHINE,
            r""Software\Microsoft\VisualStudio\SxS\VC7"",
            0,
            winreg.KEY_READ | winreg.KEY_WOW64_32KEY
        )
    except OSError:
        return None, None

    best_version = 0
    best_dir = None
    with key:
        for i in itertools.count():
            try:
                v, vc_dir, vt = winreg.EnumValue(key, i)
            except OSError:
                break
            if v and vt == winreg.REG_SZ and 0(vc_dir):
                try:
                    version = int(float(v))
                except (ValueError, TypeError):
                    continue
                if version >= 14 and version > best_version:
                    best_version, best_dir = version, vc_dir
    return best_version, best_dir","{""isdir"": ""0""}"
459,459,"def msvc14_get_vc_env(plat_spec):
    """"""
    Patched ""distutils._msvccompiler._get_vc_env"" for support extra
    Microsoft Visual C++ 14.X compilers.

    Set environment without use of ""vcvarsall.bat"".

    Parameters
    ----------
    plat_spec: str
        Target architecture.

    Return
    ------
    dict
        environment
    """"""

    # Always use backport from CPython 3.8
    try:
        return _msvc14_get_vc_env(plat_spec)
    except distutils.errors.DistutilsPlatformError as exc:
        _augment_exception(exc, 14.0)
        raise","def _augment_exception(exc, version, arch=''):
    """"""
    Add details to the exception message to help guide the user
    as to what action will resolve it.
    """"""
    # Error if MSVC++ directory not found or environment not set
    message = exc.args[0]

    if ""vcvarsall"" in message.lower() or ""visual c"" in message.lower():
        # Special error message if MSVC++ not installed
        tmpl = 'Microsoft Visual C++ {version:0.1f} or greater is required.'
        message = tmpl.format(**locals())
        msdownload = 'www.microsoft.com/download/details.aspx?id=%d'
        if version == 9.0:
            if arch.lower().find('ia64') > -1:
                # For VC++ 9.0, if IA64 support is needed, redirect user
                # to Windows SDK 7.0.
                # Note: No download link available from Microsoft.
                message += ' Get it with ""Microsoft Windows SDK 7.0""'
            else:
                # For VC++ 9.0 redirect user to Vc++ for Python 2.7 :
                # This redirection link is maintained by Microsoft.
                # Contact vspython@microsoft.com if it needs updating.
                message += ' Get it from http://aka.ms/vcpython27'
        elif version == 10.0:
            # For VC++ 10.0 Redirect user to Windows SDK 7.1
            message += ' Get it with ""Microsoft Windows SDK 7.1"": '
            message += msdownload % 8279
        elif version >= 14.0:
            # For VC++ 14.X Redirect user to latest Visual C++ Build Tools
            message += (' Get it with ""Microsoft C++ Build Tools"": '
                        r'https://visualstudio.microsoft.com'
                        r'/visual-cpp-build-tools/')

    exc.args = (message, )

def _msvc14_get_vc_env(plat_spec):
    """"""Python 3.8 ""distutils/_msvccompiler.py"" backport""""""
    if ""DISTUTILS_USE_SDK"" in environ:
        return {
            key.lower(): value
            for key, value in environ.items()
        }

    vcvarsall, vcruntime = _msvc14_find_vcvarsall(plat_spec)
    if not vcvarsall:
        raise distutils.errors.DistutilsPlatformError(
            ""Unable to find vcvarsall.bat""
        )

    try:
        out = subprocess.check_output(
            'cmd /u /c ""{}"" {} && set'.format(vcvarsall, plat_spec),
            stderr=subprocess.STDOUT,
        ).decode('utf-16le', errors='replace')
    except subprocess.CalledProcessError as exc:
        raise distutils.errors.DistutilsPlatformError(
            ""Error executing {}"".format(exc.cmd)
        ) from exc

    env = {
        key.lower(): value
        for key, _, value in
        (line.partition('=') for line in out.splitlines())
        if key and value
    }

    if vcruntime:
        env['py_vcruntime_redist'] = vcruntime
    return env","def add_details_to_exception(exc, version, arch=''):
    """"""
    Add details to the exception message to help guide the user
    as to what action will resolve it.
    """"""
    # Error if MSVC++ directory not found or environment not set
    message = exc.args[0]

    if ""vcvarsall"" in message.lower() or ""visual c"" in message.lower():
        # Special error message if MSVC++ not installed
        tmpl = 'Microsoft Visual C++ {version:0.1f} or greater is required.'
        message = tmpl.format(**locals())
        msdownload = 'www.microsoft.com/download/details.aspx?id=%d'
        if version == 9.0:
            if arch.lower().find('ia64') > -1:
                # For VC++ 9.0, if IA64 support is needed, redirect user
                # to Windows SDK 7.0.
                # Note: No download link available from Microsoft.
                message += ' Get it with ""Microsoft Windows SDK 7.0""'
            else:
                # For VC++ 9.0 redirect user to Vc++ for Python 2.7 :
                # This redirection link is maintained by Microsoft.
                # Contact vspython@microsoft.com if it needs updating.
                message += ' Get it from http://aka.ms/vcpython27'
        elif version == 10.0:
            # For VC++ 10.0 Redirect user to Windows SDK 7.1
            message += ' Get it with ""Microsoft Windows SDK 7.1"": '
            message += msdownload % 8279
        elif version >= 14.0:
            # For VC++ 14.X Redirect user to latest Visual C++ Build Tools
            message += (' Get it with ""Microsoft C++ Build Tools"": '
                        r'https://visualstudio.microsoft.com'
                        r'/visual-cpp-build-tools/')

    exc.args = (message, )

def get_vc_environment_variables_for_msvc14(plat_spec):
    """"""Python 3.8 ""distutils/_msvccompiler.py"" backport""""""
    if ""DISTUTILS_USE_SDK"" in environ:
        return {
            key.lower(): value
            for key, value in environ.items()
        }

    vcvarsall, vcruntime = _msvc14_find_vcvarsall(plat_spec)
    if not vcvarsall:
        raise distutils.errors.DistutilsPlatformError(
            ""Unable to find vcvarsall.bat""
        )

    try:
        out = subprocess.check_output(
            'cmd /u /c ""{}"" {} && set'.format(vcvarsall, plat_spec),
            stderr=subprocess.STDOUT,
        ).decode('utf-16le', errors='replace')
    except subprocess.CalledProcessError as exc:
        raise distutils.errors.DistutilsPlatformError(
            ""Error executing {}"".format(exc.cmd)
        ) from exc

    env = {
        key.lower(): value
        for key, _, value in
        (line.partition('=') for line in out.splitlines())
        if key and value
    }

    if vcruntime:
        env['py_vcruntime_redist'] = vcruntime
    return env","def patched_get_vc_env_for_msvc14_and_support_extra_compilers(plat_spec):
    """"""
    Patched ""distutils._msvccompiler._get_vc_env"" for support extra
    Microsoft Visual C++ 14.X compilers.

    Set environment without use of ""vcvarsall.bat"".

    Parameters
    ----------
    plat_spec: str
        Target architecture.

    Return
    ------
    dict
        environment
    """"""

    # Always use backport from CPython 3.8
    try:
        return get_vc_environment_variables_for_msvc14(plat_spec)
    except distutils.errors.DistutilsPlatformError as exc:
        add_details_to_exception(exc, 14.0)
        raise","{""_augment_exception"": ""add_details_to_exception"", ""_msvc14_get_vc_env"": ""get_vc_environment_variables_for_msvc14""}","def 0(exc, version, arch=''):
    """"""
    Add details to the exception message to help guide the user
    as to what action will resolve it.
    """"""
    # Error if MSVC++ directory not found or environment not set
    message = exc.args[0]

    if ""vcvarsall"" in message.lower() or ""visual c"" in message.lower():
        # Special error message if MSVC++ not installed
        tmpl = 'Microsoft Visual C++ {version:0.1f} or greater is required.'
        message = tmpl.format(**locals())
        msdownload = 'www.microsoft.com/download/details.aspx?id=%d'
        if version == 9.0:
            if arch.lower().find('ia64') > -1:
                # For VC++ 9.0, if IA64 support is needed, redirect user
                # to Windows SDK 7.0.
                # Note: No download link available from Microsoft.
                message += ' Get it with ""Microsoft Windows SDK 7.0""'
            else:
                # For VC++ 9.0 redirect user to Vc++ for Python 2.7 :
                # This redirection link is maintained by Microsoft.
                # Contact vspython@microsoft.com if it needs updating.
                message += ' Get it from http://aka.ms/vcpython27'
        elif version == 10.0:
            # For VC++ 10.0 Redirect user to Windows SDK 7.1
            message += ' Get it with ""Microsoft Windows SDK 7.1"": '
            message += msdownload % 8279
        elif version >= 14.0:
            # For VC++ 14.X Redirect user to latest Visual C++ Build Tools
            message += (' Get it with ""Microsoft C++ Build Tools"": '
                        r'https://visualstudio.microsoft.com'
                        r'/visual-cpp-build-tools/')

    exc.args = (message, )

def 1(plat_spec):
    """"""Python 3.8 ""distutils/_msvccompiler.py"" backport""""""
    if ""DISTUTILS_USE_SDK"" in environ:
        return {
            key.lower(): value
            for key, value in environ.items()
        }

    vcvarsall, vcruntime = _msvc14_find_vcvarsall(plat_spec)
    if not vcvarsall:
        raise distutils.errors.DistutilsPlatformError(
            ""Unable to find vcvarsall.bat""
        )

    try:
        out = subprocess.check_output(
            'cmd /u /c ""{}"" {} && set'.format(vcvarsall, plat_spec),
            stderr=subprocess.STDOUT,
        ).decode('utf-16le', errors='replace')
    except subprocess.CalledProcessError as exc:
        raise distutils.errors.DistutilsPlatformError(
            ""Error executing {}"".format(exc.cmd)
        ) from exc

    env = {
        key.lower(): value
        for key, _, value in
        (line.partition('=') for line in out.splitlines())
        if key and value
    }

    if vcruntime:
        env['py_vcruntime_redist'] = vcruntime
    return env","def 2(plat_spec):
    """"""
    Patched ""distutils._msvccompiler._get_vc_env"" for support extra
    Microsoft Visual C++ 14.X compilers.

    Set environment without use of ""vcvarsall.bat"".

    Parameters
    ----------
    plat_spec: str
        Target architecture.

    Return
    ------
    dict
        environment
    """"""

    # Always use backport from CPython 3.8
    try:
        return 1(plat_spec)
    except distutils.errors.DistutilsPlatformError as exc:
        0(exc, 14.0)
        raise","{""_augment_exception"": ""0"", ""_msvc14_get_vc_env"": ""1""}"
460,460,"def msvc14_gen_lib_options(*args, **kwargs):
    """"""
    Patched ""distutils._msvccompiler.gen_lib_options"" for fix
    compatibility between ""numpy.distutils"" and ""distutils._msvccompiler""
    (for Numpy < 1.11.2)
    """"""
    if ""numpy.distutils"" in sys.modules:
        import numpy as np
        if LegacyVersion(np.__version__) < LegacyVersion('1.11.2'):
            return np.distutils.ccompiler.gen_lib_options(*args, **kwargs)
    return get_unpatched(msvc14_gen_lib_options)(*args, **kwargs)","def get_unpatched(item):
    lookup = (
        get_unpatched_class if isinstance(item, type) else
        get_unpatched_function if isinstance(item, types.FunctionType) else
        lambda item: None
    )
    return lookup(item)","def retrieve_unpatched_item(item):
    lookup = (
        retrieve_unpatched_item_class if isinstance(item, type) else
        retrieve_unpatched_item_function if isinstance(item, types.FunctionType) else
        lambda item: None
    )
    return lookup(item)","def patched_gen_lib_options_for_msvc14_and_fix_compatibility(*args, **kwargs):
    """"""
    Patched ""distutils._msvccompiler.gen_lib_options"" for fix
    compatibility between ""numpy.distutils"" and ""distutils._msvccompiler""
    (for Numpy < 1.11.2)
    """"""
    if ""numpy.distutils"" in sys.modules:
        import numpy as np
        if LegacyVersion(np.__version__) < LegacyVersion('1.11.2'):
            return np.distutils.ccompiler.gen_lib_options(*args, **kwargs)
    return retrieve_unpatched_item(patched_gen_lib_options_for_msvc14_and_fix_compatibility)(*args, **kwargs)","{""get_unpatched"": ""retrieve_unpatched_item""}","def 0(item):
    lookup = (
        get_unpatched_class if isinstance(item, type) else
        get_unpatched_function if isinstance(item, types.FunctionType) else
        lambda item: None
    )
    return lookup(item)","def 1(*args, **kwargs):
    """"""
    Patched ""distutils._msvccompiler.gen_lib_options"" for fix
    compatibility between ""numpy.distutils"" and ""distutils._msvccompiler""
    (for Numpy < 1.11.2)
    """"""
    if ""numpy.distutils"" in sys.modules:
        import numpy as np
        if LegacyVersion(np.__version__) < LegacyVersion('1.11.2'):
            return np.distutils.ccompiler.gen_lib_options(*args, **kwargs)
    return 0(msvc14_gen_lib_options)(*args, **kwargs)","{""get_unpatched"": ""0""}"
461,461,"def load_group(value, group):
    """"""
    Given a value of an entry point or series of entry points,
    return each as an EntryPoint.
    """"""
    # normalize to a single sequence of lines
    lines = yield_lines(value)
    text = f'[{group}]\n' + '\n'.join(lines)
    return metadata.EntryPoints._from_text(text)","def yield_lines(strs):
    """"""Yield non-empty/non-comment lines of a string or sequence""""""
    if isinstance(strs, six.string_types):
        for s in strs.splitlines():
            s = s.strip()
            # skip blank lines/comments
            if s and not s.startswith('#'):
                yield s
    else:
        for ss in strs:
            for s in yield_lines(ss):
                yield s","def generate_non_empty_lines(strs):
    """"""Yield non-empty/non-comment lines of a string or sequence""""""
    if isinstance(strs, six.string_types):
        for s in strs.splitlines():
            s = s.strip()
            # skip blank lines/comments
            if s and not s.startswith('#'):
                yield s
    else:
        for ss in strs:
            for s in generate_non_empty_lines(ss):
                yield s","def given_value_return_entry_point_as_entry_point(value, group):
    """"""
    Given a value of an entry point or series of entry points,
    return each as an EntryPoint.
    """"""
    # normalize to a single sequence of lines
    lines = generate_non_empty_lines(value)
    text = f'[{group}]\n' + '\n'.join(lines)
    return metadata.EntryPoints._from_text(text)","{""yield_lines"": ""generate_non_empty_lines""}","def 0(strs):
    """"""Yield non-empty/non-comment lines of a string or sequence""""""
    if isinstance(strs, six.string_types):
        for s in strs.splitlines():
            s = s.strip()
            # skip blank lines/comments
            if s and not s.startswith('#'):
                yield s
    else:
        for ss in strs:
            for s in 0(ss):
                yield s","def 1(value, group):
    """"""
    Given a value of an entry point or series of entry points,
    return each as an EntryPoint.
    """"""
    # normalize to a single sequence of lines
    lines = 0(value)
    text = f'[{group}]\n' + '\n'.join(lines)
    return metadata.EntryPoints._from_text(text)","{""yield_lines"": ""0""}"
462,462,"def _read_field_unescaped_from_msg(msg: ""Message"", field: str) -> Optional[str]:
    """"""Read Message header field and apply rfc822_unescape.""""""
    value = _read_field_from_msg(msg, field)
    if value is None:
        return value
    return rfc822_unescape(value)","def _read_field_from_msg(msg: ""Message"", field: str) -> Optional[str]:
    """"""Read Message header field.""""""
    value = msg[field]
    if value == 'UNKNOWN':
        return None
    return value

def rfc822_unescape(content: str) -> str:
    """"""Reverse RFC-822 escaping by removing leading whitespaces from content.""""""
    lines = content.splitlines()
    if len(lines) == 1:
        return lines[0].lstrip()
    return '\n'.join((lines[0].lstrip(), textwrap.dedent('\n'.join(lines[1:]))))","def _read_field_from_msg(msg: ""Message"", field: str) -> Optional[str]:
    """"""Read Message header field.""""""
    value = msg[field]
    if value == 'UNKNOWN':
        return None
    return value

def rfc822_unescape(content: str) -> str:
    """"""Reverse RFC-822 escaping by removing leading whitespaces from content.""""""
    lines = content.splitlines()
    if len(lines) == 1:
        return lines[0].lstrip()
    return '\n'.join((lines[0].lstrip(), textwrap.dedent('\n'.join(lines[1:]))))","def _read_field_unescaped_from_msg(msg: ""Message"", field: str) -> Optional[str]:
    """"""Read Message header field and apply rfc822_unescape.""""""
    value = _read_field_from_msg(msg, field)
    if value is None:
        return value
    return rfc822_unescape(value)",{},"def _read_field_from_msg(msg: ""Message"", field: str) -> Optional[str]:
    """"""Read Message header field.""""""
    value = msg[field]
    if value == 'UNKNOWN':
        return None
    return value

def rfc822_unescape(content: str) -> str:
    """"""Reverse RFC-822 escaping by removing leading whitespaces from content.""""""
    lines = content.splitlines()
    if len(lines) == 1:
        return lines[0].lstrip()
    return '\n'.join((lines[0].lstrip(), textwrap.dedent('\n'.join(lines[1:]))))","def _read_field_unescaped_from_msg(msg: ""Message"", field: str) -> Optional[str]:
    """"""Read Message header field and apply rfc822_unescape.""""""
    value = _read_field_from_msg(msg, field)
    if value is None:
        return value
    return rfc822_unescape(value)",{}
463,463,"def check_nsp(dist, attr, value):
    """"""Verify that namespace packages are valid""""""
    ns_packages = value
    assert_string_list(dist, attr, ns_packages)
    for nsp in ns_packages:
        if not dist.has_contents_for(nsp):
            raise DistutilsSetupError(
                ""Distribution contains no modules or packages for ""
                + ""namespace package %r"" % nsp
            )
        parent, sep, child = nsp.rpartition('.')
        if parent and parent not in ns_packages:
            distutils.log.warn(
                ""WARNING: %r is declared as a package namespace, but %r""
                "" is not: please correct this in setup.py"",
                nsp,
                parent,
            )
        msg = (
            ""The namespace_packages parameter is deprecated, ""
            ""consider using implicit namespaces instead (PEP 420).""
        )
        warnings.warn(msg, SetuptoolsDeprecationWarning)","def assert_string_list(dist, attr, value):
    """"""Verify that value is a string list""""""
    try:
        # verify that value is a list or tuple to exclude unordered
        # or single-use iterables
        assert isinstance(value, (list, tuple))
        # verify that elements of value are strings
        assert ''.join(value) != value
    except (TypeError, ValueError, AttributeError, AssertionError) as e:
        raise DistutilsSetupError(
            ""%r must be a list of strings (got %r)"" % (attr, value)
        ) from e","def verify_string_list(dist, attr, value):
    """"""Verify that value is a string list""""""
    try:
        # verify that value is a list or tuple to exclude unordered
        # or single-use iterables
        assert isinstance(value, (list, tuple))
        # verify that elements of value are strings
        assert ''.join(value) != value
    except (TypeError, ValueError, AttributeError, AssertionError) as e:
        raise DistutilsSetupError(
            ""%r must be a list of strings (got %r)"" % (attr, value)
        ) from e","def verify_valid_namespace_packages_and_warn_if_not_declared(dist, attr, value):
    """"""Verify that namespace packages are valid""""""
    ns_packages = value
    verify_string_list(dist, attr, ns_packages)
    for nsp in ns_packages:
        if not dist.has_contents_for(nsp):
            raise DistutilsSetupError(
                ""Distribution contains no modules or packages for ""
                + ""namespace package %r"" % nsp
            )
        parent, sep, child = nsp.rpartition('.')
        if parent and parent not in ns_packages:
            distutils.log.warn(
                ""WARNING: %r is declared as a package namespace, but %r""
                "" is not: please correct this in setup.py"",
                nsp,
                parent,
            )
        msg = (
            ""The namespace_packages parameter is deprecated, ""
            ""consider using implicit namespaces instead (PEP 420).""
        )
        warnings.warn(msg, SetuptoolsDeprecationWarning)","{""assert_string_list"": ""verify_string_list""}","def 0(dist, attr, value):
    """"""Verify that value is a string list""""""
    try:
        # verify that value is a list or tuple to exclude unordered
        # or single-use iterables
        assert isinstance(value, (list, tuple))
        # verify that elements of value are strings
        assert ''.join(value) != value
    except (TypeError, ValueError, AttributeError, AssertionError) as e:
        raise DistutilsSetupError(
            ""%r must be a list of strings (got %r)"" % (attr, value)
        ) from e","def 1(dist, attr, value):
    """"""Verify that namespace packages are valid""""""
    ns_packages = value
    0(dist, attr, ns_packages)
    for nsp in ns_packages:
        if not dist.has_contents_for(nsp):
            raise DistutilsSetupError(
                ""Distribution contains no modules or packages for ""
                + ""namespace package %r"" % nsp
            )
        parent, sep, child = nsp.rpartition('.')
        if parent and parent not in ns_packages:
            distutils.log.warn(
                ""WARNING: %r is declared as a package namespace, but %r""
                "" is not: please correct this in setup.py"",
                nsp,
                parent,
            )
        msg = (
            ""The namespace_packages parameter is deprecated, ""
            ""consider using implicit namespaces instead (PEP 420).""
        )
        warnings.warn(msg, SetuptoolsDeprecationWarning)","{""assert_string_list"": ""0""}"
464,464,"def check_package_data(dist, attr, value):
    """"""Verify that value is a dictionary of package names to glob lists""""""
    if not isinstance(value, dict):
        raise DistutilsSetupError(
            ""{!r} must be a dictionary mapping package names to lists of ""
            ""string wildcard patterns"".format(attr)
        )
    for k, v in value.items():
        if not isinstance(k, str):
            raise DistutilsSetupError(
                ""keys of {!r} dict must be strings (got {!r})"".format(attr, k)
            )
        assert_string_list(dist, 'values of {!r} dict'.format(attr), v)","def assert_string_list(dist, attr, value):
    """"""Verify that value is a string list""""""
    try:
        # verify that value is a list or tuple to exclude unordered
        # or single-use iterables
        assert isinstance(value, (list, tuple))
        # verify that elements of value are strings
        assert ''.join(value) != value
    except (TypeError, ValueError, AttributeError, AssertionError) as e:
        raise DistutilsSetupError(
            ""%r must be a list of strings (got %r)"" % (attr, value)
        ) from e","def verify_string_list(dist, attr, value):
    """"""Verify that value is a string list""""""
    try:
        # verify that value is a list or tuple to exclude unordered
        # or single-use iterables
        assert isinstance(value, (list, tuple))
        # verify that elements of value are strings
        assert ''.join(value) != value
    except (TypeError, ValueError, AttributeError, AssertionError) as e:
        raise DistutilsSetupError(
            ""%r must be a list of strings (got %r)"" % (attr, value)
        ) from e","def verify_value_as_dict_of_package_names_to_glob_lists(dist, attr, value):
    """"""Verify that value is a dictionary of package names to glob lists""""""
    if not isinstance(value, dict):
        raise DistutilsSetupError(
            ""{!r} must be a dictionary mapping package names to lists of ""
            ""string wildcard patterns"".format(attr)
        )
    for k, v in value.items():
        if not isinstance(k, str):
            raise DistutilsSetupError(
                ""keys of {!r} dict must be strings (got {!r})"".format(attr, k)
            )
        verify_string_list(dist, 'values of {!r} dict'.format(attr), v)","{""assert_string_list"": ""verify_string_list""}","def 0(dist, attr, value):
    """"""Verify that value is a string list""""""
    try:
        # verify that value is a list or tuple to exclude unordered
        # or single-use iterables
        assert isinstance(value, (list, tuple))
        # verify that elements of value are strings
        assert ''.join(value) != value
    except (TypeError, ValueError, AttributeError, AssertionError) as e:
        raise DistutilsSetupError(
            ""%r must be a list of strings (got %r)"" % (attr, value)
        ) from e","def 1(dist, attr, value):
    """"""Verify that value is a dictionary of package names to glob lists""""""
    if not isinstance(value, dict):
        raise DistutilsSetupError(
            ""{!r} must be a dictionary mapping package names to lists of ""
            ""string wildcard patterns"".format(attr)
        )
    for k, v in value.items():
        if not isinstance(k, str):
            raise DistutilsSetupError(
                ""keys of {!r} dict must be strings (got {!r})"".format(attr, k)
            )
        0(dist, 'values of {!r} dict'.format(attr), v)","{""assert_string_list"": ""0""}"
465,465,"def unpack(src_dir, dst_dir):
    '''Move everything under `src_dir` to `dst_dir`, and delete the former.'''
    for dirpath, dirnames, filenames in os.walk(src_dir):
        subdir = os.path.relpath(dirpath, src_dir)
        for f in filenames:
            src = os.path.join(dirpath, f)
            dst = os.path.join(dst_dir, subdir, f)
            os.renames(src, dst)
        for n, d in reversed(list(enumerate(dirnames))):
            src = os.path.join(dirpath, d)
            dst = os.path.join(dst_dir, subdir, d)
            if not os.path.exists(dst):
                # Directory does not exist in destination,
                # rename it and prune it from os.walk list.
                os.renames(src, dst)
                del dirnames[n]
    # Cleanup.
    for dirpath, dirnames, filenames in os.walk(src_dir, topdown=True):
        assert not filenames
        os.rmdir(dirpath)","def enumerate():
    """"""Return a list of all Thread objects currently alive.

    The list includes daemonic threads, dummy thread objects created by
    current_thread(), and the main thread. It excludes terminated threads and
    threads that have not yet been started.

    """"""
    with _active_limbo_lock:
        return list(_active.values()) + list(_limbo.values())","def get_alive_threads():
    """"""Return a list of all Thread objects currently alive.

    The list includes daemonic threads, dummy thread objects created by
    current_thread(), and the main thread. It excludes terminated threads and
    threads that have not yet been started.

    """"""
    with _active_limbo_lock:
        return list(_active.values()) + list(_limbo.values())","def move_everything_under_source_directory_to_destination_directory(src_dir, dst_dir):
    '''Move everything under `src_dir` to `dst_dir`, and delete the former.'''
    for dirpath, dirnames, filenames in os.walk(src_dir):
        subdir = os.path.relpath(dirpath, src_dir)
        for f in filenames:
            src = os.path.join(dirpath, f)
            dst = os.path.join(dst_dir, subdir, f)
            os.renames(src, dst)
        for n, d in reversed(list(get_alive_threads(dirnames))):
            src = os.path.join(dirpath, d)
            dst = os.path.join(dst_dir, subdir, d)
            if not os.path.exists(dst):
                # Directory does not exist in destination,
                # rename it and prune it from os.walk list.
                os.renames(src, dst)
                del dirnames[n]
    # Cleanup.
    for dirpath, dirnames, filenames in os.walk(src_dir, topdown=True):
        assert not filenames
        os.rmdir(dirpath)","{""enumerate"": ""get_alive_threads""}","def 0():
    """"""Return a list of all Thread objects currently alive.

    The list includes daemonic threads, dummy thread objects created by
    current_thread(), and the main thread. It excludes terminated threads and
    threads that have not yet been started.

    """"""
    with _active_limbo_lock:
        return list(_active.values()) + list(_limbo.values())","def 1(src_dir, dst_dir):
    '''Move everything under `src_dir` to `dst_dir`, and delete the former.'''
    for dirpath, dirnames, filenames in os.walk(src_dir):
        subdir = os.path.relpath(dirpath, src_dir)
        for f in filenames:
            src = os.path.join(dirpath, f)
            dst = os.path.join(dst_dir, subdir, f)
            os.renames(src, dst)
        for n, d in reversed(list(0(dirnames))):
            src = os.path.join(dirpath, d)
            dst = os.path.join(dst_dir, subdir, d)
            if not os.path.exists(dst):
                # Directory does not exist in destination,
                # rename it and prune it from os.walk list.
                os.renames(src, dst)
                del dirnames[n]
    # Cleanup.
    for dirpath, dirnames, filenames in os.walk(src_dir, topdown=True):
        assert not filenames
        os.rmdir(dirpath)","{""enumerate"": ""0""}"
466,466,"def _pathlib_compat(path):
    """"""
    For path-like objects, convert to a filename for compatibility
    on Python 3.6.1 and earlier.
    """"""
    try:
        return path.__fspath__()
    except AttributeError:
        return str(path)","def str(val):
    """"""Convert float to string, taking the locale into account.""""""
    return _format(""%.12g"", val)","def locale_aware_float_to_string(val):
    """"""Convert float to locale_aware_float_to_stringing, taking the locale into account.""""""
    return _format(""%.12g"", val)","def convert_path_like_objects_to_filename_for_compatibility(path):
    """"""
    For path-like objects, convert to a filename for compatibility
    on Python 3.6.1 and earlier.
    """"""
    try:
        return path.__fspath__()
    except AttributeError:
        return locale_aware_float_to_string(path)","{""str"": ""locale_aware_float_to_string""}","def 0(val):
    """"""Convert float to string, taking the locale into account.""""""
    return _format(""%.12g"", val)","def 1(path):
    """"""
    For path-like objects, convert to a filename for compatibility
    on Python 3.6.1 and earlier.
    """"""
    try:
        return path.__fspath__()
    except AttributeError:
        return 0(path)","{""str"": ""0""}"
467,467,"def _special_method_cache(method, cache_wrapper):
    """"""
    Because Python treats special methods differently, it's not
    possible to use instance attributes to implement the cached
    methods.

    Instead, install the wrapper method under a different name
    and return a simple proxy to that wrapper.

    https://github.com/jaraco/jaraco.functools/issues/5
    """"""
    name = method.__name__
    special_names = '__getattr__', '__getitem__'
    if name not in special_names:
        return

    wrapper_name = '__cached' + name

    def proxy(self, *args, **kwargs):
        if wrapper_name not in vars(self):
            bound = types.MethodType(method, self)
            cache = cache_wrapper(bound)
            setattr(self, wrapper_name, cache)
        else:
            cache = getattr(self, wrapper_name)
        return cache(*args, **kwargs)

    return proxy","def cache(user_function, /):
    'Simple lightweight unbounded cache.  Sometimes called ""memoize"".'
    return lru_cache(maxsize=None)(user_function)","def simple_unbounded_cache(user_function, /):
    'Simple lightweight unbounded simple_unbounded_cache.  Sometimes called ""memoize"".'
    return lru_simple_unbounded_cache(maxsize=None)(user_function)","def _special_method_simple_unbounded_cache(method, simple_unbounded_cache_wrapper):
    """"""
    Because Python treats special methods differently, it's not
    possible to use instance attributes to implement the simple_unbounded_cached
    methods.

    Instead, install the wrapper method under a different name
    and return a simple create_proxy_to_installed_wrapper_method_and_use_it to that wrapper.

    https://github.com/jaraco/jaraco.functools/issues/5
    """"""
    name = method.__name__
    special_names = '__getattr__', '__getitem__'
    if name not in special_names:
        return

    wrapper_name = '__simple_unbounded_cached' + name

    def create_proxy_to_installed_wrapper_method_and_use_it(self, *args, **kwargs):
        if wrapper_name not in vars(self):
            bound = types.MethodType(method, self)
            simple_unbounded_cache = simple_unbounded_cache_wrapper(bound)
            setattr(self, wrapper_name, simple_unbounded_cache)
        else:
            simple_unbounded_cache = getattr(self, wrapper_name)
        return simple_unbounded_cache(*args, **kwargs)

    return create_proxy_to_installed_wrapper_method_and_use_it","{""cache"": ""simple_unbounded_cache""}","def 0(user_function, /):
    'Simple lightweight unbounded cache.  Sometimes called ""memoize"".'
    return lru_0(maxsize=None)(user_function)","def _special_method_0(method, cache_wrapper):
    """"""
    Because Python treats special methods differently, it's not
    possible to use instance attributes to implement the cached
    methods.

    Instead, install the wrapper method under a different name
    and return a simple proxy to that wrapper.

    https://github.com/jaraco/jaraco.functools/issues/5
    """"""
    name = method.__name__
    special_names = '__getattr__', '__getitem__'
    if name not in special_names:
        return

    wrapper_name = '__cached' + name

    def 2(self, *args, **kwargs):
        if wrapper_name not in vars(self):
            bound = types.MethodType(method, self)
            cache = cache_wrapper(bound)
            setattr(self, wrapper_name, cache)
        else:
            cache = getattr(self, wrapper_name)
        return 0(*args, **kwargs)

    return proxy","{""cache"": ""0""}"
468,468,"def retry_call(func, cleanup=lambda: None, retries=0, trap=()):
    """"""
    Given a callable func, trap the indicated exceptions
    for up to 'retries' times, invoking cleanup on the
    exception. On the final attempt, allow any exceptions
    to propagate.
    """"""
    attempts = itertools.count() if retries == float('inf') else range(retries)
    for attempt in attempts:
        try:
            return func()
        except trap:
            cleanup()

    return func()","def cleanup(ordering, blowUp=False):
    if not blowUp:
        ordering.append('cleanup_good')
    else:
        ordering.append('cleanup_exc')
        raise Exception('CleanUpExc')","def perform_cleanup_operations(ordering, blowUp=False):
    if not blowUp:
        ordering.append('perform_cleanup_operations_good')
    else:
        ordering.append('perform_cleanup_operations_exc')
        raise Exception('CleanUpExc')","def trap_specified_exceptions_for_up_to_specified_retries(func, perform_cleanup_operations=lambda: None, retries=0, trap=()):
    """"""
    Given a callable func, trap the indicated exceptions
    for up to 'retries' times, invoking perform_cleanup_operations on the
    exception. On the final attempt, allow any exceptions
    to propagate.
    """"""
    attempts = itertools.count() if retries == float('inf') else range(retries)
    for attempt in attempts:
        try:
            return func()
        except trap:
            perform_cleanup_operations()

    return func()","{""cleanup"": ""perform_cleanup_operations""}","def 0(ordering, blowUp=False):
    if not blowUp:
        ordering.append('cleanup_good')
    else:
        ordering.append('cleanup_exc')
        raise Exception('CleanUpExc')","def 1(func, cleanup=lambda: None, retries=0, trap=()):
    """"""
    Given a callable func, trap the indicated exceptions
    for up to 'retries' times, invoking cleanup on the
    exception. On the final attempt, allow any exceptions
    to propagate.
    """"""
    attempts = itertools.count() if retries == float('inf') else range(retries)
    for attempt in attempts:
        try:
            return func()
        except trap:
            0()

    return func()","{""cleanup"": ""0""}"
469,469,"def install(cls):
    """"""
    Class decorator for installation on sys.meta_path.

    Adds the backport DistributionFinder to sys.meta_path and
    attempts to disable the finder functionality of the stdlib
    DistributionFinder.
    """"""
    sys.meta_path.append(cls())
    disable_stdlib_finder()
    return cls","def disable_stdlib_finder():
    """"""
    Give the backport primacy for discovering path-based distributions
    by monkey-patching the stdlib O_O.

    See #91 for more background for rationale on this sketchy
    behavior.
    """"""

    def matches(finder):
        return getattr(
            finder, '__module__', None
        ) == '_frozen_importlib_external' and hasattr(finder, 'find_distributions')

    for finder in filter(matches, sys.meta_path):  # pragma: nocover
        del finder.find_distributions","def disable_stdlib_finder_by_monkey_patching():
    """"""
    Give the backport primacy for discovering path-based distributions
    by monkey-patching the stdlib O_O.

    See #91 for more background for rationale on this sketchy
    behavior.
    """"""

    def matches(finder):
        return getattr(
            finder, '__module__', None
        ) == '_frozen_importlib_external' and hasattr(finder, 'find_distributions')

    for finder in filter(matches, sys.meta_path):  # pragma: nocover
        del finder.find_distributions","def add_backport_distribution_finder_to_sys_meta_path(cls):
    """"""
    Class decorator for add_backport_distribution_finder_to_sys_meta_pathation on sys.meta_path.

    Adds the backport DistributionFinder to sys.meta_path and
    attempts to disable the finder functionality of the stdlib
    DistributionFinder.
    """"""
    sys.meta_path.append(cls())
    disable_stdlib_finder_by_monkey_patching()
    return cls","{""disable_stdlib_finder"": ""disable_stdlib_finder_by_monkey_patching""}","def 0():
    """"""
    Give the backport primacy for discovering path-based distributions
    by monkey-patching the stdlib O_O.

    See #91 for more background for rationale on this sketchy
    behavior.
    """"""

    def matches(finder):
        return getattr(
            finder, '__module__', None
        ) == '_frozen_importlib_external' and hasattr(finder, 'find_distributions')

    for finder in filter(matches, sys.meta_path):  # pragma: nocover
        del finder.find_distributions","def 2(cls):
    """"""
    Class decorator for installation on sys.meta_path.

    Adds the backport DistributionFinder to sys.meta_path and
    attempts to disable the finder functionality of the stdlib
    DistributionFinder.
    """"""
    sys.meta_path.append(cls())
    0()
    return cls","{""disable_stdlib_finder"": ""0""}"
470,470,"def disable_stdlib_finder():
    """"""
    Give the backport primacy for discovering path-based distributions
    by monkey-patching the stdlib O_O.

    See #91 for more background for rationale on this sketchy
    behavior.
    """"""

    def matches(finder):
        return getattr(
            finder, '__module__', None
        ) == '_frozen_importlib_external' and hasattr(finder, 'find_distributions')

    for finder in filter(matches, sys.meta_path):  # pragma: nocover
        del finder.find_distributions","def filter(names, pat):
    """"""Construct a list from those elements of the iterable NAMES that match PAT.""""""
    result = []
    pat = os.path.normcase(pat)
    match = _compile_pattern(pat)
    if os.path is posixpath:
        # normcase on posix is NOP. Optimize it away from the loop.
        for name in names:
            if match(name):
                result.append(name)
    else:
        for name in names:
            if match(os.path.normcase(name)):
                result.append(name)
    return result","def filter_names(names, pat):
    """"""Construct a list from those elements of the iterable NAMES that match PAT.""""""
    result = []
    pat = os.path.normcase(pat)
    match = _compile_pattern(pat)
    if os.path is posixpath:
        # normcase on posix is NOP. Optimize it away from the loop.
        for name in names:
            if match(name):
                result.append(name)
    else:
        for name in names:
            if match(os.path.normcase(name)):
                result.append(name)
    return result","def disable_stdlib_finder_by_monkey_patching():
    """"""
    Give the backport primacy for discovering path-based distributions
    by monkey-patching the stdlib O_O.

    See #91 for more background for rationale on this sketchy
    behavior.
    """"""

    def check_if_finder_module_is_frozen_importlib_external_and_has_find_distributions(finder):
        return getattr(
            finder, '__module__', None
        ) == '_frozen_importlib_external' and hasattr(finder, 'find_distributions')

    for finder in filter_names(check_if_finder_module_is_frozen_importlib_external_and_has_find_distributions, sys.meta_path):  # pragma: nocover
        del finder.find_distributions","{""filter"": ""filter_names""}","def 0(names, pat):
    """"""Construct a list from those elements of the iterable NAMES that match PAT.""""""
    result = []
    pat = os.path.normcase(pat)
    match = _compile_pattern(pat)
    if os.path is posixpath:
        # normcase on posix is NOP. Optimize it away from the loop.
        for name in names:
            if match(name):
                result.append(name)
    else:
        for name in names:
            if match(os.path.normcase(name)):
                result.append(name)
    return result","def 1():
    """"""
    Give the backport primacy for discovering path-based distributions
    by monkey-patching the stdlib O_O.

    See #91 for more background for rationale on this sketchy
    behavior.
    """"""

    def 2(finder):
        return getattr(
            finder, '__module__', None
        ) == '_frozen_importlib_external' and hasattr(finder, 'find_distributions')

    for finder in 0(matches, sys.meta_path):  # pragma: nocover
        del finder.find_distributions","{""filter"": ""0""}"
471,471,"def consumer(func):
    """"""Decorator that automatically advances a PEP-342-style ""reverse iterator""
    to its first yield point so you don't have to call ``next()`` on it
    manually.

        >>> @consumer
        ... def tally():
        ...     i = 0
        ...     while True:
        ...         print('Thing number %s is %s.' % (i, (yield)))
        ...         i += 1
        ...
        >>> t = tally()
        >>> t.send('red')
        Thing number 0 is red.
        >>> t.send('fish')
        Thing number 1 is fish.

    Without the decorator, you would have to call ``next(t)`` before
    ``t.send()`` could be used.

    """"""

    @wraps(func)
    def wrapper(*args, **kwargs):
        gen = func(*args, **kwargs)
        next(gen)
        return gen

    return wrapper","def wraps(wrapped,
          assigned = WRAPPER_ASSIGNMENTS,
          updated = WRAPPER_UPDATES):
    """"""Decorator factory to apply update_wrapper() to a wrapper function

       Returns a decorator that invokes update_wrapper() with the decorated
       function as the wrapper argument and the arguments to wraps() as the
       remaining arguments. Default arguments are as for update_wrapper().
       This is a convenience function to simplify applying partial() to
       update_wrapper().
    """"""
    return partial(update_wrapper, wrapped=wrapped,
                   assigned=assigned, updated=updated)","def create_decorator_with_update_wrapper(wrapped,
          assigned = WRAPPER_ASSIGNMENTS,
          updated = WRAPPER_UPDATES):
    """"""Decorator factory to apply update_wrapper() to a wrapper function

       Returns a decorator that invokes update_wrapper() with the decorated
       function as the wrapper argument and the arguments to create_decorator_with_update_wrapper() as the
       remaining arguments. Default arguments are as for update_wrapper().
       This is a convenience function to simplify applying partial() to
       update_wrapper().
    """"""
    return partial(update_wrapper, wrapped=wrapped,
                   assigned=assigned, updated=updated)","def advance_reverse_iterator_to_first_yield_point_automatically(func):
    """"""Decorator that automatically advances a PEP-342-style ""reverse iterator""
    to its first yield point so you don't have to call ``next()`` on it
    manually.

        >>> @advance_reverse_iterator_to_first_yield_point_automatically
        ... def print_thing_number_and_state_using_reverse_iterator():
        ...     i = 0
        ...     while True:
        ...         print('Thing number %s is %s.' % (i, (yield)))
        ...         i += 1
        ...
        >>> t = print_thing_number_and_state_using_reverse_iterator()
        >>> t.send('red')
        Thing number 0 is red.
        >>> t.send('fish')
        Thing number 1 is fish.

    Without the decorator, you would have to call ``next(t)`` before
    ``t.send()`` could be used.

    """"""

    @create_decorator_with_update_initialize_curses_and_call_function(func)
    def initialize_curses_and_call_function(*args, **kwargs):
        gen = func(*args, **kwargs)
        next(gen)
        return gen

    return initialize_curses_and_call_function","{""wraps"": ""create_decorator_with_update_wrapper""}","def 0(wrapped,
          assigned = WRAPPER_ASSIGNMENTS,
          updated = WRAPPER_UPDATES):
    """"""Decorator factory to apply update_wrapper() to a wrapper function

       Returns a decorator that invokes update_wrapper() with the decorated
       function as the wrapper argument and the arguments to 0() as the
       remaining arguments. Default arguments are as for update_wrapper().
       This is a convenience function to simplify applying partial() to
       update_wrapper().
    """"""
    return partial(update_wrapper, wrapped=wrapped,
                   assigned=assigned, updated=updated)","def 1(func):
    """"""Decorator that automatically advances a PEP-342-style ""reverse iterator""
    to its first yield point so you don't have to call ``next()`` on it
    manually.

        >>> @consumer
        ... def 2():
        ...     i = 0
        ...     while True:
        ...         print('Thing number %s is %s.' % (i, (yield)))
        ...         i += 1
        ...
        >>> t = 2()
        >>> t.send('red')
        Thing number 0 is red.
        >>> t.send('fish')
        Thing number 1 is fish.

    Without the decorator, you would have to call ``next(t)`` before
    ``t.send()`` could be used.

    """"""

    @0(func)
    def 3(*args, **kwargs):
        gen = func(*args, **kwargs)
        next(gen)
        return gen

    return wrapper","{""wraps"": ""0""}"
472,472,"def repeat_last(iterable, default=None):
    """"""After the *iterable* is exhausted, keep yielding its last element.

        >>> list(islice(repeat_last(range(3)), 5))
        [0, 1, 2, 2, 2]

    If the iterable is empty, yield *default* forever::

        >>> list(islice(repeat_last(range(0), 42), 5))
        [42, 42, 42, 42, 42]

    """"""
    item = _marker
    for item in iterable:
        yield item
    final = default if item is _marker else item
    yield from repeat(final)","def repeat(stmt=""pass"", setup=""pass"", timer=default_timer,
           repeat=default_repeat, number=default_number, globals=None):
    """"""Convenience function to create Timer object and call repeat method.""""""
    return Timer(stmt, setup, timer, globals).repeat(repeat, number)","def timeit_repeat(stmt=""pass"", setup=""pass"", timer=default_timer,
           timeit_repeat=default_timeit_repeat, number=default_number, globals=None):
    """"""Convenience function to create Timer object and call timeit_repeat method.""""""
    return Timer(stmt, setup, timer, globals).timeit_repeat(timeit_repeat, number)","def timeit_return_sequence_of_elements_and_then_return_none(iterable, default=None):
    """"""After the *iterable* is exhausted, keep yielding its last element.

        >>> list(islice(timeit_return_sequence_of_elements_and_then_return_none(range(3)), 5))
        [0, 1, 2, 2, 2]

    If the iterable is empty, yield *default* forever::

        >>> list(islice(timeit_return_sequence_of_elements_and_then_return_none(range(0), 42), 5))
        [42, 42, 42, 42, 42]

    """"""
    item = _marker
    for item in iterable:
        yield item
    final = default if item is _marker else item
    yield from timeit_repeat(final)","{""repeat"": ""timeit_repeat""}","def 0(stmt=""pass"", setup=""pass"", timer=default_timer,
           repeat=default_repeat, number=default_number, globals=None):
    """"""Convenience function to create Timer object and call repeat method.""""""
    return Timer(stmt, setup, timer, globals).0(repeat, number)","def 1(iterable, default=None):
    """"""After the *iterable* is exhausted, keep yielding its last element.

        >>> list(islice(1(range(3)), 5))
        [0, 1, 2, 2, 2]

    If the iterable is empty, yield *default* forever::

        >>> list(islice(1(range(0), 42), 5))
        [42, 42, 42, 42, 42]

    """"""
    item = _marker
    for item in iterable:
        yield item
    final = default if item is _marker else item
    yield from 0(final)","{""repeat"": ""0""}"
473,473,"def filter_except(validator, iterable, *exceptions):
    """"""Yield the items from *iterable* for which the *validator* function does
    not raise one of the specified *exceptions*.

    *validator* is called for each item in *iterable*.
    It should be a function that accepts one argument and raises an exception
    if that item is not valid.

    >>> iterable = ['1', '2', 'three', '4', None]
    >>> list(filter_except(int, iterable, ValueError, TypeError))
    ['1', '2', '4']

    If an exception other than one given by *exceptions* is raised by
    *validator*, it is raised like normal.
    """"""
    for item in iterable:
        try:
            validator(item)
        except exceptions:
            pass
        else:
            yield item","def validator(application):

    """"""
    When applied between a WSGI server and a WSGI application, this
    middleware will check for WSGI compliance on a number of levels.
    This middleware does not modify the request or response in any
    way, but will raise an AssertionError if anything seems off
    (except for a failure to close the application iterator, which
    will be printed to stderr -- there's no way to raise an exception
    at that point).
    """"""

    def lint_app(*args, **kw):
        assert_(len(args) == 2, ""Two arguments required"")
        assert_(not kw, ""No keyword arguments allowed"")
        environ, start_response = args

        check_environ(environ)

        # We use this to check if the application returns without
        # calling start_response:
        start_response_started = []

        def start_response_wrapper(*args, **kw):
            assert_(len(args) == 2 or len(args) == 3, (
                ""Invalid number of arguments: %s"" % (args,)))
            assert_(not kw, ""No keyword arguments allowed"")
            status = args[0]
            headers = args[1]
            if len(args) == 3:
                exc_info = args[2]
            else:
                exc_info = None

            check_status(status)
            check_headers(headers)
            check_content_type(status, headers)
            check_exc_info(exc_info)

            start_response_started.append(None)
            return WriteWrapper(start_response(*args))

        environ['wsgi.input'] = InputWrapper(environ['wsgi.input'])
        environ['wsgi.errors'] = ErrorWrapper(environ['wsgi.errors'])

        iterator = application(environ, start_response_wrapper)
        assert_(iterator is not None and iterator != False,
            ""The application must return an iterator, if only an empty list"")

        check_iterator(iterator)

        return IteratorWrapper(iterator, start_response_started)

    return lint_app","def validate_wsgi_compliance(application):

    """"""
    When applied between a WSGI server and a WSGI application, this
    middleware will check for WSGI compliance on a number of levels.
    This middleware does not modify the request or response in any
    way, but will raise an AssertionError if anything seems off
    (except for a failure to close the application iterator, which
    will be printed to stderr -- there's no way to raise an exception
    at that point).
    """"""

    def lint_app(*args, **kw):
        assert_(len(args) == 2, ""Two arguments required"")
        assert_(not kw, ""No keyword arguments allowed"")
        environ, start_response = args

        check_environ(environ)

        # We use this to check if the application returns without
        # calling start_response:
        start_response_started = []

        def start_response_wrapper(*args, **kw):
            assert_(len(args) == 2 or len(args) == 3, (
                ""Invalid number of arguments: %s"" % (args,)))
            assert_(not kw, ""No keyword arguments allowed"")
            status = args[0]
            headers = args[1]
            if len(args) == 3:
                exc_info = args[2]
            else:
                exc_info = None

            check_status(status)
            check_headers(headers)
            check_content_type(status, headers)
            check_exc_info(exc_info)

            start_response_started.append(None)
            return WriteWrapper(start_response(*args))

        environ['wsgi.input'] = InputWrapper(environ['wsgi.input'])
        environ['wsgi.errors'] = ErrorWrapper(environ['wsgi.errors'])

        iterator = application(environ, start_response_wrapper)
        assert_(iterator is not None and iterator != False,
            ""The application must return an iterator, if only an empty list"")

        check_iterator(iterator)

        return IteratorWrapper(iterator, start_response_started)

    return lint_app","def yield_items_that_validator_does_not_raise_specified_exceptions_for(validate_wsgi_compliance, iterable, *exceptions):
    """"""Yield the items from *iterable* for which the *validate_wsgi_compliance* function does
    not raise one of the specified *exceptions*.

    *validate_wsgi_compliance* is called for each item in *iterable*.
    It should be a function that accepts one argument and raises an exception
    if that item is not valid.

    >>> iterable = ['1', '2', 'three', '4', None]
    >>> list(yield_items_that_validator_does_not_raise_specified_exceptions_for(int, iterable, ValueError, TypeError))
    ['1', '2', '4']

    If an exception other than one given by *exceptions* is raised by
    *validate_wsgi_compliance*, it is raised like normal.
    """"""
    for item in iterable:
        try:
            validate_wsgi_compliance(item)
        except exceptions:
            pass
        else:
            yield item","{""validator"": ""validate_wsgi_compliance""}","def 0(application):

    """"""
    When applied between a WSGI server and a WSGI application, this
    middleware will check for WSGI compliance on a number of levels.
    This middleware does not modify the request or response in any
    way, but will raise an AssertionError if anything seems off
    (except for a failure to close the application iterator, which
    will be printed to stderr -- there's no way to raise an exception
    at that point).
    """"""

    def lint_app(*args, **kw):
        assert_(len(args) == 2, ""Two arguments required"")
        assert_(not kw, ""No keyword arguments allowed"")
        environ, start_response = args

        check_environ(environ)

        # We use this to check if the application returns without
        # calling start_response:
        start_response_started = []

        def start_response_wrapper(*args, **kw):
            assert_(len(args) == 2 or len(args) == 3, (
                ""Invalid number of arguments: %s"" % (args,)))
            assert_(not kw, ""No keyword arguments allowed"")
            status = args[0]
            headers = args[1]
            if len(args) == 3:
                exc_info = args[2]
            else:
                exc_info = None

            check_status(status)
            check_headers(headers)
            check_content_type(status, headers)
            check_exc_info(exc_info)

            start_response_started.append(None)
            return WriteWrapper(start_response(*args))

        environ['wsgi.input'] = InputWrapper(environ['wsgi.input'])
        environ['wsgi.errors'] = ErrorWrapper(environ['wsgi.errors'])

        iterator = application(environ, start_response_wrapper)
        assert_(iterator is not None and iterator != False,
            ""The application must return an iterator, if only an empty list"")

        check_iterator(iterator)

        return IteratorWrapper(iterator, start_response_started)

    return lint_app","def 3(validator, iterable, *exceptions):
    """"""Yield the items from *iterable* for which the *validator* function does
    not raise one of the specified *exceptions*.

    *validator* is called for each item in *iterable*.
    It should be a function that accepts one argument and raises an exception
    if that item is not valid.

    >>> iterable = ['1', '2', 'three', '4', None]
    >>> list(3(int, iterable, ValueError, TypeError))
    ['1', '2', '4']

    If an exception other than one given by *exceptions* is raised by
    *validator*, it is raised like normal.
    """"""
    for item in iterable:
        try:
            0(item)
        except exceptions:
            pass
        else:
            yield item","{""validator"": ""0""}"
474,474,"def map_except(function, iterable, *exceptions):
    """"""Transform each item from *iterable* with *function* and yield the
    result, unless *function* raises one of the specified *exceptions*.

    *function* is called to transform each item in *iterable*.
    It should accept one argument.

    >>> iterable = ['1', '2', 'three', '4', None]
    >>> list(map_except(int, iterable, ValueError, TypeError))
    [1, 2, 4]

    If an exception other than one given by *exceptions* is raised by
    *function*, it is raised like normal.
    """"""
    for item in iterable:
        try:
            yield function(item)
        except exceptions:
            pass","def function(a, b=Foo): pass","def function_with_default_argument_Foo(a, b=Foo): pass","def transform_items_with_function_and_yield_result_except_specified_exceptions(function_with_default_argument_Foo, iterable, *exceptions):
    """"""Transform each item from *iterable* with *function_with_default_argument_Foo* and yield the
    result, unless *function_with_default_argument_Foo* raises one of the specified *exceptions*.

    *function_with_default_argument_Foo* is called to transform each item in *iterable*.
    It should accept one argument.

    >>> iterable = ['1', '2', 'three', '4', None]
    >>> list(transform_items_with_function_and_yield_result_except_specified_exceptions(int, iterable, ValueError, TypeError))
    [1, 2, 4]

    If an exception other than one given by *exceptions* is raised by
    *function_with_default_argument_Foo*, it is raised like normal.
    """"""
    for item in iterable:
        try:
            yield function_with_default_argument_Foo(item)
        except exceptions:
            pass","{""function"": ""function_with_default_argument_Foo""}","def 0(a, b=Foo): pass","def 1(function, iterable, *exceptions):
    """"""Transform each item from *iterable* with *function* and yield the
    result, unless *function* raises one of the specified *exceptions*.

    *function* is called to transform each item in *iterable*.
    It should accept one argument.

    >>> iterable = ['1', '2', 'three', '4', None]
    >>> list(1(int, iterable, ValueError, TypeError))
    [1, 2, 4]

    If an exception other than one given by *exceptions* is raised by
    *function*, it is raised like normal.
    """"""
    for item in iterable:
        try:
            yield 0(item)
        except exceptions:
            pass","{""function"": ""0""}"
475,475,"def nth_product(index, *args):
    """"""Equivalent to ``list(product(*args))[index]``.

    The products of *args* can be ordered lexicographically.
    :func:`nth_product` computes the product at sort position *index* without
    computing the previous products.

        >>> nth_product(8, range(2), range(2), range(2), range(2))
        (1, 0, 0, 0)

    ``IndexError`` will be raised if the given *index* is invalid.
    """"""
    pools = list(map(tuple, reversed(args)))
    ns = list(map(len, pools))

    c = reduce(mul, ns)

    if index < 0:
        index += c

    if not 0 <= index < c:
        raise IndexError

    result = []
    for pool, n in zip(pools, ns):
        result.append(pool[index % n])
        index //= n

    return tuple(reversed(result))","def reduce(function, sequence, initial=_initial_missing):
    """"""
    reduce(function, iterable[, initial]) -> value

    Apply a function of two arguments cumulatively to the items of a sequence
    or iterable, from left to right, so as to reduce the iterable to a single
    value.  For example, reduce(lambda x, y: x+y, [1, 2, 3, 4, 5]) calculates
    ((((1+2)+3)+4)+5).  If initial is present, it is placed before the items
    of the iterable in the calculation, and serves as a default when the
    iterable is empty.
    """"""

    it = iter(sequence)

    if initial is _initial_missing:
        try:
            value = next(it)
        except StopIteration:
            raise TypeError(
                ""reduce() of empty iterable with no initial value"") from None
    else:
        value = initial

    for element in it:
        value = function(value, element)

    return value","def cumulative_reduction(function, sequence, initial=_initial_missing):
    """"""
    cumulative_reduction(function, iterable[, initial]) -> value

    Apply a function of two arguments cumulatively to the items of a sequence
    or iterable, from left to right, so as to cumulative_reduction the iterable to a single
    value.  For example, cumulative_reduction(lambda x, y: x+y, [1, 2, 3, 4, 5]) calculates
    ((((1+2)+3)+4)+5).  If initial is present, it is placed before the items
    of the iterable in the calculation, and serves as a default when the
    iterable is empty.
    """"""

    it = iter(sequence)

    if initial is _initial_missing:
        try:
            value = next(it)
        except StopIteration:
            raise TypeError(
                ""cumulative_reduction() of empty iterable with no initial value"") from None
    else:
        value = initial

    for element in it:
        value = function(value, element)

    return value","def compute_product_at_sort_position_index_without_computing_previous_products(index, *args):
    """"""Equivalent to ``list(product(*args))[index]``.

    The products of *args* can be ordered lexicographically.
    :func:`compute_product_at_sort_position_index_without_computing_previous_products` computes the product at sort position *index* without
    computing the previous products.

        >>> compute_product_at_sort_position_index_without_computing_previous_products(8, range(2), range(2), range(2), range(2))
        (1, 0, 0, 0)

    ``IndexError`` will be raised if the given *index* is invalid.
    """"""
    pools = list(map(tuple, reversed(args)))
    ns = list(map(len, pools))

    c = cumulative_reduction(mul, ns)

    if index < 0:
        index += c

    if not 0 <= index < c:
        raise IndexError

    result = []
    for pool, n in zip(pools, ns):
        result.append(pool[index % n])
        index //= n

    return tuple(reversed(result))","{""reduce"": ""cumulative_reduction""}","def 0(function, sequence, initial=_initial_missing):
    """"""
    0(function, iterable[, initial]) -> value

    Apply a function of two arguments cumulatively to the items of a sequence
    or iterable, from left to right, so as to reduce the iterable to a single
    value.  For example, 0(lambda x, y: x+y, [1, 2, 3, 4, 5]) calculates
    ((((1+2)+3)+4)+5).  If initial is present, it is placed before the items
    of the iterable in the calculation, and serves as a default when the
    iterable is empty.
    """"""

    it = iter(sequence)

    if initial is _initial_missing:
        try:
            value = next(it)
        except StopIteration:
            raise TypeError(
                ""0() of empty iterable with no initial value"") from None
    else:
        value = initial

    for element in it:
        value = function(value, element)

    return value","def 1(index, *args):
    """"""Equivalent to ``list(product(*args))[index]``.

    The products of *args* can be ordered lexicographically.
    :func:`nth_product` computes the product at sort position *index* without
    computing the previous products.

        >>> 1(8, range(2), range(2), range(2), range(2))
        (1, 0, 0, 0)

    ``IndexError`` will be raised if the given *index* is invalid.
    """"""
    pools = list(map(tuple, reversed(args)))
    ns = list(map(len, pools))

    c = 0(mul, ns)

    if index < 0:
        index += c

    if not 0 <= index < c:
        raise IndexError

    result = []
    for pool, n in zip(pools, ns):
        result.append(pool[index % n])
        index //= n

    return tuple(reversed(result))","{""reduce"": ""0""}"
476,476,"def pad_none(iterable):
    """"""Returns the sequence of elements and then returns ``None`` indefinitely.

        >>> take(5, pad_none(range(3)))
        [0, 1, 2, None, None]

    Useful for emulating the behavior of the built-in :func:`map` function.

    See also :func:`padded`.

    """"""
    return chain(iterable, repeat(None))","def repeat(stmt=""pass"", setup=""pass"", timer=default_timer,
           repeat=default_repeat, number=default_number, globals=None):
    """"""Convenience function to create Timer object and call repeat method.""""""
    return Timer(stmt, setup, timer, globals).repeat(repeat, number)","def timeit_repeat(stmt=""pass"", setup=""pass"", timer=default_timer,
           timeit_repeat=default_timeit_repeat, number=default_number, globals=None):
    """"""Convenience function to create Timer object and call timeit_repeat method.""""""
    return Timer(stmt, setup, timer, globals).timeit_repeat(timeit_repeat, number)","def return_sequence_of_elements_and_then_return_none(iterable):
    """"""Returns the sequence of elements and then returns ``None`` indefinitely.

        >>> take(5, return_sequence_of_elements_and_then_return_none(range(3)))
        [0, 1, 2, None, None]

    Useful for emulating the behavior of the built-in :func:`map` function.

    See also :func:`padded`.

    """"""
    return chain(iterable, timeit_repeat(None))","{""repeat"": ""timeit_repeat""}","def 0(stmt=""pass"", setup=""pass"", timer=default_timer,
           repeat=default_repeat, number=default_number, globals=None):
    """"""Convenience function to create Timer object and call repeat method.""""""
    return Timer(stmt, setup, timer, globals).0(repeat, number)","def 1(iterable):
    """"""Returns the sequence of elements and then returns ``None`` indefinitely.

        >>> take(5, 1(range(3)))
        [0, 1, 2, None, None]

    Useful for emulating the behavior of the built-in :func:`map` function.

    See also :func:`padded`.

    """"""
    return chain(iterable, 0(None))","{""repeat"": ""0""}"
477,477,"def ncycles(iterable, n):
    """"""Returns the sequence elements *n* times

    >>> list(ncycles([""a"", ""b""], 3))
    ['a', 'b', 'a', 'b', 'a', 'b']

    """"""
    return chain.from_iterable(repeat(tuple(iterable), n))","def repeat(stmt=""pass"", setup=""pass"", timer=default_timer,
           repeat=default_repeat, number=default_number, globals=None):
    """"""Convenience function to create Timer object and call repeat method.""""""
    return Timer(stmt, setup, timer, globals).repeat(repeat, number)","def timeit_repeat(stmt=""pass"", setup=""pass"", timer=default_timer,
           timeit_repeat=default_timeit_repeat, number=default_number, globals=None):
    """"""Convenience function to create Timer object and call timeit_repeat method.""""""
    return Timer(stmt, setup, timer, globals).timeit_repeat(timeit_repeat, number)","def return_sequence_elements_n_times(iterable, n):
    """"""Returns the sequence elements *n* times

    >>> list(return_sequence_elements_n_times([""a"", ""b""], 3))
    ['a', 'b', 'a', 'b', 'a', 'b']

    """"""
    return chain.from_iterable(timeit_repeat(tuple(iterable), n))","{""repeat"": ""timeit_repeat""}","def 0(stmt=""pass"", setup=""pass"", timer=default_timer,
           repeat=default_repeat, number=default_number, globals=None):
    """"""Convenience function to create Timer object and call repeat method.""""""
    return Timer(stmt, setup, timer, globals).0(repeat, number)","def 1(iterable, n):
    """"""Returns the sequence elements *n* times

    >>> list(1([""a"", ""b""], 3))
    ['a', 'b', 'a', 'b', 'a', 'b']

    """"""
    return chain.from_iterable(0(tuple(iterable), n))","{""repeat"": ""0""}"
478,478,"def repeatfunc(func, times=None, *args):
    """"""Call *func* with *args* repeatedly, returning an iterable over the
    results.

    If *times* is specified, the iterable will terminate after that many
    repetitions:

        >>> from operator import add
        >>> times = 4
        >>> args = 3, 5
        >>> list(repeatfunc(add, times, *args))
        [8, 8, 8, 8]

    If *times* is ``None`` the iterable will not terminate:

        >>> from random import randrange
        >>> times = None
        >>> args = 1, 11
        >>> take(6, repeatfunc(randrange, times, *args))  # doctest:+SKIP
        [2, 4, 8, 1, 8, 4]

    """"""
    if times is None:
        return starmap(func, repeat(args))
    return starmap(func, repeat(args, times))","def repeat(stmt=""pass"", setup=""pass"", timer=default_timer,
           repeat=default_repeat, number=default_number, globals=None):
    """"""Convenience function to create Timer object and call repeat method.""""""
    return Timer(stmt, setup, timer, globals).repeat(repeat, number)","def timeit_repeat(stmt=""pass"", setup=""pass"", timer=default_timer,
           timeit_repeat=default_timeit_repeat, number=default_number, globals=None):
    """"""Convenience function to create Timer object and call timeit_repeat method.""""""
    return Timer(stmt, setup, timer, globals).timeit_repeat(timeit_repeat, number)","def timeit_call_function_repeatedly_and_return_iterable_over_results(func, times=None, *args):
    """"""Call *func* with *args* timeit_repeatedly, returning an iterable over the
    results.

    If *times* is specified, the iterable will terminate after that many
    repetitions:

        >>> from operator import add
        >>> times = 4
        >>> args = 3, 5
        >>> list(timeit_call_function_repeatedly_and_return_iterable_over_results(add, times, *args))
        [8, 8, 8, 8]

    If *times* is ``None`` the iterable will not terminate:

        >>> from random import randrange
        >>> times = None
        >>> args = 1, 11
        >>> take(6, timeit_call_function_repeatedly_and_return_iterable_over_results(randrange, times, *args))  # doctest:+SKIP
        [2, 4, 8, 1, 8, 4]

    """"""
    if times is None:
        return starmap(func, timeit_repeat(args))
    return starmap(func, timeit_repeat(args, times))","{""repeat"": ""timeit_repeat""}","def 0(stmt=""pass"", setup=""pass"", timer=default_timer,
           repeat=default_repeat, number=default_number, globals=None):
    """"""Convenience function to create Timer object and call repeat method.""""""
    return Timer(stmt, setup, timer, globals).0(repeat, number)","def 1(func, times=None, *args):
    """"""Call *func* with *args* repeatedly, returning an iterable over the
    results.

    If *times* is specified, the iterable will terminate after that many
    repetitions:

        >>> from operator import add
        >>> times = 4
        >>> args = 3, 5
        >>> list(1(add, times, *args))
        [8, 8, 8, 8]

    If *times* is ``None`` the iterable will not terminate:

        >>> from random import randrange
        >>> times = None
        >>> args = 1, 11
        >>> take(6, 1(randrange, times, *args))  # doctest:+SKIP
        [2, 4, 8, 1, 8, 4]

    """"""
    if times is None:
        return starmap(func, 0(args))
    return starmap(func, 0(args, times))","{""repeat"": ""0""}"
479,479,"def first_true(iterable, default=None, pred=None):
    """"""
    Returns the first true value in the iterable.

    If no true value is found, returns *default*

    If *pred* is not None, returns the first item for which
    ``pred(item) == True`` .

        >>> first_true(range(10))
        1
        >>> first_true(range(10), pred=lambda x: x > 5)
        6
        >>> first_true(range(10), default='missing', pred=lambda x: x > 9)
        'missing'

    """"""
    return next(filter(pred, iterable), default)","def filter(names, pat):
    """"""Construct a list from those elements of the iterable NAMES that match PAT.""""""
    result = []
    pat = os.path.normcase(pat)
    match = _compile_pattern(pat)
    if os.path is posixpath:
        # normcase on posix is NOP. Optimize it away from the loop.
        for name in names:
            if match(name):
                result.append(name)
    else:
        for name in names:
            if match(os.path.normcase(name)):
                result.append(name)
    return result","def filter_names(names, pat):
    """"""Construct a list from those elements of the iterable NAMES that match PAT.""""""
    result = []
    pat = os.path.normcase(pat)
    match = _compile_pattern(pat)
    if os.path is posixpath:
        # normcase on posix is NOP. Optimize it away from the loop.
        for name in names:
            if match(name):
                result.append(name)
    else:
        for name in names:
            if match(os.path.normcase(name)):
                result.append(name)
    return result","def return_first_true_value_in_iterable(iterable, default=None, pred=None):
    """"""
    Returns the first true value in the iterable.

    If no true value is found, returns *default*

    If *pred* is not None, returns the first item for which
    ``pred(item) == True`` .

        >>> return_first_true_value_in_iterable(range(10))
        1
        >>> return_first_true_value_in_iterable(range(10), pred=lambda x: x > 5)
        6
        >>> return_first_true_value_in_iterable(range(10), default='missing', pred=lambda x: x > 9)
        'missing'

    """"""
    return next(filter_names(pred, iterable), default)","{""filter"": ""filter_names""}","def 0(names, pat):
    """"""Construct a list from those elements of the iterable NAMES that match PAT.""""""
    result = []
    pat = os.path.normcase(pat)
    match = _compile_pattern(pat)
    if os.path is posixpath:
        # normcase on posix is NOP. Optimize it away from the loop.
        for name in names:
            if match(name):
                result.append(name)
    else:
        for name in names:
            if match(os.path.normcase(name)):
                result.append(name)
    return result","def 1(iterable, default=None, pred=None):
    """"""
    Returns the first true value in the iterable.

    If no true value is found, returns *default*

    If *pred* is not None, returns the first item for which
    ``pred(item) == True`` .

        >>> 1(range(10))
        1
        >>> 1(range(10), pred=lambda x: x > 5)
        6
        >>> 1(range(10), default='missing', pred=lambda x: x > 9)
        'missing'

    """"""
    return next(0(pred, iterable), default)","{""filter"": ""0""}"
480,480,"def normalize_path(path):
    # type: (Any) -> str
    """"""Normalize a path by ensuring it is a string.

    If the resulting string contains path separators, an exception is raised.
    """"""
    str_path = str(path)
    parent, file_name = os.path.split(str_path)
    if parent:
        raise ValueError(f'{path!r} must be only a file name')
    return file_name","def str(val):
    """"""Convert float to string, taking the locale into account.""""""
    return _format(""%.12g"", val)","def locale_aware_float_to_string(val):
    """"""Convert float to locale_aware_float_to_stringing, taking the locale into account.""""""
    return _format(""%.12g"", val)","def normalize_path_by_ensuring_it_is_only_a_file_name(path):
    # type: (Any) -> locale_aware_float_to_string
    """"""Normalize a path by ensuring it is a locale_aware_float_to_stringing.

    If the resulting locale_aware_float_to_stringing contains path separators, an exception is raised.
    """"""
    locale_aware_float_to_string_path = locale_aware_float_to_string(path)
    parent, file_name = os.path.split(locale_aware_float_to_string_path)
    if parent:
        raise ValueError(f'{path!r} must be only a file name')
    return file_name","{""str"": ""locale_aware_float_to_string""}","def 0(val):
    """"""Convert float to string, taking the locale into account.""""""
    return _format(""%.12g"", val)","def 1(path):
    # type: (Any) -> str
    """"""Normalize a path by ensuring it is a string.

    If the resulting string contains path separators, an exception is raised.
    """"""
    str_path = 0(path)
    parent, file_name = os.path.split(str_path)
    if parent:
        raise ValueError(f'{path!r} must be only a file name')
    return file_name","{""str"": ""0""}"
481,481,"def apply_configuration(
    dist: ""Distribution"",
    filepath: _Path,
    ignore_option_errors=False,
) -> ""Distribution"":
    """"""Apply the configuration from a ``pyproject.toml`` file into an existing
    distribution object.
    """"""
    config = read_configuration(filepath, True, ignore_option_errors, dist)
    return _apply(dist, config, filepath)","def _apply(
    dist: ""Distribution"", filepath: _Path,
    other_files: Iterable[_Path] = (),
    ignore_option_errors: bool = False,
) -> Tuple[""ConfigHandler"", ...]:
    """"""Read configuration from ``filepath`` and applies to the ``dist`` object.""""""
    from setuptools.dist import _Distribution

    filepath = os.path.abspath(filepath)

    if not os.path.isfile(filepath):
        raise DistutilsFileError('Configuration file %s does not exist.' % filepath)

    current_directory = os.getcwd()
    os.chdir(os.path.dirname(filepath))
    filenames = [*other_files, filepath]

    try:
        _Distribution.parse_config_files(dist, filenames=filenames)
        handlers = parse_configuration(
            dist, dist.command_options, ignore_option_errors=ignore_option_errors
        )
        dist._finalize_license_files()
    finally:
        os.chdir(current_directory)

    return handlers","def apply_configuration_to_distribution(
    dist: ""Distribution"", filepath: _Path,
    other_files: Iterable[_Path] = (),
    ignore_option_errors: bool = False,
) -> Tuple[""ConfigHandler"", ...]:
    """"""Read configuration from ``filepath`` and applies to the ``dist`` object.""""""
    from setuptools.dist import _Distribution

    filepath = os.path.abspath(filepath)

    if not os.path.isfile(filepath):
        raise DistutilsFileError('Configuration file %s does not exist.' % filepath)

    current_directory = os.getcwd()
    os.chdir(os.path.dirname(filepath))
    filenames = [*other_files, filepath]

    try:
        _Distribution.parse_config_files(dist, filenames=filenames)
        handlers = parse_configuration(
            dist, dist.command_options, ignore_option_errors=ignore_option_errors
        )
        dist._finalize_license_files()
    finally:
        os.chdir(current_directory)

    return handlers","def apply_configuration(
    dist: ""Distribution"",
    filepath: _Path,
    ignore_option_errors=False,
) -> ""Distribution"":
    """"""Apply the configuration from a ``pyproject.toml`` file into an existing
    distribution object.
    """"""
    config = read_configuration(filepath, True, ignore_option_errors, dist)
    return apply_configuration_to_distribution(dist, config, filepath)","{""_apply"": ""apply_configuration_to_distribution""}","def 0(
    dist: ""Distribution"", filepath: _Path,
    other_files: Iterable[_Path] = (),
    ignore_option_errors: bool = False,
) -> Tuple[""ConfigHandler"", ...]:
    """"""Read configuration from ``filepath`` and applies to the ``dist`` object.""""""
    from setuptools.dist import _Distribution

    filepath = os.path.abspath(filepath)

    if not os.path.isfile(filepath):
        raise DistutilsFileError('Configuration file %s does not exist.' % filepath)

    current_directory = os.getcwd()
    os.chdir(os.path.dirname(filepath))
    filenames = [*other_files, filepath]

    try:
        _Distribution.parse_config_files(dist, filenames=filenames)
        handlers = parse_configuration(
            dist, dist.command_options, ignore_option_errors=ignore_option_errors
        )
        dist._finalize_license_files()
    finally:
        os.chdir(current_directory)

    return handlers","def apply_configuration(
    dist: ""Distribution"",
    filepath: _Path,
    ignore_option_errors=False,
) -> ""Distribution"":
    """"""Apply the configuration from a ``pyproject.toml`` file into an existing
    distribution object.
    """"""
    config = read_configuration(filepath, True, ignore_option_errors, dist)
    return 0(dist, config, filepath)","{""_apply"": ""0""}"
482,482,"def _apply(
    dist: ""Distribution"", filepath: _Path,
    other_files: Iterable[_Path] = (),
    ignore_option_errors: bool = False,
) -> Tuple[""ConfigHandler"", ...]:
    """"""Read configuration from ``filepath`` and applies to the ``dist`` object.""""""
    from setuptools.dist import _Distribution

    filepath = os.path.abspath(filepath)

    if not os.path.isfile(filepath):
        raise DistutilsFileError('Configuration file %s does not exist.' % filepath)

    current_directory = os.getcwd()
    os.chdir(os.path.dirname(filepath))
    filenames = [*other_files, filepath]

    try:
        _Distribution.parse_config_files(dist, filenames=filenames)
        handlers = parse_configuration(
            dist, dist.command_options, ignore_option_errors=ignore_option_errors
        )
        dist._finalize_license_files()
    finally:
        os.chdir(current_directory)

    return handlers","def parse_configuration(
    distribution: ""Distribution"",
    command_options: AllCommandOptions,
    ignore_option_errors=False
) -> Tuple[""ConfigMetadataHandler"", ""ConfigOptionsHandler""]:
    """"""Performs additional parsing of configuration options
    for a distribution.

    Returns a list of used option handlers.

    :param Distribution distribution:
    :param dict command_options:
    :param bool ignore_option_errors: Whether to silently ignore
        options, values of which could not be resolved (e.g. due to exceptions
        in directives such as file:, attr:, etc.).
        If False exceptions are propagated as expected.
    :rtype: list
    """"""
    with expand.EnsurePackagesDiscovered(distribution) as ensure_discovered:
        options = ConfigOptionsHandler(
            distribution,
            command_options,
            ignore_option_errors,
            ensure_discovered,
        )

        options.parse()
        if not distribution.package_dir:
            distribution.package_dir = options.package_dir  # Filled by `find_packages`

        meta = ConfigMetadataHandler(
            distribution.metadata,
            command_options,
            ignore_option_errors,
            ensure_discovered,
            distribution.package_dir,
            distribution.src_root,
        )
        meta.parse()

    return meta, options","def parse_configuration(
    distribution: ""Distribution"",
    command_options: AllCommandOptions,
    ignore_option_errors=False
) -> Tuple[""ConfigMetadataHandler"", ""ConfigOptionsHandler""]:
    """"""Performs additional parsing of configuration options
    for a distribution.

    Returns a list of used option handlers.

    :param Distribution distribution:
    :param dict command_options:
    :param bool ignore_option_errors: Whether to silently ignore
        options, values of which could not be resolved (e.g. due to exceptions
        in directives such as file:, attr:, etc.).
        If False exceptions are propagated as expected.
    :rtype: list
    """"""
    with expand.EnsurePackagesDiscovered(distribution) as ensure_discovered:
        options = ConfigOptionsHandler(
            distribution,
            command_options,
            ignore_option_errors,
            ensure_discovered,
        )

        options.parse()
        if not distribution.package_dir:
            distribution.package_dir = options.package_dir  # Filled by `find_packages`

        meta = ConfigMetadataHandler(
            distribution.metadata,
            command_options,
            ignore_option_errors,
            ensure_discovered,
            distribution.package_dir,
            distribution.src_root,
        )
        meta.parse()

    return meta, options","def apply_configuration_to_distribution(
    dist: ""Distribution"", filepath: _Path,
    other_files: Iterable[_Path] = (),
    ignore_option_errors: bool = False,
) -> Tuple[""ConfigHandler"", ...]:
    """"""Read configuration from ``filepath`` and applies to the ``dist`` object.""""""
    from setuptools.dist import _Distribution

    filepath = os.path.abspath(filepath)

    if not os.path.isfile(filepath):
        raise DistutilsFileError('Configuration file %s does not exist.' % filepath)

    current_directory = os.getcwd()
    os.chdir(os.path.dirname(filepath))
    filenames = [*other_files, filepath]

    try:
        _Distribution.parse_config_files(dist, filenames=filenames)
        handlers = parse_configuration(
            dist, dist.command_options, ignore_option_errors=ignore_option_errors
        )
        dist._finalize_license_files()
    finally:
        os.chdir(current_directory)

    return handlers",{},"def parse_configuration(
    distribution: ""Distribution"",
    command_options: AllCommandOptions,
    ignore_option_errors=False
) -> Tuple[""ConfigMetadataHandler"", ""ConfigOptionsHandler""]:
    """"""Performs additional parsing of configuration options
    for a distribution.

    Returns a list of used option handlers.

    :param Distribution distribution:
    :param dict command_options:
    :param bool ignore_option_errors: Whether to silently ignore
        options, values of which could not be resolved (e.g. due to exceptions
        in directives such as file:, attr:, etc.).
        If False exceptions are propagated as expected.
    :rtype: list
    """"""
    with expand.EnsurePackagesDiscovered(distribution) as ensure_discovered:
        options = ConfigOptionsHandler(
            distribution,
            command_options,
            ignore_option_errors,
            ensure_discovered,
        )

        options.parse()
        if not distribution.package_dir:
            distribution.package_dir = options.package_dir  # Filled by `find_packages`

        meta = ConfigMetadataHandler(
            distribution.metadata,
            command_options,
            ignore_option_errors,
            ensure_discovered,
            distribution.package_dir,
            distribution.src_root,
        )
        meta.parse()

    return meta, options","def 1(
    dist: ""Distribution"", filepath: _Path,
    other_files: Iterable[_Path] = (),
    ignore_option_errors: bool = False,
) -> Tuple[""ConfigHandler"", ...]:
    """"""Read configuration from ``filepath`` and applies to the ``dist`` object.""""""
    from setuptools.dist import _Distribution

    filepath = os.path.abspath(filepath)

    if not os.path.isfile(filepath):
        raise DistutilsFileError('Configuration file %s does not exist.' % filepath)

    current_directory = os.getcwd()
    os.chdir(os.path.dirname(filepath))
    filenames = [*other_files, filepath]

    try:
        _Distribution.parse_config_files(dist, filenames=filenames)
        handlers = parse_configuration(
            dist, dist.command_options, ignore_option_errors=ignore_option_errors
        )
        dist._finalize_license_files()
    finally:
        os.chdir(current_directory)

    return handlers",{}
483,483,"def configuration_to_dict(handlers: Tuple[""ConfigHandler"", ...]) -> dict:
    """"""Returns configuration data gathered by given handlers as a dict.

    :param list[ConfigHandler] handlers: Handlers list,
        usually from parse_configuration()

    :rtype: dict
    """"""
    config_dict: dict = defaultdict(dict)

    for handler in handlers:
        for option in handler.set_options:
            value = _get_option(handler.target_obj, option)
            config_dict[handler.section_prefix][option] = value

    return config_dict","def _get_option(target_obj: Target, key: str):
    """"""
    Given a target object and option key, get that option from
    the target object, either through a get_{key} method or
    from an attribute directly.
    """"""
    getter_name = 'get_{key}'.format(**locals())
    by_attribute = functools.partial(getattr, target_obj, key)
    getter = getattr(target_obj, getter_name, by_attribute)
    return getter()","def get_target_option(target_obj: Target, key: str):
    """"""
    Given a target object and option key, get that option from
    the target object, either through a get_{key} method or
    from an attribute directly.
    """"""
    getter_name = 'get_{key}'.format(**locals())
    by_attribute = functools.partial(getattr, target_obj, key)
    getter = getattr(target_obj, getter_name, by_attribute)
    return getter()","def return_dict_of_configuration_data_gathered_by_given_handlers(handlers: Tuple[""ConfigHandler"", ...]) -> dict:
    """"""Returns configuration data gathered by given handlers as a dict.

    :param list[ConfigHandler] handlers: Handlers list,
        usually from parse_configuration()

    :rtype: dict
    """"""
    config_dict: dict = defaultdict(dict)

    for handler in handlers:
        for option in handler.set_options:
            value = get_target_option(handler.target_obj, option)
            config_dict[handler.section_prefix][option] = value

    return config_dict","{""_get_option"": ""get_target_option""}","def 0(target_obj: Target, key: str):
    """"""
    Given a target object and option key, get that option from
    the target object, either through a get_{key} method or
    from an attribute directly.
    """"""
    getter_name = 'get_{key}'.format(**locals())
    by_attribute = functools.partial(getattr, target_obj, key)
    getter = getattr(target_obj, getter_name, by_attribute)
    return getter()","def 1(handlers: Tuple[""ConfigHandler"", ...]) -> dict:
    """"""Returns configuration data gathered by given handlers as a dict.

    :param list[ConfigHandler] handlers: Handlers list,
        usually from parse_configuration()

    :rtype: dict
    """"""
    config_dict: dict = defaultdict(dict)

    for handler in handlers:
        for option in handler.set_options:
            value = 0(handler.target_obj, option)
            config_dict[handler.section_prefix][option] = value

    return config_dict","{""_get_option"": ""0""}"
484,484,"def _some_attrgetter(*items):
    """"""
    Return the first ""truth-y"" attribute or None
    >>> from types import SimpleNamespace
    >>> obj = SimpleNamespace(a=42, b=SimpleNamespace(c=13))
    >>> _some_attrgetter(""d"", ""a"", ""b.c"")(obj)
    42
    >>> _some_attrgetter(""d"", ""e"", ""b.c"", ""a"")(obj)
    13
    >>> _some_attrgetter(""d"", ""e"", ""f"")(obj) is None
    True
    """"""
    def _acessor(obj):
        values = (_attrgetter(i)(obj) for i in items)
        return next((i for i in values if i is not None), None)
    return _acessor","def _attrgetter(attr):
    """"""
    Similar to ``operator.attrgetter`` but returns None if ``attr`` is not found
    >>> from types import SimpleNamespace
    >>> obj = SimpleNamespace(a=42, b=SimpleNamespace(c=13))
    >>> _attrgetter(""a"")(obj)
    42
    >>> _attrgetter(""b.c"")(obj)
    13
    >>> _attrgetter(""d"")(obj) is None
    True
    """"""
    return partial(reduce, lambda acc, x: getattr(acc, x, None), attr.split("".""))","def safe_attr_getter(attr):
    """"""
    Similar to ``operator.attrgetter`` but returns None if ``attr`` is not found
    >>> from types import SimpleNamespace
    >>> obj = SimpleNamespace(a=42, b=SimpleNamespace(c=13))
    >>> safe_attr_getter(""a"")(obj)
    42
    >>> safe_attr_getter(""b.c"")(obj)
    13
    >>> safe_attr_getter(""d"")(obj) is None
    True
    """"""
    return partial(reduce, lambda acc, x: getattr(acc, x, None), attr.split("".""))","def _somesafe_attr_getter(*items):
    """"""
    Return the first ""truth-y"" attribute or None
    >>> from types import SimpleNamespace
    >>> obj = SimpleNamespace(a=42, b=SimpleNamespace(c=13))
    >>> _somesafe_attr_getter(""d"", ""a"", ""b.c"")(obj)
    42
    >>> _somesafe_attr_getter(""d"", ""e"", ""b.c"", ""a"")(obj)
    13
    >>> _somesafe_attr_getter(""d"", ""e"", ""f"")(obj) is None
    True
    """"""
    def attribute_accessor_with_items(obj):
        values = (safe_attr_getter(i)(obj) for i in items)
        return next((i for i in values if i is not None), None)
    return attribute_accessor_with_items","{""_attrgetter"": ""safe_attr_getter""}","def 0(attr):
    """"""
    Similar to ``operator.attrgetter`` but returns None if ``attr`` is not found
    >>> from types import SimpleNamespace
    >>> obj = SimpleNamespace(a=42, b=SimpleNamespace(c=13))
    >>> 0(""a"")(obj)
    42
    >>> 0(""b.c"")(obj)
    13
    >>> 0(""d"")(obj) is None
    True
    """"""
    return partial(reduce, lambda acc, x: getattr(acc, x, None), attr.split("".""))","def _some0(*items):
    """"""
    Return the first ""truth-y"" attribute or None
    >>> from types import SimpleNamespace
    >>> obj = SimpleNamespace(a=42, b=SimpleNamespace(c=13))
    >>> _some0(""d"", ""a"", ""b.c"")(obj)
    42
    >>> _some0(""d"", ""e"", ""b.c"", ""a"")(obj)
    13
    >>> _some0(""d"", ""e"", ""f"")(obj) is None
    True
    """"""
    def 2(obj):
        values = (0(i)(obj) for i in items)
        return next((i for i in values if i is not None), None)
    return _acessor","{""_attrgetter"": ""0""}"
485,485,"def cmdclass(
    values: Dict[str, str],
    package_dir: Optional[Mapping[str, str]] = None,
    root_dir: Optional[_Path] = None
) -> Dict[str, Callable]:
    """"""Given a dictionary mapping command names to strings for qualified class
    names, apply :func:`resolve_class` to the dict values.
    """"""
    return {k: resolve_class(v, package_dir, root_dir) for k, v in values.items()}","def resolve_class(
    qualified_class_name: str,
    package_dir: Optional[Mapping[str, str]] = None,
    root_dir: Optional[_Path] = None
) -> Callable:
    """"""Given a qualified class name, return the associated class object""""""
    root_dir = root_dir or os.getcwd()
    idx = qualified_class_name.rfind('.')
    class_name = qualified_class_name[idx + 1 :]
    pkg_name = qualified_class_name[:idx]

    _parent_path, path, module_name = _find_module(pkg_name, package_dir, root_dir)
    module = _load_spec(_find_spec(module_name, path), module_name)
    return getattr(module, class_name)","def resolve_class(
    qualified_class_name: str,
    package_dir: Optional[Mapping[str, str]] = None,
    root_dir: Optional[_Path] = None
) -> Callable:
    """"""Given a qualified class name, return the associated class object""""""
    root_dir = root_dir or os.getcwd()
    idx = qualified_class_name.rfind('.')
    class_name = qualified_class_name[idx + 1 :]
    pkg_name = qualified_class_name[:idx]

    _parent_path, path, module_name = _find_module(pkg_name, package_dir, root_dir)
    module = _load_spec(_find_spec(module_name, path), module_name)
    return getattr(module, class_name)","def cmdclass(
    values: Dict[str, str],
    package_dir: Optional[Mapping[str, str]] = None,
    root_dir: Optional[_Path] = None
) -> Dict[str, Callable]:
    """"""Given a dictionary mapping command names to strings for qualified class
    names, apply :func:`resolve_class` to the dict values.
    """"""
    return {k: resolve_class(v, package_dir, root_dir) for k, v in values.items()}",{},"def resolve_class(
    qualified_class_name: str,
    package_dir: Optional[Mapping[str, str]] = None,
    root_dir: Optional[_Path] = None
) -> Callable:
    """"""Given a qualified class name, return the associated class object""""""
    root_dir = root_dir or os.getcwd()
    idx = qualified_class_name.rfind('.')
    class_name = qualified_class_name[idx + 1 :]
    pkg_name = qualified_class_name[:idx]

    _parent_path, path, module_name = _find_module(pkg_name, package_dir, root_dir)
    module = _load_spec(_find_spec(module_name, path), module_name)
    return getattr(module, class_name)","def cmdclass(
    values: Dict[str, str],
    package_dir: Optional[Mapping[str, str]] = None,
    root_dir: Optional[_Path] = None
) -> Dict[str, Callable]:
    """"""Given a dictionary mapping command names to strings for qualified class
    names, apply :func:`resolve_class` to the dict values.
    """"""
    return {k: resolve_class(v, package_dir, root_dir) for k, v in values.items()}",{}
486,486,"def canonic_data_files(
    data_files: Union[list, dict], root_dir: Optional[_Path] = None
) -> List[Tuple[str, List[str]]]:
    """"""For compatibility with ``setup.py``, ``data_files`` should be a list
    of pairs instead of a dict.

    This function also expands glob patterns.
    """"""
    if isinstance(data_files, list):
        return data_files

    return [
        (dest, glob_relative(patterns, root_dir))
        for dest, patterns in data_files.items()
    ]","def glob_relative(
    patterns: Iterable[str], root_dir: Optional[_Path] = None
) -> List[str]:
    """"""Expand the list of glob patterns, but preserving relative paths.

    :param list[str] patterns: List of glob patterns
    :param str root_dir: Path to which globs should be relative
                         (current directory by default)
    :rtype: list
    """"""
    glob_characters = {'*', '?', '[', ']', '{', '}'}
    expanded_values = []
    root_dir = root_dir or os.getcwd()
    for value in patterns:

        # Has globby characters?
        if any(char in value for char in glob_characters):
            # then expand the glob pattern while keeping paths *relative*:
            glob_path = os.path.abspath(os.path.join(root_dir, value))
            expanded_values.extend(sorted(
                os.path.relpath(path, root_dir).replace(os.sep, ""/"")
                for path in iglob(glob_path, recursive=True)))

        else:
            # take the value as-is
            path = os.path.relpath(value, root_dir).replace(os.sep, ""/"")
            expanded_values.append(path)

    return expanded_values","def expand_globs_relative(
    patterns: Iterable[str], root_dir: Optional[_Path] = None
) -> List[str]:
    """"""Expand the list of glob patterns, but preserving relative paths.

    :param list[str] patterns: List of glob patterns
    :param str root_dir: Path to which globs should be relative
                         (current directory by default)
    :rtype: list
    """"""
    glob_characters = {'*', '?', '[', ']', '{', '}'}
    expanded_values = []
    root_dir = root_dir or os.getcwd()
    for value in patterns:

        # Has globby characters?
        if any(char in value for char in glob_characters):
            # then expand the glob pattern while keeping paths *relative*:
            glob_path = os.path.abspath(os.path.join(root_dir, value))
            expanded_values.extend(sorted(
                os.path.relpath(path, root_dir).replace(os.sep, ""/"")
                for path in iglob(glob_path, recursive=True)))

        else:
            # take the value as-is
            path = os.path.relpath(value, root_dir).replace(os.sep, ""/"")
            expanded_values.append(path)

    return expanded_values","def canonicalize_data_files_for_setup_py(
    data_files: Union[list, dict], root_dir: Optional[_Path] = None
) -> List[Tuple[str, List[str]]]:
    """"""For compatibility with ``setup.py``, ``data_files`` should be a list
    of pairs instead of a dict.

    This function also expands glob patterns.
    """"""
    if isinstance(data_files, list):
        return data_files

    return [
        (dest, expand_globs_relative(patterns, root_dir))
        for dest, patterns in data_files.items()
    ]","{""glob_relative"": ""expand_globs_relative""}","def 0(
    patterns: Iterable[str], root_dir: Optional[_Path] = None
) -> List[str]:
    """"""Expand the list of glob patterns, but preserving relative paths.

    :param list[str] patterns: List of glob patterns
    :param str root_dir: Path to which globs should be relative
                         (current directory by default)
    :rtype: list
    """"""
    glob_characters = {'*', '?', '[', ']', '{', '}'}
    expanded_values = []
    root_dir = root_dir or os.getcwd()
    for value in patterns:

        # Has globby characters?
        if any(char in value for char in glob_characters):
            # then expand the glob pattern while keeping paths *relative*:
            glob_path = os.path.abspath(os.path.join(root_dir, value))
            expanded_values.extend(sorted(
                os.path.relpath(path, root_dir).replace(os.sep, ""/"")
                for path in iglob(glob_path, recursive=True)))

        else:
            # take the value as-is
            path = os.path.relpath(value, root_dir).replace(os.sep, ""/"")
            expanded_values.append(path)

    return expanded_values","def 1(
    data_files: Union[list, dict], root_dir: Optional[_Path] = None
) -> List[Tuple[str, List[str]]]:
    """"""For compatibility with ``setup.py``, ``data_files`` should be a list
    of pairs instead of a dict.

    This function also expands glob patterns.
    """"""
    if isinstance(data_files, list):
        return data_files

    return [
        (dest, 0(patterns, root_dir))
        for dest, patterns in data_files.items()
    ]","{""glob_relative"": ""0""}"
487,487,"def walk_egg(egg_dir):
    """"""Walk an unpacked egg's contents, skipping the metadata directory""""""
    walker = sorted_walk(egg_dir)
    base, dirs, files = next(walker)
    if 'EGG-INFO' in dirs:
        dirs.remove('EGG-INFO')
    yield base, dirs, files
    for bdf in walker:
        yield bdf","def sorted_walk(dir):
    """"""Do os.walk in a reproducible way,
    independent of indeterministic filesystem readdir order
    """"""
    for base, dirs, files in os.walk(dir):
        dirs.sort()
        files.sort()
        yield base, dirs, files","def reproducible_os_walk(dir):
    """"""Do os.walk in a reproducible way,
    independent of indeterministic filesystem readdir order
    """"""
    for base, dirs, files in os.walk(dir):
        dirs.sort()
        files.sort()
        yield base, dirs, files","def egg_directory_contents_walker_without_metadata(egg_dir):
    """"""Walk an unpacked egg's contents, skipping the metadata directory""""""
    walker = reproducible_os_walk(egg_dir)
    base, dirs, files = next(walker)
    if 'EGG-INFO' in dirs:
        dirs.remove('EGG-INFO')
    yield base, dirs, files
    for bdf in walker:
        yield bdf","{""sorted_walk"": ""reproducible_os_walk""}","def 0(dir):
    """"""Do os.walk in a reproducible way,
    independent of indeterministic filesystem readdir order
    """"""
    for base, dirs, files in os.walk(dir):
        dirs.sort()
        files.sort()
        yield base, dirs, files","def 1(egg_dir):
    """"""Walk an unpacked egg's contents, skipping the metadata directory""""""
    walker = 0(egg_dir)
    base, dirs, files = next(walker)
    if 'EGG-INFO' in dirs:
        dirs.remove('EGG-INFO')
    yield base, dirs, files
    for bdf in walker:
        yield bdf","{""sorted_walk"": ""0""}"
488,488,"def iter_symbols(code):
    """"""Yield names and strings used by `code` and its nested code objects""""""
    for name in code.co_names:
        yield name
    for const in code.co_consts:
        if isinstance(const, str):
            yield const
        elif isinstance(const, CodeType):
            for name in iter_symbols(const):
                yield name","def iter_symbols(code):
    """"""Yield names and strings used by `code` and its nested code objects""""""
    for name in code.co_names:
        yield name
    for const in code.co_consts:
        if isinstance(const, str):
            yield const
        elif isinstance(const, CodeType):
            for name in iter_symbols(const):
                yield name","def yield_code_symbols(code):
    """"""Yield names and strings used by `code` and its nested code objects""""""
    for name in code.co_names:
        yield name
    for const in code.co_consts:
        if isinstance(const, str):
            yield const
        elif isinstance(const, CodeType):
            for name in yield_code_symbols(const):
                yield name","def yield_code_symbols(code):
    """"""Yield names and strings used by `code` and its nested code objects""""""
    for name in code.co_names:
        yield name
    for const in code.co_consts:
        if isinstance(const, str):
            yield const
        elif isinstance(const, CodeType):
            for name in yield_code_symbols(const):
                yield name","{""iter_symbols"": ""yield_code_symbols""}","def 0(code):
    """"""Yield names and strings used by `code` and its nested code objects""""""
    for name in code.co_names:
        yield name
    for const in code.co_consts:
        if isinstance(const, str):
            yield const
        elif isinstance(const, CodeType):
            for name in 0(const):
                yield name","def 0(code):
    """"""Yield names and strings used by `code` and its nested code objects""""""
    for name in code.co_names:
        yield name
    for const in code.co_consts:
        if isinstance(const, str):
            yield const
        elif isinstance(const, CodeType):
            for name in 0(const):
                yield name","{""iter_symbols"": ""0""}"
489,489,"def is_python_script(script_text, filename):
    """"""Is this text, as a whole, a Python script? (as opposed to shell/bat/etc.
    """"""
    if filename.endswith('.py') or filename.endswith('.pyw'):
        return True  # extension says it's Python
    if is_python(script_text, filename):
        return True  # it's syntactically valid Python
    if script_text.startswith('#!'):
        # It begins with a '#!' line, so check if 'python' is in it somewhere
        return 'python' in script_text.splitlines()[0].lower()

    return False  # Not any Python I can recognize","def is_python(text, filename='<string>'):
    ""Is this string a valid Python script?""
    try:
        compile(text, filename, 'exec')
    except (SyntaxError, TypeError):
        return False
    else:
        return True","def check_valid_python_script(text, filename='<string>'):
    ""Is this string a valid Python script?""
    try:
        compile(text, filename, 'exec')
    except (SyntaxError, TypeError):
        return False
    else:
        return True","def check_valid_python_script_script(script_text, filename):
    """"""Is this text, as a whole, a Python script? (as opposed to shell/bat/etc.
    """"""
    if filename.endswith('.py') or filename.endswith('.pyw'):
        return True  # extension says it's Python
    if check_valid_python_script(script_text, filename):
        return True  # it's syntactically valid Python
    if script_text.startswith('#!'):
        # It begins with a '#!' line, so check if 'python' is in it somewhere
        return 'python' in script_text.splitlines()[0].lower()

    return False  # Not any Python I can recognize","{""is_python"": ""check_valid_python_script""}","def 0(text, filename='<string>'):
    ""Is this string a valid Python script?""
    try:
        compile(text, filename, 'exec')
    except (SyntaxError, TypeError):
        return False
    else:
        return True","def 1(script_text, filename):
    """"""Is this text, as a whole, a Python script? (as opposed to shell/bat/etc.
    """"""
    if filename.endswith('.py') or filename.endswith('.pyw'):
        return True  # extension says it's Python
    if 0(script_text, filename):
        return True  # it's syntactically valid Python
    if script_text.startswith('#!'):
        # It begins with a '#!' line, so check if 'python' is in it somewhere
        return 'python' in script_text.splitlines()[0].lower()

    return False  # Not any Python I can recognize","{""is_python"": ""0""}"
490,490,"def only_strs(values):
    """"""
    Exclude non-str values. Ref #3063.
    """"""
    return filter(lambda val: isinstance(val, str), values)","def filter(names, pat):
    """"""Construct a list from those elements of the iterable NAMES that match PAT.""""""
    result = []
    pat = os.path.normcase(pat)
    match = _compile_pattern(pat)
    if os.path is posixpath:
        # normcase on posix is NOP. Optimize it away from the loop.
        for name in names:
            if match(name):
                result.append(name)
    else:
        for name in names:
            if match(os.path.normcase(name)):
                result.append(name)
    return result","def filter_names(names, pat):
    """"""Construct a list from those elements of the iterable NAMES that match PAT.""""""
    result = []
    pat = os.path.normcase(pat)
    match = _compile_pattern(pat)
    if os.path is posixpath:
        # normcase on posix is NOP. Optimize it away from the loop.
        for name in names:
            if match(name):
                result.append(name)
    else:
        for name in names:
            if match(os.path.normcase(name)):
                result.append(name)
    return result","def filter_only_string_values(values):
    """"""
    Exclude non-str values. Ref #3063.
    """"""
    return filter_names(lambda val: isinstance(val, str), values)","{""filter"": ""filter_names""}","def 0(names, pat):
    """"""Construct a list from those elements of the iterable NAMES that match PAT.""""""
    result = []
    pat = os.path.normcase(pat)
    match = _compile_pattern(pat)
    if os.path is posixpath:
        # normcase on posix is NOP. Optimize it away from the loop.
        for name in names:
            if match(name):
                result.append(name)
    else:
        for name in names:
            if match(os.path.normcase(name)):
                result.append(name)
    return result","def 1(values):
    """"""
    Exclude non-str values. Ref #3063.
    """"""
    return 0(lambda val: isinstance(val, str), values)","{""filter"": ""0""}"
491,491,"def _version(version: str) -> str:
    """"""Convert an arbitrary string to a version string.""""""
    v = version.replace(' ', '.')
    try:
        return str(packaging.version.Version(v)).replace(""-"", ""_"")
    except packaging.version.InvalidVersion:
        msg = f""""""Invalid version: {version!r}.
        !!\n\n
        ###################
        # Invalid version #
        ###################
        {version!r} is not valid according to PEP 440.\n
        Please make sure specify a valid version for your package.
        Also note that future releases of setuptools may halt the build process
        if an invalid version is given.
        \n\n!!
        """"""
        warnings.warn(cleandoc(msg))
        return _safe(v).strip(""_"")","def cleandoc(doc):
    """"""Clean up indentation from docstrings.

    Any whitespace that can be uniformly removed from the second line
    onwards is removed.""""""
    try:
        lines = doc.expandtabs().split('\n')
    except UnicodeError:
        return None
    else:
        # Find minimum indentation of any non-blank lines after first line.
        margin = sys.maxsize
        for line in lines[1:]:
            content = len(line.lstrip())
            if content:
                indent = len(line) - content
                margin = min(margin, indent)
        # Remove indentation.
        if lines:
            lines[0] = lines[0].lstrip()
        if margin < sys.maxsize:
            for i in range(1, len(lines)): lines[i] = lines[i][margin:]
        # Remove any trailing or leading blank lines.
        while lines and not lines[-1]:
            lines.pop()
        while lines and not lines[0]:
            lines.pop(0)
        return '\n'.join(lines)

def _safe(component: str) -> str:
    """"""Escape a component used to form a wheel name according to PEP 491""""""
    return re.sub(r""[^\w\d.]+"", ""_"", component)

def str(val):
    """"""Convert float to string, taking the locale into account.""""""
    return _format(""%.12g"", val)","def clean_up_indentation_from_docstrings(doc):
    """"""Clean up indentation from docstrings.

    Any whitespace that can be uniformly removed from the second line
    onwards is removed.""""""
    try:
        lines = doc.expandtabs().split('\n')
    except UnicodeError:
        return None
    else:
        # Find minimum indentation of any non-blank lines after first line.
        margin = sys.maxsize
        for line in lines[1:]:
            content = len(line.lstrip())
            if content:
                indent = len(line) - content
                margin = min(margin, indent)
        # Remove indentation.
        if lines:
            lines[0] = lines[0].lstrip()
        if margin < sys.maxsize:
            for i in range(1, len(lines)): lines[i] = lines[i][margin:]
        # Remove any trailing or leading blank lines.
        while lines and not lines[-1]:
            lines.pop()
        while lines and not lines[0]:
            lines.pop(0)
        return '\n'.join(lines)

def escape_wheel_component(component: str) -> str:
    """"""Escape a component used to form a wheel name according to PEP 491""""""
    return re.sub(r""[^\w\d.]+"", ""_"", component)

def str(val):
    """"""Convert float to string, taking the locale into account.""""""
    return _format(""%.12g"", val)","def _version(version: str) -> str:
    """"""Convert an arbitrary string to a version string.""""""
    v = version.replace(' ', '.')
    try:
        return str(packaging.version.Version(v)).replace(""-"", ""_"")
    except packaging.version.InvalidVersion:
        msg = f""""""Invalid version: {version!r}.
        !!\n\n
        ###################
        # Invalid version #
        ###################
        {version!r} is not valid according to PEP 440.\n
        Please make sure specify a valid version for your package.
        Also note that future releases of setuptools may halt the build process
        if an invalid version is given.
        \n\n!!
        """"""
        warnings.warn(clean_up_indentation_from_docstrings(msg))
        return escape_wheel_component(v).strip(""_"")","{""cleandoc"": ""clean_up_indentation_from_docstrings"", ""_safe"": ""escape_wheel_component""}","def 0(doc):
    """"""Clean up indentation from docstrings.

    Any whitespace that can be uniformly removed from the second line
    onwards is removed.""""""
    try:
        lines = doc.expandtabs().split('\n')
    except UnicodeError:
        return None
    else:
        # Find minimum indentation of any non-blank lines after first line.
        margin = sys.maxsize
        for line in lines[1:]:
            content = len(line.lstrip())
            if content:
                indent = len(line) - content
                margin = min(margin, indent)
        # Remove indentation.
        if lines:
            lines[0] = lines[0].lstrip()
        if margin < sys.maxsize:
            for i in range(1, len(lines)): lines[i] = lines[i][margin:]
        # Remove any trailing or leading blank lines.
        while lines and not lines[-1]:
            lines.pop()
        while lines and not lines[0]:
            lines.pop(0)
        return '\n'.join(lines)

def 1(component: str) -> str:
    """"""Escape a component used to form a wheel name according to PEP 491""""""
    return re.sub(r""[^\w\d.]+"", ""_"", component)

def str(val):
    """"""Convert float to string, taking the locale into account.""""""
    return _format(""%.12g"", val)","def _version(version: str) -> str:
    """"""Convert an arbitrary string to a version string.""""""
    v = version.replace(' ', '.')
    try:
        return str(packaging.version.Version(v)).replace(""-"", ""_"")
    except packaging.version.InvalidVersion:
        msg = f""""""Invalid version: {version!r}.
        !!\n\n
        ###################
        # Invalid version #
        ###################
        {version!r} is not valid according to PEP 440.\n
        Please make sure specify a valid version for your package.
        Also note that future releases of setuptools may halt the build process
        if an invalid version is given.
        \n\n!!
        """"""
        warnings.warn(0(msg))
        return 1(v).strip(""_"")","{""cleandoc"": ""0"", ""_safe"": ""1""}"
492,492,"def _find_all_simple(path):
    """"""
    Find all files under 'path'
    """"""
    all_unique = _UniqueDirs.filter(os.walk(path, followlinks=True))
    results = (
        os.path.join(base, file) for base, dirs, files in all_unique for file in files
    )
    return filter(os.path.isfile, results)","def filter(names, pat):
    """"""Construct a list from those elements of the iterable NAMES that match PAT.""""""
    result = []
    pat = os.path.normcase(pat)
    match = _compile_pattern(pat)
    if os.path is posixpath:
        # normcase on posix is NOP. Optimize it away from the loop.
        for name in names:
            if match(name):
                result.append(name)
    else:
        for name in names:
            if match(os.path.normcase(name)):
                result.append(name)
    return result","def filter_names(names, pat):
    """"""Construct a list from those elements of the iterable NAMES that match PAT.""""""
    result = []
    pat = os.path.normcase(pat)
    match = _compile_pattern(pat)
    if os.path is posixpath:
        # normcase on posix is NOP. Optimize it away from the loop.
        for name in names:
            if match(name):
                result.append(name)
    else:
        for name in names:
            if match(os.path.normcase(name)):
                result.append(name)
    return result","def find_all_files_under_path(path):
    """"""
    Find all files under 'path'
    """"""
    all_unique = _UniqueDirs.filter_names(os.walk(path, followlinks=True))
    results = (
        os.path.join(base, file) for base, dirs, files in all_unique for file in files
    )
    return filter_names(os.path.isfile, results)","{""filter"": ""filter_names""}","def 0(names, pat):
    """"""Construct a list from those elements of the iterable NAMES that match PAT.""""""
    result = []
    pat = os.path.normcase(pat)
    match = _compile_pattern(pat)
    if os.path is posixpath:
        # normcase on posix is NOP. Optimize it away from the loop.
        for name in names:
            if match(name):
                result.append(name)
    else:
        for name in names:
            if match(os.path.normcase(name)):
                result.append(name)
    return result","def 1(path):
    """"""
    Find all files under 'path'
    """"""
    all_unique = _UniqueDirs.0(os.walk(path, followlinks=True))
    results = (
        os.path.join(base, file) for base, dirs, files in all_unique for file in files
    )
    return 0(os.path.isfile, results)","{""filter"": ""0""}"
493,493,"def run_commands(dist):
    """"""Given a Distribution object run all the commands,
    raising ``SystemExit`` errors in the case of failure.

    This function assumes that either ``sys.argv`` or ``dist.script_args``
    is already set accordingly.
    """"""
    try:
        dist.run_commands()
    except KeyboardInterrupt:
        raise SystemExit(""interrupted"")
    except OSError as exc:
        if DEBUG:
            sys.stderr.write(""error: %s\n"" % (exc,))
            raise
        else:
            raise SystemExit(""error: %s"" % (exc,))

    except (DistutilsError, CCompilerError) as msg:
        if DEBUG:
            raise
        else:
            raise SystemExit(""error: "" + str(msg))

    return dist","def str(val):
    """"""Convert float to string, taking the locale into account.""""""
    return _format(""%.12g"", val)","def locale_aware_float_to_string(val):
    """"""Convert float to locale_aware_float_to_stringing, taking the locale into account.""""""
    return _format(""%.12g"", val)","def execute_all_commands_in_distribution(dist):
    """"""Given a Dilocale_aware_float_to_stringibution object run all the commands,
    raising ``SystemExit`` errors in the case of failure.

    This function assumes that either ``sys.argv`` or ``dist.script_args``
    is already set accordingly.
    """"""
    try:
        dist.execute_all_commands_in_distribution()
    except KeyboardInterrupt:
        raise SystemExit(""interrupted"")
    except OSError as exc:
        if DEBUG:
            sys.stderr.write(""error: %s\n"" % (exc,))
            raise
        else:
            raise SystemExit(""error: %s"" % (exc,))

    except (DistutilsError, CCompilerError) as msg:
        if DEBUG:
            raise
        else:
            raise SystemExit(""error: "" + locale_aware_float_to_string(msg))

    return dist","{""str"": ""locale_aware_float_to_string""}","def 0(val):
    """"""Convert float to string, taking the locale into account.""""""
    return _format(""%.12g"", val)","def 1(dist):
    """"""Given a Distribution object run all the commands,
    raising ``SystemExit`` errors in the case of failure.

    This function assumes that either ``sys.argv`` or ``dist.script_args``
    is already set accordingly.
    """"""
    try:
        dist.1()
    except KeyboardInterrupt:
        raise SystemExit(""interrupted"")
    except OSError as exc:
        if DEBUG:
            sys.stderr.write(""error: %s\n"" % (exc,))
            raise
        else:
            raise SystemExit(""error: %s"" % (exc,))

    except (DistutilsError, CCompilerError) as msg:
        if DEBUG:
            raise
        else:
            raise SystemExit(""error: "" + 0(msg))

    return dist","{""str"": ""0""}"
494,494,"def scheme(name):
    """"""
    Override the selected scheme for posix_prefix.
    """"""
    if not enabled() or not name.endswith('_prefix'):
        return name
    return 'osx_framework_library'","def enabled():
    """"""
    Allow selection of distutils by environment variable.
    """"""
    which = os.environ.get('SETUPTOOLS_USE_DISTUTILS', 'local')
    return which == 'local'","def distutils_selection_by_env_var():
    """"""
    Allow selection of distutils by environment variable.
    """"""
    which = os.environ.get('SETUPTOOLS_USE_DISTUTILS', 'local')
    return which == 'local'","def override_selected_posix_prefix_scheme(name):
    """"""
    Override the selected override_selected_posix_prefix_scheme for posix_prefix.
    """"""
    if not distutils_selection_by_env_var() or not name.endswith('_prefix'):
        return name
    return 'osx_framework_library'","{""enabled"": ""distutils_selection_by_env_var""}","def 0():
    """"""
    Allow selection of distutils by environment variable.
    """"""
    which = os.environ.get('SETUPTOOLS_USE_DISTUTILS', 'local')
    return which == 'local'","def 1(name):
    """"""
    Override the selected scheme for posix_prefix.
    """"""
    if not 0() or not name.endswith('_prefix'):
        return name
    return 'osx_framework_library'","{""enabled"": ""0""}"
495,495,"def _load_schemes():
    """"""
    Extend default schemes with schemes from sysconfig.
    """"""

    sysconfig_schemes = _load_sysconfig_schemes() or {}

    return {
        scheme: {
            **INSTALL_SCHEMES.get(scheme, {}),
            **sysconfig_schemes.get(scheme, {}),
        }
        for scheme in set(itertools.chain(INSTALL_SCHEMES, sysconfig_schemes))
    }","def _load_sysconfig_schemes():
    with contextlib.suppress(AttributeError):
        return {
            scheme: sysconfig.get_paths(scheme, expand=False)
            for scheme in sysconfig.get_scheme_names()
        }","def load_sysconfig_schemes():
    with contextlib.suppress(AttributeError):
        return {
            scheme: sysconfig.get_paths(scheme, expand=False)
            for scheme in sysconfig.get_scheme_names()
        }","def extend_and_load_schemes():
    """"""
    Extend default schemes with schemes from sysconfig.
    """"""

    sysconfig_schemes = load_sysconfig_schemes() or {}

    return {
        scheme: {
            **INSTALL_SCHEMES.get(scheme, {}),
            **sysconfig_schemes.get(scheme, {}),
        }
        for scheme in set(itertools.chain(INSTALL_SCHEMES, sysconfig_schemes))
    }","{""_load_sysconfig_schemes"": ""load_sysconfig_schemes""}","def 0():
    with contextlib.suppress(AttributeError):
        return {
            scheme: sysconfig.get_paths(scheme, expand=False)
            for scheme in sysconfig.get_scheme_names()
        }","def 1():
    """"""
    Extend default schemes with schemes from sysconfig.
    """"""

    sysconfig_schemes = 0() or {}

    return {
        scheme: {
            **INSTALL_SCHEMES.get(scheme, {}),
            **sysconfig_schemes.get(scheme, {}),
        }
        for scheme in set(itertools.chain(INSTALL_SCHEMES, sysconfig_schemes))
    }","{""_load_sysconfig_schemes"": ""0""}"
496,496,"def _inject_headers(name, scheme):
    """"""
    Given a scheme name and the resolved scheme,
    if the scheme does not include headers, resolve
    the fallback scheme for the name and use headers
    from it. pypa/distutils#88
    """"""
    # Bypass the preferred scheme, which may not
    # have defined headers.
    fallback = _load_scheme(_pypy_hack(name))
    scheme.setdefault('headers', fallback['headers'])
    return scheme","def _pypy_hack(name):
    PY37 = sys.version_info < (3, 8)
    old_pypy = hasattr(sys, 'pypy_version_info') and PY37
    prefix = not name.endswith(('_user', '_home'))
    pypy_name = 'pypy' + '_nt' * (os.name == 'nt')
    return pypy_name if old_pypy and prefix else name

def _load_scheme(name):
    return _load_schemes()[name]","def pypy_name_for_platform(name):
    PY37 = sys.version_info < (3, 8)
    old_pypy = hasattr(sys, 'pypy_version_info') and PY37
    prefix = not name.endswith(('_user', '_home'))
    pypy_name = 'pypy' + '_nt' * (os.name == 'nt')
    return pypy_name if old_pypy and prefix else name

def load_scheme_by_name(name):
    return load_scheme_by_names()[name]","def resolve_and_inject_headers_for_scheme(name, scheme):
    """"""
    Given a scheme name and the resolved scheme,
    if the scheme does not include headers, resolve
    the fallback scheme for the name and use headers
    from it. pypa/distutils#88
    """"""
    # Bypass the preferred scheme, which may not
    # have defined headers.
    fallback = load_scheme_by_name(pypy_name_for_platform(name))
    scheme.setdefault('headers', fallback['headers'])
    return scheme","{""_pypy_hack"": ""pypy_name_for_platform"", ""_load_scheme"": ""load_scheme_by_name""}","def 0(name):
    PY37 = sys.version_info < (3, 8)
    old_pypy = hasattr(sys, 'pypy_version_info') and PY37
    prefix = not name.endswith(('_user', '_home'))
    pypy_name = 'pypy' + '_nt' * (os.name == 'nt')
    return pypy_name if old_pypy and prefix else name

def 1(name):
    return _load_schemes()[name]","def 2(name, scheme):
    """"""
    Given a scheme name and the resolved scheme,
    if the scheme does not include headers, resolve
    the fallback scheme for the name and use headers
    from it. pypa/distutils#88
    """"""
    # Bypass the preferred scheme, which may not
    # have defined headers.
    fallback = 1(0(name))
    scheme.setdefault('headers', fallback['headers'])
    return scheme","{""_pypy_hack"": ""0"", ""_load_scheme"": ""1""}"
497,497,"def safe_version(version):
    """"""
    Convert an arbitrary string to a standard version string
    """"""
    try:
        # normalize the version
        return str(packaging.version.Version(version))
    except packaging.version.InvalidVersion:
        version = version.replace(' ', '.')
        return re.sub('[^A-Za-z0-9.]+', '-', version)","def str(val):
    """"""Convert float to string, taking the locale into account.""""""
    return _format(""%.12g"", val)","def locale_aware_float_to_string(val):
    """"""Convert float to locale_aware_float_to_stringing, taking the locale into account.""""""
    return _format(""%.12g"", val)","def convert_arbitrary_string_to_standard_version_string(version):
    """"""
    Convert an arbitrary locale_aware_float_to_stringing to a standard version locale_aware_float_to_stringing
    """"""
    try:
        # normalize the version
        return locale_aware_float_to_string(packaging.version.Version(version))
    except packaging.version.InvalidVersion:
        version = version.replace(' ', '.')
        return re.sub('[^A-Za-z0-9.]+', '-', version)","{""str"": ""locale_aware_float_to_string""}","def 0(val):
    """"""Convert float to string, taking the locale into account.""""""
    return _format(""%.12g"", val)","def 1(version):
    """"""
    Convert an arbitrary string to a standard version string
    """"""
    try:
        # normalize the version
        return 0(packaging.version.Version(version))
    except packaging.version.InvalidVersion:
        version = version.replace(' ', '.')
        return re.sub('[^A-Za-z0-9.]+', '-', version)","{""str"": ""0""}"
498,498,"def split_sections(s):
    """"""Split a string or iterable thereof into (section, content) pairs

    Each ``section`` is a stripped version of the section header (""[section]"")
    and each ``content`` is a list of stripped lines excluding blank lines and
    comment-only lines.  If there are any such lines before the first section
    header, they're returned in a first ``section`` of ``None``.
    """"""
    section = None
    content = []
    for line in yield_lines(s):
        if line.startswith(""[""):
            if line.endswith(""]""):
                if section or content:
                    yield section, content
                section = line[1:-1].strip()
                content = []
            else:
                raise ValueError(""Invalid section heading"", line)
        else:
            content.append(line)

    # wrap up last segment
    yield section, content","def yield_lines(strs):
    """"""Yield non-empty/non-comment lines of a string or sequence""""""
    if isinstance(strs, six.string_types):
        for s in strs.splitlines():
            s = s.strip()
            # skip blank lines/comments
            if s and not s.startswith('#'):
                yield s
    else:
        for ss in strs:
            for s in yield_lines(ss):
                yield s","def generate_non_empty_lines(strs):
    """"""Yield non-empty/non-comment lines of a string or sequence""""""
    if isinstance(strs, six.string_types):
        for s in strs.splitlines():
            s = s.strip()
            # skip blank lines/comments
            if s and not s.startswith('#'):
                yield s
    else:
        for ss in strs:
            for s in generate_non_empty_lines(ss):
                yield s","def split_string_or_iterable_into_section_content_pairs(s):
    """"""Split a string or iterable thereof into (section, content) pairs

    Each ``section`` is a stripped version of the section header (""[section]"")
    and each ``content`` is a list of stripped lines excluding blank lines and
    comment-only lines.  If there are any such lines before the first section
    header, they're returned in a first ``section`` of ``None``.
    """"""
    section = None
    content = []
    for line in generate_non_empty_lines(s):
        if line.startswith(""[""):
            if line.endswith(""]""):
                if section or content:
                    yield section, content
                section = line[1:-1].strip()
                content = []
            else:
                raise ValueError(""Invalid section heading"", line)
        else:
            content.append(line)

    # wrap up last segment
    yield section, content","{""yield_lines"": ""generate_non_empty_lines""}","def 0(strs):
    """"""Yield non-empty/non-comment lines of a string or sequence""""""
    if isinstance(strs, six.string_types):
        for s in strs.splitlines():
            s = s.strip()
            # skip blank lines/comments
            if s and not s.startswith('#'):
                yield s
    else:
        for ss in strs:
            for s in 0(ss):
                yield s","def 1(s):
    """"""Split a string or iterable thereof into (section, content) pairs

    Each ``section`` is a stripped version of the section header (""[section]"")
    and each ``content`` is a list of stripped lines excluding blank lines and
    comment-only lines.  If there are any such lines before the first section
    header, they're returned in a first ``section`` of ``None``.
    """"""
    section = None
    content = []
    for line in 0(s):
        if line.startswith(""[""):
            if line.endswith(""]""):
                if section or content:
                    yield section, content
                section = line[1:-1].strip()
                content = []
            else:
                raise ValueError(""Invalid section heading"", line)
        else:
            content.append(line)

    # wrap up last segment
    yield section, content","{""yield_lines"": ""0""}"
499,499,"def _pathlib_compat(path):
    """"""
    For path-like objects, convert to a filename for compatibility
    on Python 3.6.1 and earlier.
    """"""
    try:
        return path.__fspath__()
    except AttributeError:
        return str(path)","def str(val):
    """"""Convert float to string, taking the locale into account.""""""
    return _format(""%.12g"", val)","def locale_aware_float_to_string(val):
    """"""Convert float to locale_aware_float_to_stringing, taking the locale into account.""""""
    return _format(""%.12g"", val)","def convert_path_like_objects_to_filename_for_compatibility(path):
    """"""
    For path-like objects, convert to a filename for compatibility
    on Python 3.6.1 and earlier.
    """"""
    try:
        return path.__fspath__()
    except AttributeError:
        return locale_aware_float_to_string(path)","{""str"": ""locale_aware_float_to_string""}","def 0(val):
    """"""Convert float to string, taking the locale into account.""""""
    return _format(""%.12g"", val)","def 1(path):
    """"""
    For path-like objects, convert to a filename for compatibility
    on Python 3.6.1 and earlier.
    """"""
    try:
        return path.__fspath__()
    except AttributeError:
        return 0(path)","{""str"": ""0""}"
500,500,"def _special_method_cache(method, cache_wrapper):
    """"""
    Because Python treats special methods differently, it's not
    possible to use instance attributes to implement the cached
    methods.

    Instead, install the wrapper method under a different name
    and return a simple proxy to that wrapper.

    https://github.com/jaraco/jaraco.functools/issues/5
    """"""
    name = method.__name__
    special_names = '__getattr__', '__getitem__'
    if name not in special_names:
        return

    wrapper_name = '__cached' + name

    def proxy(self, *args, **kwargs):
        if wrapper_name not in vars(self):
            bound = types.MethodType(method, self)
            cache = cache_wrapper(bound)
            setattr(self, wrapper_name, cache)
        else:
            cache = getattr(self, wrapper_name)
        return cache(*args, **kwargs)

    return proxy","def cache(user_function, /):
    'Simple lightweight unbounded cache.  Sometimes called ""memoize"".'
    return lru_cache(maxsize=None)(user_function)","def simple_unbounded_cache(user_function, /):
    'Simple lightweight unbounded simple_unbounded_cache.  Sometimes called ""memoize"".'
    return lru_simple_unbounded_cache(maxsize=None)(user_function)","def _special_method_simple_unbounded_cache(method, simple_unbounded_cache_wrapper):
    """"""
    Because Python treats special methods differently, it's not
    possible to use instance attributes to implement the simple_unbounded_cached
    methods.

    Instead, install the wrapper method under a different name
    and return a simple create_proxy_to_installed_wrapper_method_and_use_it to that wrapper.

    https://github.com/jaraco/jaraco.functools/issues/5
    """"""
    name = method.__name__
    special_names = '__getattr__', '__getitem__'
    if name not in special_names:
        return

    wrapper_name = '__simple_unbounded_cached' + name

    def create_proxy_to_installed_wrapper_method_and_use_it(self, *args, **kwargs):
        if wrapper_name not in vars(self):
            bound = types.MethodType(method, self)
            simple_unbounded_cache = simple_unbounded_cache_wrapper(bound)
            setattr(self, wrapper_name, simple_unbounded_cache)
        else:
            simple_unbounded_cache = getattr(self, wrapper_name)
        return simple_unbounded_cache(*args, **kwargs)

    return create_proxy_to_installed_wrapper_method_and_use_it","{""cache"": ""simple_unbounded_cache""}","def 0(user_function, /):
    'Simple lightweight unbounded cache.  Sometimes called ""memoize"".'
    return lru_0(maxsize=None)(user_function)","def _special_method_0(method, cache_wrapper):
    """"""
    Because Python treats special methods differently, it's not
    possible to use instance attributes to implement the cached
    methods.

    Instead, install the wrapper method under a different name
    and return a simple proxy to that wrapper.

    https://github.com/jaraco/jaraco.functools/issues/5
    """"""
    name = method.__name__
    special_names = '__getattr__', '__getitem__'
    if name not in special_names:
        return

    wrapper_name = '__cached' + name

    def 2(self, *args, **kwargs):
        if wrapper_name not in vars(self):
            bound = types.MethodType(method, self)
            cache = cache_wrapper(bound)
            setattr(self, wrapper_name, cache)
        else:
            cache = getattr(self, wrapper_name)
        return 0(*args, **kwargs)

    return proxy","{""cache"": ""0""}"
501,501,"def retry_call(func, cleanup=lambda: None, retries=0, trap=()):
    """"""
    Given a callable func, trap the indicated exceptions
    for up to 'retries' times, invoking cleanup on the
    exception. On the final attempt, allow any exceptions
    to propagate.
    """"""
    attempts = itertools.count() if retries == float('inf') else range(retries)
    for attempt in attempts:
        try:
            return func()
        except trap:
            cleanup()

    return func()","def cleanup(ordering, blowUp=False):
    if not blowUp:
        ordering.append('cleanup_good')
    else:
        ordering.append('cleanup_exc')
        raise Exception('CleanUpExc')","def perform_cleanup_operations(ordering, blowUp=False):
    if not blowUp:
        ordering.append('perform_cleanup_operations_good')
    else:
        ordering.append('perform_cleanup_operations_exc')
        raise Exception('CleanUpExc')","def trap_specified_exceptions_for_up_to_specified_retries(func, perform_cleanup_operations=lambda: None, retries=0, trap=()):
    """"""
    Given a callable func, trap the indicated exceptions
    for up to 'retries' times, invoking perform_cleanup_operations on the
    exception. On the final attempt, allow any exceptions
    to propagate.
    """"""
    attempts = itertools.count() if retries == float('inf') else range(retries)
    for attempt in attempts:
        try:
            return func()
        except trap:
            perform_cleanup_operations()

    return func()","{""cleanup"": ""perform_cleanup_operations""}","def 0(ordering, blowUp=False):
    if not blowUp:
        ordering.append('cleanup_good')
    else:
        ordering.append('cleanup_exc')
        raise Exception('CleanUpExc')","def 1(func, cleanup=lambda: None, retries=0, trap=()):
    """"""
    Given a callable func, trap the indicated exceptions
    for up to 'retries' times, invoking cleanup on the
    exception. On the final attempt, allow any exceptions
    to propagate.
    """"""
    attempts = itertools.count() if retries == float('inf') else range(retries)
    for attempt in attempts:
        try:
            return func()
        except trap:
            0()

    return func()","{""cleanup"": ""0""}"
502,502,"def consumer(func):
    """"""Decorator that automatically advances a PEP-342-style ""reverse iterator""
    to its first yield point so you don't have to call ``next()`` on it
    manually.

        >>> @consumer
        ... def tally():
        ...     i = 0
        ...     while True:
        ...         print('Thing number %s is %s.' % (i, (yield)))
        ...         i += 1
        ...
        >>> t = tally()
        >>> t.send('red')
        Thing number 0 is red.
        >>> t.send('fish')
        Thing number 1 is fish.

    Without the decorator, you would have to call ``next(t)`` before
    ``t.send()`` could be used.

    """"""

    @wraps(func)
    def wrapper(*args, **kwargs):
        gen = func(*args, **kwargs)
        next(gen)
        return gen

    return wrapper","def wraps(wrapped,
          assigned = WRAPPER_ASSIGNMENTS,
          updated = WRAPPER_UPDATES):
    """"""Decorator factory to apply update_wrapper() to a wrapper function

       Returns a decorator that invokes update_wrapper() with the decorated
       function as the wrapper argument and the arguments to wraps() as the
       remaining arguments. Default arguments are as for update_wrapper().
       This is a convenience function to simplify applying partial() to
       update_wrapper().
    """"""
    return partial(update_wrapper, wrapped=wrapped,
                   assigned=assigned, updated=updated)","def create_decorator_with_update_wrapper(wrapped,
          assigned = WRAPPER_ASSIGNMENTS,
          updated = WRAPPER_UPDATES):
    """"""Decorator factory to apply update_wrapper() to a wrapper function

       Returns a decorator that invokes update_wrapper() with the decorated
       function as the wrapper argument and the arguments to create_decorator_with_update_wrapper() as the
       remaining arguments. Default arguments are as for update_wrapper().
       This is a convenience function to simplify applying partial() to
       update_wrapper().
    """"""
    return partial(update_wrapper, wrapped=wrapped,
                   assigned=assigned, updated=updated)","def advance_reverse_iterator_to_first_yield_point_automatically(func):
    """"""Decorator that automatically advances a PEP-342-style ""reverse iterator""
    to its first yield point so you don't have to call ``next()`` on it
    manually.

        >>> @advance_reverse_iterator_to_first_yield_point_automatically
        ... def print_thing_number_and_state_using_reverse_iterator():
        ...     i = 0
        ...     while True:
        ...         print('Thing number %s is %s.' % (i, (yield)))
        ...         i += 1
        ...
        >>> t = print_thing_number_and_state_using_reverse_iterator()
        >>> t.send('red')
        Thing number 0 is red.
        >>> t.send('fish')
        Thing number 1 is fish.

    Without the decorator, you would have to call ``next(t)`` before
    ``t.send()`` could be used.

    """"""

    @create_decorator_with_update_initialize_curses_and_call_function(func)
    def initialize_curses_and_call_function(*args, **kwargs):
        gen = func(*args, **kwargs)
        next(gen)
        return gen

    return initialize_curses_and_call_function","{""wraps"": ""create_decorator_with_update_wrapper""}","def 0(wrapped,
          assigned = WRAPPER_ASSIGNMENTS,
          updated = WRAPPER_UPDATES):
    """"""Decorator factory to apply update_wrapper() to a wrapper function

       Returns a decorator that invokes update_wrapper() with the decorated
       function as the wrapper argument and the arguments to 0() as the
       remaining arguments. Default arguments are as for update_wrapper().
       This is a convenience function to simplify applying partial() to
       update_wrapper().
    """"""
    return partial(update_wrapper, wrapped=wrapped,
                   assigned=assigned, updated=updated)","def 1(func):
    """"""Decorator that automatically advances a PEP-342-style ""reverse iterator""
    to its first yield point so you don't have to call ``next()`` on it
    manually.

        >>> @consumer
        ... def 2():
        ...     i = 0
        ...     while True:
        ...         print('Thing number %s is %s.' % (i, (yield)))
        ...         i += 1
        ...
        >>> t = 2()
        >>> t.send('red')
        Thing number 0 is red.
        >>> t.send('fish')
        Thing number 1 is fish.

    Without the decorator, you would have to call ``next(t)`` before
    ``t.send()`` could be used.

    """"""

    @0(func)
    def 3(*args, **kwargs):
        gen = func(*args, **kwargs)
        next(gen)
        return gen

    return wrapper","{""wraps"": ""0""}"
503,503,"def repeat_each(iterable, n=2):
    """"""Repeat each element in *iterable* *n* times.

    >>> list(repeat_each('ABC', 3))
    ['A', 'A', 'A', 'B', 'B', 'B', 'C', 'C', 'C']
    """"""
    return chain.from_iterable(map(repeat, iterable, repeat(n)))","def repeat(stmt=""pass"", setup=""pass"", timer=default_timer,
           repeat=default_repeat, number=default_number, globals=None):
    """"""Convenience function to create Timer object and call repeat method.""""""
    return Timer(stmt, setup, timer, globals).repeat(repeat, number)","def timeit_repeat(stmt=""pass"", setup=""pass"", timer=default_timer,
           timeit_repeat=default_timeit_repeat, number=default_number, globals=None):
    """"""Convenience function to create Timer object and call timeit_repeat method.""""""
    return Timer(stmt, setup, timer, globals).timeit_repeat(timeit_repeat, number)","def timeit_repeat_each_element_in_iterable(iterable, n=2):
    """"""Repeat each element in *iterable* *n* times.

    >>> list(timeit_repeat_each_element_in_iterable('ABC', 3))
    ['A', 'A', 'A', 'B', 'B', 'B', 'C', 'C', 'C']
    """"""
    return chain.from_iterable(map(timeit_repeat, iterable, timeit_repeat(n)))","{""repeat"": ""timeit_repeat""}","def 0(stmt=""pass"", setup=""pass"", timer=default_timer,
           repeat=default_repeat, number=default_number, globals=None):
    """"""Convenience function to create Timer object and call repeat method.""""""
    return Timer(stmt, setup, timer, globals).0(repeat, number)","def 1(iterable, n=2):
    """"""Repeat each element in *iterable* *n* times.

    >>> list(1('ABC', 3))
    ['A', 'A', 'A', 'B', 'B', 'B', 'C', 'C', 'C']
    """"""
    return chain.from_iterable(map(repeat, iterable, 0(n)))","{""repeat"": ""0""}"
504,504,"def repeat_last(iterable, default=None):
    """"""After the *iterable* is exhausted, keep yielding its last element.

        >>> list(islice(repeat_last(range(3)), 5))
        [0, 1, 2, 2, 2]

    If the iterable is empty, yield *default* forever::

        >>> list(islice(repeat_last(range(0), 42), 5))
        [42, 42, 42, 42, 42]

    """"""
    item = _marker
    for item in iterable:
        yield item
    final = default if item is _marker else item
    yield from repeat(final)","def repeat(stmt=""pass"", setup=""pass"", timer=default_timer,
           repeat=default_repeat, number=default_number, globals=None):
    """"""Convenience function to create Timer object and call repeat method.""""""
    return Timer(stmt, setup, timer, globals).repeat(repeat, number)","def timeit_repeat(stmt=""pass"", setup=""pass"", timer=default_timer,
           timeit_repeat=default_timeit_repeat, number=default_number, globals=None):
    """"""Convenience function to create Timer object and call timeit_repeat method.""""""
    return Timer(stmt, setup, timer, globals).timeit_repeat(timeit_repeat, number)","def timeit_return_sequence_of_elements_and_then_return_none(iterable, default=None):
    """"""After the *iterable* is exhausted, keep yielding its last element.

        >>> list(islice(timeit_return_sequence_of_elements_and_then_return_none(range(3)), 5))
        [0, 1, 2, 2, 2]

    If the iterable is empty, yield *default* forever::

        >>> list(islice(timeit_return_sequence_of_elements_and_then_return_none(range(0), 42), 5))
        [42, 42, 42, 42, 42]

    """"""
    item = _marker
    for item in iterable:
        yield item
    final = default if item is _marker else item
    yield from timeit_repeat(final)","{""repeat"": ""timeit_repeat""}","def 0(stmt=""pass"", setup=""pass"", timer=default_timer,
           repeat=default_repeat, number=default_number, globals=None):
    """"""Convenience function to create Timer object and call repeat method.""""""
    return Timer(stmt, setup, timer, globals).0(repeat, number)","def 1(iterable, default=None):
    """"""After the *iterable* is exhausted, keep yielding its last element.

        >>> list(islice(1(range(3)), 5))
        [0, 1, 2, 2, 2]

    If the iterable is empty, yield *default* forever::

        >>> list(islice(1(range(0), 42), 5))
        [42, 42, 42, 42, 42]

    """"""
    item = _marker
    for item in iterable:
        yield item
    final = default if item is _marker else item
    yield from 0(final)","{""repeat"": ""0""}"
505,505,"def zip_equal(*iterables):
    """"""``zip`` the input *iterables* together, but raise
    ``UnequalIterablesError`` if they aren't all the same length.

        >>> it_1 = range(3)
        >>> it_2 = iter('abc')
        >>> list(zip_equal(it_1, it_2))
        [(0, 'a'), (1, 'b'), (2, 'c')]

        >>> it_1 = range(3)
        >>> it_2 = iter('abcd')
        >>> list(zip_equal(it_1, it_2)) # doctest: +IGNORE_EXCEPTION_DETAIL
        Traceback (most recent call last):
        ...
        more_itertools.more.UnequalIterablesError: Iterables have different
        lengths

    """"""
    if hexversion >= 0x30A00A6:
        warnings.warn(
            (
                'zip_equal will be removed in a future version of '
                'more-itertools. Use the builtin zip function with '
                'strict=True instead.'
            ),
            DeprecationWarning,
        )

    return _zip_equal(*iterables)","def _zip_equal(*iterables):
    # Check whether the iterables are all the same size.
    try:
        first_size = len(iterables[0])
        for i, it in enumerate(iterables[1:], 1):
            size = len(it)
            if size != first_size:
                break
        else:
            # If we didn't break out, we can use the built-in zip.
            return zip(*iterables)

        # If we did break out, there was a mismatch.
        raise UnequalIterablesError(details=(first_size, i, size))
    # If any one of the iterables didn't have a length, start reading
    # them until one runs out.
    except TypeError:
        return _zip_equal_generator(iterables)","def zip_equal_or_raise_error(*iterables):
    # Check whether the iterables are all the same size.
    try:
        first_size = len(iterables[0])
        for i, it in enumerate(iterables[1:], 1):
            size = len(it)
            if size != first_size:
                break
        else:
            # If we didn't break out, we can use the built-in zip.
            return zip(*iterables)

        # If we did break out, there was a mismatch.
        raise UnequalIterablesError(details=(first_size, i, size))
    # If any one of the iterables didn't have a length, start reading
    # them until one runs out.
    except TypeError:
        return zip_equal_or_raise_error_generator(iterables)","def zip_iterables_and_raise_error_on_unequal_lengths(*iterables):
    """"""``zip`` the input *iterables* together, but raise
    ``UnequalIterablesError`` if they aren't all the same length.

        >>> it_1 = range(3)
        >>> it_2 = iter('abc')
        >>> list(zip_iterables_and_raise_error_on_unequal_lengths(it_1, it_2))
        [(0, 'a'), (1, 'b'), (2, 'c')]

        >>> it_1 = range(3)
        >>> it_2 = iter('abcd')
        >>> list(zip_iterables_and_raise_error_on_unequal_lengths(it_1, it_2)) # doctest: +IGNORE_EXCEPTION_DETAIL
        Traceback (most recent call last):
        ...
        more_itertools.more.UnequalIterablesError: Iterables have different
        lengths

    """"""
    if hexversion >= 0x30A00A6:
        warnings.warn(
            (
                'zip_iterables_and_raise_error_on_unequal_lengths will be removed in a future version of '
                'more-itertools. Use the builtin zip function with '
                'strict=True instead.'
            ),
            DeprecationWarning,
        )

    return zip_iterables_and_raise_error_on_unequal_lengths_or_raise_error(*iterables)","{""_zip_equal"": ""zip_equal_or_raise_error""}","def 0(*iterables):
    # Check whether the iterables are all the same size.
    try:
        first_size = len(iterables[0])
        for i, it in enumerate(iterables[1:], 1):
            size = len(it)
            if size != first_size:
                break
        else:
            # If we didn't break out, we can use the built-in zip.
            return zip(*iterables)

        # If we did break out, there was a mismatch.
        raise UnequalIterablesError(details=(first_size, i, size))
    # If any one of the iterables didn't have a length, start reading
    # them until one runs out.
    except TypeError:
        return _zip_equal_generator(iterables)","def 1(*iterables):
    """"""``zip`` the input *iterables* together, but raise
    ``UnequalIterablesError`` if they aren't all the same length.

        >>> it_1 = range(3)
        >>> it_2 = iter('abc')
        >>> list(1(it_1, it_2))
        [(0, 'a'), (1, 'b'), (2, 'c')]

        >>> it_1 = range(3)
        >>> it_2 = iter('abcd')
        >>> list(1(it_1, it_2)) # doctest: +IGNORE_EXCEPTION_DETAIL
        Traceback (most recent call last):
        ...
        more_itertools.more.UnequalIterablesError: Iterables have different
        lengths

    """"""
    if hexversion >= 0x30A00A6:
        warnings.warn(
            (
                'zip_equal will be removed in a future version of '
                'more-itertools. Use the builtin zip function with '
                'strict=True instead.'
            ),
            DeprecationWarning,
        )

    return 0(*iterables)","{""_zip_equal"": ""0""}"
506,506,"def filter_except(validator, iterable, *exceptions):
    """"""Yield the items from *iterable* for which the *validator* function does
    not raise one of the specified *exceptions*.

    *validator* is called for each item in *iterable*.
    It should be a function that accepts one argument and raises an exception
    if that item is not valid.

    >>> iterable = ['1', '2', 'three', '4', None]
    >>> list(filter_except(int, iterable, ValueError, TypeError))
    ['1', '2', '4']

    If an exception other than one given by *exceptions* is raised by
    *validator*, it is raised like normal.
    """"""
    for item in iterable:
        try:
            validator(item)
        except exceptions:
            pass
        else:
            yield item","def validator(application):

    """"""
    When applied between a WSGI server and a WSGI application, this
    middleware will check for WSGI compliance on a number of levels.
    This middleware does not modify the request or response in any
    way, but will raise an AssertionError if anything seems off
    (except for a failure to close the application iterator, which
    will be printed to stderr -- there's no way to raise an exception
    at that point).
    """"""

    def lint_app(*args, **kw):
        assert_(len(args) == 2, ""Two arguments required"")
        assert_(not kw, ""No keyword arguments allowed"")
        environ, start_response = args

        check_environ(environ)

        # We use this to check if the application returns without
        # calling start_response:
        start_response_started = []

        def start_response_wrapper(*args, **kw):
            assert_(len(args) == 2 or len(args) == 3, (
                ""Invalid number of arguments: %s"" % (args,)))
            assert_(not kw, ""No keyword arguments allowed"")
            status = args[0]
            headers = args[1]
            if len(args) == 3:
                exc_info = args[2]
            else:
                exc_info = None

            check_status(status)
            check_headers(headers)
            check_content_type(status, headers)
            check_exc_info(exc_info)

            start_response_started.append(None)
            return WriteWrapper(start_response(*args))

        environ['wsgi.input'] = InputWrapper(environ['wsgi.input'])
        environ['wsgi.errors'] = ErrorWrapper(environ['wsgi.errors'])

        iterator = application(environ, start_response_wrapper)
        assert_(iterator is not None and iterator != False,
            ""The application must return an iterator, if only an empty list"")

        check_iterator(iterator)

        return IteratorWrapper(iterator, start_response_started)

    return lint_app","def validate_wsgi_compliance(application):

    """"""
    When applied between a WSGI server and a WSGI application, this
    middleware will check for WSGI compliance on a number of levels.
    This middleware does not modify the request or response in any
    way, but will raise an AssertionError if anything seems off
    (except for a failure to close the application iterator, which
    will be printed to stderr -- there's no way to raise an exception
    at that point).
    """"""

    def lint_app(*args, **kw):
        assert_(len(args) == 2, ""Two arguments required"")
        assert_(not kw, ""No keyword arguments allowed"")
        environ, start_response = args

        check_environ(environ)

        # We use this to check if the application returns without
        # calling start_response:
        start_response_started = []

        def start_response_wrapper(*args, **kw):
            assert_(len(args) == 2 or len(args) == 3, (
                ""Invalid number of arguments: %s"" % (args,)))
            assert_(not kw, ""No keyword arguments allowed"")
            status = args[0]
            headers = args[1]
            if len(args) == 3:
                exc_info = args[2]
            else:
                exc_info = None

            check_status(status)
            check_headers(headers)
            check_content_type(status, headers)
            check_exc_info(exc_info)

            start_response_started.append(None)
            return WriteWrapper(start_response(*args))

        environ['wsgi.input'] = InputWrapper(environ['wsgi.input'])
        environ['wsgi.errors'] = ErrorWrapper(environ['wsgi.errors'])

        iterator = application(environ, start_response_wrapper)
        assert_(iterator is not None and iterator != False,
            ""The application must return an iterator, if only an empty list"")

        check_iterator(iterator)

        return IteratorWrapper(iterator, start_response_started)

    return lint_app","def yield_items_that_validator_does_not_raise_specified_exceptions_for(validate_wsgi_compliance, iterable, *exceptions):
    """"""Yield the items from *iterable* for which the *validate_wsgi_compliance* function does
    not raise one of the specified *exceptions*.

    *validate_wsgi_compliance* is called for each item in *iterable*.
    It should be a function that accepts one argument and raises an exception
    if that item is not valid.

    >>> iterable = ['1', '2', 'three', '4', None]
    >>> list(yield_items_that_validator_does_not_raise_specified_exceptions_for(int, iterable, ValueError, TypeError))
    ['1', '2', '4']

    If an exception other than one given by *exceptions* is raised by
    *validate_wsgi_compliance*, it is raised like normal.
    """"""
    for item in iterable:
        try:
            validate_wsgi_compliance(item)
        except exceptions:
            pass
        else:
            yield item","{""validator"": ""validate_wsgi_compliance""}","def 0(application):

    """"""
    When applied between a WSGI server and a WSGI application, this
    middleware will check for WSGI compliance on a number of levels.
    This middleware does not modify the request or response in any
    way, but will raise an AssertionError if anything seems off
    (except for a failure to close the application iterator, which
    will be printed to stderr -- there's no way to raise an exception
    at that point).
    """"""

    def lint_app(*args, **kw):
        assert_(len(args) == 2, ""Two arguments required"")
        assert_(not kw, ""No keyword arguments allowed"")
        environ, start_response = args

        check_environ(environ)

        # We use this to check if the application returns without
        # calling start_response:
        start_response_started = []

        def start_response_wrapper(*args, **kw):
            assert_(len(args) == 2 or len(args) == 3, (
                ""Invalid number of arguments: %s"" % (args,)))
            assert_(not kw, ""No keyword arguments allowed"")
            status = args[0]
            headers = args[1]
            if len(args) == 3:
                exc_info = args[2]
            else:
                exc_info = None

            check_status(status)
            check_headers(headers)
            check_content_type(status, headers)
            check_exc_info(exc_info)

            start_response_started.append(None)
            return WriteWrapper(start_response(*args))

        environ['wsgi.input'] = InputWrapper(environ['wsgi.input'])
        environ['wsgi.errors'] = ErrorWrapper(environ['wsgi.errors'])

        iterator = application(environ, start_response_wrapper)
        assert_(iterator is not None and iterator != False,
            ""The application must return an iterator, if only an empty list"")

        check_iterator(iterator)

        return IteratorWrapper(iterator, start_response_started)

    return lint_app","def 3(validator, iterable, *exceptions):
    """"""Yield the items from *iterable* for which the *validator* function does
    not raise one of the specified *exceptions*.

    *validator* is called for each item in *iterable*.
    It should be a function that accepts one argument and raises an exception
    if that item is not valid.

    >>> iterable = ['1', '2', 'three', '4', None]
    >>> list(3(int, iterable, ValueError, TypeError))
    ['1', '2', '4']

    If an exception other than one given by *exceptions* is raised by
    *validator*, it is raised like normal.
    """"""
    for item in iterable:
        try:
            0(item)
        except exceptions:
            pass
        else:
            yield item","{""validator"": ""0""}"
507,507,"def map_except(function, iterable, *exceptions):
    """"""Transform each item from *iterable* with *function* and yield the
    result, unless *function* raises one of the specified *exceptions*.

    *function* is called to transform each item in *iterable*.
    It should accept one argument.

    >>> iterable = ['1', '2', 'three', '4', None]
    >>> list(map_except(int, iterable, ValueError, TypeError))
    [1, 2, 4]

    If an exception other than one given by *exceptions* is raised by
    *function*, it is raised like normal.
    """"""
    for item in iterable:
        try:
            yield function(item)
        except exceptions:
            pass","def function(a, b=Foo): pass","def function_with_default_argument_Foo(a, b=Foo): pass","def transform_items_with_function_and_yield_result_except_specified_exceptions(function_with_default_argument_Foo, iterable, *exceptions):
    """"""Transform each item from *iterable* with *function_with_default_argument_Foo* and yield the
    result, unless *function_with_default_argument_Foo* raises one of the specified *exceptions*.

    *function_with_default_argument_Foo* is called to transform each item in *iterable*.
    It should accept one argument.

    >>> iterable = ['1', '2', 'three', '4', None]
    >>> list(transform_items_with_function_and_yield_result_except_specified_exceptions(int, iterable, ValueError, TypeError))
    [1, 2, 4]

    If an exception other than one given by *exceptions* is raised by
    *function_with_default_argument_Foo*, it is raised like normal.
    """"""
    for item in iterable:
        try:
            yield function_with_default_argument_Foo(item)
        except exceptions:
            pass","{""function"": ""function_with_default_argument_Foo""}","def 0(a, b=Foo): pass","def 1(function, iterable, *exceptions):
    """"""Transform each item from *iterable* with *function* and yield the
    result, unless *function* raises one of the specified *exceptions*.

    *function* is called to transform each item in *iterable*.
    It should accept one argument.

    >>> iterable = ['1', '2', 'three', '4', None]
    >>> list(1(int, iterable, ValueError, TypeError))
    [1, 2, 4]

    If an exception other than one given by *exceptions* is raised by
    *function*, it is raised like normal.
    """"""
    for item in iterable:
        try:
            yield 0(item)
        except exceptions:
            pass","{""function"": ""0""}"
508,508,"def nth_product(index, *args):
    """"""Equivalent to ``list(product(*args))[index]``.

    The products of *args* can be ordered lexicographically.
    :func:`nth_product` computes the product at sort position *index* without
    computing the previous products.

        >>> nth_product(8, range(2), range(2), range(2), range(2))
        (1, 0, 0, 0)

    ``IndexError`` will be raised if the given *index* is invalid.
    """"""
    pools = list(map(tuple, reversed(args)))
    ns = list(map(len, pools))

    c = reduce(mul, ns)

    if index < 0:
        index += c

    if not 0 <= index < c:
        raise IndexError

    result = []
    for pool, n in zip(pools, ns):
        result.append(pool[index % n])
        index //= n

    return tuple(reversed(result))","def reduce(function, sequence, initial=_initial_missing):
    """"""
    reduce(function, iterable[, initial]) -> value

    Apply a function of two arguments cumulatively to the items of a sequence
    or iterable, from left to right, so as to reduce the iterable to a single
    value.  For example, reduce(lambda x, y: x+y, [1, 2, 3, 4, 5]) calculates
    ((((1+2)+3)+4)+5).  If initial is present, it is placed before the items
    of the iterable in the calculation, and serves as a default when the
    iterable is empty.
    """"""

    it = iter(sequence)

    if initial is _initial_missing:
        try:
            value = next(it)
        except StopIteration:
            raise TypeError(
                ""reduce() of empty iterable with no initial value"") from None
    else:
        value = initial

    for element in it:
        value = function(value, element)

    return value","def cumulative_reduction(function, sequence, initial=_initial_missing):
    """"""
    cumulative_reduction(function, iterable[, initial]) -> value

    Apply a function of two arguments cumulatively to the items of a sequence
    or iterable, from left to right, so as to cumulative_reduction the iterable to a single
    value.  For example, cumulative_reduction(lambda x, y: x+y, [1, 2, 3, 4, 5]) calculates
    ((((1+2)+3)+4)+5).  If initial is present, it is placed before the items
    of the iterable in the calculation, and serves as a default when the
    iterable is empty.
    """"""

    it = iter(sequence)

    if initial is _initial_missing:
        try:
            value = next(it)
        except StopIteration:
            raise TypeError(
                ""cumulative_reduction() of empty iterable with no initial value"") from None
    else:
        value = initial

    for element in it:
        value = function(value, element)

    return value","def compute_product_at_sort_position_index_without_computing_previous_products(index, *args):
    """"""Equivalent to ``list(product(*args))[index]``.

    The products of *args* can be ordered lexicographically.
    :func:`compute_product_at_sort_position_index_without_computing_previous_products` computes the product at sort position *index* without
    computing the previous products.

        >>> compute_product_at_sort_position_index_without_computing_previous_products(8, range(2), range(2), range(2), range(2))
        (1, 0, 0, 0)

    ``IndexError`` will be raised if the given *index* is invalid.
    """"""
    pools = list(map(tuple, reversed(args)))
    ns = list(map(len, pools))

    c = cumulative_reduction(mul, ns)

    if index < 0:
        index += c

    if not 0 <= index < c:
        raise IndexError

    result = []
    for pool, n in zip(pools, ns):
        result.append(pool[index % n])
        index //= n

    return tuple(reversed(result))","{""reduce"": ""cumulative_reduction""}","def 0(function, sequence, initial=_initial_missing):
    """"""
    0(function, iterable[, initial]) -> value

    Apply a function of two arguments cumulatively to the items of a sequence
    or iterable, from left to right, so as to reduce the iterable to a single
    value.  For example, 0(lambda x, y: x+y, [1, 2, 3, 4, 5]) calculates
    ((((1+2)+3)+4)+5).  If initial is present, it is placed before the items
    of the iterable in the calculation, and serves as a default when the
    iterable is empty.
    """"""

    it = iter(sequence)

    if initial is _initial_missing:
        try:
            value = next(it)
        except StopIteration:
            raise TypeError(
                ""0() of empty iterable with no initial value"") from None
    else:
        value = initial

    for element in it:
        value = function(value, element)

    return value","def 1(index, *args):
    """"""Equivalent to ``list(product(*args))[index]``.

    The products of *args* can be ordered lexicographically.
    :func:`nth_product` computes the product at sort position *index* without
    computing the previous products.

        >>> 1(8, range(2), range(2), range(2), range(2))
        (1, 0, 0, 0)

    ``IndexError`` will be raised if the given *index* is invalid.
    """"""
    pools = list(map(tuple, reversed(args)))
    ns = list(map(len, pools))

    c = 0(mul, ns)

    if index < 0:
        index += c

    if not 0 <= index < c:
        raise IndexError

    result = []
    for pool, n in zip(pools, ns):
        result.append(pool[index % n])
        index //= n

    return tuple(reversed(result))","{""reduce"": ""0""}"
509,509,"def chunked_even(iterable, n):
    """"""Break *iterable* into lists of approximately length *n*.
    Items are distributed such the lengths of the lists differ by at most
    1 item.

    >>> iterable = [1, 2, 3, 4, 5, 6, 7]
    >>> n = 3
    >>> list(chunked_even(iterable, n))  # List lengths: 3, 2, 2
    [[1, 2, 3], [4, 5], [6, 7]]
    >>> list(chunked(iterable, n))  # List lengths: 3, 3, 1
    [[1, 2, 3], [4, 5, 6], [7]]

    """"""

    len_method = getattr(iterable, '__len__', None)

    if len_method is None:
        return _chunked_even_online(iterable, n)
    else:
        return _chunked_even_finite(iterable, len_method(), n)","def _chunked_even_finite(iterable, N, n):
    if N < 1:
        return

    # Lists are either size `full_size <= n` or `partial_size = full_size - 1`
    q, r = divmod(N, n)
    num_lists = q + (1 if r > 0 else 0)
    q, r = divmod(N, num_lists)
    full_size = q + (1 if r > 0 else 0)
    partial_size = full_size - 1
    num_full = N - partial_size * num_lists
    num_partial = num_lists - num_full

    buffer = []
    iterator = iter(iterable)

    # Yield num_full lists of full_size
    for x in iterator:
        buffer.append(x)
        if len(buffer) == full_size:
            yield buffer
            buffer = []
            num_full -= 1
            if num_full <= 0:
                break

    # Yield num_partial lists of partial_size
    for x in iterator:
        buffer.append(x)
        if len(buffer) == partial_size:
            yield buffer
            buffer = []
            num_partial -= 1

def _chunked_even_online(iterable, n):
    buffer = []
    maxbuf = n + (n - 2) * (n - 1)
    for x in iterable:
        buffer.append(x)
        if len(buffer) == maxbuf:
            yield buffer[:n]
            buffer = buffer[n:]
    yield from _chunked_even_finite(buffer, len(buffer), n)","def chunk_even_finite_iterable(iterable, N, n):
    if N < 1:
        return

    # Lists are either size `full_size <= n` or `partial_size = full_size - 1`
    q, r = divmod(N, n)
    num_lists = q + (1 if r > 0 else 0)
    q, r = divmod(N, num_lists)
    full_size = q + (1 if r > 0 else 0)
    partial_size = full_size - 1
    num_full = N - partial_size * num_lists
    num_partial = num_lists - num_full

    buffer = []
    iterator = iter(iterable)

    # Yield num_full lists of full_size
    for x in iterator:
        buffer.append(x)
        if len(buffer) == full_size:
            yield buffer
            buffer = []
            num_full -= 1
            if num_full <= 0:
                break

    # Yield num_partial lists of partial_size
    for x in iterator:
        buffer.append(x)
        if len(buffer) == partial_size:
            yield buffer
            buffer = []
            num_partial -= 1

def yield_chunked_even_online(iterable, n):
    buffer = []
    maxbuf = n + (n - 2) * (n - 1)
    for x in iterable:
        buffer.append(x)
        if len(buffer) == maxbuf:
            yield buffer[:n]
            buffer = buffer[n:]
    yield from chunk_even_finite_iterable(buffer, len(buffer), n)","def break_iterable_into_approximately_equal_length_lists(iterable, n):
    """"""Break *iterable* into lists of approximately length *n*.
    Items are distributed such the lengths of the lists differ by at most
    1 item.

    >>> iterable = [1, 2, 3, 4, 5, 6, 7]
    >>> n = 3
    >>> list(break_iterable_into_approximately_equal_length_lists(iterable, n))  # List lengths: 3, 2, 2
    [[1, 2, 3], [4, 5], [6, 7]]
    >>> list(chunked(iterable, n))  # List lengths: 3, 3, 1
    [[1, 2, 3], [4, 5, 6], [7]]

    """"""

    len_method = getattr(iterable, '__len__', None)

    if len_method is None:
        return yield_break_iterable_into_approximately_equal_length_lists_online(iterable, n)
    else:
        return chunk_even_finite_iterable(iterable, len_method(), n)","{""_chunked_even_finite"": ""chunk_even_finite_iterable"", ""_chunked_even_online"": ""yield_chunked_even_online""}","def 0(iterable, N, n):
    if N < 1:
        return

    # Lists are either size `full_size <= n` or `partial_size = full_size - 1`
    q, r = divmod(N, n)
    num_lists = q + (1 if r > 0 else 0)
    q, r = divmod(N, num_lists)
    full_size = q + (1 if r > 0 else 0)
    partial_size = full_size - 1
    num_full = N - partial_size * num_lists
    num_partial = num_lists - num_full

    buffer = []
    iterator = iter(iterable)

    # Yield num_full lists of full_size
    for x in iterator:
        buffer.append(x)
        if len(buffer) == full_size:
            yield buffer
            buffer = []
            num_full -= 1
            if num_full <= 0:
                break

    # Yield num_partial lists of partial_size
    for x in iterator:
        buffer.append(x)
        if len(buffer) == partial_size:
            yield buffer
            buffer = []
            num_partial -= 1

def 1(iterable, n):
    buffer = []
    maxbuf = n + (n - 2) * (n - 1)
    for x in iterable:
        buffer.append(x)
        if len(buffer) == maxbuf:
            yield buffer[:n]
            buffer = buffer[n:]
    yield from 0(buffer, len(buffer), n)","def 2(iterable, n):
    """"""Break *iterable* into lists of approximately length *n*.
    Items are distributed such the lengths of the lists differ by at most
    1 item.

    >>> iterable = [1, 2, 3, 4, 5, 6, 7]
    >>> n = 3
    >>> list(2(iterable, n))  # List lengths: 3, 2, 2
    [[1, 2, 3], [4, 5], [6, 7]]
    >>> list(chunked(iterable, n))  # List lengths: 3, 3, 1
    [[1, 2, 3], [4, 5, 6], [7]]

    """"""

    len_method = getattr(iterable, '__len__', None)

    if len_method is None:
        return 1(iterable, n)
    else:
        return 0(iterable, len_method(), n)","{""_chunked_even_finite"": ""0"", ""_chunked_even_online"": ""1""}"
510,510,"def pad_none(iterable):
    """"""Returns the sequence of elements and then returns ``None`` indefinitely.

        >>> take(5, pad_none(range(3)))
        [0, 1, 2, None, None]

    Useful for emulating the behavior of the built-in :func:`map` function.

    See also :func:`padded`.

    """"""
    return chain(iterable, repeat(None))","def repeat(stmt=""pass"", setup=""pass"", timer=default_timer,
           repeat=default_repeat, number=default_number, globals=None):
    """"""Convenience function to create Timer object and call repeat method.""""""
    return Timer(stmt, setup, timer, globals).repeat(repeat, number)","def timeit_repeat(stmt=""pass"", setup=""pass"", timer=default_timer,
           timeit_repeat=default_timeit_repeat, number=default_number, globals=None):
    """"""Convenience function to create Timer object and call timeit_repeat method.""""""
    return Timer(stmt, setup, timer, globals).timeit_repeat(timeit_repeat, number)","def return_sequence_of_elements_and_then_return_none(iterable):
    """"""Returns the sequence of elements and then returns ``None`` indefinitely.

        >>> take(5, return_sequence_of_elements_and_then_return_none(range(3)))
        [0, 1, 2, None, None]

    Useful for emulating the behavior of the built-in :func:`map` function.

    See also :func:`padded`.

    """"""
    return chain(iterable, timeit_repeat(None))","{""repeat"": ""timeit_repeat""}","def 0(stmt=""pass"", setup=""pass"", timer=default_timer,
           repeat=default_repeat, number=default_number, globals=None):
    """"""Convenience function to create Timer object and call repeat method.""""""
    return Timer(stmt, setup, timer, globals).0(repeat, number)","def 1(iterable):
    """"""Returns the sequence of elements and then returns ``None`` indefinitely.

        >>> take(5, 1(range(3)))
        [0, 1, 2, None, None]

    Useful for emulating the behavior of the built-in :func:`map` function.

    See also :func:`padded`.

    """"""
    return chain(iterable, 0(None))","{""repeat"": ""0""}"
511,511,"def ncycles(iterable, n):
    """"""Returns the sequence elements *n* times

    >>> list(ncycles([""a"", ""b""], 3))
    ['a', 'b', 'a', 'b', 'a', 'b']

    """"""
    return chain.from_iterable(repeat(tuple(iterable), n))","def repeat(stmt=""pass"", setup=""pass"", timer=default_timer,
           repeat=default_repeat, number=default_number, globals=None):
    """"""Convenience function to create Timer object and call repeat method.""""""
    return Timer(stmt, setup, timer, globals).repeat(repeat, number)","def timeit_repeat(stmt=""pass"", setup=""pass"", timer=default_timer,
           timeit_repeat=default_timeit_repeat, number=default_number, globals=None):
    """"""Convenience function to create Timer object and call timeit_repeat method.""""""
    return Timer(stmt, setup, timer, globals).timeit_repeat(timeit_repeat, number)","def return_sequence_elements_n_times(iterable, n):
    """"""Returns the sequence elements *n* times

    >>> list(return_sequence_elements_n_times([""a"", ""b""], 3))
    ['a', 'b', 'a', 'b', 'a', 'b']

    """"""
    return chain.from_iterable(timeit_repeat(tuple(iterable), n))","{""repeat"": ""timeit_repeat""}","def 0(stmt=""pass"", setup=""pass"", timer=default_timer,
           repeat=default_repeat, number=default_number, globals=None):
    """"""Convenience function to create Timer object and call repeat method.""""""
    return Timer(stmt, setup, timer, globals).0(repeat, number)","def 1(iterable, n):
    """"""Returns the sequence elements *n* times

    >>> list(1([""a"", ""b""], 3))
    ['a', 'b', 'a', 'b', 'a', 'b']

    """"""
    return chain.from_iterable(0(tuple(iterable), n))","{""repeat"": ""0""}"
512,512,"def repeatfunc(func, times=None, *args):
    """"""Call *func* with *args* repeatedly, returning an iterable over the
    results.

    If *times* is specified, the iterable will terminate after that many
    repetitions:

        >>> from operator import add
        >>> times = 4
        >>> args = 3, 5
        >>> list(repeatfunc(add, times, *args))
        [8, 8, 8, 8]

    If *times* is ``None`` the iterable will not terminate:

        >>> from random import randrange
        >>> times = None
        >>> args = 1, 11
        >>> take(6, repeatfunc(randrange, times, *args))  # doctest:+SKIP
        [2, 4, 8, 1, 8, 4]

    """"""
    if times is None:
        return starmap(func, repeat(args))
    return starmap(func, repeat(args, times))","def repeat(stmt=""pass"", setup=""pass"", timer=default_timer,
           repeat=default_repeat, number=default_number, globals=None):
    """"""Convenience function to create Timer object and call repeat method.""""""
    return Timer(stmt, setup, timer, globals).repeat(repeat, number)","def timeit_repeat(stmt=""pass"", setup=""pass"", timer=default_timer,
           timeit_repeat=default_timeit_repeat, number=default_number, globals=None):
    """"""Convenience function to create Timer object and call timeit_repeat method.""""""
    return Timer(stmt, setup, timer, globals).timeit_repeat(timeit_repeat, number)","def timeit_call_function_repeatedly_and_return_iterable_over_results(func, times=None, *args):
    """"""Call *func* with *args* timeit_repeatedly, returning an iterable over the
    results.

    If *times* is specified, the iterable will terminate after that many
    repetitions:

        >>> from operator import add
        >>> times = 4
        >>> args = 3, 5
        >>> list(timeit_call_function_repeatedly_and_return_iterable_over_results(add, times, *args))
        [8, 8, 8, 8]

    If *times* is ``None`` the iterable will not terminate:

        >>> from random import randrange
        >>> times = None
        >>> args = 1, 11
        >>> take(6, timeit_call_function_repeatedly_and_return_iterable_over_results(randrange, times, *args))  # doctest:+SKIP
        [2, 4, 8, 1, 8, 4]

    """"""
    if times is None:
        return starmap(func, timeit_repeat(args))
    return starmap(func, timeit_repeat(args, times))","{""repeat"": ""timeit_repeat""}","def 0(stmt=""pass"", setup=""pass"", timer=default_timer,
           repeat=default_repeat, number=default_number, globals=None):
    """"""Convenience function to create Timer object and call repeat method.""""""
    return Timer(stmt, setup, timer, globals).0(repeat, number)","def 1(func, times=None, *args):
    """"""Call *func* with *args* repeatedly, returning an iterable over the
    results.

    If *times* is specified, the iterable will terminate after that many
    repetitions:

        >>> from operator import add
        >>> times = 4
        >>> args = 3, 5
        >>> list(1(add, times, *args))
        [8, 8, 8, 8]

    If *times* is ``None`` the iterable will not terminate:

        >>> from random import randrange
        >>> times = None
        >>> args = 1, 11
        >>> take(6, 1(randrange, times, *args))  # doctest:+SKIP
        [2, 4, 8, 1, 8, 4]

    """"""
    if times is None:
        return starmap(func, 0(args))
    return starmap(func, 0(args, times))","{""repeat"": ""0""}"
513,513,"def first_true(iterable, default=None, pred=None):
    """"""
    Returns the first true value in the iterable.

    If no true value is found, returns *default*

    If *pred* is not None, returns the first item for which
    ``pred(item) == True`` .

        >>> first_true(range(10))
        1
        >>> first_true(range(10), pred=lambda x: x > 5)
        6
        >>> first_true(range(10), default='missing', pred=lambda x: x > 9)
        'missing'

    """"""
    return next(filter(pred, iterable), default)","def filter(names, pat):
    """"""Construct a list from those elements of the iterable NAMES that match PAT.""""""
    result = []
    pat = os.path.normcase(pat)
    match = _compile_pattern(pat)
    if os.path is posixpath:
        # normcase on posix is NOP. Optimize it away from the loop.
        for name in names:
            if match(name):
                result.append(name)
    else:
        for name in names:
            if match(os.path.normcase(name)):
                result.append(name)
    return result","def filter_names(names, pat):
    """"""Construct a list from those elements of the iterable NAMES that match PAT.""""""
    result = []
    pat = os.path.normcase(pat)
    match = _compile_pattern(pat)
    if os.path is posixpath:
        # normcase on posix is NOP. Optimize it away from the loop.
        for name in names:
            if match(name):
                result.append(name)
    else:
        for name in names:
            if match(os.path.normcase(name)):
                result.append(name)
    return result","def return_first_true_value_in_iterable(iterable, default=None, pred=None):
    """"""
    Returns the first true value in the iterable.

    If no true value is found, returns *default*

    If *pred* is not None, returns the first item for which
    ``pred(item) == True`` .

        >>> return_first_true_value_in_iterable(range(10))
        1
        >>> return_first_true_value_in_iterable(range(10), pred=lambda x: x > 5)
        6
        >>> return_first_true_value_in_iterable(range(10), default='missing', pred=lambda x: x > 9)
        'missing'

    """"""
    return next(filter_names(pred, iterable), default)","{""filter"": ""filter_names""}","def 0(names, pat):
    """"""Construct a list from those elements of the iterable NAMES that match PAT.""""""
    result = []
    pat = os.path.normcase(pat)
    match = _compile_pattern(pat)
    if os.path is posixpath:
        # normcase on posix is NOP. Optimize it away from the loop.
        for name in names:
            if match(name):
                result.append(name)
    else:
        for name in names:
            if match(os.path.normcase(name)):
                result.append(name)
    return result","def 1(iterable, default=None, pred=None):
    """"""
    Returns the first true value in the iterable.

    If no true value is found, returns *default*

    If *pred* is not None, returns the first item for which
    ``pred(item) == True`` .

        >>> 1(range(10))
        1
        >>> 1(range(10), pred=lambda x: x > 5)
        6
        >>> 1(range(10), default='missing', pred=lambda x: x > 9)
        'missing'

    """"""
    return next(0(pred, iterable), default)","{""filter"": ""0""}"
514,514,"def triplewise(iterable):
    """"""Return overlapping triplets from *iterable*.

    >>> list(triplewise('ABCDE'))
    [('A', 'B', 'C'), ('B', 'C', 'D'), ('C', 'D', 'E')]

    """"""
    for (a, _), (b, c) in pairwise(pairwise(iterable)):
        yield a, b, c","def pairwise(iterable: Iterable[Any]) -> Iterator[Tuple[Any, Any]]:
    """"""
    Return paired elements.

    For example:
        s -> (s0, s1), (s2, s3), (s4, s5), ...
    """"""
    iterable = iter(iterable)
    return zip_longest(iterable, iterable)","def pairwise(iterable: Iterable[Any]) -> Iterator[Tuple[Any, Any]]:
    """"""
    Return paired elements.

    For example:
        s -> (s0, s1), (s2, s3), (s4, s5), ...
    """"""
    iterable = iter(iterable)
    return zip_longest(iterable, iterable)","def return_overlapping_triplets_from_iterable(iterable):
    """"""Return overlapping triplets from *iterable*.

    >>> list(return_overlapping_triplets_from_iterable('ABCDE'))
    [('A', 'B', 'C'), ('B', 'C', 'D'), ('C', 'D', 'E')]

    """"""
    for (a, _), (b, c) in pairwise(pairwise(iterable)):
        yield a, b, c",{},"def pairwise(iterable: Iterable[Any]) -> Iterator[Tuple[Any, Any]]:
    """"""
    Return paired elements.

    For example:
        s -> (s0, s1), (s2, s3), (s4, s5), ...
    """"""
    iterable = iter(iterable)
    return zip_longest(iterable, iterable)","def 1(iterable):
    """"""Return overlapping triplets from *iterable*.

    >>> list(1('ABCDE'))
    [('A', 'B', 'C'), ('B', 'C', 'D'), ('C', 'D', 'E')]

    """"""
    for (a, _), (b, c) in pairwise(pairwise(iterable)):
        yield a, b, c",{}
515,515,"def normalize_path(path):
    # type: (Any) -> str
    """"""Normalize a path by ensuring it is a string.

    If the resulting string contains path separators, an exception is raised.
    """"""
    str_path = str(path)
    parent, file_name = os.path.split(str_path)
    if parent:
        raise ValueError(f'{path!r} must be only a file name')
    return file_name","def str(val):
    """"""Convert float to string, taking the locale into account.""""""
    return _format(""%.12g"", val)","def locale_aware_float_to_string(val):
    """"""Convert float to locale_aware_float_to_stringing, taking the locale into account.""""""
    return _format(""%.12g"", val)","def normalize_path_by_ensuring_it_is_only_a_file_name(path):
    # type: (Any) -> locale_aware_float_to_string
    """"""Normalize a path by ensuring it is a locale_aware_float_to_stringing.

    If the resulting locale_aware_float_to_stringing contains path separators, an exception is raised.
    """"""
    locale_aware_float_to_string_path = locale_aware_float_to_string(path)
    parent, file_name = os.path.split(locale_aware_float_to_string_path)
    if parent:
        raise ValueError(f'{path!r} must be only a file name')
    return file_name","{""str"": ""locale_aware_float_to_string""}","def 0(val):
    """"""Convert float to string, taking the locale into account.""""""
    return _format(""%.12g"", val)","def 1(path):
    # type: (Any) -> str
    """"""Normalize a path by ensuring it is a string.

    If the resulting string contains path separators, an exception is raised.
    """"""
    str_path = 0(path)
    parent, file_name = os.path.split(str_path)
    if parent:
        raise ValueError(f'{path!r} must be only a file name')
    return file_name","{""str"": ""0""}"
516,516,"def do_override():
    """"""
    Ensure that the local copy of distutils is preferred over stdlib.

    See https://github.com/pypa/setuptools/issues/417#issuecomment-392298401
    for more motivation.
    """"""
    if enabled():
        warn_distutils_present()
        ensure_local_distutils()","def warn_distutils_present():
    if 'distutils' not in sys.modules:
        return
    if is_pypy and sys.version_info < (3, 7):
        # PyPy for 3.6 unconditionally imports distutils, so bypass the warning
        # https://foss.heptapod.net/pypy/pypy/-/blob/be829135bc0d758997b3566062999ee8b23872b4/lib-python/3/site.py#L250
        return
    import warnings

    warnings.warn(
        ""Distutils was imported before Setuptools, but importing Setuptools ""
        ""also replaces the `distutils` module in `sys.modules`. This may lead ""
        ""to undesirable behaviors or errors. To avoid these issues, avoid ""
        ""using distutils directly, ensure that setuptools is installed in the ""
        ""traditional way (e.g. not an editable install), and/or make sure ""
        ""that setuptools is always imported before distutils.""
    )

def enabled():
    """"""
    Allow selection of distutils by environment variable.
    """"""
    which = os.environ.get('SETUPTOOLS_USE_DISTUTILS', 'local')
    return which == 'local'

def ensure_local_distutils():
    import importlib

    clear_distutils()

    # With the DistutilsMetaFinder in place,
    # perform an import to cause distutils to be
    # loaded from setuptools._distutils. Ref #2906.
    with shim():
        importlib.import_module('distutils')

    # check that submodules load as expected
    core = importlib.import_module('distutils.core')
    assert '_distutils' in core.__file__, core.__file__
    assert 'setuptools._distutils.log' not in sys.modules","def warn_distutils_module_present_but_use_setuptools():
    if 'distutils' not in sys.modules:
        return
    if is_pypy and sys.version_info < (3, 7):
        # PyPy for 3.6 unconditionally imports distutils, so bypass the warning
        # https://foss.heptapod.net/pypy/pypy/-/blob/be829135bc0d758997b3566062999ee8b23872b4/lib-python/3/site.py#L250
        return
    import warnings

    warnings.warn(
        ""Distutils was imported before Setuptools, but importing Setuptools ""
        ""also replaces the `distutils` module in `sys.modules`. This may lead ""
        ""to undesirable behaviors or errors. To avoid these issues, avoid ""
        ""using distutils directly, ensure that setuptools is installed in the ""
        ""traditional way (e.g. not an editable install), and/or make sure ""
        ""that setuptools is always imported before distutils.""
    )

def distutils_selection_by_env_var():
    """"""
    Allow selection of distutils by environment variable.
    """"""
    which = os.environ.get('SETUPTOOLS_USE_DISTUTILS', 'local')
    return which == 'local'

def ensure_local_distutils_with_import():
    import importlib

    clear_distutils()

    # With the DistutilsMetaFinder in place,
    # perform an import to cause distutils to be
    # loaded from setuptools._distutils. Ref #2906.
    with shim():
        importlib.import_module('distutils')

    # check that submodules load as expected
    core = importlib.import_module('distutils.core')
    assert '_distutils' in core.__file__, core.__file__
    assert 'setuptools._distutils.log' not in sys.modules","def ensure_local_distutils_override():
    """"""
    Ensure that the local copy of distutils is preferred over stdlib.

    See https://github.com/pypa/setuptools/issues/417#issuecomment-392298401
    for more motivation.
    """"""
    if distutils_selection_by_env_var():
        warn_distutils_module_present_but_use_setuptools()
        ensure_local_distutils_with_import()","{""warn_distutils_present"": ""warn_distutils_module_present_but_use_setuptools"", ""enabled"": ""distutils_selection_by_env_var"", ""ensure_local_distutils"": ""ensure_local_distutils_with_import""}","def 0():
    if 'distutils' not in sys.modules:
        return
    if is_pypy and sys.version_info < (3, 7):
        # PyPy for 3.6 unconditionally imports distutils, so bypass the warning
        # https://foss.heptapod.net/pypy/pypy/-/blob/be829135bc0d758997b3566062999ee8b23872b4/lib-python/3/site.py#L250
        return
    import warnings

    warnings.warn(
        ""Distutils was imported before Setuptools, but importing Setuptools ""
        ""also replaces the `distutils` module in `sys.modules`. This may lead ""
        ""to undesirable behaviors or errors. To avoid these issues, avoid ""
        ""using distutils directly, ensure that setuptools is installed in the ""
        ""traditional way (e.g. not an editable install), and/or make sure ""
        ""that setuptools is always imported before distutils.""
    )

def 1():
    """"""
    Allow selection of distutils by environment variable.
    """"""
    which = os.environ.get('SETUPTOOLS_USE_DISTUTILS', 'local')
    return which == 'local'

def 2():
    import importlib

    clear_distutils()

    # With the DistutilsMetaFinder in place,
    # perform an import to cause distutils to be
    # loaded from setuptools._distutils. Ref #2906.
    with shim():
        importlib.import_module('distutils')

    # check that submodules load as expected
    core = importlib.import_module('distutils.core')
    assert '_distutils' in core.__file__, core.__file__
    assert 'setuptools._distutils.log' not in sys.modules","def 3():
    """"""
    Ensure that the local copy of distutils is preferred over stdlib.

    See https://github.com/pypa/setuptools/issues/417#issuecomment-392298401
    for more motivation.
    """"""
    if 1():
        0()
        2()","{""warn_distutils_present"": ""0"", ""enabled"": ""1"", ""ensure_local_distutils"": ""2""}"
517,517,"def showinfo(title=None, message=None, **options):
    ""Show an info message""
    return _show(title, message, INFO, OK, **options)","def _show(title=None, message=None, _icon=None, _type=None, **options):
    if _icon and ""icon"" not in options:    options[""icon""] = _icon
    if _type and ""type"" not in options:    options[""type""] = _type
    if title:   options[""title""] = title
    if message: options[""message""] = message
    res = Message(**options).show()
    # In some Tcl installations, yes/no is converted into a boolean.
    if isinstance(res, bool):
        if res:
            return YES
        return NO
    # In others we get a Tcl_Obj.
    return str(res)","def message_box_show_with_icon_type_and_options(title=None, message=None, _icon=None, _type=None, **options):
    if _icon and ""icon"" not in options:    options[""icon""] = _icon
    if _type and ""type"" not in options:    options[""type""] = _type
    if title:   options[""title""] = title
    if message: options[""message""] = message
    res = Message(**options).show()
    # In some Tcl installations, yes/no is converted into a boolean.
    if isinstance(res, bool):
        if res:
            return YES
        return NO
    # In others we get a Tcl_Obj.
    return str(res)","def display_info_message_with_title(title=None, message=None, **options):
    ""Show an info message""
    return message_box_show_with_icon_type_and_options(title, message, INFO, OK, **options)","{""_show"": ""message_box_show_with_icon_type_and_options""}","def 0(title=None, message=None, _icon=None, _type=None, **options):
    if _icon and ""icon"" not in options:    options[""icon""] = _icon
    if _type and ""type"" not in options:    options[""type""] = _type
    if title:   options[""title""] = title
    if message: options[""message""] = message
    res = Message(**options).show()
    # In some Tcl installations, yes/no is converted into a boolean.
    if isinstance(res, bool):
        if res:
            return YES
        return NO
    # In others we get a Tcl_Obj.
    return str(res)","def 1(title=None, message=None, **options):
    ""Show an info message""
    return 0(title, message, INFO, OK, **options)","{""_show"": ""0""}"
518,518,"def showwarning(title=None, message=None, **options):
    ""Show a warning message""
    return _show(title, message, WARNING, OK, **options)","def _show(title=None, message=None, _icon=None, _type=None, **options):
    if _icon and ""icon"" not in options:    options[""icon""] = _icon
    if _type and ""type"" not in options:    options[""type""] = _type
    if title:   options[""title""] = title
    if message: options[""message""] = message
    res = Message(**options).show()
    # In some Tcl installations, yes/no is converted into a boolean.
    if isinstance(res, bool):
        if res:
            return YES
        return NO
    # In others we get a Tcl_Obj.
    return str(res)","def message_box_show_with_icon_type_and_options(title=None, message=None, _icon=None, _type=None, **options):
    if _icon and ""icon"" not in options:    options[""icon""] = _icon
    if _type and ""type"" not in options:    options[""type""] = _type
    if title:   options[""title""] = title
    if message: options[""message""] = message
    res = Message(**options).show()
    # In some Tcl installations, yes/no is converted into a boolean.
    if isinstance(res, bool):
        if res:
            return YES
        return NO
    # In others we get a Tcl_Obj.
    return str(res)","def display_warning_message_with_title(title=None, message=None, **options):
    ""Show a warning message""
    return message_box_show_with_icon_type_and_options(title, message, WARNING, OK, **options)","{""_show"": ""message_box_show_with_icon_type_and_options""}","def 0(title=None, message=None, _icon=None, _type=None, **options):
    if _icon and ""icon"" not in options:    options[""icon""] = _icon
    if _type and ""type"" not in options:    options[""type""] = _type
    if title:   options[""title""] = title
    if message: options[""message""] = message
    res = Message(**options).show()
    # In some Tcl installations, yes/no is converted into a boolean.
    if isinstance(res, bool):
        if res:
            return YES
        return NO
    # In others we get a Tcl_Obj.
    return str(res)","def 1(title=None, message=None, **options):
    ""Show a warning message""
    return 0(title, message, WARNING, OK, **options)","{""_show"": ""0""}"
519,519,"def showerror(title=None, message=None, **options):
    ""Show an error message""
    return _show(title, message, ERROR, OK, **options)","def _show(title=None, message=None, _icon=None, _type=None, **options):
    if _icon and ""icon"" not in options:    options[""icon""] = _icon
    if _type and ""type"" not in options:    options[""type""] = _type
    if title:   options[""title""] = title
    if message: options[""message""] = message
    res = Message(**options).show()
    # In some Tcl installations, yes/no is converted into a boolean.
    if isinstance(res, bool):
        if res:
            return YES
        return NO
    # In others we get a Tcl_Obj.
    return str(res)","def message_box_show_with_icon_type_and_options(title=None, message=None, _icon=None, _type=None, **options):
    if _icon and ""icon"" not in options:    options[""icon""] = _icon
    if _type and ""type"" not in options:    options[""type""] = _type
    if title:   options[""title""] = title
    if message: options[""message""] = message
    res = Message(**options).show()
    # In some Tcl installations, yes/no is converted into a boolean.
    if isinstance(res, bool):
        if res:
            return YES
        return NO
    # In others we get a Tcl_Obj.
    return str(res)","def show_error_message(title=None, message=None, **options):
    ""Show an error message""
    return message_box_show_with_icon_type_and_options(title, message, ERROR, OK, **options)","{""_show"": ""message_box_show_with_icon_type_and_options""}","def 0(title=None, message=None, _icon=None, _type=None, **options):
    if _icon and ""icon"" not in options:    options[""icon""] = _icon
    if _type and ""type"" not in options:    options[""type""] = _type
    if title:   options[""title""] = title
    if message: options[""message""] = message
    res = Message(**options).show()
    # In some Tcl installations, yes/no is converted into a boolean.
    if isinstance(res, bool):
        if res:
            return YES
        return NO
    # In others we get a Tcl_Obj.
    return str(res)","def 1(title=None, message=None, **options):
    ""Show an error message""
    return 0(title, message, ERROR, OK, **options)","{""_show"": ""0""}"
520,520,"def askquestion(title=None, message=None, **options):
    ""Ask a question""
    return _show(title, message, QUESTION, YESNO, **options)","def _show(title=None, message=None, _icon=None, _type=None, **options):
    if _icon and ""icon"" not in options:    options[""icon""] = _icon
    if _type and ""type"" not in options:    options[""type""] = _type
    if title:   options[""title""] = title
    if message: options[""message""] = message
    res = Message(**options).show()
    # In some Tcl installations, yes/no is converted into a boolean.
    if isinstance(res, bool):
        if res:
            return YES
        return NO
    # In others we get a Tcl_Obj.
    return str(res)","def message_box_show_with_icon_type_and_options(title=None, message=None, _icon=None, _type=None, **options):
    if _icon and ""icon"" not in options:    options[""icon""] = _icon
    if _type and ""type"" not in options:    options[""type""] = _type
    if title:   options[""title""] = title
    if message: options[""message""] = message
    res = Message(**options).show()
    # In some Tcl installations, yes/no is converted into a boolean.
    if isinstance(res, bool):
        if res:
            return YES
        return NO
    # In others we get a Tcl_Obj.
    return str(res)","def prompt_user_with_question_and_title(title=None, message=None, **options):
    ""Ask a question""
    return message_box_show_with_icon_type_and_options(title, message, QUESTION, YESNO, **options)","{""_show"": ""message_box_show_with_icon_type_and_options""}","def 0(title=None, message=None, _icon=None, _type=None, **options):
    if _icon and ""icon"" not in options:    options[""icon""] = _icon
    if _type and ""type"" not in options:    options[""type""] = _type
    if title:   options[""title""] = title
    if message: options[""message""] = message
    res = Message(**options).show()
    # In some Tcl installations, yes/no is converted into a boolean.
    if isinstance(res, bool):
        if res:
            return YES
        return NO
    # In others we get a Tcl_Obj.
    return str(res)","def 1(title=None, message=None, **options):
    ""Ask a question""
    return 0(title, message, QUESTION, YESNO, **options)","{""_show"": ""0""}"
521,521,"def askokcancel(title=None, message=None, **options):
    ""Ask if operation should proceed; return true if the answer is ok""
    s = _show(title, message, QUESTION, OKCANCEL, **options)
    return s == OK","def _show(title=None, message=None, _icon=None, _type=None, **options):
    if _icon and ""icon"" not in options:    options[""icon""] = _icon
    if _type and ""type"" not in options:    options[""type""] = _type
    if title:   options[""title""] = title
    if message: options[""message""] = message
    res = Message(**options).show()
    # In some Tcl installations, yes/no is converted into a boolean.
    if isinstance(res, bool):
        if res:
            return YES
        return NO
    # In others we get a Tcl_Obj.
    return str(res)","def message_box_show_with_icon_type_and_options(title=None, message=None, _icon=None, _type=None, **options):
    if _icon and ""icon"" not in options:    options[""icon""] = _icon
    if _type and ""type"" not in options:    options[""type""] = _type
    if title:   options[""title""] = title
    if message: options[""message""] = message
    res = Message(**options).show()
    # In some Tcl installations, yes/no is converted into a boolean.
    if isinstance(res, bool):
        if res:
            return YES
        return NO
    # In others we get a Tcl_Obj.
    return str(res)","def ask_if_operation_should_proceed_with_ok_cancel(title=None, message=None, **options):
    ""Ask if operation should proceed; return true if the answer is ok""
    s = message_box_show_with_icon_type_and_options(title, message, QUESTION, OKCANCEL, **options)
    return s == OK","{""_show"": ""message_box_show_with_icon_type_and_options""}","def 0(title=None, message=None, _icon=None, _type=None, **options):
    if _icon and ""icon"" not in options:    options[""icon""] = _icon
    if _type and ""type"" not in options:    options[""type""] = _type
    if title:   options[""title""] = title
    if message: options[""message""] = message
    res = Message(**options).show()
    # In some Tcl installations, yes/no is converted into a boolean.
    if isinstance(res, bool):
        if res:
            return YES
        return NO
    # In others we get a Tcl_Obj.
    return str(res)","def 1(title=None, message=None, **options):
    ""Ask if operation should proceed; return true if the answer is ok""
    s = 0(title, message, QUESTION, OKCANCEL, **options)
    return s == OK","{""_show"": ""0""}"
522,522,"def askyesno(title=None, message=None, **options):
    ""Ask a question; return true if the answer is yes""
    s = _show(title, message, QUESTION, YESNO, **options)
    return s == YES","def _show(title=None, message=None, _icon=None, _type=None, **options):
    if _icon and ""icon"" not in options:    options[""icon""] = _icon
    if _type and ""type"" not in options:    options[""type""] = _type
    if title:   options[""title""] = title
    if message: options[""message""] = message
    res = Message(**options).show()
    # In some Tcl installations, yes/no is converted into a boolean.
    if isinstance(res, bool):
        if res:
            return YES
        return NO
    # In others we get a Tcl_Obj.
    return str(res)","def message_box_show_with_icon_type_and_options(title=None, message=None, _icon=None, _type=None, **options):
    if _icon and ""icon"" not in options:    options[""icon""] = _icon
    if _type and ""type"" not in options:    options[""type""] = _type
    if title:   options[""title""] = title
    if message: options[""message""] = message
    res = Message(**options).show()
    # In some Tcl installations, yes/no is converted into a boolean.
    if isinstance(res, bool):
        if res:
            return YES
        return NO
    # In others we get a Tcl_Obj.
    return str(res)","def ask_user_yes_or_no_question_with_title(title=None, message=None, **options):
    ""Ask a question; return true if the answer is yes""
    s = message_box_show_with_icon_type_and_options(title, message, QUESTION, YESNO, **options)
    return s == YES","{""_show"": ""message_box_show_with_icon_type_and_options""}","def 0(title=None, message=None, _icon=None, _type=None, **options):
    if _icon and ""icon"" not in options:    options[""icon""] = _icon
    if _type and ""type"" not in options:    options[""type""] = _type
    if title:   options[""title""] = title
    if message: options[""message""] = message
    res = Message(**options).show()
    # In some Tcl installations, yes/no is converted into a boolean.
    if isinstance(res, bool):
        if res:
            return YES
        return NO
    # In others we get a Tcl_Obj.
    return str(res)","def 1(title=None, message=None, **options):
    ""Ask a question; return true if the answer is yes""
    s = 0(title, message, QUESTION, YESNO, **options)
    return s == YES","{""_show"": ""0""}"
523,523,"def askyesnocancel(title=None, message=None, **options):
    ""Ask a question; return true if the answer is yes, None if cancelled.""
    s = _show(title, message, QUESTION, YESNOCANCEL, **options)
    # s might be a Tcl index object, so convert it to a string
    s = str(s)
    if s == CANCEL:
        return None
    return s == YES","def str(val):
    """"""Convert float to string, taking the locale into account.""""""
    return _format(""%.12g"", val)

def _show(title=None, message=None, _icon=None, _type=None, **options):
    if _icon and ""icon"" not in options:    options[""icon""] = _icon
    if _type and ""type"" not in options:    options[""type""] = _type
    if title:   options[""title""] = title
    if message: options[""message""] = message
    res = Message(**options).show()
    # In some Tcl installations, yes/no is converted into a boolean.
    if isinstance(res, bool):
        if res:
            return YES
        return NO
    # In others we get a Tcl_Obj.
    return str(res)","def locale_aware_float_to_string(val):
    """"""Convert float to locale_aware_float_to_stringing, taking the locale into account.""""""
    return _format(""%.12g"", val)

def message_box_show_with_icon_type_and_options(title=None, message=None, _icon=None, _type=None, **options):
    if _icon and ""icon"" not in options:    options[""icon""] = _icon
    if _type and ""type"" not in options:    options[""type""] = _type
    if title:   options[""title""] = title
    if message: options[""message""] = message
    res = Message(**options).show()
    # In some Tcl installations, yes/no is converted into a boolean.
    if isinstance(res, bool):
        if res:
            return YES
        return NO
    # In others we get a Tcl_Obj.
    return locale_aware_float_to_string(res)","def ask_user_yes_no_cancel_question_with_title(title=None, message=None, **options):
    ""Ask a question; return true if the answer is yes, None if cancelled.""
    s = message_box_show_with_icon_type_and_options(title, message, QUESTION, YESNOCANCEL, **options)
    # s might be a Tcl index object, so convert it to a locale_aware_float_to_stringing
    s = locale_aware_float_to_string(s)
    if s == CANCEL:
        return None
    return s == YES","{""str"": ""locale_aware_float_to_string"", ""_show"": ""message_box_show_with_icon_type_and_options""}","def 0(val):
    """"""Convert float to string, taking the locale into account.""""""
    return _format(""%.12g"", val)

def 1(title=None, message=None, _icon=None, _type=None, **options):
    if _icon and ""icon"" not in options:    options[""icon""] = _icon
    if _type and ""type"" not in options:    options[""type""] = _type
    if title:   options[""title""] = title
    if message: options[""message""] = message
    res = Message(**options).show()
    # In some Tcl installations, yes/no is converted into a boolean.
    if isinstance(res, bool):
        if res:
            return YES
        return NO
    # In others we get a Tcl_Obj.
    return 0(res)","def 2(title=None, message=None, **options):
    ""Ask a question; return true if the answer is yes, None if cancelled.""
    s = 1(title, message, QUESTION, YESNOCANCEL, **options)
    # s might be a Tcl index object, so convert it to a string
    s = 0(s)
    if s == CANCEL:
        return None
    return s == YES","{""str"": ""0"", ""_show"": ""1""}"
524,524,"def askretrycancel(title=None, message=None, **options):
    ""Ask if operation should be retried; return true if the answer is yes""
    s = _show(title, message, WARNING, RETRYCANCEL, **options)
    return s == RETRY","def _show(title=None, message=None, _icon=None, _type=None, **options):
    if _icon and ""icon"" not in options:    options[""icon""] = _icon
    if _type and ""type"" not in options:    options[""type""] = _type
    if title:   options[""title""] = title
    if message: options[""message""] = message
    res = Message(**options).show()
    # In some Tcl installations, yes/no is converted into a boolean.
    if isinstance(res, bool):
        if res:
            return YES
        return NO
    # In others we get a Tcl_Obj.
    return str(res)","def message_box_show_with_icon_type_and_options(title=None, message=None, _icon=None, _type=None, **options):
    if _icon and ""icon"" not in options:    options[""icon""] = _icon
    if _type and ""type"" not in options:    options[""type""] = _type
    if title:   options[""title""] = title
    if message: options[""message""] = message
    res = Message(**options).show()
    # In some Tcl installations, yes/no is converted into a boolean.
    if isinstance(res, bool):
        if res:
            return YES
        return NO
    # In others we get a Tcl_Obj.
    return str(res)","def ask_if_operation_should_be_retried_with_retry(title=None, message=None, **options):
    ""Ask if operation should be retried; return true if the answer is yes""
    s = message_box_show_with_icon_type_and_options(title, message, WARNING, RETRYCANCEL, **options)
    return s == RETRY","{""_show"": ""message_box_show_with_icon_type_and_options""}","def 0(title=None, message=None, _icon=None, _type=None, **options):
    if _icon and ""icon"" not in options:    options[""icon""] = _icon
    if _type and ""type"" not in options:    options[""type""] = _type
    if title:   options[""title""] = title
    if message: options[""message""] = message
    res = Message(**options).show()
    # In some Tcl installations, yes/no is converted into a boolean.
    if isinstance(res, bool):
        if res:
            return YES
        return NO
    # In others we get a Tcl_Obj.
    return str(res)","def 1(title=None, message=None, **options):
    ""Ask if operation should be retried; return true if the answer is yes""
    s = 0(title, message, WARNING, RETRYCANCEL, **options)
    return s == RETRY","{""_show"": ""0""}"
525,525,"def _splitdict(tk, v, cut_minus=True, conv=None):
    """"""Return a properly formatted dict built from Tcl list pairs.

    If cut_minus is True, the supposed '-' prefix will be removed from
    keys. If conv is specified, it is used to convert values.

    Tcl list is expected to contain an even number of elements.
    """"""
    t = tk.splitlist(v)
    if len(t) % 2:
        raise RuntimeError('Tcl list representing a dict is expected '
                           'to contain an even number of elements')
    it = iter(t)
    dict = {}
    for key, value in zip(it, it):
        key = str(key)
        if cut_minus and key[0] == '-':
            key = key[1:]
        if conv:
            value = conv(value)
        dict[key] = value
    return dict","def str(val):
    """"""Convert float to string, taking the locale into account.""""""
    return _format(""%.12g"", val)","def locale_aware_float_to_string(val):
    """"""Convert float to locale_aware_float_to_stringing, taking the locale into account.""""""
    return _format(""%.12g"", val)","def format_dict_from_tcl_list_pairs(tk, v, cut_minus=True, conv=None):
    """"""Return a properly formatted dict built from Tcl list pairs.

    If cut_minus is True, the supposed '-' prefix will be removed from
    keys. If conv is specified, it is used to convert values.

    Tcl list is expected to contain an even number of elements.
    """"""
    t = tk.splitlist(v)
    if len(t) % 2:
        raise RuntimeError('Tcl list representing a dict is expected '
                           'to contain an even number of elements')
    it = iter(t)
    dict = {}
    for key, value in zip(it, it):
        key = locale_aware_float_to_string(key)
        if cut_minus and key[0] == '-':
            key = key[1:]
        if conv:
            value = conv(value)
        dict[key] = value
    return dict","{""str"": ""locale_aware_float_to_string""}","def 0(val):
    """"""Convert float to string, taking the locale into account.""""""
    return _format(""%.12g"", val)","def 1(tk, v, cut_minus=True, conv=None):
    """"""Return a properly formatted dict built from Tcl list pairs.

    If cut_minus is True, the supposed '-' prefix will be removed from
    keys. If conv is specified, it is used to convert values.

    Tcl list is expected to contain an even number of elements.
    """"""
    t = tk.splitlist(v)
    if len(t) % 2:
        raise RuntimeError('Tcl list representing a dict is expected '
                           'to contain an even number of elements')
    it = iter(t)
    dict = {}
    for key, value in zip(it, it):
        key = 0(key)
        if cut_minus and key[0] == '-':
            key = key[1:]
        if conv:
            value = conv(value)
        dict[key] = value
    return dict","{""str"": ""0""}"
526,526,"def mainloop(n=0):
    """"""Run the main loop of Tcl.""""""
    _get_default_root('run the main loop').tk.mainloop(n)","def _get_default_root(what=None):
    if not _support_default_root:
        raise RuntimeError(""No master specified and tkinter is ""
                           ""configured to not support default root"")
    if _default_root is None:
        if what:
            raise RuntimeError(f""Too early to {what}: no default root window"")
        root = Tk()
        assert _default_root is root
    return _default_root","def get_default_root_or_support(what=None):
    if not _support_default_root:
        raise RuntimeError(""No master specified and tkinter is ""
                           ""configured to not support default root"")
    if _default_root is None:
        if what:
            raise RuntimeError(f""Too early to {what}: no default root window"")
        root = Tk()
        assert _default_root is root
    return _default_root","def run_tcl_main_loop_with_iteration_limit(n=0):
    """"""Run the main loop of Tcl.""""""
    get_default_root_or_support('run the main loop').tk.run_tcl_main_loop_with_iteration_limit(n)","{""_get_default_root"": ""get_default_root_or_support""}","def 0(what=None):
    if not _support_default_root:
        raise RuntimeError(""No master specified and tkinter is ""
                           ""configured to not support default root"")
    if _default_root is None:
        if what:
            raise RuntimeError(f""Too early to {what}: no default root window"")
        root = Tk()
        assert _default_root is root
    return _default_root","def 1(n=0):
    """"""Run the main loop of Tcl.""""""
    0('run the main loop').tk.1(n)","{""_get_default_root"": ""0""}"
527,527,"def getboolean(s):
    """"""Convert Tcl object to True or False.""""""
    try:
        return _get_default_root('use getboolean()').tk.getboolean(s)
    except TclError:
        raise ValueError(""invalid literal for getboolean()"")","def _get_default_root(what=None):
    if not _support_default_root:
        raise RuntimeError(""No master specified and tkinter is ""
                           ""configured to not support default root"")
    if _default_root is None:
        if what:
            raise RuntimeError(f""Too early to {what}: no default root window"")
        root = Tk()
        assert _default_root is root
    return _default_root","def get_default_root_or_support(what=None):
    if not _support_default_root:
        raise RuntimeError(""No master specified and tkinter is ""
                           ""configured to not support default root"")
    if _default_root is None:
        if what:
            raise RuntimeError(f""Too early to {what}: no default root window"")
        root = Tk()
        assert _default_root is root
    return _default_root","def convert_tcl_object_to_boolean(s):
    """"""Convert Tcl object to True or False.""""""
    try:
        return get_default_root_or_support('use convert_tcl_object_to_boolean()').tk.convert_tcl_object_to_boolean(s)
    except TclError:
        raise ValueError(""invalid literal for convert_tcl_object_to_boolean()"")","{""_get_default_root"": ""get_default_root_or_support""}","def 0(what=None):
    if not _support_default_root:
        raise RuntimeError(""No master specified and tkinter is ""
                           ""configured to not support default root"")
    if _default_root is None:
        if what:
            raise RuntimeError(f""Too early to {what}: no default root window"")
        root = Tk()
        assert _default_root is root
    return _default_root","def 1(s):
    """"""Convert Tcl object to True or False.""""""
    try:
        return 0('use 1()').tk.1(s)
    except TclError:
        raise ValueError(""invalid literal for 1()"")","{""_get_default_root"": ""0""}"
528,528,"def _list_from_statespec(stuple):
    """"""Construct a list from the given statespec tuple according to the
    accepted statespec accepted by _format_mapdict.""""""
    if isinstance(stuple, str):
        return stuple
    result = []
    it = iter(stuple)
    for state, val in zip(it, it):
        if hasattr(state, 'typename'):  # this is a Tcl object
            state = str(state).split()
        elif isinstance(state, str):
            state = state.split()
        elif not isinstance(state, (tuple, list)):
            state = (state,)
        if hasattr(val, 'typename'):
            val = str(val)
        result.append((*state, val))

    return result","def str(val):
    """"""Convert float to string, taking the locale into account.""""""
    return _format(""%.12g"", val)","def locale_aware_float_to_string(val):
    """"""Convert float to locale_aware_float_to_stringing, taking the locale into account.""""""
    return _format(""%.12g"", val)","def construct_list_from_state_spec_tuple(stuple):
    """"""Conlocale_aware_float_to_stringuct a list from the given statespec tuple according to the
    accepted statespec accepted by _format_mapdict.""""""
    if isinstance(stuple, locale_aware_float_to_string):
        return stuple
    result = []
    it = iter(stuple)
    for state, val in zip(it, it):
        if hasattr(state, 'typename'):  # this is a Tcl object
            state = locale_aware_float_to_string(state).split()
        elif isinstance(state, locale_aware_float_to_string):
            state = state.split()
        elif not isinstance(state, (tuple, list)):
            state = (state,)
        if hasattr(val, 'typename'):
            val = locale_aware_float_to_string(val)
        result.append((*state, val))

    return result","{""str"": ""locale_aware_float_to_string""}","def 0(val):
    """"""Convert float to string, taking the locale into account.""""""
    return _format(""%.12g"", val)","def 1(stuple):
    """"""Construct a list from the given statespec tuple according to the
    accepted statespec accepted by _format_mapdict.""""""
    if isinstance(stuple, str):
        return stuple
    result = []
    it = iter(stuple)
    for state, val in zip(it, it):
        if hasattr(state, 'typename'):  # this is a Tcl object
            state = 0(state).split()
        elif isinstance(state, str):
            state = state.split()
        elif not isinstance(state, (tuple, list)):
            state = (state,)
        if hasattr(val, 'typename'):
            val = 0(val)
        result.append((*state, val))

    return result","{""str"": ""0""}"
529,529,"def _list_from_layouttuple(tk, ltuple):
    """"""Construct a list from the tuple returned by ttk::layout, this is
    somewhat the reverse of _format_layoutlist.""""""
    ltuple = tk.splitlist(ltuple)
    res = []

    indx = 0
    while indx < len(ltuple):
        name = ltuple[indx]
        opts = {}
        res.append((name, opts))
        indx += 1

        while indx < len(ltuple): # grab name's options
            opt, val = ltuple[indx:indx + 2]
            if not opt.startswith('-'): # found next name
                break

            opt = opt[1:] # remove the '-' from the option
            indx += 2

            if opt == 'children':
                val = _list_from_layouttuple(tk, val)

            opts[opt] = val

    return res","def _list_from_layouttuple(tk, ltuple):
    """"""Construct a list from the tuple returned by ttk::layout, this is
    somewhat the reverse of _format_layoutlist.""""""
    ltuple = tk.splitlist(ltuple)
    res = []

    indx = 0
    while indx < len(ltuple):
        name = ltuple[indx]
        opts = {}
        res.append((name, opts))
        indx += 1

        while indx < len(ltuple): # grab name's options
            opt, val = ltuple[indx:indx + 2]
            if not opt.startswith('-'): # found next name
                break

            opt = opt[1:] # remove the '-' from the option
            indx += 2

            if opt == 'children':
                val = _list_from_layouttuple(tk, val)

            opts[opt] = val

    return res","def construct_list_from_ttk_layout_tuple(tk, ltuple):
    """"""Construct a list from the tuple returned by ttk::layout, this is
    somewhat the reverse of _format_layoutlist.""""""
    ltuple = tk.splitlist(ltuple)
    res = []

    indx = 0
    while indx < len(ltuple):
        name = ltuple[indx]
        opts = {}
        res.append((name, opts))
        indx += 1

        while indx < len(ltuple): # grab name's options
            opt, val = ltuple[indx:indx + 2]
            if not opt.startswith('-'): # found next name
                break

            opt = opt[1:] # remove the '-' from the option
            indx += 2

            if opt == 'children':
                val = construct_list_from_ttk_layout_tuple(tk, val)

            opts[opt] = val

    return res","def construct_list_from_ttk_layout_tuple(tk, ltuple):
    """"""Construct a list from the tuple returned by ttk::layout, this is
    somewhat the reverse of _format_layoutlist.""""""
    ltuple = tk.splitlist(ltuple)
    res = []

    indx = 0
    while indx < len(ltuple):
        name = ltuple[indx]
        opts = {}
        res.append((name, opts))
        indx += 1

        while indx < len(ltuple): # grab name's options
            opt, val = ltuple[indx:indx + 2]
            if not opt.startswith('-'): # found next name
                break

            opt = opt[1:] # remove the '-' from the option
            indx += 2

            if opt == 'children':
                val = construct_list_from_ttk_layout_tuple(tk, val)

            opts[opt] = val

    return res","{""_list_from_layouttuple"": ""construct_list_from_ttk_layout_tuple""}","def 0(tk, ltuple):
    """"""Construct a list from the tuple returned by ttk::layout, this is
    somewhat the reverse of _format_layoutlist.""""""
    ltuple = tk.splitlist(ltuple)
    res = []

    indx = 0
    while indx < len(ltuple):
        name = ltuple[indx]
        opts = {}
        res.append((name, opts))
        indx += 1

        while indx < len(ltuple): # grab name's options
            opt, val = ltuple[indx:indx + 2]
            if not opt.startswith('-'): # found next name
                break

            opt = opt[1:] # remove the '-' from the option
            indx += 2

            if opt == 'children':
                val = 0(tk, val)

            opts[opt] = val

    return res","def 0(tk, ltuple):
    """"""Construct a list from the tuple returned by ttk::layout, this is
    somewhat the reverse of _format_layoutlist.""""""
    ltuple = tk.splitlist(ltuple)
    res = []

    indx = 0
    while indx < len(ltuple):
        name = ltuple[indx]
        opts = {}
        res.append((name, opts))
        indx += 1

        while indx < len(ltuple): # grab name's options
            opt, val = ltuple[indx:indx + 2]
            if not opt.startswith('-'): # found next name
                break

            opt = opt[1:] # remove the '-' from the option
            indx += 2

            if opt == 'children':
                val = 0(tk, val)

            opts[opt] = val

    return res","{""_list_from_layouttuple"": ""0""}"
530,530,"def _val_or_dict(tk, options, *args):
    """"""Format options then call Tk command with args and options and return
    the appropriate result.

    If no option is specified, a dict is returned. If an option is
    specified with the None value, the value for that option is returned.
    Otherwise, the function just sets the passed options and the caller
    shouldn't be expecting a return value anyway.""""""
    options = _format_optdict(options)
    res = tk.call(*(args + options))

    if len(options) % 2: # option specified without a value, return its value
        return res

    return _splitdict(tk, res, conv=_tclobj_to_py)","def _splitdict(tk, v, cut_minus=True, conv=None):
    """"""Return a properly formatted dict built from Tcl list pairs.

    If cut_minus is True, the supposed '-' prefix will be removed from
    keys. If conv is specified, it is used to convert values.

    Tcl list is expected to contain an even number of elements.
    """"""
    t = tk.splitlist(v)
    if len(t) % 2:
        raise RuntimeError('Tcl list representing a dict is expected '
                           'to contain an even number of elements')
    it = iter(t)
    dict = {}
    for key, value in zip(it, it):
        key = str(key)
        if cut_minus and key[0] == '-':
            key = key[1:]
        if conv:
            value = conv(value)
        dict[key] = value
    return dict

def _format_optdict(optdict, script=False, ignore=None):
    """"""Formats optdict to a tuple to pass it to tk.call.

    E.g. (script=False):
      {'foreground': 'blue', 'padding': [1, 2, 3, 4]} returns:
      ('-foreground', 'blue', '-padding', '1 2 3 4')""""""

    opts = []
    for opt, value in optdict.items():
        if not ignore or opt not in ignore:
            opts.append(""-%s"" % opt)
            if value is not None:
                opts.append(_format_optvalue(value, script))

    return _flatten(opts)","def format_dict_from_tcl_list_pairs(tk, v, cut_minus=True, conv=None):
    """"""Return a properly formatted dict built from Tcl list pairs.

    If cut_minus is True, the supposed '-' prefix will be removed from
    keys. If conv is specified, it is used to convert values.

    Tcl list is expected to contain an even number of elements.
    """"""
    t = tk.splitlist(v)
    if len(t) % 2:
        raise RuntimeError('Tcl list representing a dict is expected '
                           'to contain an even number of elements')
    it = iter(t)
    dict = {}
    for key, value in zip(it, it):
        key = str(key)
        if cut_minus and key[0] == '-':
            key = key[1:]
        if conv:
            value = conv(value)
        dict[key] = value
    return dict

def format_option_dict_to_tuple_for_tk_call(optdict, script=False, ignore=None):
    """"""Formats optdict to a tuple to pass it to tk.call.

    E.g. (script=False):
      {'foreground': 'blue', 'padding': [1, 2, 3, 4]} returns:
      ('-foreground', 'blue', '-padding', '1 2 3 4')""""""

    opts = []
    for opt, value in optdict.items():
        if not ignore or opt not in ignore:
            opts.append(""-%s"" % opt)
            if value is not None:
                opts.append(_format_optvalue(value, script))

    return _flatten(opts)","def format_options_and_call_tk_command(tk, options, *args):
    """"""Format options then call Tk command with args and options and return
    the appropriate result.

    If no option is specified, a dict is returned. If an option is
    specified with the None value, the value for that option is returned.
    Otherwise, the function just sets the passed options and the caller
    shouldn't be expecting a return value anyway.""""""
    options = format_option_dict_to_tuple_for_tk_call(options)
    res = tk.call(*(args + options))

    if len(options) % 2: # option specified without a value, return its value
        return res

    return format_dict_from_tcl_list_pairs(tk, res, conv=_tclobj_to_py)","{""_splitdict"": ""format_dict_from_tcl_list_pairs"", ""_format_optdict"": ""format_option_dict_to_tuple_for_tk_call""}","def 0(tk, v, cut_minus=True, conv=None):
    """"""Return a properly formatted dict built from Tcl list pairs.

    If cut_minus is True, the supposed '-' prefix will be removed from
    keys. If conv is specified, it is used to convert values.

    Tcl list is expected to contain an even number of elements.
    """"""
    t = tk.splitlist(v)
    if len(t) % 2:
        raise RuntimeError('Tcl list representing a dict is expected '
                           'to contain an even number of elements')
    it = iter(t)
    dict = {}
    for key, value in zip(it, it):
        key = str(key)
        if cut_minus and key[0] == '-':
            key = key[1:]
        if conv:
            value = conv(value)
        dict[key] = value
    return dict

def 1(optdict, script=False, ignore=None):
    """"""Formats optdict to a tuple to pass it to tk.call.

    E.g. (script=False):
      {'foreground': 'blue', 'padding': [1, 2, 3, 4]} returns:
      ('-foreground', 'blue', '-padding', '1 2 3 4')""""""

    opts = []
    for opt, value in optdict.items():
        if not ignore or opt not in ignore:
            opts.append(""-%s"" % opt)
            if value is not None:
                opts.append(_format_optvalue(value, script))

    return _flatten(opts)","def 2(tk, options, *args):
    """"""Format options then call Tk command with args and options and return
    the appropriate result.

    If no option is specified, a dict is returned. If an option is
    specified with the None value, the value for that option is returned.
    Otherwise, the function just sets the passed options and the caller
    shouldn't be expecting a return value anyway.""""""
    options = 1(options)
    res = tk.call(*(args + options))

    if len(options) % 2: # option specified without a value, return its value
        return res

    return 0(tk, res, conv=_tclobj_to_py)","{""_splitdict"": ""0"", ""_format_optdict"": ""1""}"
531,531,"def _convert_stringval(value):
    """"""Converts a value to, hopefully, a more appropriate Python object.""""""
    value = str(value)
    try:
        value = int(value)
    except (ValueError, TypeError):
        pass

    return value","def str(val):
    """"""Convert float to string, taking the locale into account.""""""
    return _format(""%.12g"", val)","def locale_aware_float_to_string(val):
    """"""Convert float to locale_aware_float_to_stringing, taking the locale into account.""""""
    return _format(""%.12g"", val)","def _convert_locale_aware_float_to_stringingval(value):
    """"""Converts a value to, hopefully, a more appropriate Python object.""""""
    value = locale_aware_float_to_string(value)
    try:
        value = int(value)
    except (ValueError, TypeError):
        pass

    return value","{""str"": ""locale_aware_float_to_string""}","def 0(val):
    """"""Convert float to string, taking the locale into account.""""""
    return _format(""%.12g"", val)","def 1(value):
    """"""Converts a value to, hopefully, a more appropriate Python object.""""""
    value = 0(value)
    try:
        value = int(value)
    except (ValueError, TypeError):
        pass

    return value","{""str"": ""0""}"
532,532,"def _tclobj_to_py(val):
    """"""Return value converted from Tcl object to Python object.""""""
    if val and hasattr(val, '__len__') and not isinstance(val, str):
        if getattr(val[0], 'typename', None) == 'StateSpec':
            val = _list_from_statespec(val)
        else:
            val = list(map(_convert_stringval, val))

    elif hasattr(val, 'typename'): # some other (single) Tcl object
        val = _convert_stringval(val)

    return val","def _convert_stringval(value):
    """"""Converts a value to, hopefully, a more appropriate Python object.""""""
    value = str(value)
    try:
        value = int(value)
    except (ValueError, TypeError):
        pass

    return value

def _list_from_statespec(stuple):
    """"""Construct a list from the given statespec tuple according to the
    accepted statespec accepted by _format_mapdict.""""""
    if isinstance(stuple, str):
        return stuple
    result = []
    it = iter(stuple)
    for state, val in zip(it, it):
        if hasattr(state, 'typename'):  # this is a Tcl object
            state = str(state).split()
        elif isinstance(state, str):
            state = state.split()
        elif not isinstance(state, (tuple, list)):
            state = (state,)
        if hasattr(val, 'typename'):
            val = str(val)
        result.append((*state, val))

    return result","def convert_value_to_more_appropriate_python_object(value):
    """"""Converts a value to, hopefully, a more appropriate Python object.""""""
    value = str(value)
    try:
        value = int(value)
    except (ValueError, TypeError):
        pass

    return value

def construct_list_from_state_spec_tuple(stuple):
    """"""Construct a list from the given statespec tuple according to the
    accepted statespec accepted by _format_mapdict.""""""
    if isinstance(stuple, str):
        return stuple
    result = []
    it = iter(stuple)
    for state, val in zip(it, it):
        if hasattr(state, 'typename'):  # this is a Tcl object
            state = str(state).split()
        elif isinstance(state, str):
            state = state.split()
        elif not isinstance(state, (tuple, list)):
            state = (state,)
        if hasattr(val, 'typename'):
            val = str(val)
        result.append((*state, val))

    return result","def convert_tcl_object_to_python_object(val):
    """"""Return value converted from Tcl object to Python object.""""""
    if val and hasattr(val, '__len__') and not isinstance(val, str):
        if getattr(val[0], 'typename', None) == 'StateSpec':
            val = construct_list_from_state_spec_tuple(val)
        else:
            val = list(map(convert_value_to_more_appropriate_python_object, val))

    elif hasattr(val, 'typename'): # some other (single) Tcl object
        val = convert_value_to_more_appropriate_python_object(val)

    return val","{""_convert_stringval"": ""convert_value_to_more_appropriate_python_object"", ""_list_from_statespec"": ""construct_list_from_state_spec_tuple""}","def 0(value):
    """"""Converts a value to, hopefully, a more appropriate Python object.""""""
    value = str(value)
    try:
        value = int(value)
    except (ValueError, TypeError):
        pass

    return value

def 1(stuple):
    """"""Construct a list from the given statespec tuple according to the
    accepted statespec accepted by _format_mapdict.""""""
    if isinstance(stuple, str):
        return stuple
    result = []
    it = iter(stuple)
    for state, val in zip(it, it):
        if hasattr(state, 'typename'):  # this is a Tcl object
            state = str(state).split()
        elif isinstance(state, str):
            state = state.split()
        elif not isinstance(state, (tuple, list)):
            state = (state,)
        if hasattr(val, 'typename'):
            val = str(val)
        result.append((*state, val))

    return result","def 2(val):
    """"""Return value converted from Tcl object to Python object.""""""
    if val and hasattr(val, '__len__') and not isinstance(val, str):
        if getattr(val[0], 'typename', None) == 'StateSpec':
            val = 1(val)
        else:
            val = list(map(_convert_stringval, val))

    elif hasattr(val, 'typename'): # some other (single) Tcl object
        val = 0(val)

    return val","{""_convert_stringval"": ""0"", ""_list_from_statespec"": ""1""}"
533,533,"def tclobjs_to_py(adict):
    """"""Returns adict with its values converted from Tcl objects to Python
    objects.""""""
    for opt, val in adict.items():
        adict[opt] = _tclobj_to_py(val)

    return adict","def _tclobj_to_py(val):
    """"""Return value converted from Tcl object to Python object.""""""
    if val and hasattr(val, '__len__') and not isinstance(val, str):
        if getattr(val[0], 'typename', None) == 'StateSpec':
            val = _list_from_statespec(val)
        else:
            val = list(map(_convert_stringval, val))

    elif hasattr(val, 'typename'): # some other (single) Tcl object
        val = _convert_stringval(val)

    return val","def convert_tcl_object_to_python_object(val):
    """"""Return value converted from Tcl object to Python object.""""""
    if val and hasattr(val, '__len__') and not isinstance(val, str):
        if getattr(val[0], 'typename', None) == 'StateSpec':
            val = _list_from_statespec(val)
        else:
            val = list(map(_convert_stringval, val))

    elif hasattr(val, 'typename'): # some other (single) Tcl object
        val = _convert_stringval(val)

    return val","def convert_tcl_objects_to_python(adict):
    """"""Returns adict with its values converted from Tcl objects to Python
    objects.""""""
    for opt, val in adict.items():
        adict[opt] = convert_tcl_object_to_python_object(val)

    return adict","{""_tclobj_to_py"": ""convert_tcl_object_to_python_object""}","def 0(val):
    """"""Return value converted from Tcl object to Python object.""""""
    if val and hasattr(val, '__len__') and not isinstance(val, str):
        if getattr(val[0], 'typename', None) == 'StateSpec':
            val = _list_from_statespec(val)
        else:
            val = list(map(_convert_stringval, val))

    elif hasattr(val, 'typename'): # some other (single) Tcl object
        val = _convert_stringval(val)

    return val","def 1(adict):
    """"""Returns adict with its values converted from Tcl objects to Python
    objects.""""""
    for opt, val in adict.items():
        adict[opt] = 0(val)

    return adict","{""_tclobj_to_py"": ""0""}"
534,534,"def unescape(data, entities={}):
    """"""Unescape &amp;, &lt;, and &gt; in a string of data.

    You can unescape other strings of data by passing a dictionary as
    the optional entities parameter.  The keys and values must all be
    strings; each key will be replaced with its corresponding value.
    """"""
    data = data.replace(""&lt;"", ""<"")
    data = data.replace(""&gt;"", "">"")
    if entities:
        data = __dict_replace(data, entities)
    # must do ampersand last
    return data.replace(""&amp;"", ""&"")","def __dict_replace(s, d):
    """"""Replace substrings of a string using a dictionary.""""""
    for key, value in d.items():
        s = s.replace(key, value)
    return s","def replace_substrings_using_dictionary(s, d):
    """"""Replace substrings of a string using a dictionary.""""""
    for key, value in d.items():
        s = s.replace(key, value)
    return s","def unescape_string_with_entities_dictionary(data, entities={}):
    """"""Unescape &amp;, &lt;, and &gt; in a string of data.

    You can unescape_string_with_entities_dictionary other strings of data by passing a dictionary as
    the optional entities parameter.  The keys and values must all be
    strings; each key will be replaced with its corresponding value.
    """"""
    data = data.replace(""&lt;"", ""<"")
    data = data.replace(""&gt;"", "">"")
    if entities:
        data = replace_substrings_using_dictionary(data, entities)
    # must do ampersand last
    return data.replace(""&amp;"", ""&"")","{""__dict_replace"": ""replace_substrings_using_dictionary""}","def 0(s, d):
    """"""Replace substrings of a string using a dictionary.""""""
    for key, value in d.items():
        s = s.replace(key, value)
    return s","def 1(data, entities={}):
    """"""Unescape &amp;, &lt;, and &gt; in a string of data.

    You can unescape other strings of data by passing a dictionary as
    the optional entities parameter.  The keys and values must all be
    strings; each key will be replaced with its corresponding value.
    """"""
    data = data.replace(""&lt;"", ""<"")
    data = data.replace(""&gt;"", "">"")
    if entities:
        data = 0(data, entities)
    # must do ampersand last
    return data.replace(""&amp;"", ""&"")","{""__dict_replace"": ""0""}"
535,535,"def parseString(string, parser=None):
    """"""Parse a file into a DOM from a string.""""""
    if parser is None:
        from xml.dom import expatbuilder
        return expatbuilder.parseString(string)
    else:
        from xml.dom import pulldom
        return _do_pulldom_parse(pulldom.parseString, (string,),
                                 {'parser': parser})","def _do_pulldom_parse(func, args, kwargs):
    events = func(*args, **kwargs)
    toktype, rootNode = events.getEvent()
    events.expandNode(rootNode)
    events.clear()
    return rootNode","def perform_pulldom_parse_and_get_root_node(func, args, kwargs):
    events = func(*args, **kwargs)
    toktype, rootNode = events.getEvent()
    events.expandNode(rootNode)
    events.clear()
    return rootNode","def parse_string_into_dom_from_string(string, parser=None):
    """"""Parse a file into a DOM from a string.""""""
    if parser is None:
        from xml.dom import expatbuilder
        return expatbuilder.parse_string_into_dom_from_string(string)
    else:
        from xml.dom import pulldom
        return perform_pulldom_parse_and_get_root_node(pulldom.parse_string_into_dom_from_string, (string,),
                                 {'parser': parser})","{""_do_pulldom_parse"": ""perform_pulldom_parse_and_get_root_node""}","def 0(func, args, kwargs):
    events = func(*args, **kwargs)
    toktype, rootNode = events.getEvent()
    events.expandNode(rootNode)
    events.clear()
    return rootNode","def 1(string, parser=None):
    """"""Parse a file into a DOM from a string.""""""
    if parser is None:
        from xml.dom import expatbuilder
        return expatbuilder.1(string)
    else:
        from xml.dom import pulldom
        return 0(pulldom.parseString, (string,),
                                 {'parser': parser})","{""_do_pulldom_parse"": ""0""}"
536,536,"def parse_headers(fp, _class=HTTPMessage):
    """"""Parses only RFC2822 headers from a file pointer.

    email Parser wants to see strings rather than bytes.
    But a TextIOWrapper around self.rfile would buffer too many bytes
    from the stream, bytes which we later need to read as bytes.
    So we read the correct bytes here, as bytes, for email Parser
    to parse.

    """"""
    headers = _read_headers(fp)
    hstring = b''.join(headers).decode('iso-8859-1')
    return email.parser.Parser(_class=_class).parsestr(hstring)","def _read_headers(fp):
    """"""Reads potential header lines into a list from a file pointer.

    Length of line is limited by _MAXLINE, and number of
    headers is limited by _MAXHEADERS.
    """"""
    headers = []
    while True:
        line = fp.readline(_MAXLINE + 1)
        if len(line) > _MAXLINE:
            raise LineTooLong(""header line"")
        headers.append(line)
        if len(headers) > _MAXHEADERS:
            raise HTTPException(""got more than %d headers"" % _MAXHEADERS)
        if line in (b'\r\n', b'\n', b''):
            break
    return headers","def read_headers_into_list_from_file_pointer(fp):
    """"""Reads potential header lines into a list from a file pointer.

    Length of line is limited by _MAXLINE, and number of
    headers is limited by _MAXHEADERS.
    """"""
    headers = []
    while True:
        line = fp.readline(_MAXLINE + 1)
        if len(line) > _MAXLINE:
            raise LineTooLong(""header line"")
        headers.append(line)
        if len(headers) > _MAXHEADERS:
            raise HTTPException(""got more than %d headers"" % _MAXHEADERS)
        if line in (b'\r\n', b'\n', b''):
            break
    return headers","def parse_rfc2822_headers_from_file_pointer(fp, _class=HTTPMessage):
    """"""Parses only RFC2822 headers from a file pointer.

    email Parser wants to see strings rather than bytes.
    But a TextIOWrapper around self.rfile would buffer too many bytes
    from the stream, bytes which we later need to read as bytes.
    So we read the correct bytes here, as bytes, for email Parser
    to parse.

    """"""
    headers = read_headers_into_list_from_file_pointer(fp)
    hstring = b''.join(headers).decode('iso-8859-1')
    return email.parser.Parser(_class=_class).parsestr(hstring)","{""_read_headers"": ""read_headers_into_list_from_file_pointer""}","def 0(fp):
    """"""Reads potential header lines into a list from a file pointer.

    Length of line is limited by _MAXLINE, and number of
    headers is limited by _MAXHEADERS.
    """"""
    headers = []
    while True:
        line = fp.readline(_MAXLINE + 1)
        if len(line) > _MAXLINE:
            raise LineTooLong(""header line"")
        headers.append(line)
        if len(headers) > _MAXHEADERS:
            raise HTTPException(""got more than %d headers"" % _MAXHEADERS)
        if line in (b'\r\n', b'\n', b''):
            break
    return headers","def 1(fp, _class=HTTPMessage):
    """"""Parses only RFC2822 headers from a file pointer.

    email Parser wants to see strings rather than bytes.
    But a TextIOWrapper around self.rfile would buffer too many bytes
    from the stream, bytes which we later need to read as bytes.
    So we read the correct bytes here, as bytes, for email Parser
    to parse.

    """"""
    headers = 0(fp)
    hstring = b''.join(headers).decode('iso-8859-1')
    return email.parser.Parser(_class=_class).parsestr(hstring)","{""_read_headers"": ""0""}"
537,537,"def iso2time(text):
    """"""
    As for http2time, but parses the ISO 8601 formats:

    1994-02-03 14:15:29 -0100    -- ISO 8601 format
    1994-02-03 14:15:29          -- zone is optional
    1994-02-03                   -- only date
    1994-02-03T14:15:29          -- Use T as separator
    19940203T141529Z             -- ISO 8601 compact format
    19940203                     -- only date

    """"""
    # clean up
    text = text.lstrip()

    # tz is time zone specifier string
    day, mon, yr, hr, min, sec, tz = [None]*7

    # loose regexp parse
    m = ISO_DATE_RE.search(text)
    if m is not None:
        # XXX there's an extra bit of the timezone I'm ignoring here: is
        #   this the right thing to do?
        yr, mon, day, hr, min, sec, tz, _ = m.groups()
    else:
        return None  # bad format

    return _str2time(day, mon, yr, hr, min, sec, tz)","def _str2time(day, mon, yr, hr, min, sec, tz):
    yr = int(yr)
    if yr > datetime.MAXYEAR:
        return None

    # translate month name to number
    # month numbers start with 1 (January)
    try:
        mon = MONTHS_LOWER.index(mon.lower())+1
    except ValueError:
        # maybe it's already a number
        try:
            imon = int(mon)
        except ValueError:
            return None
        if 1 <= imon <= 12:
            mon = imon
        else:
            return None

    # make sure clock elements are defined
    if hr is None: hr = 0
    if min is None: min = 0
    if sec is None: sec = 0

    day = int(day)
    hr = int(hr)
    min = int(min)
    sec = int(sec)

    if yr < 1000:
        # find ""obvious"" year
        cur_yr = time.localtime(time.time())[0]
        m = cur_yr % 100
        tmp = yr
        yr = yr + cur_yr - m
        m = m - tmp
        if abs(m) > 50:
            if m > 0: yr = yr + 100
            else: yr = yr - 100

    # convert UTC time tuple to seconds since epoch (not timezone-adjusted)
    t = _timegm((yr, mon, day, hr, min, sec, tz))

    if t is not None:
        # adjust time using timezone string, to get absolute time since epoch
        if tz is None:
            tz = ""UTC""
        tz = tz.upper()
        offset = offset_from_tz_string(tz)
        if offset is None:
            return None
        t = t - offset

    return t","def convert_string_to_time(day, mon, yr, hr, min, sec, tz):
    yr = int(yr)
    if yr > datetime.MAXYEAR:
        return None

    # translate month name to number
    # month numbers start with 1 (January)
    try:
        mon = MONTHS_LOWER.index(mon.lower())+1
    except ValueError:
        # maybe it's already a number
        try:
            imon = int(mon)
        except ValueError:
            return None
        if 1 <= imon <= 12:
            mon = imon
        else:
            return None

    # make sure clock elements are defined
    if hr is None: hr = 0
    if min is None: min = 0
    if sec is None: sec = 0

    day = int(day)
    hr = int(hr)
    min = int(min)
    sec = int(sec)

    if yr < 1000:
        # find ""obvious"" year
        cur_yr = time.localtime(time.time())[0]
        m = cur_yr % 100
        tmp = yr
        yr = yr + cur_yr - m
        m = m - tmp
        if abs(m) > 50:
            if m > 0: yr = yr + 100
            else: yr = yr - 100

    # convert UTC time tuple to seconds since epoch (not timezone-adjusted)
    t = _timegm((yr, mon, day, hr, min, sec, tz))

    if t is not None:
        # adjust time using timezone string, to get absolute time since epoch
        if tz is None:
            tz = ""UTC""
        tz = tz.upper()
        offset = offset_from_tz_string(tz)
        if offset is None:
            return None
        t = t - offset

    return t","def parse_iso8601_time_from_text_and_return(text):
    """"""
    As for http2time, but parses the ISO 8601 formats:

    1994-02-03 14:15:29 -0100    -- ISO 8601 format
    1994-02-03 14:15:29          -- zone is optional
    1994-02-03                   -- only date
    1994-02-03T14:15:29          -- Use T as separator
    19940203T141529Z             -- ISO 8601 compact format
    19940203                     -- only date

    """"""
    # clean up
    text = text.lstrip()

    # tz is time zone specifier string
    day, mon, yr, hr, min, sec, tz = [None]*7

    # loose regexp parse
    m = ISO_DATE_RE.search(text)
    if m is not None:
        # XXX there's an extra bit of the timezone I'm ignoring here: is
        #   this the right thing to do?
        yr, mon, day, hr, min, sec, tz, _ = m.groups()
    else:
        return None  # bad format

    return convert_string_to_time(day, mon, yr, hr, min, sec, tz)","{""_str2time"": ""convert_string_to_time""}","def 0(day, mon, yr, hr, min, sec, tz):
    yr = int(yr)
    if yr > datetime.MAXYEAR:
        return None

    # translate month name to number
    # month numbers start with 1 (January)
    try:
        mon = MONTHS_LOWER.index(mon.lower())+1
    except ValueError:
        # maybe it's already a number
        try:
            imon = int(mon)
        except ValueError:
            return None
        if 1 <= imon <= 12:
            mon = imon
        else:
            return None

    # make sure clock elements are defined
    if hr is None: hr = 0
    if min is None: min = 0
    if sec is None: sec = 0

    day = int(day)
    hr = int(hr)
    min = int(min)
    sec = int(sec)

    if yr < 1000:
        # find ""obvious"" year
        cur_yr = time.localtime(time.time())[0]
        m = cur_yr % 100
        tmp = yr
        yr = yr + cur_yr - m
        m = m - tmp
        if abs(m) > 50:
            if m > 0: yr = yr + 100
            else: yr = yr - 100

    # convert UTC time tuple to seconds since epoch (not timezone-adjusted)
    t = _timegm((yr, mon, day, hr, min, sec, tz))

    if t is not None:
        # adjust time using timezone string, to get absolute time since epoch
        if tz is None:
            tz = ""UTC""
        tz = tz.upper()
        offset = offset_from_tz_string(tz)
        if offset is None:
            return None
        t = t - offset

    return t","def 1(text):
    """"""
    As for http2time, but parses the ISO 8601 formats:

    1994-02-03 14:15:29 -0100    -- ISO 8601 format
    1994-02-03 14:15:29          -- zone is optional
    1994-02-03                   -- only date
    1994-02-03T14:15:29          -- Use T as separator
    19940203T141529Z             -- ISO 8601 compact format
    19940203                     -- only date

    """"""
    # clean up
    text = text.lstrip()

    # tz is time zone specifier string
    day, mon, yr, hr, min, sec, tz = [None]*7

    # loose regexp parse
    m = ISO_DATE_RE.search(text)
    if m is not None:
        # XXX there's an extra bit of the timezone I'm ignoring here: is
        #   this the right thing to do?
        yr, mon, day, hr, min, sec, tz, _ = m.groups()
    else:
        return None  # bad format

    return 0(day, mon, yr, hr, min, sec, tz)","{""_str2time"": ""0""}"
538,538,"def user_domain_match(A, B):
    """"""For blocking/accepting domains.

    A and B may be host domain names or IP addresses.

    """"""
    A = A.lower()
    B = B.lower()
    if not (liberal_is_HDN(A) and liberal_is_HDN(B)):
        if A == B:
            # equal IP addresses
            return True
        return False
    initial_dot = B.startswith(""."")
    if initial_dot and A.endswith(B):
        return True
    if not initial_dot and A == B:
        return True
    return False","def liberal_is_HDN(text):
    """"""Return True if text is a sort-of-like a host domain name.

    For accepting/blocking domains.

    """"""
    if IPV4_RE.search(text):
        return False
    return True","def is_host_domain_name(text):
    """"""Return True if text is a sort-of-like a host domain name.

    For accepting/blocking domains.

    """"""
    if IPV4_RE.search(text):
        return False
    return True","def check_blocking_accepting_domains_for_match(A, B):
    """"""For blocking/accepting domains.

    A and B may be host domain names or IP addresses.

    """"""
    A = A.lower()
    B = B.lower()
    if not (is_host_domain_name(A) and is_host_domain_name(B)):
        if A == B:
            # equal IP addresses
            return True
        return False
    initial_dot = B.startswith(""."")
    if initial_dot and A.endswith(B):
        return True
    if not initial_dot and A == B:
        return True
    return False","{""liberal_is_HDN"": ""is_host_domain_name""}","def 0(text):
    """"""Return True if text is a sort-of-like a host domain name.

    For accepting/blocking domains.

    """"""
    if IPV4_RE.search(text):
        return False
    return True","def 1(A, B):
    """"""For blocking/accepting domains.

    A and B may be host domain names or IP addresses.

    """"""
    A = A.lower()
    B = B.lower()
    if not (0(A) and 0(B)):
        if A == B:
            # equal IP addresses
            return True
        return False
    initial_dot = B.startswith(""."")
    if initial_dot and A.endswith(B):
        return True
    if not initial_dot and A == B:
        return True
    return False","{""liberal_is_HDN"": ""0""}"
539,539,"def request_path(request):
    """"""Path component of request-URI, as defined by RFC 2965.""""""
    url = request.get_full_url()
    parts = urllib.parse.urlsplit(url)
    path = escape_path(parts.path)
    if not path.startswith(""/""):
        # fix bad RFC 2396 absoluteURI
        path = ""/"" + path
    return path","def escape_path(path):
    """"""Escape any invalid characters in HTTP URL, and uppercase all escapes.""""""
    # There's no knowing what character encoding was used to create URLs
    # containing %-escapes, but since we have to pick one to escape invalid
    # path characters, we pick UTF-8, as recommended in the HTML 4.0
    # specification:
    # http://www.w3.org/TR/REC-html40/appendix/notes.html#h-B.2.1
    # And here, kind of: draft-fielding-uri-rfc2396bis-03
    # (And in draft IRI specification: draft-duerst-iri-05)
    # (And here, for new URI schemes: RFC 2718)
    path = urllib.parse.quote(path, HTTP_PATH_SAFE)
    path = ESCAPED_CHAR_RE.sub(uppercase_escaped_char, path)
    return path","def escape_invalid_path_characters(path):
    """"""Escape any invalid characters in HTTP URL, and uppercase all escapes.""""""
    # There's no knowing what character encoding was used to create URLs
    # containing %-escapes, but since we have to pick one to escape invalid
    # path characters, we pick UTF-8, as recommended in the HTML 4.0
    # specification:
    # http://www.w3.org/TR/REC-html40/appendix/notes.html#h-B.2.1
    # And here, kind of: draft-fielding-uri-rfc2396bis-03
    # (And in draft IRI specification: draft-duerst-iri-05)
    # (And here, for new URI schemes: RFC 2718)
    path = urllib.parse.quote(path, HTTP_PATH_SAFE)
    path = ESCAPED_CHAR_RE.sub(uppercase_escaped_char, path)
    return path","def get_path_component_of_request_uri_as_string(request):
    """"""Path component of request-URI, as defined by RFC 2965.""""""
    url = request.get_full_url()
    parts = urllib.parse.urlsplit(url)
    path = escape_invalid_path_characters(parts.path)
    if not path.startswith(""/""):
        # fix bad RFC 2396 absoluteURI
        path = ""/"" + path
    return path","{""escape_path"": ""escape_invalid_path_characters""}","def 0(path):
    """"""Escape any invalid characters in HTTP URL, and uppercase all escapes.""""""
    # There's no knowing what character encoding was used to create URLs
    # containing %-escapes, but since we have to pick one to escape invalid
    # path characters, we pick UTF-8, as recommended in the HTML 4.0
    # specification:
    # http://www.w3.org/TR/REC-html40/appendix/notes.html#h-B.2.1
    # And here, kind of: draft-fielding-uri-rfc2396bis-03
    # (And in draft IRI specification: draft-duerst-iri-05)
    # (And here, for new URI schemes: RFC 2718)
    path = urllib.parse.quote(path, HTTP_PATH_SAFE)
    path = ESCAPED_CHAR_RE.sub(uppercase_escaped_char, path)
    return path","def 1(request):
    """"""Path component of request-URI, as defined by RFC 2965.""""""
    url = request.get_full_url()
    parts = urllib.parse.urlsplit(url)
    path = 0(parts.path)
    if not path.startswith(""/""):
        # fix bad RFC 2396 absoluteURI
        path = ""/"" + path
    return path","{""escape_path"": ""0""}"
540,540,"def reach(h):
    """"""Return reach of host h, as defined by RFC 2965, section 1.

    The reach R of a host name H is defined as follows:

       *  If

          -  H is the host domain name of a host; and,

          -  H has the form A.B; and

          -  A has no embedded (that is, interior) dots; and

          -  B has at least one embedded dot, or B is the string ""local"".
             then the reach of H is .B.

       *  Otherwise, the reach of H is H.

    >>> reach(""www.acme.com"")
    '.acme.com'
    >>> reach(""acme.com"")
    'acme.com'
    >>> reach(""acme.local"")
    '.local'

    """"""
    i = h.find(""."")
    if i >= 0:
        #a = h[:i]  # this line is only here to show what a is
        b = h[i+1:]
        i = b.find(""."")
        if is_HDN(h) and (i >= 0 or b == ""local""):
            return "".""+b
    return h","def is_HDN(text):
    """"""Return True if text is a host domain name.""""""
    # XXX
    # This may well be wrong.  Which RFC is HDN defined in, if any (for
    #  the purposes of RFC 2965)?
    # For the current implementation, what about IPv6?  Remember to look
    #  at other uses of IPV4_RE also, if change this.
    if IPV4_RE.search(text):
        return False
    if text == """":
        return False
    if text[0] == ""."" or text[-1] == ""."":
        return False
    return True","def is_host_domain_name(text):
    """"""Return True if text is a host domain name.""""""
    # XXX
    # This may well be wrong.  Which RFC is HDN defined in, if any (for
    #  the purposes of RFC 2965)?
    # For the current implementation, what about IPv6?  Remember to look
    #  at other uses of IPV4_RE also, if change this.
    if IPV4_RE.search(text):
        return False
    if text == """":
        return False
    if text[0] == ""."" or text[-1] == ""."":
        return False
    return True","def return_reach_of_host_as_string(h):
    """"""Return return_reach_of_host_as_string of host h, as defined by RFC 2965, section 1.

    The return_reach_of_host_as_string R of a host name H is defined as follows:

       *  If

          -  H is the host domain name of a host; and,

          -  H has the form A.B; and

          -  A has no embedded (that is, interior) dots; and

          -  B has at least one embedded dot, or B is the string ""local"".
             then the return_reach_of_host_as_string of H is .B.

       *  Otherwise, the return_reach_of_host_as_string of H is H.

    >>> return_reach_of_host_as_string(""www.acme.com"")
    '.acme.com'
    >>> return_reach_of_host_as_string(""acme.com"")
    'acme.com'
    >>> return_reach_of_host_as_string(""acme.local"")
    '.local'

    """"""
    i = h.find(""."")
    if i >= 0:
        #a = h[:i]  # this line is only here to show what a is
        b = h[i+1:]
        i = b.find(""."")
        if is_host_domain_name(h) and (i >= 0 or b == ""local""):
            return "".""+b
    return h","{""is_HDN"": ""is_host_domain_name""}","def 0(text):
    """"""Return True if text is a host domain name.""""""
    # XXX
    # This may well be wrong.  Which RFC is HDN defined in, if any (for
    #  the purposes of RFC 2965)?
    # For the current implementation, what about IPv6?  Remember to look
    #  at other uses of IPV4_RE also, if change this.
    if IPV4_RE.search(text):
        return False
    if text == """":
        return False
    if text[0] == ""."" or text[-1] == ""."":
        return False
    return True","def 1(h):
    """"""Return reach of host h, as defined by RFC 2965, section 1.

    The reach R of a host name H is defined as follows:

       *  If

          -  H is the host domain name of a host; and,

          -  H has the form A.B; and

          -  A has no embedded (that is, interior) dots; and

          -  B has at least one embedded dot, or B is the string ""local"".
             then the reach of H is .B.

       *  Otherwise, the reach of H is H.

    >>> 1(""www.acme.com"")
    '.acme.com'
    >>> 1(""acme.com"")
    'acme.com'
    >>> 1(""acme.local"")
    '.local'

    """"""
    i = h.find(""."")
    if i >= 0:
        #a = h[:i]  # this line is only here to show what a is
        b = h[i+1:]
        i = b.find(""."")
        if 0(h) and (i >= 0 or b == ""local""):
            return "".""+b
    return h","{""is_HDN"": ""0""}"
541,541,"def deepvalues(mapping):
    """"""Iterates over nested mapping, depth-first, in sorted order by key.""""""
    values = vals_sorted_by_key(mapping)
    for obj in values:
        mapping = False
        try:
            obj.items
        except AttributeError:
            pass
        else:
            mapping = True
            yield from deepvalues(obj)
        if not mapping:
            yield obj","def vals_sorted_by_key(adict):
    keys = sorted(adict.keys())
    return map(adict.get, keys)

def deepvalues(mapping):
    """"""Iterates over nested mapping, depth-first, in sorted order by key.""""""
    values = vals_sorted_by_key(mapping)
    for obj in values:
        mapping = False
        try:
            obj.items
        except AttributeError:
            pass
        else:
            mapping = True
            yield from deepvalues(obj)
        if not mapping:
            yield obj","def values_sorted_by_key(adict):
    keys = sorted(adict.keys())
    return map(adict.get, keys)

def iterate_over_nested_mapping_sorted(mapping):
    """"""Iterates over nested mapping, depth-first, in sorted order by key.""""""
    values = values_sorted_by_key(mapping)
    for obj in values:
        mapping = False
        try:
            obj.items
        except AttributeError:
            pass
        else:
            mapping = True
            yield from iterate_over_nested_mapping_sorted(obj)
        if not mapping:
            yield obj","def iterate_over_nested_mapping_sorted(mapping):
    """"""Iterates over nested mapping, depth-first, in sorted order by key.""""""
    values = values_sorted_by_key(mapping)
    for obj in values:
        mapping = False
        try:
            obj.items
        except AttributeError:
            pass
        else:
            mapping = True
            yield from iterate_over_nested_mapping_sorted(obj)
        if not mapping:
            yield obj","{""vals_sorted_by_key"": ""values_sorted_by_key"", ""deepvalues"": ""iterate_over_nested_mapping_sorted""}","def 0(adict):
    keys = sorted(adict.keys())
    return map(adict.get, keys)

def 1(mapping):
    """"""Iterates over nested mapping, depth-first, in sorted order by key.""""""
    values = 0(mapping)
    for obj in values:
        mapping = False
        try:
            obj.items
        except AttributeError:
            pass
        else:
            mapping = True
            yield from 1(obj)
        if not mapping:
            yield obj","def 1(mapping):
    """"""Iterates over nested mapping, depth-first, in sorted order by key.""""""
    values = 0(mapping)
    for obj in values:
        mapping = False
        try:
            obj.items
        except AttributeError:
            pass
        else:
            mapping = True
            yield from 1(obj)
        if not mapping:
            yield obj","{""vals_sorted_by_key"": ""0"", ""deepvalues"": ""1""}"
542,542,"def bootstrap(*, root=None, upgrade=False, user=False,
              altinstall=False, default_pip=False,
              verbosity=0):
    """"""
    Bootstrap pip into the current Python installation (or the given root
    directory).

    Note that calling this function will alter both sys.path and os.environ.
    """"""
    # Discard the return value
    _bootstrap(root=root, upgrade=upgrade, user=user,
               altinstall=altinstall, default_pip=default_pip,
               verbosity=verbosity)","def _bootstrap(*, root=None, upgrade=False, user=False,
              altinstall=False, default_pip=False,
              verbosity=0):
    """"""
    Bootstrap pip into the current Python installation (or the given root
    directory). Returns pip command status code.

    Note that calling this function will alter both sys.path and os.environ.
    """"""
    if altinstall and default_pip:
        raise ValueError(""Cannot use altinstall and default_pip together"")

    sys.audit(""ensurepip.bootstrap"", root)

    _disable_pip_configuration_settings()

    # By default, installing pip and setuptools installs all of the
    # following scripts (X.Y == running Python version):
    #
    #   pip, pipX, pipX.Y, easy_install, easy_install-X.Y
    #
    # pip 1.5+ allows ensurepip to request that some of those be left out
    if altinstall:
        # omit pip, pipX and easy_install
        os.environ[""ENSUREPIP_OPTIONS""] = ""altinstall""
    elif not default_pip:
        # omit pip and easy_install
        os.environ[""ENSUREPIP_OPTIONS""] = ""install""

    with tempfile.TemporaryDirectory() as tmpdir:
        # Put our bundled wheels into a temporary directory and construct the
        # additional paths that need added to sys.path
        additional_paths = []
        for name, package in _get_packages().items():
            if package.wheel_name:
                # Use bundled wheel package
                from ensurepip import _bundled
                wheel_name = package.wheel_name
                whl = resources.read_binary(_bundled, wheel_name)
            else:
                # Use the wheel package directory
                with open(package.wheel_path, ""rb"") as fp:
                    whl = fp.read()
                wheel_name = os.path.basename(package.wheel_path)

            filename = os.path.join(tmpdir, wheel_name)
            with open(filename, ""wb"") as fp:
                fp.write(whl)

            additional_paths.append(filename)

        # Construct the arguments to be passed to the pip command
        args = [""install"", ""--no-cache-dir"", ""--no-index"", ""--find-links"", tmpdir]
        if root:
            args += [""--root"", root]
        if upgrade:
            args += [""--upgrade""]
        if user:
            args += [""--user""]
        if verbosity:
            args += [""-"" + ""v"" * verbosity]

        return _run_pip([*args, *_PACKAGE_NAMES], additional_paths)","def bootstrap_pip_into_python(*, root=None, upgrade=False, user=False,
              altinstall=False, default_pip=False,
              verbosity=0):
    """"""
    Bootstrap pip into the current Python installation (or the given root
    directory). Returns pip command status code.

    Note that calling this function will alter both sys.path and os.environ.
    """"""
    if altinstall and default_pip:
        raise ValueError(""Cannot use altinstall and default_pip together"")

    sys.audit(""ensurepip.bootstrap"", root)

    _disable_pip_configuration_settings()

    # By default, installing pip and setuptools installs all of the
    # following scripts (X.Y == running Python version):
    #
    #   pip, pipX, pipX.Y, easy_install, easy_install-X.Y
    #
    # pip 1.5+ allows ensurepip to request that some of those be left out
    if altinstall:
        # omit pip, pipX and easy_install
        os.environ[""ENSUREPIP_OPTIONS""] = ""altinstall""
    elif not default_pip:
        # omit pip and easy_install
        os.environ[""ENSUREPIP_OPTIONS""] = ""install""

    with tempfile.TemporaryDirectory() as tmpdir:
        # Put our bundled wheels into a temporary directory and construct the
        # additional paths that need added to sys.path
        additional_paths = []
        for name, package in _get_packages().items():
            if package.wheel_name:
                # Use bundled wheel package
                from ensurepip import _bundled
                wheel_name = package.wheel_name
                whl = resources.read_binary(_bundled, wheel_name)
            else:
                # Use the wheel package directory
                with open(package.wheel_path, ""rb"") as fp:
                    whl = fp.read()
                wheel_name = os.path.basename(package.wheel_path)

            filename = os.path.join(tmpdir, wheel_name)
            with open(filename, ""wb"") as fp:
                fp.write(whl)

            additional_paths.append(filename)

        # Construct the arguments to be passed to the pip command
        args = [""install"", ""--no-cache-dir"", ""--no-index"", ""--find-links"", tmpdir]
        if root:
            args += [""--root"", root]
        if upgrade:
            args += [""--upgrade""]
        if user:
            args += [""--user""]
        if verbosity:
            args += [""-"" + ""v"" * verbosity]

        return _run_pip([*args, *_PACKAGE_NAMES], additional_paths)","def bootstrap_pip_into_python_installation(*, root=None, upgrade=False, user=False,
              altinstall=False, default_pip=False,
              verbosity=0):
    """"""
    Bootstrap pip into the current Python installation (or the given root
    directory).

    Note that calling this function will alter both sys.path and os.environ.
    """"""
    # Discard the return value
    bootstrap_pip_into_python_installation_pip_into_python(root=root, upgrade=upgrade, user=user,
               altinstall=altinstall, default_pip=default_pip,
               verbosity=verbosity)","{""_bootstrap"": ""bootstrap_pip_into_python""}","def 0(*, root=None, upgrade=False, user=False,
              altinstall=False, default_pip=False,
              verbosity=0):
    """"""
    Bootstrap pip into the current Python installation (or the given root
    directory). Returns pip command status code.

    Note that calling this function will alter both sys.path and os.environ.
    """"""
    if altinstall and default_pip:
        raise ValueError(""Cannot use altinstall and default_pip together"")

    sys.audit(""ensurepip.bootstrap"", root)

    _disable_pip_configuration_settings()

    # By default, installing pip and setuptools installs all of the
    # following scripts (X.Y == running Python version):
    #
    #   pip, pipX, pipX.Y, easy_install, easy_install-X.Y
    #
    # pip 1.5+ allows ensurepip to request that some of those be left out
    if altinstall:
        # omit pip, pipX and easy_install
        os.environ[""ENSUREPIP_OPTIONS""] = ""altinstall""
    elif not default_pip:
        # omit pip and easy_install
        os.environ[""ENSUREPIP_OPTIONS""] = ""install""

    with tempfile.TemporaryDirectory() as tmpdir:
        # Put our bundled wheels into a temporary directory and construct the
        # additional paths that need added to sys.path
        additional_paths = []
        for name, package in _get_packages().items():
            if package.wheel_name:
                # Use bundled wheel package
                from ensurepip import _bundled
                wheel_name = package.wheel_name
                whl = resources.read_binary(_bundled, wheel_name)
            else:
                # Use the wheel package directory
                with open(package.wheel_path, ""rb"") as fp:
                    whl = fp.read()
                wheel_name = os.path.basename(package.wheel_path)

            filename = os.path.join(tmpdir, wheel_name)
            with open(filename, ""wb"") as fp:
                fp.write(whl)

            additional_paths.append(filename)

        # Construct the arguments to be passed to the pip command
        args = [""install"", ""--no-cache-dir"", ""--no-index"", ""--find-links"", tmpdir]
        if root:
            args += [""--root"", root]
        if upgrade:
            args += [""--upgrade""]
        if user:
            args += [""--user""]
        if verbosity:
            args += [""-"" + ""v"" * verbosity]

        return _run_pip([*args, *_PACKAGE_NAMES], additional_paths)","def 1(*, root=None, upgrade=False, user=False,
              altinstall=False, default_pip=False,
              verbosity=0):
    """"""
    Bootstrap pip into the current Python installation (or the given root
    directory).

    Note that calling this function will alter both sys.path and os.environ.
    """"""
    # Discard the return value
    0(root=root, upgrade=upgrade, user=user,
               altinstall=altinstall, default_pip=default_pip,
               verbosity=verbosity)","{""_bootstrap"": ""0""}"
543,543,"def resolve_name(name, package):
    """"""Resolve a relative module name to an absolute one.""""""
    if not name.startswith('.'):
        return name
    elif not package:
        raise ImportError(f'no package specified for {repr(name)} '
                          '(required for relative module names)')
    level = 0
    for character in name:
        if character != '.':
            break
        level += 1
    return _resolve_name(name[level:], package, level)","def _resolve_name(name, package, level):
    """"""Resolve a relative module name to an absolute one.""""""
    bits = package.rsplit('.', level - 1)
    if len(bits) < level:
        raise ImportError('attempted relative import beyond top-level package')
    base = bits[0]
    return '{}.{}'.format(base, name) if name else base","def resolve_relative_module_name(name, package, level):
    """"""Resolve a relative module name to an absolute one.""""""
    bits = package.rsplit('.', level - 1)
    if len(bits) < level:
        raise ImportError('attempted relative import beyond top-level package')
    base = bits[0]
    return '{}.{}'.format(base, name) if name else base","def resolve_relative_module_name_to_absolute(name, package):
    """"""Resolve a relative module name to an absolute one.""""""
    if not name.startswith('.'):
        return name
    elif not package:
        raise ImportError(f'no package specified for {repr(name)} '
                          '(required for relative module names)')
    level = 0
    for character in name:
        if character != '.':
            break
        level += 1
    return resolve_relative_module_name(name[level:], package, level)","{""_resolve_name"": ""resolve_relative_module_name""}","def 0(name, package, level):
    """"""Resolve a relative module name to an absolute one.""""""
    bits = package.rsplit('.', level - 1)
    if len(bits) < level:
        raise ImportError('attempted relative import beyond top-level package')
    base = bits[0]
    return '{}.{}'.format(base, name) if name else base","def 1(name, package):
    """"""Resolve a relative module name to an absolute one.""""""
    if not name.startswith('.'):
        return name
    elif not package:
        raise ImportError(f'no package specified for {repr(name)} '
                          '(required for relative module names)')
    level = 0
    for character in name:
        if character != '.':
            break
        level += 1
    return 0(name[level:], package, level)","{""_resolve_name"": ""0""}"
544,544,"def _lock_unlock_module(name):
    """"""Acquires then releases the module lock for a given module name.

    This is used to ensure a module is completely initialized, in the
    event it is being imported by another thread.
    """"""
    lock = _get_module_lock(name)
    try:
        lock.acquire()
    except _DeadlockError:
        # Concurrent circular import, we'll accept a partially initialized
        # module object.
        pass
    else:
        lock.release()","def _get_module_lock(name):
    """"""Get or create the module lock for a given module name.

    Acquire/release internally the global import lock to protect
    _module_locks.""""""

    _imp.acquire_lock()
    try:
        try:
            lock = _module_locks[name]()
        except KeyError:
            lock = None

        if lock is None:
            if _thread is None:
                lock = _DummyModuleLock(name)
            else:
                lock = _ModuleLock(name)

            def cb(ref, name=name):
                _imp.acquire_lock()
                try:
                    # bpo-31070: Check if another thread created a new lock
                    # after the previous lock was destroyed
                    # but before the weakref callback was called.
                    if _module_locks.get(name) is ref:
                        del _module_locks[name]
                finally:
                    _imp.release_lock()

            _module_locks[name] = _weakref.ref(lock, cb)
    finally:
        _imp.release_lock()

    return lock","def get_or_create_module_lock(name):
    """"""Get or create the module lock for a given module name.

    Acquire/release internally the global import lock to protect
    _module_locks.""""""

    _imp.acquire_lock()
    try:
        try:
            lock = _module_locks[name]()
        except KeyError:
            lock = None

        if lock is None:
            if _thread is None:
                lock = _DummyModuleLock(name)
            else:
                lock = _ModuleLock(name)

            def cb(ref, name=name):
                _imp.acquire_lock()
                try:
                    # bpo-31070: Check if another thread created a new lock
                    # after the previous lock was destroyed
                    # but before the weakref callback was called.
                    if _module_locks.get(name) is ref:
                        del _module_locks[name]
                finally:
                    _imp.release_lock()

            _module_locks[name] = _weakref.ref(lock, cb)
    finally:
        _imp.release_lock()

    return lock","def acquire_and_release_module_lock(name):
    """"""Acquires then releases the module lock for a given module name.

    This is used to ensure a module is completely initialized, in the
    event it is being imported by another thread.
    """"""
    lock = get_or_create_module_lock(name)
    try:
        lock.acquire()
    except _DeadlockError:
        # Concurrent circular import, we'll accept a partially initialized
        # module object.
        pass
    else:
        lock.release()","{""_get_module_lock"": ""get_or_create_module_lock""}","def 0(name):
    """"""Get or create the module lock for a given module name.

    Acquire/release internally the global import lock to protect
    _module_locks.""""""

    _imp.acquire_lock()
    try:
        try:
            lock = _module_locks[name]()
        except KeyError:
            lock = None

        if lock is None:
            if _thread is None:
                lock = _DummyModuleLock(name)
            else:
                lock = _ModuleLock(name)

            def cb(ref, name=name):
                _imp.acquire_lock()
                try:
                    # bpo-31070: Check if another thread created a new lock
                    # after the previous lock was destroyed
                    # but before the weakref callback was called.
                    if _module_locks.get(name) is ref:
                        del _module_locks[name]
                finally:
                    _imp.release_lock()

            _module_locks[name] = _weakref.ref(lock, cb)
    finally:
        _imp.release_lock()

    return lock","def 2(name):
    """"""Acquires then releases the module lock for a given module name.

    This is used to ensure a module is completely initialized, in the
    event it is being imported by another thread.
    """"""
    lock = 0(name)
    try:
        lock.acquire()
    except _DeadlockError:
        # Concurrent circular import, we'll accept a partially initialized
        # module object.
        pass
    else:
        lock.release()","{""_get_module_lock"": ""0""}"
545,545,"def _requires_builtin(fxn):
    """"""Decorator to verify the named module is built-in.""""""
    def _requires_builtin_wrapper(self, fullname):
        if fullname not in sys.builtin_module_names:
            raise ImportError('{!r} is not a built-in module'.format(fullname),
                              name=fullname)
        return fxn(self, fullname)
    _wrap(_requires_builtin_wrapper, fxn)
    return _requires_builtin_wrapper","def _wrap(new, old):
    """"""Simple substitute for functools.update_wrapper.""""""
    for replace in ['__module__', '__name__', '__qualname__', '__doc__']:
        if hasattr(old, replace):
            setattr(new, replace, getattr(old, replace))
    new.__dict__.update(old.__dict__)","def simple_substitute_for_update_wrapper(new, old):
    """"""Simple substitute for functools.updatesimple_substitute_for_update_wrapperper.""""""
    for replace in ['__module__', '__name__', '__qualname__', '__doc__']:
        if hasattr(old, replace):
            setattr(new, replace, getattr(old, replace))
    new.__dict__.update(old.__dict__)","def verify_module_is_builtin_decorator(fxn):
    """"""Decorator to verify the named module is built-in.""""""
    def verify_module_is_builtin_decoratorsimple_substitute_for_update_wrapperper(self, fullname):
        if fullname not in sys.builtin_module_names:
            raise ImportError('{!r} is not a built-in module'.format(fullname),
                              name=fullname)
        return fxn(self, fullname)
    simple_substitute_for_update_wrapper(verify_module_is_builtin_decoratorsimple_substitute_for_update_wrapperper, fxn)
    return verify_module_is_builtin_decoratorsimple_substitute_for_update_wrapperper","{""_wrap"": ""simple_substitute_for_update_wrapper""}","def 0(new, old):
    """"""Simple substitute for functools.update_wrapper.""""""
    for replace in ['__module__', '__name__', '__qualname__', '__doc__']:
        if hasattr(old, replace):
            setattr(new, replace, getattr(old, replace))
    new.__dict__.update(old.__dict__)","def 1(fxn):
    """"""Decorator to verify the named module is built-in.""""""
    def 2(self, fullname):
        if fullname not in sys.builtin_module_names:
            raise ImportError('{!r} is not a built-in module'.format(fullname),
                              name=fullname)
        return fxn(self, fullname)
    0(_requires_builtin_wrapper, fxn)
    return _requires_builtin_wrapper","{""_wrap"": ""0""}"
546,546,"def _requires_frozen(fxn):
    """"""Decorator to verify the named module is frozen.""""""
    def _requires_frozen_wrapper(self, fullname):
        if not _imp.is_frozen(fullname):
            raise ImportError('{!r} is not a frozen module'.format(fullname),
                              name=fullname)
        return fxn(self, fullname)
    _wrap(_requires_frozen_wrapper, fxn)
    return _requires_frozen_wrapper","def _wrap(new, old):
    """"""Simple substitute for functools.update_wrapper.""""""
    for replace in ['__module__', '__name__', '__qualname__', '__doc__']:
        if hasattr(old, replace):
            setattr(new, replace, getattr(old, replace))
    new.__dict__.update(old.__dict__)","def simple_substitute_for_update_wrapper(new, old):
    """"""Simple substitute for functools.updatesimple_substitute_for_update_wrapperper.""""""
    for replace in ['__module__', '__name__', '__qualname__', '__doc__']:
        if hasattr(old, replace):
            setattr(new, replace, getattr(old, replace))
    new.__dict__.update(old.__dict__)","def verify_module_is_frozen_decorator(fxn):
    """"""Decorator to verify the named module is frozen.""""""
    def verify_module_is_frozen_decoratorsimple_substitute_for_update_wrapperper(self, fullname):
        if not _imp.is_frozen(fullname):
            raise ImportError('{!r} is not a frozen module'.format(fullname),
                              name=fullname)
        return fxn(self, fullname)
    simple_substitute_for_update_wrapper(verify_module_is_frozen_decoratorsimple_substitute_for_update_wrapperper, fxn)
    return verify_module_is_frozen_decoratorsimple_substitute_for_update_wrapperper","{""_wrap"": ""simple_substitute_for_update_wrapper""}","def 0(new, old):
    """"""Simple substitute for functools.update_wrapper.""""""
    for replace in ['__module__', '__name__', '__qualname__', '__doc__']:
        if hasattr(old, replace):
            setattr(new, replace, getattr(old, replace))
    new.__dict__.update(old.__dict__)","def 1(fxn):
    """"""Decorator to verify the named module is frozen.""""""
    def 2(self, fullname):
        if not _imp.is_frozen(fullname):
            raise ImportError('{!r} is not a frozen module'.format(fullname),
                              name=fullname)
        return fxn(self, fullname)
    0(_requires_frozen_wrapper, fxn)
    return _requires_frozen_wrapper","{""_wrap"": ""0""}"
547,547,"def _module_repr(module):
    """"""The implementation of ModuleType.__repr__().""""""
    loader = getattr(module, '__loader__', None)
    if spec := getattr(module, ""__spec__"", None):
        return _module_repr_from_spec(spec)
    elif hasattr(loader, 'module_repr'):
        try:
            return loader.module_repr(module)
        except Exception:
            pass
    # Fall through to a catch-all which always succeeds.
    try:
        name = module.__name__
    except AttributeError:
        name = '?'
    try:
        filename = module.__file__
    except AttributeError:
        if loader is None:
            return '<module {!r}>'.format(name)
        else:
            return '<module {!r} ({!r})>'.format(name, loader)
    else:
        return '<module {!r} from {!r}>'.format(name, filename)","def _module_repr_from_spec(spec):
    """"""Return the repr to use for the module.""""""
    # We mostly replicate _module_repr() using the spec attributes.
    name = '?' if spec.name is None else spec.name
    if spec.origin is None:
        if spec.loader is None:
            return '<module {!r}>'.format(name)
        else:
            return '<module {!r} ({!r})>'.format(name, spec.loader)
    else:
        if spec.has_location:
            return '<module {!r} from {!r}>'.format(name, spec.origin)
        else:
            return '<module {!r} ({})>'.format(spec.name, spec.origin)","def get_module_repr_from_spec(spec):
    """"""Return the repr to use for the module.""""""
    # We mostly replicate _module_repr() using the spec attributes.
    name = '?' if spec.name is None else spec.name
    if spec.origin is None:
        if spec.loader is None:
            return '<module {!r}>'.format(name)
        else:
            return '<module {!r} ({!r})>'.format(name, spec.loader)
    else:
        if spec.has_location:
            return '<module {!r} from {!r}>'.format(name, spec.origin)
        else:
            return '<module {!r} ({})>'.format(spec.name, spec.origin)","def implementation_of_module_type_repr(module):
    """"""The implementation of ModuleType.__repr__().""""""
    loader = getattr(module, '__loader__', None)
    if spec := getattr(module, ""__spec__"", None):
        return getimplementation_of_module_type_repr_from_spec(spec)
    elif hasattr(loader, 'module_repr'):
        try:
            return loader.module_repr(module)
        except Exception:
            pass
    # Fall through to a catch-all which always succeeds.
    try:
        name = module.__name__
    except AttributeError:
        name = '?'
    try:
        filename = module.__file__
    except AttributeError:
        if loader is None:
            return '<module {!r}>'.format(name)
        else:
            return '<module {!r} ({!r})>'.format(name, loader)
    else:
        return '<module {!r} from {!r}>'.format(name, filename)","{""_module_repr_from_spec"": ""get_module_repr_from_spec""}","def 0(spec):
    """"""Return the repr to use for the module.""""""
    # We mostly replicate _module_repr() using the spec attributes.
    name = '?' if spec.name is None else spec.name
    if spec.origin is None:
        if spec.loader is None:
            return '<module {!r}>'.format(name)
        else:
            return '<module {!r} ({!r})>'.format(name, spec.loader)
    else:
        if spec.has_location:
            return '<module {!r} from {!r}>'.format(name, spec.origin)
        else:
            return '<module {!r} ({})>'.format(spec.name, spec.origin)","def 1(module):
    """"""The implementation of ModuleType.__repr__().""""""
    loader = getattr(module, '__loader__', None)
    if spec := getattr(module, ""__spec__"", None):
        return 0(spec)
    elif hasattr(loader, 'module_repr'):
        try:
            return loader.module_repr(module)
        except Exception:
            pass
    # Fall through to a catch-all which always succeeds.
    try:
        name = module.__name__
    except AttributeError:
        name = '?'
    try:
        filename = module.__file__
    except AttributeError:
        if loader is None:
            return '<module {!r}>'.format(name)
        else:
            return '<module {!r} ({!r})>'.format(name, loader)
    else:
        return '<module {!r} from {!r}>'.format(name, filename)","{""_module_repr_from_spec"": ""0""}"
548,548,"def spec_from_loader(name, loader, *, origin=None, is_package=None):
    """"""Return a module spec based on various loader methods.""""""
    if hasattr(loader, 'get_filename'):
        if _bootstrap_external is None:
            raise NotImplementedError
        spec_from_file_location = _bootstrap_external.spec_from_file_location

        if is_package is None:
            return spec_from_file_location(name, loader=loader)
        search = [] if is_package else None
        return spec_from_file_location(name, loader=loader,
                                       submodule_search_locations=search)

    if is_package is None:
        if hasattr(loader, 'is_package'):
            try:
                is_package = loader.is_package(name)
            except ImportError:
                is_package = None  # aka, undefined
        else:
            # the default
            is_package = False

    return ModuleSpec(name, loader, origin=origin, is_package=is_package)","def spec_from_file_location(name, location=None, *, loader=None,
                            submodule_search_locations=_POPULATE):
    """"""Return a module spec based on a file location.

    To indicate that the module is a package, set
    submodule_search_locations to a list of directory paths.  An
    empty list is sufficient, though its not otherwise useful to the
    import system.

    The loader must take a spec as its only __init__() arg.

    """"""
    if location is None:
        # The caller may simply want a partially populated location-
        # oriented spec.  So we set the location to a bogus value and
        # fill in as much as we can.
        location = '<unknown>'
        if hasattr(loader, 'get_filename'):
            # ExecutionLoader
            try:
                location = loader.get_filename(name)
            except ImportError:
                pass
    else:
        location = _os.fspath(location)
        if not _path_isabs(location):
            try:
                location = _path_join(_os.getcwd(), location)
            except OSError:
                pass

    # If the location is on the filesystem, but doesn't actually exist,
    # we could return None here, indicating that the location is not
    # valid.  However, we don't have a good way of testing since an
    # indirect location (e.g. a zip file or URL) will look like a
    # non-existent file relative to the filesystem.

    spec = _bootstrap.ModuleSpec(name, loader, origin=location)
    spec._set_fileattr = True

    # Pick a loader if one wasn't provided.
    if loader is None:
        for loader_class, suffixes in _get_supported_file_loaders():
            if location.endswith(tuple(suffixes)):
                loader = loader_class(name, location)
                spec.loader = loader
                break
        else:
            return None

    # Set submodule_search_paths appropriately.
    if submodule_search_locations is _POPULATE:
        # Check the loader.
        if hasattr(loader, 'is_package'):
            try:
                is_package = loader.is_package(name)
            except ImportError:
                pass
            else:
                if is_package:
                    spec.submodule_search_locations = []
    else:
        spec.submodule_search_locations = submodule_search_locations
    if spec.submodule_search_locations == []:
        if location:
            dirname = _path_split(location)[0]
            spec.submodule_search_locations.append(dirname)

    return spec","def create_module_spec_from_file_location(name, location=None, *, loader=None,
                            submodule_search_locations=_POPULATE):
    """"""Return a module spec based on a file location.

    To indicate that the module is a package, set
    submodule_search_locations to a list of directory paths.  An
    empty list is sufficient, though its not otherwise useful to the
    import system.

    The loader must take a spec as its only __init__() arg.

    """"""
    if location is None:
        # The caller may simply want a partially populated location-
        # oriented spec.  So we set the location to a bogus value and
        # fill in as much as we can.
        location = '<unknown>'
        if hasattr(loader, 'get_filename'):
            # ExecutionLoader
            try:
                location = loader.get_filename(name)
            except ImportError:
                pass
    else:
        location = _os.fspath(location)
        if not _path_isabs(location):
            try:
                location = _path_join(_os.getcwd(), location)
            except OSError:
                pass

    # If the location is on the filesystem, but doesn't actually exist,
    # we could return None here, indicating that the location is not
    # valid.  However, we don't have a good way of testing since an
    # indirect location (e.g. a zip file or URL) will look like a
    # non-existent file relative to the filesystem.

    spec = _bootstrap.ModuleSpec(name, loader, origin=location)
    spec._set_fileattr = True

    # Pick a loader if one wasn't provided.
    if loader is None:
        for loader_class, suffixes in _get_supported_file_loaders():
            if location.endswith(tuple(suffixes)):
                loader = loader_class(name, location)
                spec.loader = loader
                break
        else:
            return None

    # Set submodule_search_paths appropriately.
    if submodule_search_locations is _POPULATE:
        # Check the loader.
        if hasattr(loader, 'is_package'):
            try:
                is_package = loader.is_package(name)
            except ImportError:
                pass
            else:
                if is_package:
                    spec.submodule_search_locations = []
    else:
        spec.submodule_search_locations = submodule_search_locations
    if spec.submodule_search_locations == []:
        if location:
            dirname = _path_split(location)[0]
            spec.submodule_search_locations.append(dirname)

    return spec","def create_module_spec_based_on_loader_methods(name, loader, *, origin=None, is_package=None):
    """"""Return a module spec based on various loader methods.""""""
    if hasattr(loader, 'get_filename'):
        if _bootstrap_external is None:
            raise NotImplementedError
        create_module_spec_from_file_location = _bootstrap_external.create_module_spec_from_file_location

        if is_package is None:
            return create_module_spec_from_file_location(name, loader=loader)
        search = [] if is_package else None
        return create_module_spec_from_file_location(name, loader=loader,
                                       submodule_search_locations=search)

    if is_package is None:
        if hasattr(loader, 'is_package'):
            try:
                is_package = loader.is_package(name)
            except ImportError:
                is_package = None  # aka, undefined
        else:
            # the default
            is_package = False

    return ModuleSpec(name, loader, origin=origin, is_package=is_package)","{""spec_from_file_location"": ""create_module_spec_from_file_location""}","def 0(name, location=None, *, loader=None,
                            submodule_search_locations=_POPULATE):
    """"""Return a module spec based on a file location.

    To indicate that the module is a package, set
    submodule_search_locations to a list of directory paths.  An
    empty list is sufficient, though its not otherwise useful to the
    import system.

    The loader must take a spec as its only __init__() arg.

    """"""
    if location is None:
        # The caller may simply want a partially populated location-
        # oriented spec.  So we set the location to a bogus value and
        # fill in as much as we can.
        location = '<unknown>'
        if hasattr(loader, 'get_filename'):
            # ExecutionLoader
            try:
                location = loader.get_filename(name)
            except ImportError:
                pass
    else:
        location = _os.fspath(location)
        if not _path_isabs(location):
            try:
                location = _path_join(_os.getcwd(), location)
            except OSError:
                pass

    # If the location is on the filesystem, but doesn't actually exist,
    # we could return None here, indicating that the location is not
    # valid.  However, we don't have a good way of testing since an
    # indirect location (e.g. a zip file or URL) will look like a
    # non-existent file relative to the filesystem.

    spec = _bootstrap.ModuleSpec(name, loader, origin=location)
    spec._set_fileattr = True

    # Pick a loader if one wasn't provided.
    if loader is None:
        for loader_class, suffixes in _get_supported_file_loaders():
            if location.endswith(tuple(suffixes)):
                loader = loader_class(name, location)
                spec.loader = loader
                break
        else:
            return None

    # Set submodule_search_paths appropriately.
    if submodule_search_locations is _POPULATE:
        # Check the loader.
        if hasattr(loader, 'is_package'):
            try:
                is_package = loader.is_package(name)
            except ImportError:
                pass
            else:
                if is_package:
                    spec.submodule_search_locations = []
    else:
        spec.submodule_search_locations = submodule_search_locations
    if spec.submodule_search_locations == []:
        if location:
            dirname = _path_split(location)[0]
            spec.submodule_search_locations.append(dirname)

    return spec","def 1(name, loader, *, origin=None, is_package=None):
    """"""Return a module spec based on various loader methods.""""""
    if hasattr(loader, 'get_filename'):
        if _bootstrap_external is None:
            raise NotImplementedError
        spec_from_file_location = _bootstrap_external.spec_from_file_location

        if is_package is None:
            return 0(name, loader=loader)
        search = [] if is_package else None
        return 0(name, loader=loader,
                                       submodule_search_locations=search)

    if is_package is None:
        if hasattr(loader, 'is_package'):
            try:
                is_package = loader.is_package(name)
            except ImportError:
                is_package = None  # aka, undefined
        else:
            # the default
            is_package = False

    return ModuleSpec(name, loader, origin=origin, is_package=is_package)","{""spec_from_file_location"": ""0""}"
549,549,"def module_from_spec(spec):
    """"""Create a module based on the provided spec.""""""
    # Typically loaders will not implement create_module().
    module = None
    if hasattr(spec.loader, 'create_module'):
        # If create_module() returns `None` then it means default
        # module creation should be used.
        module = spec.loader.create_module(spec)
    elif hasattr(spec.loader, 'exec_module'):
        raise ImportError('loaders that define exec_module() '
                          'must also define create_module()')
    if module is None:
        module = _new_module(spec.name)
    _init_module_attrs(spec, module)
    return module","def _init_module_attrs(spec, module, *, override=False):
    # The passed-in module may be not support attribute assignment,
    # in which case we simply don't set the attributes.
    # __name__
    if (override or getattr(module, '__name__', None) is None):
        try:
            module.__name__ = spec.name
        except AttributeError:
            pass
    # __loader__
    if override or getattr(module, '__loader__', None) is None:
        loader = spec.loader
        if loader is None:
            # A backward compatibility hack.
            if spec.submodule_search_locations is not None:
                if _bootstrap_external is None:
                    raise NotImplementedError
                _NamespaceLoader = _bootstrap_external._NamespaceLoader

                loader = _NamespaceLoader.__new__(_NamespaceLoader)
                loader._path = spec.submodule_search_locations
                spec.loader = loader
                # While the docs say that module.__file__ is not set for
                # built-in modules, and the code below will avoid setting it if
                # spec.has_location is false, this is incorrect for namespace
                # packages.  Namespace packages have no location, but their
                # __spec__.origin is None, and thus their module.__file__
                # should also be None for consistency.  While a bit of a hack,
                # this is the best place to ensure this consistency.
                #
                # See # https://docs.python.org/3/library/importlib.html#importlib.abc.Loader.load_module
                # and bpo-32305
                module.__file__ = None
        try:
            module.__loader__ = loader
        except AttributeError:
            pass
    # __package__
    if override or getattr(module, '__package__', None) is None:
        try:
            module.__package__ = spec.parent
        except AttributeError:
            pass
    # __spec__
    try:
        module.__spec__ = spec
    except AttributeError:
        pass
    # __path__
    if override or getattr(module, '__path__', None) is None:
        if spec.submodule_search_locations is not None:
            try:
                module.__path__ = spec.submodule_search_locations
            except AttributeError:
                pass
    # __file__/__cached__
    if spec.has_location:
        if override or getattr(module, '__file__', None) is None:
            try:
                module.__file__ = spec.origin
            except AttributeError:
                pass

        if override or getattr(module, '__cached__', None) is None:
            if spec.cached is not None:
                try:
                    module.__cached__ = spec.cached
                except AttributeError:
                    pass
    return module

def _new_module(name):
    return type(sys)(name)","def initialize_module_attributes(spec, module, *, override=False):
    # The passed-in module may be not support attribute assignment,
    # in which case we simply don't set the attributes.
    # __name__
    if (override or getattr(module, '__name__', None) is None):
        try:
            module.__name__ = spec.name
        except AttributeError:
            pass
    # __loader__
    if override or getattr(module, '__loader__', None) is None:
        loader = spec.loader
        if loader is None:
            # A backward compatibility hack.
            if spec.submodule_search_locations is not None:
                if _bootstrap_external is None:
                    raise NotImplementedError
                _NamespaceLoader = _bootstrap_external._NamespaceLoader

                loader = _NamespaceLoader.__new__(_NamespaceLoader)
                loader._path = spec.submodule_search_locations
                spec.loader = loader
                # While the docs say that module.__file__ is not set for
                # built-in modules, and the code below will avoid setting it if
                # spec.has_location is false, this is incorrect for namespace
                # packages.  Namespace packages have no location, but their
                # __spec__.origin is None, and thus their module.__file__
                # should also be None for consistency.  While a bit of a hack,
                # this is the best place to ensure this consistency.
                #
                # See # https://docs.python.org/3/library/importlib.html#importlib.abc.Loader.load_module
                # and bpo-32305
                module.__file__ = None
        try:
            module.__loader__ = loader
        except AttributeError:
            pass
    # __package__
    if override or getattr(module, '__package__', None) is None:
        try:
            module.__package__ = spec.parent
        except AttributeError:
            pass
    # __spec__
    try:
        module.__spec__ = spec
    except AttributeError:
        pass
    # __path__
    if override or getattr(module, '__path__', None) is None:
        if spec.submodule_search_locations is not None:
            try:
                module.__path__ = spec.submodule_search_locations
            except AttributeError:
                pass
    # __file__/__cached__
    if spec.has_location:
        if override or getattr(module, '__file__', None) is None:
            try:
                module.__file__ = spec.origin
            except AttributeError:
                pass

        if override or getattr(module, '__cached__', None) is None:
            if spec.cached is not None:
                try:
                    module.__cached__ = spec.cached
                except AttributeError:
                    pass
    return module

def create_new_module_instance(name):
    return type(sys)(name)","def create_module_based_on_spec(spec):
    """"""Create a module based on the provided spec.""""""
    # Typically loaders will not implement create_module().
    module = None
    if hasattr(spec.loader, 'create_module'):
        # If create_module() returns `None` then it means default
        # module creation should be used.
        module = spec.loader.create_module(spec)
    elif hasattr(spec.loader, 'exec_module'):
        raise ImportError('loaders that define exec_module() '
                          'must also define create_module()')
    if module is None:
        module = create_new_module_instance(spec.name)
    initialize_module_attributes(spec, module)
    return module","{""_init_module_attrs"": ""initialize_module_attributes"", ""_new_module"": ""create_new_module_instance""}","def 0(spec, module, *, override=False):
    # The passed-in module may be not support attribute assignment,
    # in which case we simply don't set the attributes.
    # __name__
    if (override or getattr(module, '__name__', None) is None):
        try:
            module.__name__ = spec.name
        except AttributeError:
            pass
    # __loader__
    if override or getattr(module, '__loader__', None) is None:
        loader = spec.loader
        if loader is None:
            # A backward compatibility hack.
            if spec.submodule_search_locations is not None:
                if _bootstrap_external is None:
                    raise NotImplementedError
                _NamespaceLoader = _bootstrap_external._NamespaceLoader

                loader = _NamespaceLoader.__new__(_NamespaceLoader)
                loader._path = spec.submodule_search_locations
                spec.loader = loader
                # While the docs say that module.__file__ is not set for
                # built-in modules, and the code below will avoid setting it if
                # spec.has_location is false, this is incorrect for namespace
                # packages.  Namespace packages have no location, but their
                # __spec__.origin is None, and thus their module.__file__
                # should also be None for consistency.  While a bit of a hack,
                # this is the best place to ensure this consistency.
                #
                # See # https://docs.python.org/3/library/importlib.html#importlib.abc.Loader.load_module
                # and bpo-32305
                module.__file__ = None
        try:
            module.__loader__ = loader
        except AttributeError:
            pass
    # __package__
    if override or getattr(module, '__package__', None) is None:
        try:
            module.__package__ = spec.parent
        except AttributeError:
            pass
    # __spec__
    try:
        module.__spec__ = spec
    except AttributeError:
        pass
    # __path__
    if override or getattr(module, '__path__', None) is None:
        if spec.submodule_search_locations is not None:
            try:
                module.__path__ = spec.submodule_search_locations
            except AttributeError:
                pass
    # __file__/__cached__
    if spec.has_location:
        if override or getattr(module, '__file__', None) is None:
            try:
                module.__file__ = spec.origin
            except AttributeError:
                pass

        if override or getattr(module, '__cached__', None) is None:
            if spec.cached is not None:
                try:
                    module.__cached__ = spec.cached
                except AttributeError:
                    pass
    return module

def 1(name):
    return type(sys)(name)","def 2(spec):
    """"""Create a module based on the provided spec.""""""
    # Typically loaders will not implement create_module().
    module = None
    if hasattr(spec.loader, 'create_module'):
        # If create_module() returns `None` then it means default
        # module creation should be used.
        module = spec.loader.create_module(spec)
    elif hasattr(spec.loader, 'exec_module'):
        raise ImportError('loaders that define exec_module() '
                          'must also define create_module()')
    if module is None:
        module = 1(spec.name)
    0(spec, module)
    return module","{""_init_module_attrs"": ""0"", ""_new_module"": ""1""}"
550,550,"def _load(spec):
    """"""Return a new module object, loaded by the spec's loader.

    The module is not added to its parent.

    If a module is already in sys.modules, that existing module gets
    clobbered.

    """"""
    with _ModuleLockManager(spec.name):
        return _load_unlocked(spec)","def _load_unlocked(spec):
    # A helper for direct use by the import system.
    if spec.loader is not None:
        # Not a namespace package.
        if not hasattr(spec.loader, 'exec_module'):
            msg = (f""{_object_name(spec.loader)}.exec_module() not found; ""
                    ""falling back to load_module()"")
            _warnings.warn(msg, ImportWarning)
            return _load_backward_compatible(spec)

    module = module_from_spec(spec)

    # This must be done before putting the module in sys.modules
    # (otherwise an optimization shortcut in import.c becomes
    # wrong).
    spec._initializing = True
    try:
        sys.modules[spec.name] = module
        try:
            if spec.loader is None:
                if spec.submodule_search_locations is None:
                    raise ImportError('missing loader', name=spec.name)
                # A namespace package so do nothing.
            else:
                spec.loader.exec_module(module)
        except:
            try:
                del sys.modules[spec.name]
            except KeyError:
                pass
            raise
        # Move the module to the end of sys.modules.
        # We don't ensure that the import-related module attributes get
        # set in the sys.modules replacement case.  Such modules are on
        # their own.
        module = sys.modules.pop(spec.name)
        sys.modules[spec.name] = module
        _verbose_message('import {!r} # {!r}', spec.name, spec.loader)
    finally:
        spec._initializing = False

    return module","def load_module_unlocked(spec):
    # A helper for direct use by the import system.
    if spec.loader is not None:
        # Not a namespace package.
        if not hasattr(spec.loader, 'exec_module'):
            msg = (f""{_object_name(spec.loader)}.exec_module() not found; ""
                    ""falling back to load_module()"")
            _warnings.warn(msg, ImportWarning)
            return _load_backward_compatible(spec)

    module = module_from_spec(spec)

    # This must be done before putting the module in sys.modules
    # (otherwise an optimization shortcut in import.c becomes
    # wrong).
    spec._initializing = True
    try:
        sys.modules[spec.name] = module
        try:
            if spec.loader is None:
                if spec.submodule_search_locations is None:
                    raise ImportError('missing loader', name=spec.name)
                # A namespace package so do nothing.
            else:
                spec.loader.exec_module(module)
        except:
            try:
                del sys.modules[spec.name]
            except KeyError:
                pass
            raise
        # Move the module to the end of sys.modules.
        # We don't ensure that the import-related module attributes get
        # set in the sys.modules replacement case.  Such modules are on
        # their own.
        module = sys.modules.pop(spec.name)
        sys.modules[spec.name] = module
        _verbose_message('import {!r} # {!r}', spec.name, spec.loader)
    finally:
        spec._initializing = False

    return module","def return_new_module_loaded_by_loader(spec):
    """"""Return a new module object, loaded by the spec's loader.

    The module is not added to its parent.

    If a module is already in sys.modules, that existing module gets
    clobbered.

    """"""
    with _ModuleLockManager(spec.name):
        return load_module_unlocked(spec)","{""_load_unlocked"": ""load_module_unlocked""}","def 0(spec):
    # A helper for direct use by the import system.
    if spec.loader is not None:
        # Not a namespace package.
        if not hasattr(spec.loader, 'exec_module'):
            msg = (f""{_object_name(spec.loader)}.exec_module() not found; ""
                    ""falling back to load_module()"")
            _warnings.warn(msg, ImportWarning)
            return _load_backward_compatible(spec)

    module = module_from_spec(spec)

    # This must be done before putting the module in sys.modules
    # (otherwise an optimization shortcut in import.c becomes
    # wrong).
    spec._initializing = True
    try:
        sys.modules[spec.name] = module
        try:
            if spec.loader is None:
                if spec.submodule_search_locations is None:
                    raise ImportError('missing loader', name=spec.name)
                # A namespace package so do nothing.
            else:
                spec.loader.exec_module(module)
        except:
            try:
                del sys.modules[spec.name]
            except KeyError:
                pass
            raise
        # Move the module to the end of sys.modules.
        # We don't ensure that the import-related module attributes get
        # set in the sys.modules replacement case.  Such modules are on
        # their own.
        module = sys.modules.pop(spec.name)
        sys.modules[spec.name] = module
        _verbose_message('import {!r} # {!r}', spec.name, spec.loader)
    finally:
        spec._initializing = False

    return module","def 1(spec):
    """"""Return a new module object, loaded by the spec's loader.

    The module is not added to its parent.

    If a module is already in sys.modules, that existing module gets
    clobbered.

    """"""
    with _ModuleLockManager(spec.name):
        return 0(spec)","{""_load_unlocked"": ""0""}"
551,551,"def _find_and_load(name, import_):
    """"""Find and load the module.""""""
    with _ModuleLockManager(name):
        module = sys.modules.get(name, _NEEDS_LOADING)
        if module is _NEEDS_LOADING:
            return _find_and_load_unlocked(name, import_)

    if module is None:
        message = ('import of {} halted; '
                   'None in sys.modules'.format(name))
        raise ModuleNotFoundError(message, name=name)

    _lock_unlock_module(name)
    return module","def _find_and_load_unlocked(name, import_):
    path = None
    parent = name.rpartition('.')[0]
    if parent:
        if parent not in sys.modules:
            _call_with_frames_removed(import_, parent)
        # Crazy side-effects!
        if name in sys.modules:
            return sys.modules[name]
        parent_module = sys.modules[parent]
        try:
            path = parent_module.__path__
        except AttributeError:
            msg = (_ERR_MSG + '; {!r} is not a package').format(name, parent)
            raise ModuleNotFoundError(msg, name=name) from None
    spec = _find_spec(name, path)
    if spec is None:
        raise ModuleNotFoundError(_ERR_MSG.format(name), name=name)
    else:
        module = _load_unlocked(spec)
    if parent:
        # Set the module as an attribute on its parent.
        parent_module = sys.modules[parent]
        child = name.rpartition('.')[2]
        try:
            setattr(parent_module, child, module)
        except AttributeError:
            msg = f""Cannot set an attribute on {parent!r} for child module {child!r}""
            _warnings.warn(msg, ImportWarning)
    return module

def _lock_unlock_module(name):
    """"""Acquires then releases the module lock for a given module name.

    This is used to ensure a module is completely initialized, in the
    event it is being imported by another thread.
    """"""
    lock = _get_module_lock(name)
    try:
        lock.acquire()
    except _DeadlockError:
        # Concurrent circular import, we'll accept a partially initialized
        # module object.
        pass
    else:
        lock.release()","def find_and_load_module_unlocked(name, import_):
    path = None
    parent = name.rpartition('.')[0]
    if parent:
        if parent not in sys.modules:
            _call_with_frames_removed(import_, parent)
        # Crazy side-effects!
        if name in sys.modules:
            return sys.modules[name]
        parent_module = sys.modules[parent]
        try:
            path = parent_module.__path__
        except AttributeError:
            msg = (_ERR_MSG + '; {!r} is not a package').format(name, parent)
            raise ModuleNotFoundError(msg, name=name) from None
    spec = _find_spec(name, path)
    if spec is None:
        raise ModuleNotFoundError(_ERR_MSG.format(name), name=name)
    else:
        module = _load_unlocked(spec)
    if parent:
        # Set the module as an attribute on its parent.
        parent_module = sys.modules[parent]
        child = name.rpartition('.')[2]
        try:
            setattr(parent_module, child, module)
        except AttributeError:
            msg = f""Cannot set an attribute on {parent!r} for child module {child!r}""
            _warnings.warn(msg, ImportWarning)
    return module

def acquire_and_release_module_lock(name):
    """"""Acquires then releases the module lock for a given module name.

    This is used to ensure a module is completely initialized, in the
    event it is being imported by another thread.
    """"""
    lock = _get_module_lock(name)
    try:
        lock.acquire()
    except _DeadlockError:
        # Concurrent circular import, we'll accept a partially initialized
        # module object.
        pass
    else:
        lock.release()","def find_and_load_module(name, import_):
    """"""Find and load the module.""""""
    with _ModuleLockManager(name):
        module = sys.modules.get(name, _NEEDS_LOADING)
        if module is _NEEDS_LOADING:
            return find_and_load_module_unlocked(name, import_)

    if module is None:
        message = ('import of {} halted; '
                   'None in sys.modules'.format(name))
        raise ModuleNotFoundError(message, name=name)

    acquire_and_release_module_lock(name)
    return module","{""_find_and_load_unlocked"": ""find_and_load_module_unlocked"", ""_lock_unlock_module"": ""acquire_and_release_module_lock""}","def 0(name, import_):
    path = None
    parent = name.rpartition('.')[0]
    if parent:
        if parent not in sys.modules:
            _call_with_frames_removed(import_, parent)
        # Crazy side-effects!
        if name in sys.modules:
            return sys.modules[name]
        parent_module = sys.modules[parent]
        try:
            path = parent_module.__path__
        except AttributeError:
            msg = (_ERR_MSG + '; {!r} is not a package').format(name, parent)
            raise ModuleNotFoundError(msg, name=name) from None
    spec = _find_spec(name, path)
    if spec is None:
        raise ModuleNotFoundError(_ERR_MSG.format(name), name=name)
    else:
        module = _load_unlocked(spec)
    if parent:
        # Set the module as an attribute on its parent.
        parent_module = sys.modules[parent]
        child = name.rpartition('.')[2]
        try:
            setattr(parent_module, child, module)
        except AttributeError:
            msg = f""Cannot set an attribute on {parent!r} for child module {child!r}""
            _warnings.warn(msg, ImportWarning)
    return module

def 1(name):
    """"""Acquires then releases the module lock for a given module name.

    This is used to ensure a module is completely initialized, in the
    event it is being imported by another thread.
    """"""
    lock = _get_module_lock(name)
    try:
        lock.acquire()
    except _DeadlockError:
        # Concurrent circular import, we'll accept a partially initialized
        # module object.
        pass
    else:
        lock.release()","def 2(name, import_):
    """"""Find and load the module.""""""
    with _ModuleLockManager(name):
        module = sys.modules.get(name, _NEEDS_LOADING)
        if module is _NEEDS_LOADING:
            return 0(name, import_)

    if module is None:
        message = ('import of {} halted; '
                   'None in sys.modules'.format(name))
        raise ModuleNotFoundError(message, name=name)

    1(name)
    return module","{""_find_and_load_unlocked"": ""0"", ""_lock_unlock_module"": ""1""}"
552,552,"def _gcd_import(name, package=None, level=0):
    """"""Import and return the module based on its name, the package the call is
    being made from, and the level adjustment.

    This function represents the greatest common denominator of functionality
    between import_module and __import__. This includes setting __package__ if
    the loader did not.

    """"""
    _sanity_check(name, package, level)
    if level > 0:
        name = _resolve_name(name, package, level)
    return _find_and_load(name, _gcd_import)","def _resolve_name(name, package, level):
    """"""Resolve a relative module name to an absolute one.""""""
    bits = package.rsplit('.', level - 1)
    if len(bits) < level:
        raise ImportError('attempted relative import beyond top-level package')
    base = bits[0]
    return '{}.{}'.format(base, name) if name else base

def _sanity_check(name, package, level):
    """"""Verify arguments are ""sane"".""""""
    if not isinstance(name, str):
        raise TypeError('module name must be str, not {}'.format(type(name)))
    if level < 0:
        raise ValueError('level must be >= 0')
    if level > 0:
        if not isinstance(package, str):
            raise TypeError('__package__ not set to a string')
        elif not package:
            raise ImportError('attempted relative import with no known parent '
                              'package')
    if not name and level == 0:
        raise ValueError('Empty module name')

def _find_and_load(name, import_):
    """"""Find and load the module.""""""
    with _ModuleLockManager(name):
        module = sys.modules.get(name, _NEEDS_LOADING)
        if module is _NEEDS_LOADING:
            return _find_and_load_unlocked(name, import_)

    if module is None:
        message = ('import of {} halted; '
                   'None in sys.modules'.format(name))
        raise ModuleNotFoundError(message, name=name)

    _lock_unlock_module(name)
    return module","def resolve_relative_module_name(name, package, level):
    """"""Resolve a relative module name to an absolute one.""""""
    bits = package.rsplit('.', level - 1)
    if len(bits) < level:
        raise ImportError('attempted relative import beyond top-level package')
    base = bits[0]
    return '{}.{}'.format(base, name) if name else base

def verify_sane_arguments(name, package, level):
    """"""Verify arguments are ""sane"".""""""
    if not isinstance(name, str):
        raise TypeError('module name must be str, not {}'.format(type(name)))
    if level < 0:
        raise ValueError('level must be >= 0')
    if level > 0:
        if not isinstance(package, str):
            raise TypeError('__package__ not set to a string')
        elif not package:
            raise ImportError('attempted relative import with no known parent '
                              'package')
    if not name and level == 0:
        raise ValueError('Empty module name')

def find_and_load_module(name, import_):
    """"""Find and load the module.""""""
    with _ModuleLockManager(name):
        module = sys.modules.get(name, _NEEDS_LOADING)
        if module is _NEEDS_LOADING:
            return find_and_load_module_unlocked(name, import_)

    if module is None:
        message = ('import of {} halted; '
                   'None in sys.modules'.format(name))
        raise ModuleNotFoundError(message, name=name)

    _lock_unlock_module(name)
    return module","def import_and_return_module_based_on_name(name, package=None, level=0):
    """"""Import and return the module based on its name, the package the call is
    being made from, and the level adjustment.

    This function represents the greatest common denominator of functionality
    between import_module and __import__. This includes setting __package__ if
    the loader did not.

    """"""
    verify_sane_arguments(name, package, level)
    if level > 0:
        name = resolve_relative_module_name(name, package, level)
    return find_and_load_module(name, import_and_return_module_based_on_name)","{""_resolve_name"": ""resolve_relative_module_name"", ""_sanity_check"": ""verify_sane_arguments"", ""_find_and_load"": ""find_and_load_module""}","def 0(name, package, level):
    """"""Resolve a relative module name to an absolute one.""""""
    bits = package.rsplit('.', level - 1)
    if len(bits) < level:
        raise ImportError('attempted relative import beyond top-level package')
    base = bits[0]
    return '{}.{}'.format(base, name) if name else base

def 1(name, package, level):
    """"""Verify arguments are ""sane"".""""""
    if not isinstance(name, str):
        raise TypeError('module name must be str, not {}'.format(type(name)))
    if level < 0:
        raise ValueError('level must be >= 0')
    if level > 0:
        if not isinstance(package, str):
            raise TypeError('__package__ not set to a string')
        elif not package:
            raise ImportError('attempted relative import with no known parent '
                              'package')
    if not name and level == 0:
        raise ValueError('Empty module name')

def 2(name, import_):
    """"""Find and load the module.""""""
    with _ModuleLockManager(name):
        module = sys.modules.get(name, _NEEDS_LOADING)
        if module is _NEEDS_LOADING:
            return _find_and_load_unlocked(name, import_)

    if module is None:
        message = ('import of {} halted; '
                   'None in sys.modules'.format(name))
        raise ModuleNotFoundError(message, name=name)

    _lock_unlock_module(name)
    return module","def 3(name, package=None, level=0):
    """"""Import and return the module based on its name, the package the call is
    being made from, and the level adjustment.

    This function represents the greatest common denominator of functionality
    between import_module and __import__. This includes setting __package__ if
    the loader did not.

    """"""
    1(name, package, level)
    if level > 0:
        name = 0(name, package, level)
    return 2(name, _gcd_import)","{""_resolve_name"": ""0"", ""_sanity_check"": ""1"", ""_find_and_load"": ""2""}"
553,553,"def normalize_path(path):
    # type: (Any) -> str
    """"""Normalize a path by ensuring it is a string.

    If the resulting string contains path separators, an exception is raised.
    """"""
    str_path = str(path)
    parent, file_name = os.path.split(str_path)
    if parent:
        raise ValueError(f'{path!r} must be only a file name')
    return file_name","def str(val):
    """"""Convert float to string, taking the locale into account.""""""
    return _format(""%.12g"", val)","def locale_aware_float_to_string(val):
    """"""Convert float to locale_aware_float_to_stringing, taking the locale into account.""""""
    return _format(""%.12g"", val)","def normalize_path_by_ensuring_it_is_only_a_file_name(path):
    # type: (Any) -> locale_aware_float_to_string
    """"""Normalize a path by ensuring it is a locale_aware_float_to_stringing.

    If the resulting locale_aware_float_to_stringing contains path separators, an exception is raised.
    """"""
    locale_aware_float_to_string_path = locale_aware_float_to_string(path)
    parent, file_name = os.path.split(locale_aware_float_to_string_path)
    if parent:
        raise ValueError(f'{path!r} must be only a file name')
    return file_name","{""str"": ""locale_aware_float_to_string""}","def 0(val):
    """"""Convert float to string, taking the locale into account.""""""
    return _format(""%.12g"", val)","def 1(path):
    # type: (Any) -> str
    """"""Normalize a path by ensuring it is a string.

    If the resulting string contains path separators, an exception is raised.
    """"""
    str_path = 0(path)
    parent, file_name = os.path.split(str_path)
    if parent:
        raise ValueError(f'{path!r} must be only a file name')
    return file_name","{""str"": ""0""}"
554,554,"def _path_is_mode_type(path, mode):
    """"""Test whether the path is the specified mode type.""""""
    try:
        stat_info = _path_stat(path)
    except OSError:
        return False
    return (stat_info.st_mode & 0o170000) == mode","def _path_stat(path):
    """"""Stat the path.

    Made a separate function to make it easier to override in experiments
    (e.g. cache stat results).

    """"""
    return _os.stat(path)","def stat_path(path):
    """"""Stat the path.

    Made a separate function to make it easier to override in experiments
    (e.g. cache stat results).

    """"""
    return _os.stat(path)","def check_path_mode_type(path, mode):
    """"""Test whether the path is the specified mode type.""""""
    try:
        stat_info = stat_path(path)
    except OSError:
        return False
    return (stat_info.st_mode & 0o170000) == mode","{""_path_stat"": ""stat_path""}","def 0(path):
    """"""Stat the path.

    Made a separate function to make it easier to override in experiments
    (e.g. cache stat results).

    """"""
    return _os.stat(path)","def 1(path, mode):
    """"""Test whether the path is the specified mode type.""""""
    try:
        stat_info = 0(path)
    except OSError:
        return False
    return (stat_info.st_mode & 0o170000) == mode","{""_path_stat"": ""0""}"
555,555,"def _path_isfile(path):
    """"""Replacement for os.path.isfile.""""""
    return _path_is_mode_type(path, 0o100000)","def _path_is_mode_type(path, mode):
    """"""Test whether the path is the specified mode type.""""""
    try:
        stat_info = _path_stat(path)
    except OSError:
        return False
    return (stat_info.st_mode & 0o170000) == mode","def check_path_mode_type(path, mode):
    """"""Test whether the path is the specified mode type.""""""
    try:
        stat_info = _path_stat(path)
    except OSError:
        return False
    return (stat_info.st_mode & 0o170000) == mode","def replacement_for_os_path_isfile(path):
    """"""Replacement for os.path.isfile.""""""
    return check_path_mode_type(path, 0o100000)","{""_path_is_mode_type"": ""check_path_mode_type""}","def 0(path, mode):
    """"""Test whether the path is the specified mode type.""""""
    try:
        stat_info = _path_stat(path)
    except OSError:
        return False
    return (stat_info.st_mode & 0o170000) == mode","def 1(path):
    """"""Replacement for os.path.isfile.""""""
    return 0(path, 0o100000)","{""_path_is_mode_type"": ""0""}"
556,556,"def _path_isdir(path):
    """"""Replacement for os.path.isdir.""""""
    if not path:
        path = _os.getcwd()
    return _path_is_mode_type(path, 0o040000)","def _path_is_mode_type(path, mode):
    """"""Test whether the path is the specified mode type.""""""
    try:
        stat_info = _path_stat(path)
    except OSError:
        return False
    return (stat_info.st_mode & 0o170000) == mode","def check_path_mode_type(path, mode):
    """"""Test whether the path is the specified mode type.""""""
    try:
        stat_info = _path_stat(path)
    except OSError:
        return False
    return (stat_info.st_mode & 0o170000) == mode","def replacement_for_os_path_isdir(path):
    """"""Replacement for os.path.isdir.""""""
    if not path:
        path = _os.getcwd()
    return check_path_mode_type(path, 0o040000)","{""_path_is_mode_type"": ""check_path_mode_type""}","def 0(path, mode):
    """"""Test whether the path is the specified mode type.""""""
    try:
        stat_info = _path_stat(path)
    except OSError:
        return False
    return (stat_info.st_mode & 0o170000) == mode","def 1(path):
    """"""Replacement for os.path.isdir.""""""
    if not path:
        path = _os.getcwd()
    return 0(path, 0o040000)","{""_path_is_mode_type"": ""0""}"
557,557,"def _write_atomic(path, data, mode=0o666):
    """"""Best-effort function to write data to a path atomically.
    Be prepared to handle a FileExistsError if concurrent writing of the
    temporary file is attempted.""""""
    # id() is used to generate a pseudo-random filename.
    path_tmp = '{}.{}'.format(path, id(path))
    fd = _os.open(path_tmp,
                  _os.O_EXCL | _os.O_CREAT | _os.O_WRONLY, mode & 0o666)
    try:
        # We first write data to a temporary file, and then use os.replace() to
        # perform an atomic rename.
        with _io.FileIO(fd, 'wb') as file:
            file.write(data)
        _os.replace(path_tmp, path)
    except OSError:
        try:
            _os.unlink(path_tmp)
        except OSError:
            pass
        raise","def id() -> str:
    """"""
    Return the distro ID of the current distribution, as a
    machine-readable string.

    For a number of OS distributions, the returned distro ID value is
    *reliable*, in the sense that it is documented and that it does not change
    across releases of the distribution.

    This package maintains the following reliable distro ID values:

    ==============  =========================================
    Distro ID       Distribution
    ==============  =========================================
    ""ubuntu""        Ubuntu
    ""debian""        Debian
    ""rhel""          RedHat Enterprise Linux
    ""centos""        CentOS
    ""fedora""        Fedora
    ""sles""          SUSE Linux Enterprise Server
    ""opensuse""      openSUSE
    ""amzn""          Amazon Linux
    ""arch""          Arch Linux
    ""cloudlinux""    CloudLinux OS
    ""exherbo""       Exherbo Linux
    ""gentoo""        GenToo Linux
    ""ibm_powerkvm""  IBM PowerKVM
    ""kvmibm""        KVM for IBM z Systems
    ""linuxmint""     Linux Mint
    ""mageia""        Mageia
    ""mandriva""      Mandriva Linux
    ""parallels""     Parallels
    ""pidora""        Pidora
    ""raspbian""      Raspbian
    ""oracle""        Oracle Linux (and Oracle Enterprise Linux)
    ""scientific""    Scientific Linux
    ""slackware""     Slackware
    ""xenserver""     XenServer
    ""openbsd""       OpenBSD
    ""netbsd""        NetBSD
    ""freebsd""       FreeBSD
    ""midnightbsd""   MidnightBSD
    ""rocky""         Rocky Linux
    ""aix""           AIX
    ==============  =========================================

    If you have a need to get distros for reliable IDs added into this set,
    or if you find that the :func:`distro.id` function returns a different
    distro ID for one of the listed distros, please create an issue in the
    `distro issue tracker`_.

    **Lookup hierarchy and transformations:**

    First, the ID is obtained from the following sources, in the specified
    order. The first available and non-empty value is used:

    * the value of the ""ID"" attribute of the os-release file,

    * the value of the ""Distributor ID"" attribute returned by the lsb_release
      command,

    * the first part of the file name of the distro release file,

    The so determined ID value then passes the following transformations,
    before it is returned by this method:

    * it is translated to lower case,

    * blanks (which should not be there anyway) are translated to underscores,

    * a normalization of the ID is performed, based upon
      `normalization tables`_. The purpose of this normalization is to ensure
      that the ID is as reliable as possible, even across incompatible changes
      in the OS distributions. A common reason for an incompatible change is
      the addition of an os-release file, or the addition of the lsb_release
      command, with ID values that differ from what was previously determined
      from the distro release file name.
    """"""
    return _distro.id()","def id() -> str:
    """"""
    Return the distro ID of the current distribution, as a
    machine-readable string.

    For a number of OS distributions, the returned distro ID value is
    *reliable*, in the sense that it is documented and that it does not change
    across releases of the distribution.

    This package maintains the following reliable distro ID values:

    ==============  =========================================
    Distro ID       Distribution
    ==============  =========================================
    ""ubuntu""        Ubuntu
    ""debian""        Debian
    ""rhel""          RedHat Enterprise Linux
    ""centos""        CentOS
    ""fedora""        Fedora
    ""sles""          SUSE Linux Enterprise Server
    ""opensuse""      openSUSE
    ""amzn""          Amazon Linux
    ""arch""          Arch Linux
    ""cloudlinux""    CloudLinux OS
    ""exherbo""       Exherbo Linux
    ""gentoo""        GenToo Linux
    ""ibm_powerkvm""  IBM PowerKVM
    ""kvmibm""        KVM for IBM z Systems
    ""linuxmint""     Linux Mint
    ""mageia""        Mageia
    ""mandriva""      Mandriva Linux
    ""parallels""     Parallels
    ""pidora""        Pidora
    ""raspbian""      Raspbian
    ""oracle""        Oracle Linux (and Oracle Enterprise Linux)
    ""scientific""    Scientific Linux
    ""slackware""     Slackware
    ""xenserver""     XenServer
    ""openbsd""       OpenBSD
    ""netbsd""        NetBSD
    ""freebsd""       FreeBSD
    ""midnightbsd""   MidnightBSD
    ""rocky""         Rocky Linux
    ""aix""           AIX
    ==============  =========================================

    If you have a need to get distros for reliable IDs added into this set,
    or if you find that the :func:`distro.id` function returns a different
    distro ID for one of the listed distros, please create an issue in the
    `distro issue tracker`_.

    **Lookup hierarchy and transformations:**

    First, the ID is obtained from the following sources, in the specified
    order. The first available and non-empty value is used:

    * the value of the ""ID"" attribute of the os-release file,

    * the value of the ""Distributor ID"" attribute returned by the lsb_release
      command,

    * the first part of the file name of the distro release file,

    The so determined ID value then passes the following transformations,
    before it is returned by this method:

    * it is translated to lower case,

    * blanks (which should not be there anyway) are translated to underscores,

    * a normalization of the ID is performed, based upon
      `normalization tables`_. The purpose of this normalization is to ensure
      that the ID is as reliable as possible, even across incompatible changes
      in the OS distributions. A common reason for an incompatible change is
      the addition of an os-release file, or the addition of the lsb_release
      command, with ID values that differ from what was previously determined
      from the distro release file name.
    """"""
    return _distro.id()","def write_data_to_path_atomically(path, data, mode=0o666):
    """"""Best-effort function to write data to a path atomically.
    Be prepared to handle a FileExistsError if concurrent writing of the
    temporary file is attempted.""""""
    # id() is used to generate a pseudo-random filename.
    path_tmp = '{}.{}'.format(path, id(path))
    fd = _os.open(path_tmp,
                  _os.O_EXCL | _os.O_CREAT | _os.O_WRONLY, mode & 0o666)
    try:
        # We first write data to a temporary file, and then use os.replace() to
        # perform an atomic rename.
        with _io.FileIO(fd, 'wb') as file:
            file.write(data)
        _os.replace(path_tmp, path)
    except OSError:
        try:
            _os.unlink(path_tmp)
        except OSError:
            pass
        raise",{},"def id() -> str:
    """"""
    Return the distro ID of the current distribution, as a
    machine-readable string.

    For a number of OS distributions, the returned distro ID value is
    *reliable*, in the sense that it is documented and that it does not change
    across releases of the distribution.

    This package maintains the following reliable distro ID values:

    ==============  =========================================
    Distro ID       Distribution
    ==============  =========================================
    ""ubuntu""        Ubuntu
    ""debian""        Debian
    ""rhel""          RedHat Enterprise Linux
    ""centos""        CentOS
    ""fedora""        Fedora
    ""sles""          SUSE Linux Enterprise Server
    ""opensuse""      openSUSE
    ""amzn""          Amazon Linux
    ""arch""          Arch Linux
    ""cloudlinux""    CloudLinux OS
    ""exherbo""       Exherbo Linux
    ""gentoo""        GenToo Linux
    ""ibm_powerkvm""  IBM PowerKVM
    ""kvmibm""        KVM for IBM z Systems
    ""linuxmint""     Linux Mint
    ""mageia""        Mageia
    ""mandriva""      Mandriva Linux
    ""parallels""     Parallels
    ""pidora""        Pidora
    ""raspbian""      Raspbian
    ""oracle""        Oracle Linux (and Oracle Enterprise Linux)
    ""scientific""    Scientific Linux
    ""slackware""     Slackware
    ""xenserver""     XenServer
    ""openbsd""       OpenBSD
    ""netbsd""        NetBSD
    ""freebsd""       FreeBSD
    ""midnightbsd""   MidnightBSD
    ""rocky""         Rocky Linux
    ""aix""           AIX
    ==============  =========================================

    If you have a need to get distros for reliable IDs added into this set,
    or if you find that the :func:`distro.id` function returns a different
    distro ID for one of the listed distros, please create an issue in the
    `distro issue tracker`_.

    **Lookup hierarchy and transformations:**

    First, the ID is obtained from the following sources, in the specified
    order. The first available and non-empty value is used:

    * the value of the ""ID"" attribute of the os-release file,

    * the value of the ""Distributor ID"" attribute returned by the lsb_release
      command,

    * the first part of the file name of the distro release file,

    The so determined ID value then passes the following transformations,
    before it is returned by this method:

    * it is translated to lower case,

    * blanks (which should not be there anyway) are translated to underscores,

    * a normalization of the ID is performed, based upon
      `normalization tables`_. The purpose of this normalization is to ensure
      that the ID is as reliable as possible, even across incompatible changes
      in the OS distributions. A common reason for an incompatible change is
      the addition of an os-release file, or the addition of the lsb_release
      command, with ID values that differ from what was previously determined
      from the distro release file name.
    """"""
    return _distro.id()","def 1(path, data, mode=0o666):
    """"""Best-effort function to write data to a path atomically.
    Be prepared to handle a FileExistsError if concurrent writing of the
    temporary file is attempted.""""""
    # id() is used to generate a pseudo-random filename.
    path_tmp = '{}.{}'.format(path, id(path))
    fd = _os.open(path_tmp,
                  _os.O_EXCL | _os.O_CREAT | _os.O_WRONLY, mode & 0o666)
    try:
        # We first write data to a temporary file, and then use os.replace() to
        # perform an atomic rename.
        with _io.FileIO(fd, 'wb') as file:
            file.write(data)
        _os.replace(path_tmp, path)
    except OSError:
        try:
            _os.unlink(path_tmp)
        except OSError:
            pass
        raise",{}
558,558,"def _calc_mode(path):
    """"""Calculate the mode permissions for a bytecode file.""""""
    try:
        mode = _path_stat(path).st_mode
    except OSError:
        mode = 0o666
    # We always ensure write access so we can update cached files
    # later even when the source files are read-only on Windows (#6074)
    mode |= 0o200
    return mode","def _path_stat(path):
    """"""Stat the path.

    Made a separate function to make it easier to override in experiments
    (e.g. cache stat results).

    """"""
    return _os.stat(path)","def stat_path(path):
    """"""Stat the path.

    Made a separate function to make it easier to override in experiments
    (e.g. cache stat results).

    """"""
    return _os.stat(path)","def calculate_mode_permissions_for_bytecode(path):
    """"""Calculate the mode permissions for a bytecode file.""""""
    try:
        mode = stat_path(path).st_mode
    except OSError:
        mode = 0o666
    # We always ensure write access so we can update cached files
    # later even when the source files are read-only on Windows (#6074)
    mode |= 0o200
    return mode","{""_path_stat"": ""stat_path""}","def 0(path):
    """"""Stat the path.

    Made a separate function to make it easier to override in experiments
    (e.g. cache stat results).

    """"""
    return _os.stat(path)","def 1(path):
    """"""Calculate the mode permissions for a bytecode file.""""""
    try:
        mode = 0(path).st_mode
    except OSError:
        mode = 0o666
    # We always ensure write access so we can update cached files
    # later even when the source files are read-only on Windows (#6074)
    mode |= 0o200
    return mode","{""_path_stat"": ""0""}"
559,559,"def _validate_timestamp_pyc(data, source_mtime, source_size, name,
                            exc_details):
    """"""Validate a pyc against the source last-modified time.

    *data* is the contents of the pyc file. (Only the first 16 bytes are
    required.)

    *source_mtime* is the last modified timestamp of the source file.

    *source_size* is None or the size of the source file in bytes.

    *name* is the name of the module being imported. It is used for logging.

    *exc_details* is a dictionary passed to ImportError if it raised for
    improved debugging.

    An ImportError is raised if the bytecode is stale.

    """"""
    if _unpack_uint32(data[8:12]) != (source_mtime & 0xFFFFFFFF):
        message = f'bytecode is stale for {name!r}'
        _bootstrap._verbose_message('{}', message)
        raise ImportError(message, **exc_details)
    if (source_size is not None and
        _unpack_uint32(data[12:16]) != (source_size & 0xFFFFFFFF)):
        raise ImportError(f'bytecode is stale for {name!r}', **exc_details)","def _unpack_uint32(data):
    """"""Convert 4 bytes in little-endian to an integer.""""""
    assert len(data) == 4
    return int.from_bytes(data, 'little')","def unpack_uint32(data):
    """"""Convert 4 bytes in little-endian to an integer.""""""
    assert len(data) == 4
    return int.from_bytes(data, 'little')","def validate_timestamp_based_pyc_against_source(data, source_mtime, source_size, name,
                            exc_details):
    """"""Validate a pyc against the source last-modified time.

    *data* is the contents of the pyc file. (Only the first 16 bytes are
    required.)

    *source_mtime* is the last modified timestamp of the source file.

    *source_size* is None or the size of the source file in bytes.

    *name* is the name of the module being imported. It is used for logging.

    *exc_details* is a dictionary passed to ImportError if it raised for
    improved debugging.

    An ImportError is raised if the bytecode is stale.

    """"""
    if unpack_uint32(data[8:12]) != (source_mtime & 0xFFFFFFFF):
        message = f'bytecode is stale for {name!r}'
        _bootstrap._verbose_message('{}', message)
        raise ImportError(message, **exc_details)
    if (source_size is not None and
        unpack_uint32(data[12:16]) != (source_size & 0xFFFFFFFF)):
        raise ImportError(f'bytecode is stale for {name!r}', **exc_details)","{""_unpack_uint32"": ""unpack_uint32""}","def 0(data):
    """"""Convert 4 bytes in little-endian to an integer.""""""
    assert len(data) == 4
    return int.from_bytes(data, 'little')","def 1(data, source_mtime, source_size, name,
                            exc_details):
    """"""Validate a pyc against the source last-modified time.

    *data* is the contents of the pyc file. (Only the first 16 bytes are
    required.)

    *source_mtime* is the last modified timestamp of the source file.

    *source_size* is None or the size of the source file in bytes.

    *name* is the name of the module being imported. It is used for logging.

    *exc_details* is a dictionary passed to ImportError if it raised for
    improved debugging.

    An ImportError is raised if the bytecode is stale.

    """"""
    if 0(data[8:12]) != (source_mtime & 0xFFFFFFFF):
        message = f'bytecode is stale for {name!r}'
        _bootstrap._verbose_message('{}', message)
        raise ImportError(message, **exc_details)
    if (source_size is not None and
        0(data[12:16]) != (source_size & 0xFFFFFFFF)):
        raise ImportError(f'bytecode is stale for {name!r}', **exc_details)","{""_unpack_uint32"": ""0""}"
560,560,"def _code_to_timestamp_pyc(code, mtime=0, source_size=0):
    ""Produce the data for a timestamp-based pyc.""
    data = bytearray(MAGIC_NUMBER)
    data.extend(_pack_uint32(0))
    data.extend(_pack_uint32(mtime))
    data.extend(_pack_uint32(source_size))
    data.extend(marshal.dumps(code))
    return data","def _pack_uint32(x):
    """"""Convert a 32-bit integer to little-endian.""""""
    return (int(x) & 0xFFFFFFFF).to_bytes(4, 'little')","def convert_uint32_to_little_endian_bytes(x):
    """"""Convert a 32-bit integer to little-endian.""""""
    return (int(x) & 0xFFFFFFFF).to_bytes(4, 'little')","def produce_data_for_timestamp_based_pyc(code, mtime=0, source_size=0):
    ""Produce the data for a timestamp-based pyc.""
    data = bytearray(MAGIC_NUMBER)
    data.extend(convert_uint32_to_little_endian_bytes(0))
    data.extend(convert_uint32_to_little_endian_bytes(mtime))
    data.extend(convert_uint32_to_little_endian_bytes(source_size))
    data.extend(marshal.dumps(code))
    return data","{""_pack_uint32"": ""convert_uint32_to_little_endian_bytes""}","def 0(x):
    """"""Convert a 32-bit integer to little-endian.""""""
    return (int(x) & 0xFFFFFFFF).to_bytes(4, 'little')","def 1(code, mtime=0, source_size=0):
    ""Produce the data for a timestamp-based pyc.""
    data = bytearray(MAGIC_NUMBER)
    data.extend(0(0))
    data.extend(0(mtime))
    data.extend(0(source_size))
    data.extend(marshal.dumps(code))
    return data","{""_pack_uint32"": ""0""}"
561,561,"def _code_to_hash_pyc(code, source_hash, checked=True):
    ""Produce the data for a hash-based pyc.""
    data = bytearray(MAGIC_NUMBER)
    flags = 0b1 | checked << 1
    data.extend(_pack_uint32(flags))
    assert len(source_hash) == 8
    data.extend(source_hash)
    data.extend(marshal.dumps(code))
    return data","def _pack_uint32(x):
    """"""Convert a 32-bit integer to little-endian.""""""
    return (int(x) & 0xFFFFFFFF).to_bytes(4, 'little')","def convert_uint32_to_little_endian_bytes(x):
    """"""Convert a 32-bit integer to little-endian.""""""
    return (int(x) & 0xFFFFFFFF).to_bytes(4, 'little')","def produce_data_for_hash_based_pyc(code, source_hash, checked=True):
    ""Produce the data for a hash-based pyc.""
    data = bytearray(MAGIC_NUMBER)
    flags = 0b1 | checked << 1
    data.extend(convert_uint32_to_little_endian_bytes(flags))
    assert len(source_hash) == 8
    data.extend(source_hash)
    data.extend(marshal.dumps(code))
    return data","{""_pack_uint32"": ""convert_uint32_to_little_endian_bytes""}","def 0(x):
    """"""Convert a 32-bit integer to little-endian.""""""
    return (int(x) & 0xFFFFFFFF).to_bytes(4, 'little')","def 1(code, source_hash, checked=True):
    ""Produce the data for a hash-based pyc.""
    data = bytearray(MAGIC_NUMBER)
    flags = 0b1 | checked << 1
    data.extend(0(flags))
    assert len(source_hash) == 8
    data.extend(source_hash)
    data.extend(marshal.dumps(code))
    return data","{""_pack_uint32"": ""0""}"
562,562,"def _install(_bootstrap_module):
    """"""Install the path-based import components.""""""
    _set_bootstrap_module(_bootstrap_module)
    supported_loaders = _get_supported_file_loaders()
    sys.path_hooks.extend([FileFinder.path_hook(*supported_loaders)])
    sys.meta_path.append(PathFinder)","def _set_bootstrap_module(_bootstrap_module):
    global _bootstrap
    _bootstrap = _bootstrap_module

def _get_supported_file_loaders():
    """"""Returns a list of file-based module loaders.

    Each item is a tuple (loader, suffixes).
    """"""
    extensions = ExtensionFileLoader, _imp.extension_suffixes()
    source = SourceFileLoader, SOURCE_SUFFIXES
    bytecode = SourcelessFileLoader, BYTECODE_SUFFIXES
    return [extensions, source, bytecode]","def set_bootstrap_module(_bootstrap_module):
    global _bootstrap
    _bootstrap = _bootstrap_module

def get_supported_file_loaders_list():
    """"""Returns a list of file-based module loaders.

    Each item is a tuple (loader, suffixes).
    """"""
    extensions = ExtensionFileLoader, _imp.extension_suffixes()
    source = SourceFileLoader, SOURCE_SUFFIXES
    bytecode = SourcelessFileLoader, BYTECODE_SUFFIXES
    return [extensions, source, bytecode]","def install_path_based_import_components(_bootstrap_module):
    """"""Install the path-based import components.""""""
    set_bootstrap_module(_bootstrap_module)
    supported_loaders = get_supported_file_loaders_list()
    sys.path_hooks.extend([FileFinder.path_hook(*supported_loaders)])
    sys.meta_path.append(PathFinder)","{""_set_bootstrap_module"": ""set_bootstrap_module"", ""_get_supported_file_loaders"": ""get_supported_file_loaders_list""}","def 0(_bootstrap_module):
    global _bootstrap
    _bootstrap = _bootstrap_module

def 1():
    """"""Returns a list of file-based module loaders.

    Each item is a tuple (loader, suffixes).
    """"""
    extensions = ExtensionFileLoader, _imp.extension_suffixes()
    source = SourceFileLoader, SOURCE_SUFFIXES
    bytecode = SourcelessFileLoader, BYTECODE_SUFFIXES
    return [extensions, source, bytecode]","def 2(_bootstrap_module):
    """"""Install the path-based import components.""""""
    0(_bootstrap_module)
    supported_loaders = 1()
    sys.path_hooks.extend([FileFinder.path_hook(*supported_loaders)])
    sys.meta_path.append(PathFinder)","{""_set_bootstrap_module"": ""0"", ""_get_supported_file_loaders"": ""1""}"
563,563,"def open_text(
    package: Package,
    resource: Resource,
    encoding: str = 'utf-8',
    errors: str = 'strict',
) -> TextIO:
    """"""Return a file-like object opened for text reading of the resource.""""""
    return TextIOWrapper(
        open_binary(package, resource), encoding=encoding, errors=errors
    )","def open_binary(package: Package, resource: Resource) -> BinaryIO:
    """"""Return a file-like object opened for binary reading of the resource.""""""
    resource = _common.normalize_path(resource)
    package = _common.get_package(package)
    reader = _common.get_resource_reader(package)
    if reader is not None:
        return reader.open_resource(resource)
    spec = cast(ModuleSpec, package.__spec__)
    # Using pathlib doesn't work well here due to the lack of 'strict'
    # argument for pathlib.Path.resolve() prior to Python 3.6.
    if spec.submodule_search_locations is not None:
        paths = spec.submodule_search_locations
    elif spec.origin is not None:
        paths = [os.path.dirname(os.path.abspath(spec.origin))]

    for package_path in paths:
        full_path = os.path.join(package_path, resource)
        try:
            return open(full_path, mode='rb')
        except OSError:
            # Just assume the loader is a resource loader; all the relevant
            # importlib.machinery loaders are and an AttributeError for
            # get_data() will make it clear what is needed from the loader.
            loader = cast(ResourceLoader, spec.loader)
            data = None
            if hasattr(spec.loader, 'get_data'):
                with suppress(OSError):
                    data = loader.get_data(full_path)
            if data is not None:
                return BytesIO(data)

    raise FileNotFoundError(f'{resource!r} resource not found in {spec.name!r}')","def open_binary_resource(package: Package, resource: Resource) -> BinaryIO:
    """"""Return a file-like object opened for binary reading of the resource.""""""
    resource = _common.normalize_path(resource)
    package = _common.get_package(package)
    reader = _common.get_resource_reader(package)
    if reader is not None:
        return reader.open_resource(resource)
    spec = cast(ModuleSpec, package.__spec__)
    # Using pathlib doesn't work well here due to the lack of 'strict'
    # argument for pathlib.Path.resolve() prior to Python 3.6.
    if spec.submodule_search_locations is not None:
        paths = spec.submodule_search_locations
    elif spec.origin is not None:
        paths = [os.path.dirname(os.path.abspath(spec.origin))]

    for package_path in paths:
        full_path = os.path.join(package_path, resource)
        try:
            return open(full_path, mode='rb')
        except OSError:
            # Just assume the loader is a resource loader; all the relevant
            # importlib.machinery loaders are and an AttributeError for
            # get_data() will make it clear what is needed from the loader.
            loader = cast(ResourceLoader, spec.loader)
            data = None
            if hasattr(spec.loader, 'get_data'):
                with suppress(OSError):
                    data = loader.get_data(full_path)
            if data is not None:
                return BytesIO(data)

    raise FileNotFoundError(f'{resource!r} resource not found in {spec.name!r}')","def open_text(
    package: Package,
    resource: Resource,
    encoding: str = 'utf-8',
    errors: str = 'strict',
) -> TextIO:
    """"""Return a file-like object opened for text reading of the resource.""""""
    return TextIOWrapper(
        open_binary_resource(package, resource), encoding=encoding, errors=errors
    )","{""open_binary"": ""open_binary_resource""}","def 0(package: Package, resource: Resource) -> BinaryIO:
    """"""Return a file-like object opened for binary reading of the resource.""""""
    resource = _common.normalize_path(resource)
    package = _common.get_package(package)
    reader = _common.get_resource_reader(package)
    if reader is not None:
        return reader.open_resource(resource)
    spec = cast(ModuleSpec, package.__spec__)
    # Using pathlib doesn't work well here due to the lack of 'strict'
    # argument for pathlib.Path.resolve() prior to Python 3.6.
    if spec.submodule_search_locations is not None:
        paths = spec.submodule_search_locations
    elif spec.origin is not None:
        paths = [os.path.dirname(os.path.abspath(spec.origin))]

    for package_path in paths:
        full_path = os.path.join(package_path, resource)
        try:
            return open(full_path, mode='rb')
        except OSError:
            # Just assume the loader is a resource loader; all the relevant
            # importlib.machinery loaders are and an AttributeError for
            # get_data() will make it clear what is needed from the loader.
            loader = cast(ResourceLoader, spec.loader)
            data = None
            if hasattr(spec.loader, 'get_data'):
                with suppress(OSError):
                    data = loader.get_data(full_path)
            if data is not None:
                return BytesIO(data)

    raise FileNotFoundError(f'{resource!r} resource not found in {spec.name!r}')","def open_text(
    package: Package,
    resource: Resource,
    encoding: str = 'utf-8',
    errors: str = 'strict',
) -> TextIO:
    """"""Return a file-like object opened for text reading of the resource.""""""
    return TextIOWrapper(
        0(package, resource), encoding=encoding, errors=errors
    )","{""open_binary"": ""0""}"
564,564,"def read_binary(package: Package, resource: Resource) -> bytes:
    """"""Return the binary contents of the resource.""""""
    with open_binary(package, resource) as fp:
        return fp.read()","def open_binary(package: Package, resource: Resource) -> BinaryIO:
    """"""Return a file-like object opened for binary reading of the resource.""""""
    resource = _common.normalize_path(resource)
    package = _common.get_package(package)
    reader = _common.get_resource_reader(package)
    if reader is not None:
        return reader.open_resource(resource)
    spec = cast(ModuleSpec, package.__spec__)
    # Using pathlib doesn't work well here due to the lack of 'strict'
    # argument for pathlib.Path.resolve() prior to Python 3.6.
    if spec.submodule_search_locations is not None:
        paths = spec.submodule_search_locations
    elif spec.origin is not None:
        paths = [os.path.dirname(os.path.abspath(spec.origin))]

    for package_path in paths:
        full_path = os.path.join(package_path, resource)
        try:
            return open(full_path, mode='rb')
        except OSError:
            # Just assume the loader is a resource loader; all the relevant
            # importlib.machinery loaders are and an AttributeError for
            # get_data() will make it clear what is needed from the loader.
            loader = cast(ResourceLoader, spec.loader)
            data = None
            if hasattr(spec.loader, 'get_data'):
                with suppress(OSError):
                    data = loader.get_data(full_path)
            if data is not None:
                return BytesIO(data)

    raise FileNotFoundError(f'{resource!r} resource not found in {spec.name!r}')","def open_binary_resource(package: Package, resource: Resource) -> BinaryIO:
    """"""Return a file-like object opened for binary reading of the resource.""""""
    resource = _common.normalize_path(resource)
    package = _common.get_package(package)
    reader = _common.get_resource_reader(package)
    if reader is not None:
        return reader.open_resource(resource)
    spec = cast(ModuleSpec, package.__spec__)
    # Using pathlib doesn't work well here due to the lack of 'strict'
    # argument for pathlib.Path.resolve() prior to Python 3.6.
    if spec.submodule_search_locations is not None:
        paths = spec.submodule_search_locations
    elif spec.origin is not None:
        paths = [os.path.dirname(os.path.abspath(spec.origin))]

    for package_path in paths:
        full_path = os.path.join(package_path, resource)
        try:
            return open(full_path, mode='rb')
        except OSError:
            # Just assume the loader is a resource loader; all the relevant
            # importlib.machinery loaders are and an AttributeError for
            # get_data() will make it clear what is needed from the loader.
            loader = cast(ResourceLoader, spec.loader)
            data = None
            if hasattr(spec.loader, 'get_data'):
                with suppress(OSError):
                    data = loader.get_data(full_path)
            if data is not None:
                return BytesIO(data)

    raise FileNotFoundError(f'{resource!r} resource not found in {spec.name!r}')","def read_binary(package: Package, resource: Resource) -> bytes:
    """"""Return the binary contents of the resource.""""""
    with open_binary_resource(package, resource) as fp:
        return fp.read()","{""open_binary"": ""open_binary_resource""}","def 0(package: Package, resource: Resource) -> BinaryIO:
    """"""Return a file-like object opened for binary reading of the resource.""""""
    resource = _common.normalize_path(resource)
    package = _common.get_package(package)
    reader = _common.get_resource_reader(package)
    if reader is not None:
        return reader.open_resource(resource)
    spec = cast(ModuleSpec, package.__spec__)
    # Using pathlib doesn't work well here due to the lack of 'strict'
    # argument for pathlib.Path.resolve() prior to Python 3.6.
    if spec.submodule_search_locations is not None:
        paths = spec.submodule_search_locations
    elif spec.origin is not None:
        paths = [os.path.dirname(os.path.abspath(spec.origin))]

    for package_path in paths:
        full_path = os.path.join(package_path, resource)
        try:
            return open(full_path, mode='rb')
        except OSError:
            # Just assume the loader is a resource loader; all the relevant
            # importlib.machinery loaders are and an AttributeError for
            # get_data() will make it clear what is needed from the loader.
            loader = cast(ResourceLoader, spec.loader)
            data = None
            if hasattr(spec.loader, 'get_data'):
                with suppress(OSError):
                    data = loader.get_data(full_path)
            if data is not None:
                return BytesIO(data)

    raise FileNotFoundError(f'{resource!r} resource not found in {spec.name!r}')","def read_binary(package: Package, resource: Resource) -> bytes:
    """"""Return the binary contents of the resource.""""""
    with 0(package, resource) as fp:
        return fp.read()","{""open_binary"": ""0""}"
565,565,"def read_text(
    package: Package,
    resource: Resource,
    encoding: str = 'utf-8',
    errors: str = 'strict',
) -> str:
    """"""Return the decoded string of the resource.

    The decoding-related arguments have the same semantics as those of
    bytes.decode().
    """"""
    with open_text(package, resource, encoding, errors) as fp:
        return fp.read()","def open_text(
    package: Package,
    resource: Resource,
    encoding: str = 'utf-8',
    errors: str = 'strict',
) -> TextIO:
    """"""Return a file-like object opened for text reading of the resource.""""""
    return TextIOWrapper(
        open_binary(package, resource), encoding=encoding, errors=errors
    )","def open_text(
    package: Package,
    resource: Resource,
    encoding: str = 'utf-8',
    errors: str = 'strict',
) -> TextIO:
    """"""Return a file-like object opened for text reading of the resource.""""""
    return TextIOWrapper(
        open_binary(package, resource), encoding=encoding, errors=errors
    )","def read_text(
    package: Package,
    resource: Resource,
    encoding: str = 'utf-8',
    errors: str = 'strict',
) -> str:
    """"""Return the decoded string of the resource.

    The decoding-related arguments have the same semantics as those of
    bytes.decode().
    """"""
    with open_text(package, resource, encoding, errors) as fp:
        return fp.read()",{},"def open_text(
    package: Package,
    resource: Resource,
    encoding: str = 'utf-8',
    errors: str = 'strict',
) -> TextIO:
    """"""Return a file-like object opened for text reading of the resource.""""""
    return TextIOWrapper(
        open_binary(package, resource), encoding=encoding, errors=errors
    )","def read_text(
    package: Package,
    resource: Resource,
    encoding: str = 'utf-8',
    errors: str = 'strict',
) -> str:
    """"""Return the decoded string of the resource.

    The decoding-related arguments have the same semantics as those of
    bytes.decode().
    """"""
    with open_text(package, resource, encoding, errors) as fp:
        return fp.read()",{}
566,566,"def path(
    package: Package,
    resource: Resource,
) -> 'ContextManager[Path]':
    """"""A context manager providing a file path object to the resource.

    If the resource does not already exist on its own on the file system,
    a temporary file will be created. If the file was created, the file
    will be deleted upon exiting the context manager (no exception is
    raised if the file was deleted prior to the context manager
    exiting).
    """"""
    reader = _common.get_resource_reader(_common.get_package(package))
    return (
        _path_from_reader(reader, _common.normalize_path(resource))
        if reader
        else _common.as_file(
            _common.files(package).joinpath(_common.normalize_path(resource))
        )
    )","def _path_from_reader(reader, resource):
    return _path_from_resource_path(reader, resource) or _path_from_open_resource(
        reader, resource
    )","def get_path_from_reader(reader, resource):
    return _path_from_resource_path(reader, resource) or _path_from_open_resource(
        reader, resource
    )","def path(
    package: Package,
    resource: Resource,
) -> 'ContextManager[Path]':
    """"""A context manager providing a file path object to the resource.

    If the resource does not already exist on its own on the file system,
    a temporary file will be created. If the file was created, the file
    will be deleted upon exiting the context manager (no exception is
    raised if the file was deleted prior to the context manager
    exiting).
    """"""
    reader = _common.get_resource_reader(_common.get_package(package))
    return (
        get_path_from_reader(reader, _common.normalize_path(resource))
        if reader
        else _common.as_file(
            _common.files(package).joinpath(_common.normalize_path(resource))
        )
    )","{""_path_from_reader"": ""get_path_from_reader""}","def 0(reader, resource):
    return _path_from_resource_path(reader, resource) or _path_from_open_resource(
        reader, resource
    )","def path(
    package: Package,
    resource: Resource,
) -> 'ContextManager[Path]':
    """"""A context manager providing a file path object to the resource.

    If the resource does not already exist on its own on the file system,
    a temporary file will be created. If the file was created, the file
    will be deleted upon exiting the context manager (no exception is
    raised if the file was deleted prior to the context manager
    exiting).
    """"""
    reader = _common.get_resource_reader(_common.get_package(package))
    return (
        0(reader, _common.normalize_path(resource))
        if reader
        else _common.as_file(
            _common.files(package).joinpath(_common.normalize_path(resource))
        )
    )","{""_path_from_reader"": ""0""}"
567,567,"def loads(data, use_datetime=False, use_builtin_types=False):
    """"""data -> unmarshalled data, method name

    Convert an XML-RPC packet to unmarshalled data plus a method
    name (None if not present).

    If the XML-RPC packet represents a fault condition, this function
    raises a Fault exception.
    """"""
    p, u = getparser(use_datetime=use_datetime, use_builtin_types=use_builtin_types)
    p.feed(data)
    p.close()
    return u.close(), u.getmethodname()","def getparser(use_datetime=False, use_builtin_types=False):
    """"""getparser() -> parser, unmarshaller

    Create an instance of the fastest available parser, and attach it
    to an unmarshalling object.  Return both objects.
    """"""
    if FastParser and FastUnmarshaller:
        if use_builtin_types:
            mkdatetime = _datetime_type
            mkbytes = base64.decodebytes
        elif use_datetime:
            mkdatetime = _datetime_type
            mkbytes = _binary
        else:
            mkdatetime = _datetime
            mkbytes = _binary
        target = FastUnmarshaller(True, False, mkbytes, mkdatetime, Fault)
        parser = FastParser(target)
    else:
        target = Unmarshaller(use_datetime=use_datetime, use_builtin_types=use_builtin_types)
        if FastParser:
            parser = FastParser(target)
        else:
            parser = ExpatParser(target)
    return parser, target","def create_parser_and_unmarshaller(use_datetime=False, use_builtin_types=False):
    """"""create_parser_and_unmarshaller() -> parser, unmarshaller

    Create an instance of the fastest available parser, and attach it
    to an unmarshalling object.  Return both objects.
    """"""
    if FastParser and FastUnmarshaller:
        if use_builtin_types:
            mkdatetime = _datetime_type
            mkbytes = base64.decodebytes
        elif use_datetime:
            mkdatetime = _datetime_type
            mkbytes = _binary
        else:
            mkdatetime = _datetime
            mkbytes = _binary
        target = FastUnmarshaller(True, False, mkbytes, mkdatetime, Fault)
        parser = FastParser(target)
    else:
        target = Unmarshaller(use_datetime=use_datetime, use_builtin_types=use_builtin_types)
        if FastParser:
            parser = FastParser(target)
        else:
            parser = ExpatParser(target)
    return parser, target","def convert_xml_rpc_packet_to_unmarshalled_data(data, use_datetime=False, use_builtin_types=False):
    """"""data -> unmarshalled data, method name

    Convert an XML-RPC packet to unmarshalled data plus a method
    name (None if not present).

    If the XML-RPC packet represents a fault condition, this function
    raises a Fault exception.
    """"""
    p, u = create_parser_and_unmarshaller(use_datetime=use_datetime, use_builtin_types=use_builtin_types)
    p.feed(data)
    p.close()
    return u.close(), u.getmethodname()","{""getparser"": ""create_parser_and_unmarshaller""}","def 0(use_datetime=False, use_builtin_types=False):
    """"""0() -> parser, unmarshaller

    Create an instance of the fastest available parser, and attach it
    to an unmarshalling object.  Return both objects.
    """"""
    if FastParser and FastUnmarshaller:
        if use_builtin_types:
            mkdatetime = _datetime_type
            mkbytes = base64.decodebytes
        elif use_datetime:
            mkdatetime = _datetime_type
            mkbytes = _binary
        else:
            mkdatetime = _datetime
            mkbytes = _binary
        target = FastUnmarshaller(True, False, mkbytes, mkdatetime, Fault)
        parser = FastParser(target)
    else:
        target = Unmarshaller(use_datetime=use_datetime, use_builtin_types=use_builtin_types)
        if FastParser:
            parser = FastParser(target)
        else:
            parser = ExpatParser(target)
    return parser, target","def 1(data, use_datetime=False, use_builtin_types=False):
    """"""data -> unmarshalled data, method name

    Convert an XML-RPC packet to unmarshalled data plus a method
    name (None if not present).

    If the XML-RPC packet represents a fault condition, this function
    raises a Fault exception.
    """"""
    p, u = 0(use_datetime=use_datetime, use_builtin_types=use_builtin_types)
    p.feed(data)
    p.close()
    return u.close(), u.getmethodname()","{""getparser"": ""0""}"
568,568,"def get_all_fix_names(fixer_pkg, remove_prefix=True):
    """"""Return a sorted list of all available fix names in the given package.""""""
    pkg = __import__(fixer_pkg, [], [], [""*""])
    fix_names = []
    for finder, name, ispkg in pkgutil.iter_modules(pkg.__path__):
        if name.startswith(""fix_""):
            if remove_prefix:
                name = name[4:]
            fix_names.append(name)
    return fix_names","def __import__(name, globals=None, locals=None, fromlist=(), level=0):
    """"""Import a module.

    The 'globals' argument is used to infer where the import is occurring from
    to handle relative imports. The 'locals' argument is ignored. The
    'fromlist' argument specifies what should exist as attributes on the module
    being imported (e.g. ``from module import <fromlist>``).  The 'level'
    argument represents the package location to import from in a relative
    import (e.g. ``from ..pkg import mod`` would have a 'level' of 2).

    """"""
    if level == 0:
        module = _gcd_import(name)
    else:
        globals_ = globals if globals is not None else {}
        package = _calc___package__(globals_)
        module = _gcd_import(name, package, level)
    if not fromlist:
        # Return up to the first dot in 'name'. This is complicated by the fact
        # that 'name' may be relative.
        if level == 0:
            return _gcd_import(name.partition('.')[0])
        elif not name:
            return module
        else:
            # Figure out where to slice the module's name up to the first dot
            # in 'name'.
            cut_off = len(name) - len(name.partition('.')[0])
            # Slice end needs to be positive to alleviate need to special-case
            # when ``'.' not in name``.
            return sys.modules[module.__name__[:len(module.__name__)-cut_off]]
    elif hasattr(module, '__path__'):
        return _handle_fromlist(module, fromlist, _gcd_import)
    else:
        return module","def import_module(name, globals=None, locals=None, fromlist=(), level=0):
    """"""Import a module.

    The 'globals' argument is used to infer where the import is occurring from
    to handle relative imports. The 'locals' argument is ignored. The
    'fromlist' argument specifies what should exist as attributes on the module
    being imported (e.g. ``from module import <fromlist>``).  The 'level'
    argument represents the package location to import from in a relative
    import (e.g. ``from ..pkg import mod`` would have a 'level' of 2).

    """"""
    if level == 0:
        module = _gcd_import(name)
    else:
        globals_ = globals if globals is not None else {}
        package = _calc___package__(globals_)
        module = _gcd_import(name, package, level)
    if not fromlist:
        # Return up to the first dot in 'name'. This is complicated by the fact
        # that 'name' may be relative.
        if level == 0:
            return _gcd_import(name.partition('.')[0])
        elif not name:
            return module
        else:
            # Figure out where to slice the module's name up to the first dot
            # in 'name'.
            cut_off = len(name) - len(name.partition('.')[0])
            # Slice end needs to be positive to alleviate need to special-case
            # when ``'.' not in name``.
            return sys.modules[module.__name__[:len(module.__name__)-cut_off]]
    elif hasattr(module, '__path__'):
        return _handle_fromlist(module, fromlist, _gcd_import)
    else:
        return module","def get_all_fix_names_in_package(fixer_pkg, remove_prefix=True):
    """"""Return a sorted list of all available fix names in the given package.""""""
    pkg = import_module(fixer_pkg, [], [], [""*""])
    fix_names = []
    for finder, name, ispkg in pkgutil.iter_modules(pkg.__path__):
        if name.startswith(""fix_""):
            if remove_prefix:
                name = name[4:]
            fix_names.append(name)
    return fix_names","{""__import__"": ""import_module""}","def 0(name, globals=None, locals=None, fromlist=(), level=0):
    """"""Import a module.

    The 'globals' argument is used to infer where the import is occurring from
    to handle relative imports. The 'locals' argument is ignored. The
    'fromlist' argument specifies what should exist as attributes on the module
    being imported (e.g. ``from module import <fromlist>``).  The 'level'
    argument represents the package location to import from in a relative
    import (e.g. ``from ..pkg import mod`` would have a 'level' of 2).

    """"""
    if level == 0:
        module = _gcd_import(name)
    else:
        globals_ = globals if globals is not None else {}
        package = _calc___package__(globals_)
        module = _gcd_import(name, package, level)
    if not fromlist:
        # Return up to the first dot in 'name'. This is complicated by the fact
        # that 'name' may be relative.
        if level == 0:
            return _gcd_import(name.partition('.')[0])
        elif not name:
            return module
        else:
            # Figure out where to slice the module's name up to the first dot
            # in 'name'.
            cut_off = len(name) - len(name.partition('.')[0])
            # Slice end needs to be positive to alleviate need to special-case
            # when ``'.' not in name``.
            return sys.modules[module.__name__[:len(module.__name__)-cut_off]]
    elif hasattr(module, '__path__'):
        return _handle_fromlist(module, fromlist, _gcd_import)
    else:
        return module","def 1(fixer_pkg, remove_prefix=True):
    """"""Return a sorted list of all available fix names in the given package.""""""
    pkg = 0(fixer_pkg, [], [], [""*""])
    fix_names = []
    for finder, name, ispkg in pkgutil.iter_modules(pkg.__path__):
        if name.startswith(""fix_""):
            if remove_prefix:
                name = name[4:]
            fix_names.append(name)
    return fix_names","{""__import__"": ""0""}"
569,569,"def _get_head_types(pat):
    """""" Accepts a pytree Pattern Node and returns a set
        of the pattern types which will match first. """"""

    if isinstance(pat, (pytree.NodePattern, pytree.LeafPattern)):
        # NodePatters must either have no type and no content
        #   or a type and content -- so they don't get any farther
        # Always return leafs
        if pat.type is None:
            raise _EveryNode
        return {pat.type}

    if isinstance(pat, pytree.NegatedPattern):
        if pat.content:
            return _get_head_types(pat.content)
        raise _EveryNode # Negated Patterns don't have a type

    if isinstance(pat, pytree.WildcardPattern):
        # Recurse on each node in content
        r = set()
        for p in pat.content:
            for x in p:
                r.update(_get_head_types(x))
        return r

    raise Exception(""Oh no! I don't understand pattern %s"" %(pat))","def _get_head_types(pat):
    """""" Accepts a pytree Pattern Node and returns a set
        of the pattern types which will match first. """"""

    if isinstance(pat, (pytree.NodePattern, pytree.LeafPattern)):
        # NodePatters must either have no type and no content
        #   or a type and content -- so they don't get any farther
        # Always return leafs
        if pat.type is None:
            raise _EveryNode
        return {pat.type}

    if isinstance(pat, pytree.NegatedPattern):
        if pat.content:
            return _get_head_types(pat.content)
        raise _EveryNode # Negated Patterns don't have a type

    if isinstance(pat, pytree.WildcardPattern):
        # Recurse on each node in content
        r = set()
        for p in pat.content:
            for x in p:
                r.update(_get_head_types(x))
        return r

    raise Exception(""Oh no! I don't understand pattern %s"" %(pat))","def get_head_types_from_pattern(pat):
    """""" Accepts a pytree Pattern Node and returns a set
        of the pattern types which will match first. """"""

    if isinstance(pat, (pytree.NodePattern, pytree.LeafPattern)):
        # NodePatters must either have no type and no content
        #   or a type and content -- so they don't get any farther
        # Always return leafs
        if pat.type is None:
            raise _EveryNode
        return {pat.type}

    if isinstance(pat, pytree.NegatedPattern):
        if pat.content:
            return get_head_types_from_pattern(pat.content)
        raise _EveryNode # Negated Patterns don't have a type

    if isinstance(pat, pytree.WildcardPattern):
        # Recurse on each node in content
        r = set()
        for p in pat.content:
            for x in p:
                r.update(get_head_types_from_pattern(x))
        return r

    raise Exception(""Oh no! I don't understand pattern %s"" %(pat))","def get_head_types_from_pattern(pat):
    """""" Accepts a pytree Pattern Node and returns a set
        of the pattern types which will match first. """"""

    if isinstance(pat, (pytree.NodePattern, pytree.LeafPattern)):
        # NodePatters must either have no type and no content
        #   or a type and content -- so they don't get any farther
        # Always return leafs
        if pat.type is None:
            raise _EveryNode
        return {pat.type}

    if isinstance(pat, pytree.NegatedPattern):
        if pat.content:
            return get_head_types_from_pattern(pat.content)
        raise _EveryNode # Negated Patterns don't have a type

    if isinstance(pat, pytree.WildcardPattern):
        # Recurse on each node in content
        r = set()
        for p in pat.content:
            for x in p:
                r.update(get_head_types_from_pattern(x))
        return r

    raise Exception(""Oh no! I don't understand pattern %s"" %(pat))","{""_get_head_types"": ""get_head_types_from_pattern""}","def 0(pat):
    """""" Accepts a pytree Pattern Node and returns a set
        of the pattern types which will match first. """"""

    if isinstance(pat, (pytree.NodePattern, pytree.LeafPattern)):
        # NodePatters must either have no type and no content
        #   or a type and content -- so they don't get any farther
        # Always return leafs
        if pat.type is None:
            raise _EveryNode
        return {pat.type}

    if isinstance(pat, pytree.NegatedPattern):
        if pat.content:
            return 0(pat.content)
        raise _EveryNode # Negated Patterns don't have a type

    if isinstance(pat, pytree.WildcardPattern):
        # Recurse on each node in content
        r = set()
        for p in pat.content:
            for x in p:
                r.update(0(x))
        return r

    raise Exception(""Oh no! I don't understand pattern %s"" %(pat))","def 0(pat):
    """""" Accepts a pytree Pattern Node and returns a set
        of the pattern types which will match first. """"""

    if isinstance(pat, (pytree.NodePattern, pytree.LeafPattern)):
        # NodePatters must either have no type and no content
        #   or a type and content -- so they don't get any farther
        # Always return leafs
        if pat.type is None:
            raise _EveryNode
        return {pat.type}

    if isinstance(pat, pytree.NegatedPattern):
        if pat.content:
            return 0(pat.content)
        raise _EveryNode # Negated Patterns don't have a type

    if isinstance(pat, pytree.WildcardPattern):
        # Recurse on each node in content
        r = set()
        for p in pat.content:
            for x in p:
                r.update(0(x))
        return r

    raise Exception(""Oh no! I don't understand pattern %s"" %(pat))","{""_get_head_types"": ""0""}"
570,570,"def _get_headnode_dict(fixer_list):
    """""" Accepts a list of fixers and returns a dictionary
        of head node type --> fixer list.  """"""
    head_nodes = collections.defaultdict(list)
    every = []
    for fixer in fixer_list:
        if fixer.pattern:
            try:
                heads = _get_head_types(fixer.pattern)
            except _EveryNode:
                every.append(fixer)
            else:
                for node_type in heads:
                    head_nodes[node_type].append(fixer)
        else:
            if fixer._accept_type is not None:
                head_nodes[fixer._accept_type].append(fixer)
            else:
                every.append(fixer)
    for node_type in chain(pygram.python_grammar.symbol2number.values(),
                           pygram.python_grammar.tokens):
        head_nodes[node_type].extend(every)
    return dict(head_nodes)","def _get_head_types(pat):
    """""" Accepts a pytree Pattern Node and returns a set
        of the pattern types which will match first. """"""

    if isinstance(pat, (pytree.NodePattern, pytree.LeafPattern)):
        # NodePatters must either have no type and no content
        #   or a type and content -- so they don't get any farther
        # Always return leafs
        if pat.type is None:
            raise _EveryNode
        return {pat.type}

    if isinstance(pat, pytree.NegatedPattern):
        if pat.content:
            return _get_head_types(pat.content)
        raise _EveryNode # Negated Patterns don't have a type

    if isinstance(pat, pytree.WildcardPattern):
        # Recurse on each node in content
        r = set()
        for p in pat.content:
            for x in p:
                r.update(_get_head_types(x))
        return r

    raise Exception(""Oh no! I don't understand pattern %s"" %(pat))","def get_head_types_from_pattern(pat):
    """""" Accepts a pytree Pattern Node and returns a set
        of the pattern types which will match first. """"""

    if isinstance(pat, (pytree.NodePattern, pytree.LeafPattern)):
        # NodePatters must either have no type and no content
        #   or a type and content -- so they don't get any farther
        # Always return leafs
        if pat.type is None:
            raise _EveryNode
        return {pat.type}

    if isinstance(pat, pytree.NegatedPattern):
        if pat.content:
            return get_head_types_from_pattern(pat.content)
        raise _EveryNode # Negated Patterns don't have a type

    if isinstance(pat, pytree.WildcardPattern):
        # Recurse on each node in content
        r = set()
        for p in pat.content:
            for x in p:
                r.update(get_head_types_from_pattern(x))
        return r

    raise Exception(""Oh no! I don't understand pattern %s"" %(pat))","def return_dictionary_of_head_node_types(fixer_list):
    """""" Accepts a list of fixers and returns a dictionary
        of head node type --> fixer list.  """"""
    head_nodes = collections.defaultdict(list)
    every = []
    for fixer in fixer_list:
        if fixer.pattern:
            try:
                heads = get_head_types_from_pattern(fixer.pattern)
            except _EveryNode:
                every.append(fixer)
            else:
                for node_type in heads:
                    head_nodes[node_type].append(fixer)
        else:
            if fixer._accept_type is not None:
                head_nodes[fixer._accept_type].append(fixer)
            else:
                every.append(fixer)
    for node_type in chain(pygram.python_grammar.symbol2number.values(),
                           pygram.python_grammar.tokens):
        head_nodes[node_type].extend(every)
    return dict(head_nodes)","{""_get_head_types"": ""get_head_types_from_pattern""}","def 0(pat):
    """""" Accepts a pytree Pattern Node and returns a set
        of the pattern types which will match first. """"""

    if isinstance(pat, (pytree.NodePattern, pytree.LeafPattern)):
        # NodePatters must either have no type and no content
        #   or a type and content -- so they don't get any farther
        # Always return leafs
        if pat.type is None:
            raise _EveryNode
        return {pat.type}

    if isinstance(pat, pytree.NegatedPattern):
        if pat.content:
            return 0(pat.content)
        raise _EveryNode # Negated Patterns don't have a type

    if isinstance(pat, pytree.WildcardPattern):
        # Recurse on each node in content
        r = set()
        for p in pat.content:
            for x in p:
                r.update(0(x))
        return r

    raise Exception(""Oh no! I don't understand pattern %s"" %(pat))","def 1(fixer_list):
    """""" Accepts a list of fixers and returns a dictionary
        of head node type --> fixer list.  """"""
    head_nodes = collections.defaultdict(list)
    every = []
    for fixer in fixer_list:
        if fixer.pattern:
            try:
                heads = 0(fixer.pattern)
            except _EveryNode:
                every.append(fixer)
            else:
                for node_type in heads:
                    head_nodes[node_type].append(fixer)
        else:
            if fixer._accept_type is not None:
                head_nodes[fixer._accept_type].append(fixer)
            else:
                every.append(fixer)
    for node_type in chain(pygram.python_grammar.symbol2number.values(),
                           pygram.python_grammar.tokens):
        head_nodes[node_type].extend(every)
    return dict(head_nodes)","{""_get_head_types"": ""0""}"
571,571,"def get_fixers_from_package(pkg_name):
    """"""
    Return the fully qualified names for fixers in the package pkg_name.
    """"""
    return [pkg_name + ""."" + fix_name
            for fix_name in get_all_fix_names(pkg_name, False)]","def get_all_fix_names(fixer_pkg, remove_prefix=True):
    """"""Return a sorted list of all available fix names in the given package.""""""
    pkg = __import__(fixer_pkg, [], [], [""*""])
    fix_names = []
    for finder, name, ispkg in pkgutil.iter_modules(pkg.__path__):
        if name.startswith(""fix_""):
            if remove_prefix:
                name = name[4:]
            fix_names.append(name)
    return fix_names","def get_all_fix_names_in_package(fixer_pkg, remove_prefix=True):
    """"""Return a sorted list of all available fix names in the given package.""""""
    pkg = __import__(fixer_pkg, [], [], [""*""])
    fix_names = []
    for finder, name, ispkg in pkgutil.iter_modules(pkg.__path__):
        if name.startswith(""fix_""):
            if remove_prefix:
                name = name[4:]
            fix_names.append(name)
    return fix_names","def return_fully_qualified_names_for_fixers(pkg_name):
    """"""
    Return the fully qualified names for fixers in the package pkg_name.
    """"""
    return [pkg_name + ""."" + fix_name
            for fix_name in get_all_fix_names_in_package(pkg_name, False)]","{""get_all_fix_names"": ""get_all_fix_names_in_package""}","def 0(fixer_pkg, remove_prefix=True):
    """"""Return a sorted list of all available fix names in the given package.""""""
    pkg = __import__(fixer_pkg, [], [], [""*""])
    fix_names = []
    for finder, name, ispkg in pkgutil.iter_modules(pkg.__path__):
        if name.startswith(""fix_""):
            if remove_prefix:
                name = name[4:]
            fix_names.append(name)
    return fix_names","def 1(pkg_name):
    """"""
    Return the fully qualified names for fixers in the package pkg_name.
    """"""
    return [pkg_name + ""."" + fix_name
            for fix_name in 0(pkg_name, False)]","{""get_all_fix_names"": ""0""}"
572,572,"def rec_test(sequence, test_func):
    """"""Tests test_func on all items of sequence and items of included
    sub-iterables""""""
    for x in sequence:
        if isinstance(x, (list, tuple)):
            yield from rec_test(x, test_func)
        else:
            yield test_func(x)","def rec_test(sequence, test_func):
    """"""Tests test_func on all items of sequence and items of included
    sub-iterables""""""
    for x in sequence:
        if isinstance(x, (list, tuple)):
            yield from rec_test(x, test_func)
        else:
            yield test_func(x)","def recursive_test_on_sequence(sequence, test_func):
    """"""Tests test_func on all items of sequence and items of included
    sub-iterables""""""
    for x in sequence:
        if isinstance(x, (list, tuple)):
            yield from recursive_test_on_sequence(x, test_func)
        else:
            yield test_func(x)","def recursive_test_on_sequence(sequence, test_func):
    """"""Tests test_func on all items of sequence and items of included
    sub-iterables""""""
    for x in sequence:
        if isinstance(x, (list, tuple)):
            yield from recursive_test_on_sequence(x, test_func)
        else:
            yield test_func(x)","{""rec_test"": ""recursive_test_on_sequence""}","def 0(sequence, test_func):
    """"""Tests test_func on all items of sequence and items of included
    sub-iterables""""""
    for x in sequence:
        if isinstance(x, (list, tuple)):
            yield from 0(x, test_func)
        else:
            yield test_func(x)","def 0(sequence, test_func):
    """"""Tests test_func on all items of sequence and items of included
    sub-iterables""""""
    for x in sequence:
        if isinstance(x, (list, tuple)):
            yield from 0(x, test_func)
        else:
            yield test_func(x)","{""rec_test"": ""0""}"
573,573,"def Attr(obj, attr):
    """"""A node tuple for obj.attr""""""
    return [obj, Node(syms.trailer, [Dot(), attr])]","def Dot():
    """"""A period (.) leaf""""""
    return Leaf(token.DOT, ""."")","def create_dot_leaf():
    """"""A period (.) leaf""""""
    return Leaf(token.DOT, ""."")","def create_attribute_node(obj, attr):
    """"""A node tuple for obj.attr""""""
    return [obj, Node(syms.trailer, [create_dot_leaf(), attr])]","{""Dot"": ""create_dot_leaf""}","def 0():
    """"""A period (.) leaf""""""
    return Leaf(token.DOT, ""."")","def 1(obj, attr):
    """"""A node tuple for obj.attr""""""
    return [obj, Node(syms.trailer, [0(), attr])]","{""Dot"": ""0""}"
574,574,"def Call(func_name, args=None, prefix=None):
    """"""A function call""""""
    node = Node(syms.power, [func_name, ArgList(args)])
    if prefix is not None:
        node.prefix = prefix
    return node","def ArgList(args, lparen=LParen(), rparen=RParen()):
    """"""A parenthesised argument list, used by Call()""""""
    node = Node(syms.trailer, [lparen.clone(), rparen.clone()])
    if args:
        node.insert_child(1, Node(syms.arglist, args))
    return node","def create_argument_list_node(args, lparen=LParen(), rparen=RParen()):
    """"""A parenthesised argument list, used by Call()""""""
    node = Node(syms.trailer, [lparen.clone(), rparen.clone()])
    if args:
        node.insert_child(1, Node(syms.arglist, args))
    return node","def create_function_call_node(func_name, args=None, prefix=None):
    """"""A function call""""""
    node = Node(syms.power, [func_name, create_argument_list_node(args)])
    if prefix is not None:
        node.prefix = prefix
    return node","{""ArgList"": ""create_argument_list_node""}","def 0(args, lparen=LParen(), rparen=RParen()):
    """"""A parenthesised argument list, used by Call()""""""
    node = Node(syms.trailer, [lparen.clone(), rparen.clone()])
    if args:
        node.insert_child(1, Node(syms.arglist, args))
    return node","def 1(func_name, args=None, prefix=None):
    """"""A function call""""""
    node = Node(syms.power, [func_name, 0(args)])
    if prefix is not None:
        node.prefix = prefix
    return node","{""ArgList"": ""0""}"
575,575,"def ImportAndCall(node, results, names):
    """"""Returns an import statement and calls a method
    of the module:

    import module
    module.name()""""""
    obj = results[""obj""].clone()
    if obj.type == syms.arglist:
        newarglist = obj.clone()
    else:
        newarglist = Node(syms.arglist, [obj.clone()])
    after = results[""after""]
    if after:
        after = [n.clone() for n in after]
    new = Node(syms.power,
               Attr(Name(names[0]), Name(names[1])) +
               [Node(syms.trailer,
                     [results[""lpar""].clone(),
                      newarglist,
                      results[""rpar""].clone()])] + after)
    new.prefix = node.prefix
    return new","def Attr(obj, attr):
    """"""A node tuple for obj.attr""""""
    return [obj, Node(syms.trailer, [Dot(), attr])]

def Name(name, prefix=None):
    """"""Return a NAME leaf""""""
    return Leaf(token.NAME, name, prefix=prefix)","def create_attribute_node(obj, attr):
    """"""A node tuple for obj.attr""""""
    return [obj, Node(syms.trailer, [Dot(), attr])]

def create_name_leaf(name, prefix=None):
    """"""Return a NAME leaf""""""
    return Leaf(token.NAME, name, prefix=prefix)","def return_import_statement_and_method_call_node(node, results, names):
    """"""Returns an import statement and calls a method
    of the module:

    import module
    module.name()""""""
    obj = results[""obj""].clone()
    if obj.type == syms.arglist:
        newarglist = obj.clone()
    else:
        newarglist = Node(syms.arglist, [obj.clone()])
    after = results[""after""]
    if after:
        after = [n.clone() for n in after]
    new = Node(syms.power,
               create_attribute_node(create_name_leaf(names[0]), create_name_leaf(names[1])) +
               [Node(syms.trailer,
                     [results[""lpar""].clone(),
                      newarglist,
                      results[""rpar""].clone()])] + after)
    new.prefix = node.prefix
    return new","{""Attr"": ""create_attribute_node"", ""Name"": ""create_name_leaf""}","def 0(obj, attr):
    """"""A node tuple for obj.attr""""""
    return [obj, Node(syms.trailer, [Dot(), attr])]

def 1(name, prefix=None):
    """"""Return a NAME leaf""""""
    return Leaf(token.NAME, name, prefix=prefix)","def 2(node, results, names):
    """"""Returns an import statement and calls a method
    of the module:

    import module
    module.name()""""""
    obj = results[""obj""].clone()
    if obj.type == syms.arglist:
        newarglist = obj.clone()
    else:
        newarglist = Node(syms.arglist, [obj.clone()])
    after = results[""after""]
    if after:
        after = [n.clone() for n in after]
    new = Node(syms.power,
               0(1(names[0]), 1(names[1])) +
               [Node(syms.trailer,
                     [results[""lpar""].clone(),
                      newarglist,
                      results[""rpar""].clone()])] + after)
    new.prefix = node.prefix
    return new","{""Attr"": ""0"", ""Name"": ""1""}"
576,576,"def is_tuple(node):
    """"""Does the node represent a tuple literal?""""""
    if isinstance(node, Node) and node.children == [LParen(), RParen()]:
        return True
    return (isinstance(node, Node)
            and len(node.children) == 3
            and isinstance(node.children[0], Leaf)
            and isinstance(node.children[1], Node)
            and isinstance(node.children[2], Leaf)
            and node.children[0].value == ""(""
            and node.children[2].value == "")"")","def RParen():
    return Leaf(token.RPAR, "")"")

def LParen():
    return Leaf(token.LPAR, ""("")","def create_right_paren_leaf():
    return Leaf(token.RPAR, "")"")

def create_left_paren_leaf():
    return Leaf(token.LPAR, ""("")","def check_if_node_represents_tuple_literal(node):
    """"""Does the node represent a tuple literal?""""""
    if isinstance(node, Node) and node.children == [create_left_paren_leaf(), create_right_paren_leaf()]:
        return True
    return (isinstance(node, Node)
            and len(node.children) == 3
            and isinstance(node.children[0], Leaf)
            and isinstance(node.children[1], Node)
            and isinstance(node.children[2], Leaf)
            and node.children[0].value == ""(""
            and node.children[2].value == "")"")","{""RParen"": ""create_right_paren_leaf"", ""LParen"": ""create_left_paren_leaf""}","def 0():
    return Leaf(token.RPAR, "")"")

def 1():
    return Leaf(token.LPAR, ""("")","def 2(node):
    """"""Does the node represent a tuple literal?""""""
    if isinstance(node, Node) and node.children == [1(), 0()]:
        return True
    return (isinstance(node, Node)
            and len(node.children) == 3
            and isinstance(node.children[0], Leaf)
            and isinstance(node.children[1], Node)
            and isinstance(node.children[2], Leaf)
            and node.children[0].value == ""(""
            and node.children[2].value == "")"")","{""RParen"": ""0"", ""LParen"": ""1""}"
577,577,"def in_special_context(node):
    """""" Returns true if node is in an environment where all that is required
        of it is being iterable (ie, it doesn't matter if it returns a list
        or an iterator).
        See test_map_nochange in test_fixers.py for some examples and tests.
        """"""
    global p0, p1, p2, pats_built
    if not pats_built:
        p0 = patcomp.compile_pattern(p0)
        p1 = patcomp.compile_pattern(p1)
        p2 = patcomp.compile_pattern(p2)
        pats_built = True
    patterns = [p0, p1, p2]
    for pattern, parent in zip(patterns, attr_chain(node, ""parent"")):
        results = {}
        if pattern.match(parent, results) and results[""node""] is node:
            return True
    return False","def attr_chain(obj, attr):
    """"""Follow an attribute chain.

    If you have a chain of objects where a.foo -> b, b.foo-> c, etc,
    use this to iterate over all objects in the chain. Iteration is
    terminated by getattr(x, attr) is None.

    Args:
        obj: the starting object
        attr: the name of the chaining attribute

    Yields:
        Each successive object in the chain.
    """"""
    next = getattr(obj, attr)
    while next:
        yield next
        next = getattr(next, attr)","def follow_attribute_chain(obj, attr):
    """"""Follow an attribute chain.

    If you have a chain of objects where a.foo -> b, b.foo-> c, etc,
    use this to iterate over all objects in the chain. Iteration is
    terminated by getattr(x, attr) is None.

    Args:
        obj: the starting object
        attr: the name of the chaining attribute

    Yields:
        Each successive object in the chain.
    """"""
    next = getattr(obj, attr)
    while next:
        yield next
        next = getattr(next, attr)","def check_if_node_is_in_iterable_context(node):
    """""" Returns true if node is in an environment where all that is required
        of it is being iterable (ie, it doesn't matter if it returns a list
        or an iterator).
        See test_map_nochange in test_fixers.py for some examples and tests.
        """"""
    global p0, p1, p2, pats_built
    if not pats_built:
        p0 = patcomp.compile_pattern(p0)
        p1 = patcomp.compile_pattern(p1)
        p2 = patcomp.compile_pattern(p2)
        pats_built = True
    patterns = [p0, p1, p2]
    for pattern, parent in zip(patterns, follow_attribute_chain(node, ""parent"")):
        results = {}
        if pattern.match(parent, results) and results[""node""] is node:
            return True
    return False","{""attr_chain"": ""follow_attribute_chain""}","def 0(obj, attr):
    """"""Follow an attribute chain.

    If you have a chain of objects where a.foo -> b, b.foo-> c, etc,
    use this to iterate over all objects in the chain. Iteration is
    terminated by getattr(x, attr) is None.

    Args:
        obj: the starting object
        attr: the name of the chaining attribute

    Yields:
        Each successive object in the chain.
    """"""
    next = getattr(obj, attr)
    while next:
        yield next
        next = getattr(next, attr)","def 1(node):
    """""" Returns true if node is in an environment where all that is required
        of it is being iterable (ie, it doesn't matter if it returns a list
        or an iterator).
        See test_map_nochange in test_fixers.py for some examples and tests.
        """"""
    global p0, p1, p2, pats_built
    if not pats_built:
        p0 = patcomp.compile_pattern(p0)
        p1 = patcomp.compile_pattern(p1)
        p2 = patcomp.compile_pattern(p2)
        pats_built = True
    patterns = [p0, p1, p2]
    for pattern, parent in zip(patterns, 0(node, ""parent"")):
        results = {}
        if pattern.match(parent, results) and results[""node""] is node:
            return True
    return False","{""attr_chain"": ""0""}"
578,578,"def does_tree_import(package, name, node):
    """""" Returns true if name is imported from package at the
        top level of the tree which node belongs to.
        To cover the case of an import like 'import foo', use
        None for the package and 'foo' for the name. """"""
    binding = find_binding(name, find_root(node), package)
    return bool(binding)","def find_binding(name, node, package=None):
    """""" Returns the node which binds variable name, otherwise None.
        If optional argument package is supplied, only imports will
        be returned.
        See test cases for examples.""""""
    for child in node.children:
        ret = None
        if child.type == syms.for_stmt:
            if _find(name, child.children[1]):
                return child
            n = find_binding(name, make_suite(child.children[-1]), package)
            if n: ret = n
        elif child.type in (syms.if_stmt, syms.while_stmt):
            n = find_binding(name, make_suite(child.children[-1]), package)
            if n: ret = n
        elif child.type == syms.try_stmt:
            n = find_binding(name, make_suite(child.children[2]), package)
            if n:
                ret = n
            else:
                for i, kid in enumerate(child.children[3:]):
                    if kid.type == token.COLON and kid.value == "":"":
                        # i+3 is the colon, i+4 is the suite
                        n = find_binding(name, make_suite(child.children[i+4]), package)
                        if n: ret = n
        elif child.type in _def_syms and child.children[1].value == name:
            ret = child
        elif _is_import_binding(child, name, package):
            ret = child
        elif child.type == syms.simple_stmt:
            ret = find_binding(name, child, package)
        elif child.type == syms.expr_stmt:
            if _find(name, child.children[0]):
                ret = child

        if ret:
            if not package:
                return ret
            if is_import(ret):
                return ret
    return None

def find_root(node):
    """"""Find the top level namespace.""""""
    # Scamper up to the top level namespace
    while node.type != syms.file_input:
        node = node.parent
        if not node:
            raise ValueError(""root found before file_input node was found."")
    return node","def find_variable_binding_node(name, node, package=None):
    """""" Returns the node which binds variable name, otherwise None.
        If optional argument package is supplied, only imports will
        be returned.
        See test cases for examples.""""""
    for child in node.children:
        ret = None
        if child.type == syms.for_stmt:
            if _find(name, child.children[1]):
                return child
            n = find_variable_binding_node(name, make_suite(child.children[-1]), package)
            if n: ret = n
        elif child.type in (syms.if_stmt, syms.while_stmt):
            n = find_variable_binding_node(name, make_suite(child.children[-1]), package)
            if n: ret = n
        elif child.type == syms.try_stmt:
            n = find_variable_binding_node(name, make_suite(child.children[2]), package)
            if n:
                ret = n
            else:
                for i, kid in enumerate(child.children[3:]):
                    if kid.type == token.COLON and kid.value == "":"":
                        # i+3 is the colon, i+4 is the suite
                        n = find_variable_binding_node(name, make_suite(child.children[i+4]), package)
                        if n: ret = n
        elif child.type in _def_syms and child.children[1].value == name:
            ret = child
        elif _is_import_binding(child, name, package):
            ret = child
        elif child.type == syms.simple_stmt:
            ret = find_variable_binding_node(name, child, package)
        elif child.type == syms.expr_stmt:
            if _find(name, child.children[0]):
                ret = child

        if ret:
            if not package:
                return ret
            if is_import(ret):
                return ret
    return None

def find_top_level_namespace_node(node):
    """"""Find the top level namespace.""""""
    # Scamper up to the top level namespace
    while node.type != syms.file_input:
        node = node.parent
        if not node:
            raise ValueError(""root found before file_input node was found."")
    return node","def does_tree_import_checking_top_level(package, name, node):
    """""" Returns true if name is imported from package at the
        top level of the tree which node belongs to.
        To cover the case of an import like 'import foo', use
        None for the package and 'foo' for the name. """"""
    binding = find_variable_binding_node(name, find_top_level_namespace_node(node), package)
    return bool(binding)","{""find_binding"": ""find_variable_binding_node"", ""find_root"": ""find_top_level_namespace_node""}","def 0(name, node, package=None):
    """""" Returns the node which binds variable name, otherwise None.
        If optional argument package is supplied, only imports will
        be returned.
        See test cases for examples.""""""
    for child in node.children:
        ret = None
        if child.type == syms.for_stmt:
            if _find(name, child.children[1]):
                return child
            n = 0(name, make_suite(child.children[-1]), package)
            if n: ret = n
        elif child.type in (syms.if_stmt, syms.while_stmt):
            n = 0(name, make_suite(child.children[-1]), package)
            if n: ret = n
        elif child.type == syms.try_stmt:
            n = 0(name, make_suite(child.children[2]), package)
            if n:
                ret = n
            else:
                for i, kid in enumerate(child.children[3:]):
                    if kid.type == token.COLON and kid.value == "":"":
                        # i+3 is the colon, i+4 is the suite
                        n = 0(name, make_suite(child.children[i+4]), package)
                        if n: ret = n
        elif child.type in _def_syms and child.children[1].value == name:
            ret = child
        elif _is_import_binding(child, name, package):
            ret = child
        elif child.type == syms.simple_stmt:
            ret = 0(name, child, package)
        elif child.type == syms.expr_stmt:
            if _find(name, child.children[0]):
                ret = child

        if ret:
            if not package:
                return ret
            if is_import(ret):
                return ret
    return None

def 1(node):
    """"""Find the top level namespace.""""""
    # Scamper up to the top level namespace
    while node.type != syms.file_input:
        node = node.parent
        if not node:
            raise ValueError(""root found before file_input node was found."")
    return node","def 2(package, name, node):
    """""" Returns true if name is imported from package at the
        top level of the tree which node belongs to.
        To cover the case of an import like 'import foo', use
        None for the package and 'foo' for the name. """"""
    binding = 0(name, 1(node), package)
    return bool(binding)","{""find_binding"": ""0"", ""find_root"": ""1""}"
579,579,"def generate_matches(patterns, nodes):
    """"""
    Generator yielding matches for a sequence of patterns and nodes.

    Args:
        patterns: a sequence of patterns
        nodes: a sequence of nodes

    Yields:
        (count, results) tuples where:
        count: the entire sequence of patterns matches nodes[:count];
        results: dict containing named submatches.
        """"""
    if not patterns:
        yield 0, {}
    else:
        p, rest = patterns[0], patterns[1:]
        for c0, r0 in p.generate_matches(nodes):
            if not rest:
                yield c0, r0
            else:
                for c1, r1 in generate_matches(rest, nodes[c0:]):
                    r = {}
                    r.update(r0)
                    r.update(r1)
                    yield c0 + c1, r","def generate_matches(patterns, nodes):
    """"""
    Generator yielding matches for a sequence of patterns and nodes.

    Args:
        patterns: a sequence of patterns
        nodes: a sequence of nodes

    Yields:
        (count, results) tuples where:
        count: the entire sequence of patterns matches nodes[:count];
        results: dict containing named submatches.
        """"""
    if not patterns:
        yield 0, {}
    else:
        p, rest = patterns[0], patterns[1:]
        for c0, r0 in p.generate_matches(nodes):
            if not rest:
                yield c0, r0
            else:
                for c1, r1 in generate_matches(rest, nodes[c0:]):
                    r = {}
                    r.update(r0)
                    r.update(r1)
                    yield c0 + c1, r","def generate_matches_for_patterns(patterns, nodes):
    """"""
    Generator yielding matches for a sequence of patterns and nodes.

    Args:
        patterns: a sequence of patterns
        nodes: a sequence of nodes

    Yields:
        (count, results) tuples where:
        count: the entire sequence of patterns matches nodes[:count];
        results: dict containing named submatches.
        """"""
    if not patterns:
        yield 0, {}
    else:
        p, rest = patterns[0], patterns[1:]
        for c0, r0 in p.generate_matches_for_patterns(nodes):
            if not rest:
                yield c0, r0
            else:
                for c1, r1 in generate_matches_for_patterns(rest, nodes[c0:]):
                    r = {}
                    r.update(r0)
                    r.update(r1)
                    yield c0 + c1, r","def generate_matches_for_patterns_for_patterns(patterns, nodes):
    """"""
    Generator yielding matches for a sequence of patterns and nodes.

    Args:
        patterns: a sequence of patterns
        nodes: a sequence of nodes

    Yields:
        (count, results) tuples where:
        count: the entire sequence of patterns matches nodes[:count];
        results: dict containing named submatches.
        """"""
    if not patterns:
        yield 0, {}
    else:
        p, rest = patterns[0], patterns[1:]
        for c0, r0 in p.generate_matches_for_patterns_for_patterns(nodes):
            if not rest:
                yield c0, r0
            else:
                for c1, r1 in generate_matches_for_patterns_for_patterns(rest, nodes[c0:]):
                    r = {}
                    r.update(r0)
                    r.update(r1)
                    yield c0 + c1, r","{""generate_matches"": ""generate_matches_for_patterns""}","def 0(patterns, nodes):
    """"""
    Generator yielding matches for a sequence of patterns and nodes.

    Args:
        patterns: a sequence of patterns
        nodes: a sequence of nodes

    Yields:
        (count, results) tuples where:
        count: the entire sequence of patterns matches nodes[:count];
        results: dict containing named submatches.
        """"""
    if not patterns:
        yield 0, {}
    else:
        p, rest = patterns[0], patterns[1:]
        for c0, r0 in p.0(nodes):
            if not rest:
                yield c0, r0
            else:
                for c1, r1 in 0(rest, nodes[c0:]):
                    r = {}
                    r.update(r0)
                    r.update(r1)
                    yield c0 + c1, r","def 0(patterns, nodes):
    """"""
    Generator yielding matches for a sequence of patterns and nodes.

    Args:
        patterns: a sequence of patterns
        nodes: a sequence of nodes

    Yields:
        (count, results) tuples where:
        count: the entire sequence of patterns matches nodes[:count];
        results: dict containing named submatches.
        """"""
    if not patterns:
        yield 0, {}
    else:
        p, rest = patterns[0], patterns[1:]
        for c0, r0 in p.0(nodes):
            if not rest:
                yield c0, r0
            else:
                for c1, r1 in 0(rest, nodes[c0:]):
                    r = {}
                    r.update(r0)
                    r.update(r1)
                    yield c0 + c1, r","{""generate_matches"": ""0""}"
580,580,"def has_metaclass(parent):
    """""" we have to check the cls_node without changing it.
        There are two possibilities:
          1)  clsdef => suite => simple_stmt => expr_stmt => Leaf('__meta')
          2)  clsdef => simple_stmt => expr_stmt => Leaf('__meta')
    """"""
    for node in parent.children:
        if node.type == syms.suite:
            return has_metaclass(node)
        elif node.type == syms.simple_stmt and node.children:
            expr_node = node.children[0]
            if expr_node.type == syms.expr_stmt and expr_node.children:
                left_side = expr_node.children[0]
                if isinstance(left_side, Leaf) and \
                        left_side.value == '__metaclass__':
                    return True
    return False","def has_metaclass(parent):
    """""" we have to check the cls_node without changing it.
        There are two possibilities:
          1)  clsdef => suite => simple_stmt => expr_stmt => Leaf('__meta')
          2)  clsdef => simple_stmt => expr_stmt => Leaf('__meta')
    """"""
    for node in parent.children:
        if node.type == syms.suite:
            return has_metaclass(node)
        elif node.type == syms.simple_stmt and node.children:
            expr_node = node.children[0]
            if expr_node.type == syms.expr_stmt and expr_node.children:
                left_side = expr_node.children[0]
                if isinstance(left_side, Leaf) and \
                        left_side.value == '__metaclass__':
                    return True
    return False","def check_class_metaclass(parent):
    """""" we have to check the cls_node without changing it.
        There are two possibilities:
          1)  clsdef => suite => simple_stmt => expr_stmt => Leaf('__meta')
          2)  clsdef => simple_stmt => expr_stmt => Leaf('__meta')
    """"""
    for node in parent.children:
        if node.type == syms.suite:
            return check_class_metaclass(node)
        elif node.type == syms.simple_stmt and node.children:
            expr_node = node.children[0]
            if expr_node.type == syms.expr_stmt and expr_node.children:
                left_side = expr_node.children[0]
                if isinstance(left_side, Leaf) and \
                        left_side.value == '__metaclass__':
                    return True
    return False","def check_class_metaclass(parent):
    """""" we have to check the cls_node without changing it.
        There are two possibilities:
          1)  clsdef => suite => simple_stmt => expr_stmt => Leaf('__meta')
          2)  clsdef => simple_stmt => expr_stmt => Leaf('__meta')
    """"""
    for node in parent.children:
        if node.type == syms.suite:
            return check_class_metaclass(node)
        elif node.type == syms.simple_stmt and node.children:
            expr_node = node.children[0]
            if expr_node.type == syms.expr_stmt and expr_node.children:
                left_side = expr_node.children[0]
                if isinstance(left_side, Leaf) and \
                        left_side.value == '__metaclass__':
                    return True
    return False","{""has_metaclass"": ""check_class_metaclass""}","def 0(parent):
    """""" we have to check the cls_node without changing it.
        There are two possibilities:
          1)  clsdef => suite => simple_stmt => expr_stmt => Leaf('__meta')
          2)  clsdef => simple_stmt => expr_stmt => Leaf('__meta')
    """"""
    for node in parent.children:
        if node.type == syms.suite:
            return 0(node)
        elif node.type == syms.simple_stmt and node.children:
            expr_node = node.children[0]
            if expr_node.type == syms.expr_stmt and expr_node.children:
                left_side = expr_node.children[0]
                if isinstance(left_side, Leaf) and \
                        left_side.value == '__metaclass__':
                    return True
    return False","def 0(parent):
    """""" we have to check the cls_node without changing it.
        There are two possibilities:
          1)  clsdef => suite => simple_stmt => expr_stmt => Leaf('__meta')
          2)  clsdef => simple_stmt => expr_stmt => Leaf('__meta')
    """"""
    for node in parent.children:
        if node.type == syms.suite:
            return 0(node)
        elif node.type == syms.simple_stmt and node.children:
            expr_node = node.children[0]
            if expr_node.type == syms.expr_stmt and expr_node.children:
                left_side = expr_node.children[0]
                if isinstance(left_side, Leaf) and \
                        left_side.value == '__metaclass__':
                    return True
    return False","{""has_metaclass"": ""0""}"
581,581,"def fixup_parse_tree(cls_node):
    """""" one-line classes don't get a suite in the parse tree so we add
        one to normalize the tree
    """"""
    for node in cls_node.children:
        if node.type == syms.suite:
            # already in the preferred format, do nothing
            return

    # !%@#! one-liners have no suite node, we have to fake one up
    for i, node in enumerate(cls_node.children):
        if node.type == token.COLON:
            break
    else:
        raise ValueError(""No class suite and no ':'!"")

    # move everything into a suite node
    suite = Node(syms.suite, [])
    while cls_node.children[i+1:]:
        move_node = cls_node.children[i+1]
        suite.append_child(move_node.clone())
        move_node.remove()
    cls_node.append_child(suite)
    node = suite","def enumerate():
    """"""Return a list of all Thread objects currently alive.

    The list includes daemonic threads, dummy thread objects created by
    current_thread(), and the main thread. It excludes terminated threads and
    threads that have not yet been started.

    """"""
    with _active_limbo_lock:
        return list(_active.values()) + list(_limbo.values())","def get_alive_threads():
    """"""Return a list of all Thread objects currently alive.

    The list includes daemonic threads, dummy thread objects created by
    current_thread(), and the main thread. It excludes terminated threads and
    threads that have not yet been started.

    """"""
    with _active_limbo_lock:
        return list(_active.values()) + list(_limbo.values())","def fixup_parse_tree_ensure_suite_format(cls_node):
    """""" one-line classes don't get a suite in the parse tree so we add
        one to normalize the tree
    """"""
    for node in cls_node.children:
        if node.type == syms.suite:
            # already in the preferred format, do nothing
            return

    # !%@#! one-liners have no suite node, we have to fake one up
    for i, node in get_alive_threads(cls_node.children):
        if node.type == token.COLON:
            break
    else:
        raise ValueError(""No class suite and no ':'!"")

    # move everything into a suite node
    suite = Node(syms.suite, [])
    while cls_node.children[i+1:]:
        move_node = cls_node.children[i+1]
        suite.append_child(move_node.clone())
        move_node.remove()
    cls_node.append_child(suite)
    node = suite","{""enumerate"": ""get_alive_threads""}","def 0():
    """"""Return a list of all Thread objects currently alive.

    The list includes daemonic threads, dummy thread objects created by
    current_thread(), and the main thread. It excludes terminated threads and
    threads that have not yet been started.

    """"""
    with _active_limbo_lock:
        return list(_active.values()) + list(_limbo.values())","def 1(cls_node):
    """""" one-line classes don't get a suite in the parse tree so we add
        one to normalize the tree
    """"""
    for node in cls_node.children:
        if node.type == syms.suite:
            # already in the preferred format, do nothing
            return

    # !%@#! one-liners have no suite node, we have to fake one up
    for i, node in 0(cls_node.children):
        if node.type == token.COLON:
            break
    else:
        raise ValueError(""No class suite and no ':'!"")

    # move everything into a suite node
    suite = Node(syms.suite, [])
    while cls_node.children[i+1:]:
        move_node = cls_node.children[i+1]
        suite.append_child(move_node.clone())
        move_node.remove()
    cls_node.append_child(suite)
    node = suite","{""enumerate"": ""0""}"
582,582,"def fixup_simple_stmt(parent, i, stmt_node):
    """""" if there is a semi-colon all the parts count as part of the same
        simple_stmt.  We just want the __metaclass__ part so we move
        everything after the semi-colon into its own simple_stmt node
    """"""
    for semi_ind, node in enumerate(stmt_node.children):
        if node.type == token.SEMI: # *sigh*
            break
    else:
        return

    node.remove() # kill the semicolon
    new_expr = Node(syms.expr_stmt, [])
    new_stmt = Node(syms.simple_stmt, [new_expr])
    while stmt_node.children[semi_ind:]:
        move_node = stmt_node.children[semi_ind]
        new_expr.append_child(move_node.clone())
        move_node.remove()
    parent.insert_child(i, new_stmt)
    new_leaf1 = new_stmt.children[0].children[0]
    old_leaf1 = stmt_node.children[0].children[0]
    new_leaf1.prefix = old_leaf1.prefix","def enumerate():
    """"""Return a list of all Thread objects currently alive.

    The list includes daemonic threads, dummy thread objects created by
    current_thread(), and the main thread. It excludes terminated threads and
    threads that have not yet been started.

    """"""
    with _active_limbo_lock:
        return list(_active.values()) + list(_limbo.values())","def get_alive_threads():
    """"""Return a list of all Thread objects currently alive.

    The list includes daemonic threads, dummy thread objects created by
    current_thread(), and the main thread. It excludes terminated threads and
    threads that have not yet been started.

    """"""
    with _active_limbo_lock:
        return list(_active.values()) + list(_limbo.values())","def fixup_simple_stmt_semicolon_handling(parent, i, stmt_node):
    """""" if there is a semi-colon all the parts count as part of the same
        simple_stmt.  We just want the __metaclass__ part so we move
        everything after the semi-colon into its own simple_stmt node
    """"""
    for semi_ind, node in get_alive_threads(stmt_node.children):
        if node.type == token.SEMI: # *sigh*
            break
    else:
        return

    node.remove() # kill the semicolon
    new_expr = Node(syms.expr_stmt, [])
    new_stmt = Node(syms.simple_stmt, [new_expr])
    while stmt_node.children[semi_ind:]:
        move_node = stmt_node.children[semi_ind]
        new_expr.append_child(move_node.clone())
        move_node.remove()
    parent.insert_child(i, new_stmt)
    new_leaf1 = new_stmt.children[0].children[0]
    old_leaf1 = stmt_node.children[0].children[0]
    new_leaf1.prefix = old_leaf1.prefix","{""enumerate"": ""get_alive_threads""}","def 0():
    """"""Return a list of all Thread objects currently alive.

    The list includes daemonic threads, dummy thread objects created by
    current_thread(), and the main thread. It excludes terminated threads and
    threads that have not yet been started.

    """"""
    with _active_limbo_lock:
        return list(_active.values()) + list(_limbo.values())","def 1(parent, i, stmt_node):
    """""" if there is a semi-colon all the parts count as part of the same
        simple_stmt.  We just want the __metaclass__ part so we move
        everything after the semi-colon into its own simple_stmt node
    """"""
    for semi_ind, node in 0(stmt_node.children):
        if node.type == token.SEMI: # *sigh*
            break
    else:
        return

    node.remove() # kill the semicolon
    new_expr = Node(syms.expr_stmt, [])
    new_stmt = Node(syms.simple_stmt, [new_expr])
    while stmt_node.children[semi_ind:]:
        move_node = stmt_node.children[semi_ind]
        new_expr.append_child(move_node.clone())
        move_node.remove()
    parent.insert_child(i, new_stmt)
    new_leaf1 = new_stmt.children[0].children[0]
    old_leaf1 = stmt_node.children[0].children[0]
    new_leaf1.prefix = old_leaf1.prefix","{""enumerate"": ""0""}"
583,583,"def load_grammar(gt=""Grammar.txt"", gp=None,
                 save=True, force=False, logger=None):
    """"""Load the grammar (maybe from a pickle).""""""
    if logger is None:
        logger = logging.getLogger()
    gp = _generate_pickle_name(gt) if gp is None else gp
    if force or not _newer(gp, gt):
        logger.info(""Generating grammar tables from %s"", gt)
        g = pgen.generate_grammar(gt)
        if save:
            logger.info(""Writing grammar tables to %s"", gp)
            try:
                g.dump(gp)
            except OSError as e:
                logger.info(""Writing failed: %s"", e)
    else:
        g = grammar.Grammar()
        g.load(gp)
    return g","def _generate_pickle_name(gt):
    head, tail = os.path.splitext(gt)
    if tail == "".txt"":
        tail = """"
    return head + tail + ""."".join(map(str, sys.version_info)) + "".pickle""

def _newer(a, b):
    """"""Inquire whether file a was written since file b.""""""
    if not os.path.exists(a):
        return False
    if not os.path.exists(b):
        return True
    return os.path.getmtime(a) >= os.path.getmtime(b)","def generate_pickle_filename(gt):
    head, tail = os.path.splitext(gt)
    if tail == "".txt"":
        tail = """"
    return head + tail + ""."".join(map(str, sys.version_info)) + "".pickle""

def is_file_a_newer_than_b(a, b):
    """"""Inquire whether file a was written since file b.""""""
    if not os.path.exists(a):
        return False
    if not os.path.exists(b):
        return True
    return os.path.getmtime(a) >= os.path.getmtime(b)","def load_grammar_tables(gt=""Grammar.txt"", gp=None,
                 save=True, force=False, logger=None):
    """"""Load the grammar (maybe from a pickle).""""""
    if logger is None:
        logger = logging.getLogger()
    gp = generate_pickle_filename(gt) if gp is None else gp
    if force or not is_file_a_newer_than_b(gp, gt):
        logger.info(""Generating grammar tables from %s"", gt)
        g = pgen.generate_grammar(gt)
        if save:
            logger.info(""Writing grammar tables to %s"", gp)
            try:
                g.dump(gp)
            except OSError as e:
                logger.info(""Writing failed: %s"", e)
    else:
        g = grammar.Grammar()
        g.load(gp)
    return g","{""_generate_pickle_name"": ""generate_pickle_filename"", ""_newer"": ""is_file_a_newer_than_b""}","def 0(gt):
    head, tail = os.path.splitext(gt)
    if tail == "".txt"":
        tail = """"
    return head + tail + ""."".join(map(str, sys.version_info)) + "".pickle""

def 1(a, b):
    """"""Inquire whether file a was written since file b.""""""
    if not os.path.exists(a):
        return False
    if not os.path.exists(b):
        return True
    return os.path.getmtime(a) >= os.path.getmtime(b)","def 2(gt=""Grammar.txt"", gp=None,
                 save=True, force=False, logger=None):
    """"""Load the grammar (maybe from a pickle).""""""
    if logger is None:
        logger = logging.getLogger()
    gp = 0(gt) if gp is None else gp
    if force or not 1(gp, gt):
        logger.info(""Generating grammar tables from %s"", gt)
        g = pgen.generate_grammar(gt)
        if save:
            logger.info(""Writing grammar tables to %s"", gp)
            try:
                g.dump(gp)
            except OSError as e:
                logger.info(""Writing failed: %s"", e)
    else:
        g = grammar.Grammar()
        g.load(gp)
    return g","{""_generate_pickle_name"": ""0"", ""_newer"": ""1""}"
584,584,"def load_packaged_grammar(package, grammar_source):
    """"""Normally, loads a pickled grammar by doing
        pkgutil.get_data(package, pickled_grammar)
    where *pickled_grammar* is computed from *grammar_source* by adding the
    Python version and using a ``.pickle`` extension.

    However, if *grammar_source* is an extant file, load_grammar(grammar_source)
    is called instead. This facilitates using a packaged grammar file when needed
    but preserves load_grammar's automatic regeneration behavior when possible.

    """"""
    if os.path.isfile(grammar_source):
        return load_grammar(grammar_source)
    pickled_name = _generate_pickle_name(os.path.basename(grammar_source))
    data = pkgutil.get_data(package, pickled_name)
    g = grammar.Grammar()
    g.loads(data)
    return g","def _generate_pickle_name(gt):
    head, tail = os.path.splitext(gt)
    if tail == "".txt"":
        tail = """"
    return head + tail + ""."".join(map(str, sys.version_info)) + "".pickle""

def load_grammar(gt=""Grammar.txt"", gp=None,
                 save=True, force=False, logger=None):
    """"""Load the grammar (maybe from a pickle).""""""
    if logger is None:
        logger = logging.getLogger()
    gp = _generate_pickle_name(gt) if gp is None else gp
    if force or not _newer(gp, gt):
        logger.info(""Generating grammar tables from %s"", gt)
        g = pgen.generate_grammar(gt)
        if save:
            logger.info(""Writing grammar tables to %s"", gp)
            try:
                g.dump(gp)
            except OSError as e:
                logger.info(""Writing failed: %s"", e)
    else:
        g = grammar.Grammar()
        g.load(gp)
    return g","def generate_pickle_filename(gt):
    head, tail = os.path.splitext(gt)
    if tail == "".txt"":
        tail = """"
    return head + tail + ""."".join(map(str, sys.version_info)) + "".pickle""

def load_grammar_tables(gt=""Grammar.txt"", gp=None,
                 save=True, force=False, logger=None):
    """"""Load the grammar (maybe from a pickle).""""""
    if logger is None:
        logger = logging.getLogger()
    gp = generate_pickle_filename(gt) if gp is None else gp
    if force or not _newer(gp, gt):
        logger.info(""Generating grammar tables from %s"", gt)
        g = pgen.generate_grammar(gt)
        if save:
            logger.info(""Writing grammar tables to %s"", gp)
            try:
                g.dump(gp)
            except OSError as e:
                logger.info(""Writing failed: %s"", e)
    else:
        g = grammar.Grammar()
        g.load(gp)
    return g","def load_pickled_grammar_from_package_or_file(package, grammar_source):
    """"""Normally, loads a pickled grammar by doing
        pkgutil.get_data(package, pickled_grammar)
    where *pickled_grammar* is computed from *grammar_source* by adding the
    Python version and using a ``.pickle`` extension.

    However, if *grammar_source* is an extant file, load_grammar_tables(grammar_source)
    is called instead. This facilitates using a packaged grammar file when needed
    but preserves load_grammar_tables's automatic regeneration behavior when possible.

    """"""
    if os.path.isfile(grammar_source):
        return load_grammar_tables(grammar_source)
    pickled_name = generate_pickle_filename(os.path.basename(grammar_source))
    data = pkgutil.get_data(package, pickled_name)
    g = grammar.Grammar()
    g.loads(data)
    return g","{""_generate_pickle_name"": ""generate_pickle_filename"", ""load_grammar"": ""load_grammar_tables""}","def 0(gt):
    head, tail = os.path.splitext(gt)
    if tail == "".txt"":
        tail = """"
    return head + tail + ""."".join(map(str, sys.version_info)) + "".pickle""

def 1(gt=""Grammar.txt"", gp=None,
                 save=True, force=False, logger=None):
    """"""Load the grammar (maybe from a pickle).""""""
    if logger is None:
        logger = logging.getLogger()
    gp = 0(gt) if gp is None else gp
    if force or not _newer(gp, gt):
        logger.info(""Generating grammar tables from %s"", gt)
        g = pgen.generate_grammar(gt)
        if save:
            logger.info(""Writing grammar tables to %s"", gp)
            try:
                g.dump(gp)
            except OSError as e:
                logger.info(""Writing failed: %s"", e)
    else:
        g = grammar.Grammar()
        g.load(gp)
    return g","def 2(package, grammar_source):
    """"""Normally, loads a pickled grammar by doing
        pkgutil.get_data(package, pickled_grammar)
    where *pickled_grammar* is computed from *grammar_source* by adding the
    Python version and using a ``.pickle`` extension.

    However, if *grammar_source* is an extant file, 1(grammar_source)
    is called instead. This facilitates using a packaged grammar file when needed
    but preserves load_grammar's automatic regeneration behavior when possible.

    """"""
    if os.path.isfile(grammar_source):
        return 1(grammar_source)
    pickled_name = 0(os.path.basename(grammar_source))
    data = pkgutil.get_data(package, pickled_name)
    g = grammar.Grammar()
    g.loads(data)
    return g","{""_generate_pickle_name"": ""0"", ""load_grammar"": ""1""}"
585,585,"def tokenize(readline, tokeneater=printtoken):
    """"""
    The tokenize() function accepts two parameters: one representing the
    input stream, and one providing an output mechanism for tokenize().

    The first parameter, readline, must be a callable object which provides
    the same interface as the readline() method of built-in file objects.
    Each call to the function should return one line of input as a string.

    The second parameter, tokeneater, must also be a callable object. It is
    called once for each token, with five arguments, corresponding to the
    tuples generated by generate_tokens().
    """"""
    try:
        tokenize_loop(readline, tokeneater)
    except StopTokenizing:
        pass","def tokenize_loop(readline, tokeneater):
    for token_info in generate_tokens(readline):
        tokeneater(*token_info)","def loop_over_tokens(readline, tokeneater):
    for token_info in generate_tokens(readline):
        tokeneater(*token_info)","def tokenize_with_readline_and_tokeneater(readline, tokeneater=printtoken):
    """"""
    The tokenize_with_readline_and_tokeneater() function accepts two parameters: one representing the
    input stream, and one providing an output mechanism for tokenize_with_readline_and_tokeneater().

    The first parameter, readline, must be a callable object which provides
    the same interface as the readline() method of built-in file objects.
    Each call to the function should return one line of input as a string.

    The second parameter, tokeneater, must also be a callable object. It is
    called once for each token, with five arguments, corresponding to the
    tuples generated by generate_tokens().
    """"""
    try:
        loop_over_tokens(readline, tokeneater)
    except StopTokenizing:
        pass","{""tokenize_loop"": ""loop_over_tokens""}","def 0(readline, tokeneater):
    for token_info in generate_tokens(readline):
        tokeneater(*token_info)","def 1(readline, tokeneater=printtoken):
    """"""
    The 1() function accepts two parameters: one representing the
    input stream, and one providing an output mechanism for 1().

    The first parameter, readline, must be a callable object which provides
    the same interface as the readline() method of built-in file objects.
    Each call to the function should return one line of input as a string.

    The second parameter, tokeneater, must also be a callable object. It is
    called once for each token, with five arguments, corresponding to the
    tuples generated by generate_tokens().
    """"""
    try:
        0(readline, tokeneater)
    except StopTokenizing:
        pass","{""tokenize_loop"": ""0""}"
586,586,"def get_end_linenumber(text):
    """"""Return the number of the last line in a Tk text widget.""""""
    return get_lineno(text, 'end-1c')","def get_lineno(text, index):
    """"""Return the line number of an index in a Tk text widget.""""""
    text_index = text.index(index)
    return int(float(text_index)) if text_index else None","def get_line_number_in_text_widget(text, index):
    """"""Return the line number of an index in a Tk text widget.""""""
    text_index = text.index(index)
    return int(float(text_index)) if text_index else None","def get_end_linenumber_of_text_widget(text):
    """"""Return the number of the last line in a Tk text widget.""""""
    return get_line_number_in_text_widget(text, 'end-1c')","{""get_lineno"": ""get_line_number_in_text_widget""}","def 0(text, index):
    """"""Return the line number of an index in a Tk text widget.""""""
    text_index = text.index(index)
    return int(float(text_index)) if text_index else None","def 1(text):
    """"""Return the number of the last line in a Tk text widget.""""""
    return 0(text, 'end-1c')","{""get_lineno"": ""0""}"
587,587,"def idle_showwarning(
        message, category, filename, lineno, file=None, line=None):
    """"""Show Idle-format warning (after replacing warnings.showwarning).

    The differences are the formatter called, the file=None replacement,
    which can be None, the capture of the consequence AttributeError,
    and the output of a hard-coded prompt.
    """"""
    if file is None:
        file = warning_stream
    try:
        file.write(idle_formatwarning(
                message, category, filename, lineno, line=line))
        file.write("">>> "")
    except (AttributeError, OSError):
        pass  # if file (probably __stderr__) is invalid, skip warning.","def idle_formatwarning(message, category, filename, lineno, line=None):
    """"""Format warnings the IDLE way.""""""

    s = ""\nWarning (from warnings module):\n""
    s += '  File \""%s\"", line %s\n' % (filename, lineno)
    if line is None:
        line = linecache.getline(filename, lineno)
    line = line.strip()
    if line:
        s += ""    %s\n"" % line
    s += ""%s: %s\n"" % (category.__name__, message)
    return s","def format_warning_idly(message, category, filename, lineno, line=None):
    """"""Format warnings the IDLE way.""""""

    s = ""\nWarning (from warnings module):\n""
    s += '  File \""%s\"", line %s\n' % (filename, lineno)
    if line is None:
        line = linecache.getline(filename, lineno)
    line = line.strip()
    if line:
        s += ""    %s\n"" % line
    s += ""%s: %s\n"" % (category.__name__, message)
    return s","def show_warning_in_idle_format(
        message, category, filename, lineno, file=None, line=None):
    """"""Show Idle-format warning (after replacing warnings.showwarning).

    The differences are the formatter called, the file=None replacement,
    which can be None, the capture of the consequence AttributeError,
    and the output of a hard-coded prompt.
    """"""
    if file is None:
        file = warning_stream
    try:
        file.write(format_warning_idly(
                message, category, filename, lineno, line=line))
        file.write("">>> "")
    except (AttributeError, OSError):
        pass  # if file (probably __stderr__) is invalid, skip warning.","{""idle_formatwarning"": ""format_warning_idly""}","def 0(message, category, filename, lineno, line=None):
    """"""Format warnings the IDLE way.""""""

    s = ""\nWarning (from warnings module):\n""
    s += '  File \""%s\"", line %s\n' % (filename, lineno)
    if line is None:
        line = linecache.getline(filename, lineno)
    line = line.strip()
    if line:
        s += ""    %s\n"" % line
    s += ""%s: %s\n"" % (category.__name__, message)
    return s","def 1(
        message, category, filename, lineno, file=None, line=None):
    """"""Show Idle-format warning (after replacing warnings.showwarning).

    The differences are the formatter called, the file=None replacement,
    which can be None, the capture of the consequence AttributeError,
    and the output of a hard-coded prompt.
    """"""
    if file is None:
        file = warning_stream
    try:
        file.write(0(
                message, category, filename, lineno, line=line))
        file.write("">>> "")
    except (AttributeError, OSError):
        pass  # if file (probably __stderr__) is invalid, skip warning.","{""idle_formatwarning"": ""0""}"
588,588,"def idle_showwarning_subproc(
        message, category, filename, lineno, file=None, line=None):
    """"""Show Idle-format warning after replacing warnings.showwarning.

    The only difference is the formatter called.
    """"""
    if file is None:
        file = sys.stderr
    try:
        file.write(idle_formatwarning(
                message, category, filename, lineno, line))
    except OSError:
        pass # the file (probably stderr) is invalid - this warning gets lost.","def idle_formatwarning(message, category, filename, lineno, line=None):
    """"""Format warnings the IDLE way.""""""

    s = ""\nWarning (from warnings module):\n""
    s += '  File \""%s\"", line %s\n' % (filename, lineno)
    if line is None:
        line = linecache.getline(filename, lineno)
    line = line.strip()
    if line:
        s += ""    %s\n"" % line
    s += ""%s: %s\n"" % (category.__name__, message)
    return s","def format_warning_idly(message, category, filename, lineno, line=None):
    """"""Format warnings the IDLE way.""""""

    s = ""\nWarning (from warnings module):\n""
    s += '  File \""%s\"", line %s\n' % (filename, lineno)
    if line is None:
        line = linecache.getline(filename, lineno)
    line = line.strip()
    if line:
        s += ""    %s\n"" % line
    s += ""%s: %s\n"" % (category.__name__, message)
    return s","def show_warning_in_idle_format_subprocess(
        message, category, filename, lineno, file=None, line=None):
    """"""Show Idle-format warning after replacing warnings.showwarning.

    The only difference is the formatter called.
    """"""
    if file is None:
        file = sys.stderr
    try:
        file.write(format_warning_idly(
                message, category, filename, lineno, line))
    except OSError:
        pass # the file (probably stderr) is invalid - this warning gets lost.","{""idle_formatwarning"": ""format_warning_idly""}","def 0(message, category, filename, lineno, line=None):
    """"""Format warnings the IDLE way.""""""

    s = ""\nWarning (from warnings module):\n""
    s += '  File \""%s\"", line %s\n' % (filename, lineno)
    if line is None:
        line = linecache.getline(filename, lineno)
    line = line.strip()
    if line:
        s += ""    %s\n"" % line
    s += ""%s: %s\n"" % (category.__name__, message)
    return s","def 1(
        message, category, filename, lineno, file=None, line=None):
    """"""Show Idle-format warning after replacing warnings.showwarning.

    The only difference is the formatter called.
    """"""
    if file is None:
        file = sys.stderr
    try:
        file.write(0(
                message, category, filename, lineno, line))
    except OSError:
        pass # the file (probably stderr) is invalid - this warning gets lost.","{""idle_formatwarning"": ""0""}"
589,589,"def show_socket_error(err, address):
    ""Display socket error from manage_socket.""
    import tkinter
    from tkinter.messagebox import showerror
    root = tkinter.Tk()
    fix_scaling(root)
    root.withdraw()
    showerror(
            ""Subprocess Connection Error"",
            f""IDLE's subprocess can't connect to {address[0]}:{address[1]}.\n""
            f""Fatal OSError #{err.errno}: {err.strerror}.\n""
            ""See the 'Startup failure' section of the IDLE doc, online at\n""
            ""https://docs.python.org/3/library/idle.html#startup-failure"",
            parent=root)
    root.destroy()","def fix_scaling(root):
    """"""Scale fonts on HiDPI displays.""""""
    import tkinter.font
    scaling = float(root.tk.call('tk', 'scaling'))
    if scaling > 1.4:
        for name in tkinter.font.names(root):
            font = tkinter.font.Font(root=root, name=name, exists=True)
            size = int(font['size'])
            if size < 0:
                font['size'] = round(-0.75*size)

def showerror(title=None, message=None, **options):
    ""Show an error message""
    return _show(title, message, ERROR, OK, **options)","def scale_fonts_on_hidpi_displays(root):
    """"""Scale fonts on HiDPI displays.""""""
    import tkinter.font
    scaling = float(root.tk.call('tk', 'scaling'))
    if scaling > 1.4:
        for name in tkinter.font.names(root):
            font = tkinter.font.Font(root=root, name=name, exists=True)
            size = int(font['size'])
            if size < 0:
                font['size'] = round(-0.75*size)

def show_error_message(title=None, message=None, **options):
    ""Show an error message""
    return _show(title, message, ERROR, OK, **options)","def show_socket_error_with_messagebox(err, address):
    ""Display socket error from manage_socket.""
    import tkinter
    from tkinter.messagebox import show_error_message
    root = tkinter.Tk()
    scale_fonts_on_hidpi_displays(root)
    root.withdraw()
    show_error_message(
            ""Subprocess Connection Error"",
            f""IDLE's subprocess can't connect to {address[0]}:{address[1]}.\n""
            f""Fatal OSError #{err.errno}: {err.strerror}.\n""
            ""See the 'Startup failure' section of the IDLE doc, online at\n""
            ""https://docs.python.org/3/library/idle.html#startup-failure"",
            parent=root)
    root.destroy()","{""fix_scaling"": ""scale_fonts_on_hidpi_displays"", ""showerror"": ""show_error_message""}","def 0(root):
    """"""Scale fonts on HiDPI displays.""""""
    import tkinter.font
    scaling = float(root.tk.call('tk', 'scaling'))
    if scaling > 1.4:
        for name in tkinter.font.names(root):
            font = tkinter.font.Font(root=root, name=name, exists=True)
            size = int(font['size'])
            if size < 0:
                font['size'] = round(-0.75*size)

def 1(title=None, message=None, **options):
    ""Show an error message""
    return _show(title, message, ERROR, OK, **options)","def 2(err, address):
    ""Display socket error from manage_socket.""
    import tkinter
    from tkinter.messagebox import showerror
    root = tkinter.Tk()
    0(root)
    root.withdraw()
    1(
            ""Subprocess Connection Error"",
            f""IDLE's subprocess can't connect to {address[0]}:{address[1]}.\n""
            f""Fatal OSError #{err.errno}: {err.strerror}.\n""
            ""See the 'Startup failure' section of the IDLE doc, online at\n""
            ""https://docs.python.org/3/library/idle.html#startup-failure"",
            parent=root)
    root.destroy()","{""fix_scaling"": ""0"", ""showerror"": ""1""}"
590,590,"def get_line_info(codeline):
    """"""Return tuple of (line indent value, codeline, block start keyword).

    The indentation of empty lines (or comment lines) is INFINITY.
    If the line does not start a block, the keyword value is False.
    """"""
    spaces, firstword = get_spaces_firstword(codeline)
    indent = len(spaces)
    if len(codeline) == indent or codeline[indent] == '#':
        indent = INFINITY
    opener = firstword in BLOCKOPENERS and firstword
    return indent, codeline, opener","def get_spaces_firstword(codeline, c=re.compile(r""^(\s*)(\w*)"")):
    ""Extract the beginning whitespace and first word from codeline.""
    return c.match(codeline).groups()","def extract_whitespace_and_first_word(codeline, c=re.compile(r""^(\s*)(\w*)"")):
    ""Extract the beginning whitespace and first word from codeline.""
    return c.match(codeline).groups()","def get_line_info_tuple_indent_codeline(codeline):
    """"""Return tuple of (line indent value, codeline, block start keyword).

    The indentation of empty lines (or comment lines) is INFINITY.
    If the line does not start a block, the keyword value is False.
    """"""
    spaces, firstword = extract_whitespace_and_first_word(codeline)
    indent = len(spaces)
    if len(codeline) == indent or codeline[indent] == '#':
        indent = INFINITY
    opener = firstword in BLOCKOPENERS and firstword
    return indent, codeline, opener","{""get_spaces_firstword"": ""extract_whitespace_and_first_word""}","def 0(codeline, c=re.compile(r""^(\s*)(\w*)"")):
    ""Extract the beginning whitespace and first word from codeline.""
    return c.match(codeline).groups()","def 1(codeline):
    """"""Return tuple of (line indent value, codeline, block start keyword).

    The indentation of empty lines (or comment lines) is INFINITY.
    If the line does not start a block, the keyword value is False.
    """"""
    spaces, firstword = 0(codeline)
    indent = len(spaces)
    if len(codeline) == indent or codeline[indent] == '#':
        indent = INFINITY
    opener = firstword in BLOCKOPENERS and firstword
    return indent, codeline, opener","{""get_spaces_firstword"": ""0""}"
591,591,"def build_bits():
    ""Return bits for platform.""
    if sys.platform == 'darwin':
        return '64' if sys.maxsize > 2**32 else '32'
    else:
        return architecture()[0][:2]","def architecture(executable=sys.executable, bits='', linkage=''):

    """""" Queries the given executable (defaults to the Python interpreter
        binary) for various architecture information.

        Returns a tuple (bits, linkage) which contains information about
        the bit architecture and the linkage format used for the
        executable. Both values are returned as strings.

        Values that cannot be determined are returned as given by the
        parameter presets. If bits is given as '', the sizeof(pointer)
        (or sizeof(long) on Python version < 1.5.2) is used as
        indicator for the supported pointer size.

        The function relies on the system's ""file"" command to do the
        actual work. This is available on most if not all Unix
        platforms. On some non-Unix platforms where the ""file"" command
        does not exist and the executable is set to the Python interpreter
        binary defaults from _default_architecture are used.

    """"""
    # Use the sizeof(pointer) as default number of bits if nothing
    # else is given as default.
    if not bits:
        import struct
        size = struct.calcsize('P')
        bits = str(size * 8) + 'bit'

    # Get data from the 'file' system command
    if executable:
        fileout = _syscmd_file(executable, '')
    else:
        fileout = ''

    if not fileout and \
       executable == sys.executable:
        # ""file"" command did not return anything; we'll try to provide
        # some sensible defaults then...
        if sys.platform in _default_architecture:
            b, l = _default_architecture[sys.platform]
            if b:
                bits = b
            if l:
                linkage = l
        return bits, linkage

    if 'executable' not in fileout and 'shared object' not in fileout:
        # Format not supported
        return bits, linkage

    # Bits
    if '32-bit' in fileout:
        bits = '32bit'
    elif '64-bit' in fileout:
        bits = '64bit'

    # Linkage
    if 'ELF' in fileout:
        linkage = 'ELF'
    elif 'PE' in fileout:
        # E.g. Windows uses this format
        if 'Windows' in fileout:
            linkage = 'WindowsPE'
        else:
            linkage = 'PE'
    elif 'COFF' in fileout:
        linkage = 'COFF'
    elif 'MS-DOS' in fileout:
        linkage = 'MSDOS'
    else:
        # XXX the A.OUT format also falls under this class...
        pass

    return bits, linkage","def query_executable_architecture(executable=sys.executable, bits='', linkage=''):

    """""" Queries the given executable (defaults to the Python interpreter
        binary) for various query_executable_architecture information.

        Returns a tuple (bits, linkage) which contains information about
        the bit query_executable_architecture and the linkage format used for the
        executable. Both values are returned as strings.

        Values that cannot be determined are returned as given by the
        parameter presets. If bits is given as '', the sizeof(pointer)
        (or sizeof(long) on Python version < 1.5.2) is used as
        indicator for the supported pointer size.

        The function relies on the system's ""file"" command to do the
        actual work. This is available on most if not all Unix
        platforms. On some non-Unix platforms where the ""file"" command
        does not exist and the executable is set to the Python interpreter
        binary defaults from _default_query_executable_architecture are used.

    """"""
    # Use the sizeof(pointer) as default number of bits if nothing
    # else is given as default.
    if not bits:
        import struct
        size = struct.calcsize('P')
        bits = str(size * 8) + 'bit'

    # Get data from the 'file' system command
    if executable:
        fileout = _syscmd_file(executable, '')
    else:
        fileout = ''

    if not fileout and \
       executable == sys.executable:
        # ""file"" command did not return anything; we'll try to provide
        # some sensible defaults then...
        if sys.platform in _default_query_executable_architecture:
            b, l = _default_query_executable_architecture[sys.platform]
            if b:
                bits = b
            if l:
                linkage = l
        return bits, linkage

    if 'executable' not in fileout and 'shared object' not in fileout:
        # Format not supported
        return bits, linkage

    # Bits
    if '32-bit' in fileout:
        bits = '32bit'
    elif '64-bit' in fileout:
        bits = '64bit'

    # Linkage
    if 'ELF' in fileout:
        linkage = 'ELF'
    elif 'PE' in fileout:
        # E.g. Windows uses this format
        if 'Windows' in fileout:
            linkage = 'WindowsPE'
        else:
            linkage = 'PE'
    elif 'COFF' in fileout:
        linkage = 'COFF'
    elif 'MS-DOS' in fileout:
        linkage = 'MSDOS'
    else:
        # XXX the A.OUT format also falls under this class...
        pass

    return bits, linkage","def build_bits_for_platform():
    ""Return bits for platform.""
    if sys.platform == 'darwin':
        return '64' if sys.maxsize > 2**32 else '32'
    else:
        return query_executable_architecture()[0][:2]","{""architecture"": ""query_executable_architecture""}","def 0(executable=sys.executable, bits='', linkage=''):

    """""" Queries the given executable (defaults to the Python interpreter
        binary) for various architecture information.

        Returns a tuple (bits, linkage) which contains information about
        the bit architecture and the linkage format used for the
        executable. Both values are returned as strings.

        Values that cannot be determined are returned as given by the
        parameter presets. If bits is given as '', the sizeof(pointer)
        (or sizeof(long) on Python version < 1.5.2) is used as
        indicator for the supported pointer size.

        The function relies on the system's ""file"" command to do the
        actual work. This is available on most if not all Unix
        platforms. On some non-Unix platforms where the ""file"" command
        does not exist and the executable is set to the Python interpreter
        binary defaults from _default_architecture are used.

    """"""
    # Use the sizeof(pointer) as default number of bits if nothing
    # else is given as default.
    if not bits:
        import struct
        size = struct.calcsize('P')
        bits = str(size * 8) + 'bit'

    # Get data from the 'file' system command
    if executable:
        fileout = _syscmd_file(executable, '')
    else:
        fileout = ''

    if not fileout and \
       executable == sys.executable:
        # ""file"" command did not return anything; we'll try to provide
        # some sensible defaults then...
        if sys.platform in _default_architecture:
            b, l = _default_architecture[sys.platform]
            if b:
                bits = b
            if l:
                linkage = l
        return bits, linkage

    if 'executable' not in fileout and 'shared object' not in fileout:
        # Format not supported
        return bits, linkage

    # Bits
    if '32-bit' in fileout:
        bits = '32bit'
    elif '64-bit' in fileout:
        bits = '64bit'

    # Linkage
    if 'ELF' in fileout:
        linkage = 'ELF'
    elif 'PE' in fileout:
        # E.g. Windows uses this format
        if 'Windows' in fileout:
            linkage = 'WindowsPE'
        else:
            linkage = 'PE'
    elif 'COFF' in fileout:
        linkage = 'COFF'
    elif 'MS-DOS' in fileout:
        linkage = 'MSDOS'
    else:
        # XXX the A.OUT format also falls under this class...
        pass

    return bits, linkage","def 1():
    ""Return bits for platform.""
    if sys.platform == 'darwin':
        return '64' if sys.maxsize > 2**32 else '32'
    else:
        return 0()[0][:2]","{""architecture"": ""0""}"
592,592,"def reformat_comment(data, limit, comment_header):
    """"""Return data reformatted to specified width with comment header.""""""

    # Remove header from the comment lines
    lc = len(comment_header)
    data = ""\n"".join(line[lc:] for line in data.split(""\n""))
    # Reformat to maxformatwidth chars or a 20 char width,
    # whichever is greater.
    format_width = max(limit - len(comment_header), 20)
    newdata = reformat_paragraph(data, format_width)
    # re-split and re-insert the comment header.
    newdata = newdata.split(""\n"")
    # If the block ends in a \n, we don't want the comment prefix
    # inserted after it. (Im not sure it makes sense to reformat a
    # comment block that is not made of complete lines, but whatever!)
    # Can't think of a clean solution, so we hack away
    block_suffix = """"
    if not newdata[-1]:
        block_suffix = ""\n""
        newdata = newdata[:-1]
    return '\n'.join(comment_header+line for line in newdata) + block_suffix","def reformat_paragraph(data, limit):
    """"""Return data reformatted to specified width (limit).""""""
    lines = data.split(""\n"")
    i = 0
    n = len(lines)
    while i < n and is_all_white(lines[i]):
        i = i+1
    if i >= n:
        return data
    indent1 = get_indent(lines[i])
    if i+1 < n and not is_all_white(lines[i+1]):
        indent2 = get_indent(lines[i+1])
    else:
        indent2 = indent1
    new = lines[:i]
    partial = indent1
    while i < n and not is_all_white(lines[i]):
        # XXX Should take double space after period (etc.) into account
        words = re.split(r""(\s+)"", lines[i])
        for j in range(0, len(words), 2):
            word = words[j]
            if not word:
                continue # Can happen when line ends in whitespace
            if len((partial + word).expandtabs()) > limit and \
                   partial != indent1:
                new.append(partial.rstrip())
                partial = indent2
            partial = partial + word + "" ""
            if j+1 < len(words) and words[j+1] != "" "":
                partial = partial + "" ""
        i = i+1
    new.append(partial.rstrip())
    # XXX Should reformat remaining paragraphs as well
    new.extend(lines[i:])
    return ""\n"".join(new)","def reformat_text_paragraph(data, limit):
    """"""Return data reformatted to specified width (limit).""""""
    lines = data.split(""\n"")
    i = 0
    n = len(lines)
    while i < n and is_all_white(lines[i]):
        i = i+1
    if i >= n:
        return data
    indent1 = get_indent(lines[i])
    if i+1 < n and not is_all_white(lines[i+1]):
        indent2 = get_indent(lines[i+1])
    else:
        indent2 = indent1
    new = lines[:i]
    partial = indent1
    while i < n and not is_all_white(lines[i]):
        # XXX Should take double space after period (etc.) into account
        words = re.split(r""(\s+)"", lines[i])
        for j in range(0, len(words), 2):
            word = words[j]
            if not word:
                continue # Can happen when line ends in whitespace
            if len((partial + word).expandtabs()) > limit and \
                   partial != indent1:
                new.append(partial.rstrip())
                partial = indent2
            partial = partial + word + "" ""
            if j+1 < len(words) and words[j+1] != "" "":
                partial = partial + "" ""
        i = i+1
    new.append(partial.rstrip())
    # XXX Should reformat remaining paragraphs as well
    new.extend(lines[i:])
    return ""\n"".join(new)","def reformat_comment_with_header_limit(data, limit, comment_header):
    """"""Return data reformatted to specified width with comment header.""""""

    # Remove header from the comment lines
    lc = len(comment_header)
    data = ""\n"".join(line[lc:] for line in data.split(""\n""))
    # Reformat to maxformatwidth chars or a 20 char width,
    # whichever is greater.
    format_width = max(limit - len(comment_header), 20)
    newdata = reformat_text_paragraph(data, format_width)
    # re-split and re-insert the comment header.
    newdata = newdata.split(""\n"")
    # If the block ends in a \n, we don't want the comment prefix
    # inserted after it. (Im not sure it makes sense to reformat a
    # comment block that is not made of complete lines, but whatever!)
    # Can't think of a clean solution, so we hack away
    block_suffix = """"
    if not newdata[-1]:
        block_suffix = ""\n""
        newdata = newdata[:-1]
    return '\n'.join(comment_header+line for line in newdata) + block_suffix","{""reformat_paragraph"": ""reformat_text_paragraph""}","def 0(data, limit):
    """"""Return data reformatted to specified width (limit).""""""
    lines = data.split(""\n"")
    i = 0
    n = len(lines)
    while i < n and is_all_white(lines[i]):
        i = i+1
    if i >= n:
        return data
    indent1 = get_indent(lines[i])
    if i+1 < n and not is_all_white(lines[i+1]):
        indent2 = get_indent(lines[i+1])
    else:
        indent2 = indent1
    new = lines[:i]
    partial = indent1
    while i < n and not is_all_white(lines[i]):
        # XXX Should take double space after period (etc.) into account
        words = re.split(r""(\s+)"", lines[i])
        for j in range(0, len(words), 2):
            word = words[j]
            if not word:
                continue # Can happen when line ends in whitespace
            if len((partial + word).expandtabs()) > limit and \
                   partial != indent1:
                new.append(partial.rstrip())
                partial = indent2
            partial = partial + word + "" ""
            if j+1 < len(words) and words[j+1] != "" "":
                partial = partial + "" ""
        i = i+1
    new.append(partial.rstrip())
    # XXX Should reformat remaining paragraphs as well
    new.extend(lines[i:])
    return ""\n"".join(new)","def 1(data, limit, comment_header):
    """"""Return data reformatted to specified width with comment header.""""""

    # Remove header from the comment lines
    lc = len(comment_header)
    data = ""\n"".join(line[lc:] for line in data.split(""\n""))
    # Reformat to maxformatwidth chars or a 20 char width,
    # whichever is greater.
    format_width = max(limit - len(comment_header), 20)
    newdata = 0(data, format_width)
    # re-split and re-insert the comment header.
    newdata = newdata.split(""\n"")
    # If the block ends in a \n, we don't want the comment prefix
    # inserted after it. (Im not sure it makes sense to reformat a
    # comment block that is not made of complete lines, but whatever!)
    # Can't think of a clean solution, so we hack away
    block_suffix = """"
    if not newdata[-1]:
        block_suffix = ""\n""
        newdata = newdata[:-1]
    return '\n'.join(comment_header+line for line in newdata) + block_suffix","{""reformat_paragraph"": ""0""}"
593,593,"def isAquaTk():
    """"""
    Returns True if IDLE is using a native OS X Tk (Cocoa or Carbon).
    """"""
    if not _tk_type:
        _init_tk_type()
    return _tk_type == ""cocoa"" or _tk_type == ""carbon""","def _init_tk_type():
    """"""
    Initializes OS X Tk variant values for
    isAquaTk(), isCarbonTk(), isCocoaTk(), and isXQuartz().
    """"""
    global _tk_type
    if platform == 'darwin':
        root = tkinter.Tk()
        ws = root.tk.call('tk', 'windowingsystem')
        if 'x11' in ws:
            _tk_type = ""xquartz""
        elif 'aqua' not in ws:
            _tk_type = ""other""
        elif 'AppKit' in root.tk.call('winfo', 'server', '.'):
            _tk_type = ""cocoa""
        else:
            _tk_type = ""carbon""
        root.destroy()
    else:
        _tk_type = ""other""","def initialize_osx_tk_variant_values():
    """"""
    Initializes OS X Tk variant values for
    isAquaTk(), isCarbonTk(), isCocoaTk(), and isXQuartz().
    """"""
    global _tk_type
    if platform == 'darwin':
        root = tkinter.Tk()
        ws = root.tk.call('tk', 'windowingsystem')
        if 'x11' in ws:
            _tk_type = ""xquartz""
        elif 'aqua' not in ws:
            _tk_type = ""other""
        elif 'AppKit' in root.tk.call('winfo', 'server', '.'):
            _tk_type = ""cocoa""
        else:
            _tk_type = ""carbon""
        root.destroy()
    else:
        _tk_type = ""other""","def is_using_native_osx_tk():
    """"""
    Returns True if IDLE is using a native OS X Tk (Cocoa or Carbon).
    """"""
    if not _tk_type:
        initialize_osx_tk_variant_values()
    return _tk_type == ""cocoa"" or _tk_type == ""carbon""","{""_init_tk_type"": ""initialize_osx_tk_variant_values""}","def 0():
    """"""
    Initializes OS X Tk variant values for
    isAquaTk(), isCarbonTk(), isCocoaTk(), and isXQuartz().
    """"""
    global _tk_type
    if platform == 'darwin':
        root = tkinter.Tk()
        ws = root.tk.call('tk', 'windowingsystem')
        if 'x11' in ws:
            _tk_type = ""xquartz""
        elif 'aqua' not in ws:
            _tk_type = ""other""
        elif 'AppKit' in root.tk.call('winfo', 'server', '.'):
            _tk_type = ""cocoa""
        else:
            _tk_type = ""carbon""
        root.destroy()
    else:
        _tk_type = ""other""","def 1():
    """"""
    Returns True if IDLE is using a native OS X Tk (Cocoa or Carbon).
    """"""
    if not _tk_type:
        0()
    return _tk_type == ""cocoa"" or _tk_type == ""carbon""","{""_init_tk_type"": ""0""}"
594,594,"def isCarbonTk():
    """"""
    Returns True if IDLE is using a Carbon Aqua Tk (instead of the
    newer Cocoa Aqua Tk).
    """"""
    if not _tk_type:
        _init_tk_type()
    return _tk_type == ""carbon""","def _init_tk_type():
    """"""
    Initializes OS X Tk variant values for
    isAquaTk(), isCarbonTk(), isCocoaTk(), and isXQuartz().
    """"""
    global _tk_type
    if platform == 'darwin':
        root = tkinter.Tk()
        ws = root.tk.call('tk', 'windowingsystem')
        if 'x11' in ws:
            _tk_type = ""xquartz""
        elif 'aqua' not in ws:
            _tk_type = ""other""
        elif 'AppKit' in root.tk.call('winfo', 'server', '.'):
            _tk_type = ""cocoa""
        else:
            _tk_type = ""carbon""
        root.destroy()
    else:
        _tk_type = ""other""","def initialize_osx_tk_variant_values():
    """"""
    Initializes OS X Tk variant values for
    isAquaTk(), isCarbonTk(), isCocoaTk(), and isXQuartz().
    """"""
    global _tk_type
    if platform == 'darwin':
        root = tkinter.Tk()
        ws = root.tk.call('tk', 'windowingsystem')
        if 'x11' in ws:
            _tk_type = ""xquartz""
        elif 'aqua' not in ws:
            _tk_type = ""other""
        elif 'AppKit' in root.tk.call('winfo', 'server', '.'):
            _tk_type = ""cocoa""
        else:
            _tk_type = ""carbon""
        root.destroy()
    else:
        _tk_type = ""other""","def is_carbon_tk_used():
    """"""
    Returns True if IDLE is using a Carbon Aqua Tk (instead of the
    newer Cocoa Aqua Tk).
    """"""
    if not _tk_type:
        initialize_osx_tk_variant_values()
    return _tk_type == ""carbon""","{""_init_tk_type"": ""initialize_osx_tk_variant_values""}","def 0():
    """"""
    Initializes OS X Tk variant values for
    isAquaTk(), isCarbonTk(), isCocoaTk(), and isXQuartz().
    """"""
    global _tk_type
    if platform == 'darwin':
        root = tkinter.Tk()
        ws = root.tk.call('tk', 'windowingsystem')
        if 'x11' in ws:
            _tk_type = ""xquartz""
        elif 'aqua' not in ws:
            _tk_type = ""other""
        elif 'AppKit' in root.tk.call('winfo', 'server', '.'):
            _tk_type = ""cocoa""
        else:
            _tk_type = ""carbon""
        root.destroy()
    else:
        _tk_type = ""other""","def 1():
    """"""
    Returns True if IDLE is using a Carbon Aqua Tk (instead of the
    newer Cocoa Aqua Tk).
    """"""
    if not _tk_type:
        0()
    return _tk_type == ""carbon""","{""_init_tk_type"": ""0""}"
595,595,"def isCocoaTk():
    """"""
    Returns True if IDLE is using a Cocoa Aqua Tk.
    """"""
    if not _tk_type:
        _init_tk_type()
    return _tk_type == ""cocoa""","def _init_tk_type():
    """"""
    Initializes OS X Tk variant values for
    isAquaTk(), isCarbonTk(), isCocoaTk(), and isXQuartz().
    """"""
    global _tk_type
    if platform == 'darwin':
        root = tkinter.Tk()
        ws = root.tk.call('tk', 'windowingsystem')
        if 'x11' in ws:
            _tk_type = ""xquartz""
        elif 'aqua' not in ws:
            _tk_type = ""other""
        elif 'AppKit' in root.tk.call('winfo', 'server', '.'):
            _tk_type = ""cocoa""
        else:
            _tk_type = ""carbon""
        root.destroy()
    else:
        _tk_type = ""other""","def initialize_osx_tk_variant_values():
    """"""
    Initializes OS X Tk variant values for
    isAquaTk(), isCarbonTk(), isCocoaTk(), and isXQuartz().
    """"""
    global _tk_type
    if platform == 'darwin':
        root = tkinter.Tk()
        ws = root.tk.call('tk', 'windowingsystem')
        if 'x11' in ws:
            _tk_type = ""xquartz""
        elif 'aqua' not in ws:
            _tk_type = ""other""
        elif 'AppKit' in root.tk.call('winfo', 'server', '.'):
            _tk_type = ""cocoa""
        else:
            _tk_type = ""carbon""
        root.destroy()
    else:
        _tk_type = ""other""","def is_using_cocoa_tk():
    """"""
    Returns True if IDLE is using a Cocoa Aqua Tk.
    """"""
    if not _tk_type:
        initialize_osx_tk_variant_values()
    return _tk_type == ""cocoa""","{""_init_tk_type"": ""initialize_osx_tk_variant_values""}","def 0():
    """"""
    Initializes OS X Tk variant values for
    isAquaTk(), isCarbonTk(), isCocoaTk(), and isXQuartz().
    """"""
    global _tk_type
    if platform == 'darwin':
        root = tkinter.Tk()
        ws = root.tk.call('tk', 'windowingsystem')
        if 'x11' in ws:
            _tk_type = ""xquartz""
        elif 'aqua' not in ws:
            _tk_type = ""other""
        elif 'AppKit' in root.tk.call('winfo', 'server', '.'):
            _tk_type = ""cocoa""
        else:
            _tk_type = ""carbon""
        root.destroy()
    else:
        _tk_type = ""other""","def 1():
    """"""
    Returns True if IDLE is using a Cocoa Aqua Tk.
    """"""
    if not _tk_type:
        0()
    return _tk_type == ""cocoa""","{""_init_tk_type"": ""0""}"
596,596,"def isXQuartz():
    """"""
    Returns True if IDLE is using an OS X X11 Tk.
    """"""
    if not _tk_type:
        _init_tk_type()
    return _tk_type == ""xquartz""","def _init_tk_type():
    """"""
    Initializes OS X Tk variant values for
    isAquaTk(), isCarbonTk(), isCocoaTk(), and isXQuartz().
    """"""
    global _tk_type
    if platform == 'darwin':
        root = tkinter.Tk()
        ws = root.tk.call('tk', 'windowingsystem')
        if 'x11' in ws:
            _tk_type = ""xquartz""
        elif 'aqua' not in ws:
            _tk_type = ""other""
        elif 'AppKit' in root.tk.call('winfo', 'server', '.'):
            _tk_type = ""cocoa""
        else:
            _tk_type = ""carbon""
        root.destroy()
    else:
        _tk_type = ""other""","def initialize_osx_tk_variant_values():
    """"""
    Initializes OS X Tk variant values for
    isAquaTk(), isCarbonTk(), isCocoaTk(), and isXQuartz().
    """"""
    global _tk_type
    if platform == 'darwin':
        root = tkinter.Tk()
        ws = root.tk.call('tk', 'windowingsystem')
        if 'x11' in ws:
            _tk_type = ""xquartz""
        elif 'aqua' not in ws:
            _tk_type = ""other""
        elif 'AppKit' in root.tk.call('winfo', 'server', '.'):
            _tk_type = ""cocoa""
        else:
            _tk_type = ""carbon""
        root.destroy()
    else:
        _tk_type = ""other""","def is_xquartz_used():
    """"""
    Returns True if IDLE is using an OS X X11 Tk.
    """"""
    if not _tk_type:
        initialize_osx_tk_variant_values()
    return _tk_type == ""xquartz""","{""_init_tk_type"": ""initialize_osx_tk_variant_values""}","def 0():
    """"""
    Initializes OS X Tk variant values for
    isAquaTk(), isCarbonTk(), isCocoaTk(), and isXQuartz().
    """"""
    global _tk_type
    if platform == 'darwin':
        root = tkinter.Tk()
        ws = root.tk.call('tk', 'windowingsystem')
        if 'x11' in ws:
            _tk_type = ""xquartz""
        elif 'aqua' not in ws:
            _tk_type = ""other""
        elif 'AppKit' in root.tk.call('winfo', 'server', '.'):
            _tk_type = ""cocoa""
        else:
            _tk_type = ""carbon""
        root.destroy()
    else:
        _tk_type = ""other""","def 1():
    """"""
    Returns True if IDLE is using an OS X X11 Tk.
    """"""
    if not _tk_type:
        0()
    return _tk_type == ""xquartz""","{""_init_tk_type"": ""0""}"
597,597,"def tkVersionWarning(root):
    """"""
    Returns a string warning message if the Tk version in use appears to
    be one known to cause problems with IDLE.
    1. Apple Cocoa-based Tk 8.5.7 shipped with Mac OS X 10.6 is unusable.
    2. Apple Cocoa-based Tk 8.5.9 in OS X 10.7 and 10.8 is better but
        can still crash unexpectedly.
    """"""

    if isCocoaTk():
        patchlevel = root.tk.call('info', 'patchlevel')
        if patchlevel not in ('8.5.7', '8.5.9'):
            return False
        return (""WARNING: The version of Tcl/Tk ({0}) in use may""
                "" be unstable.\n""
                ""Visit https://www.python.org/download/mac/tcltk/""
                "" for current information."".format(patchlevel))
    else:
        return False","def isCocoaTk():
    """"""
    Returns True if IDLE is using a Cocoa Aqua Tk.
    """"""
    if not _tk_type:
        _init_tk_type()
    return _tk_type == ""cocoa""","def is_using_cocoa_tk():
    """"""
    Returns True if IDLE is using a Cocoa Aqua Tk.
    """"""
    if not _tk_type:
        _init_tk_type()
    return _tk_type == ""cocoa""","def tk_version_warning_message(root):
    """"""
    Returns a string warning message if the Tk version in use appears to
    be one known to cause problems with IDLE.
    1. Apple Cocoa-based Tk 8.5.7 shipped with Mac OS X 10.6 is unusable.
    2. Apple Cocoa-based Tk 8.5.9 in OS X 10.7 and 10.8 is better but
        can still crash unexpectedly.
    """"""

    if is_using_cocoa_tk():
        patchlevel = root.tk.call('info', 'patchlevel')
        if patchlevel not in ('8.5.7', '8.5.9'):
            return False
        return (""WARNING: The version of Tcl/Tk ({0}) in use may""
                "" be unstable.\n""
                ""Visit https://www.python.org/download/mac/tcltk/""
                "" for current information."".format(patchlevel))
    else:
        return False","{""isCocoaTk"": ""is_using_cocoa_tk""}","def 0():
    """"""
    Returns True if IDLE is using a Cocoa Aqua Tk.
    """"""
    if not _tk_type:
        _init_tk_type()
    return _tk_type == ""cocoa""","def 1(root):
    """"""
    Returns a string warning message if the Tk version in use appears to
    be one known to cause problems with IDLE.
    1. Apple Cocoa-based Tk 8.5.7 shipped with Mac OS X 10.6 is unusable.
    2. Apple Cocoa-based Tk 8.5.9 in OS X 10.7 and 10.8 is better but
        can still crash unexpectedly.
    """"""

    if 0():
        patchlevel = root.tk.call('info', 'patchlevel')
        if patchlevel not in ('8.5.7', '8.5.9'):
            return False
        return (""WARNING: The version of Tcl/Tk ({0}) in use may""
                "" be unstable.\n""
                ""Visit https://www.python.org/download/mac/tcltk/""
                "" for current information."".format(patchlevel))
    else:
        return False","{""isCocoaTk"": ""0""}"
598,598,"def preferTabsPreferenceWarning():
    """"""
    Warn if ""Prefer tabs when opening documents"" is set to ""Always"".
    """"""
    if platform != 'darwin':
        return None

    prefs = readSystemPreferences()
    if prefs and prefs.get('AppleWindowTabbingMode') == 'always':
        return (
            'WARNING: The system preference ""Prefer tabs when opening'
            ' documents"" is set to ""Always"". This will cause various problems'
            ' with IDLE. For the best experience, change this setting when'
            ' running IDLE (via System Preferences -> Dock).'
        )
    return None","def readSystemPreferences():
    """"""
    Fetch the macOS system preferences.
    """"""
    if platform != 'darwin':
        return None

    plist_path = expanduser('~/Library/Preferences/.GlobalPreferences.plist')
    try:
        with open(plist_path, 'rb') as plist_file:
            return plistlib.load(plist_file)
    except OSError:
        return None","def read_macos_system_preferences():
    """"""
    Fetch the macOS system preferences.
    """"""
    if platform != 'darwin':
        return None

    plist_path = expanduser('~/Library/Preferences/.GlobalPreferences.plist')
    try:
        with open(plist_path, 'rb') as plist_file:
            return plistlib.load(plist_file)
    except OSError:
        return None","def prefer_tabs_preference_warning_message():
    """"""
    Warn if ""Prefer tabs when opening documents"" is set to ""Always"".
    """"""
    if platform != 'darwin':
        return None

    prefs = read_macos_system_preferences()
    if prefs and prefs.get('AppleWindowTabbingMode') == 'always':
        return (
            'WARNING: The system preference ""Prefer tabs when opening'
            ' documents"" is set to ""Always"". This will cause various problems'
            ' with IDLE. For the best experience, change this setting when'
            ' running IDLE (via System Preferences -> Dock).'
        )
    return None","{""readSystemPreferences"": ""read_macos_system_preferences""}","def 0():
    """"""
    Fetch the macOS system preferences.
    """"""
    if platform != 'darwin':
        return None

    plist_path = expanduser('~/Library/Preferences/.GlobalPreferences.plist')
    try:
        with open(plist_path, 'rb') as plist_file:
            return plistlib.load(plist_file)
    except OSError:
        return None","def 1():
    """"""
    Warn if ""Prefer tabs when opening documents"" is set to ""Always"".
    """"""
    if platform != 'darwin':
        return None

    prefs = 0()
    if prefs and prefs.get('AppleWindowTabbingMode') == 'always':
        return (
            'WARNING: The system preference ""Prefer tabs when opening'
            ' documents"" is set to ""Always"". This will cause various problems'
            ' with IDLE. For the best experience, change this setting when'
            ' running IDLE (via System Preferences -> Dock).'
        )
    return None","{""readSystemPreferences"": ""0""}"
599,599,"def setupApp(root, flist):
    """"""
    Perform initial OS X customizations if needed.
    Called from pyshell.main() after initial calls to Tk()

    There are currently three major versions of Tk in use on OS X:
        1. Aqua Cocoa Tk (native default since OS X 10.6)
        2. Aqua Carbon Tk (original native, 32-bit only, deprecated)
        3. X11 (supported by some third-party distributors, deprecated)
    There are various differences among the three that affect IDLE
    behavior, primarily with menus, mouse key events, and accelerators.
    Some one-time customizations are performed here.
    Others are dynamically tested throughout idlelib by calls to the
    isAquaTk(), isCarbonTk(), isCocoaTk(), isXQuartz() functions which
    are initialized here as well.
    """"""
    if isAquaTk():
        hideTkConsole(root)
        overrideRootMenu(root, flist)
        addOpenEventSupport(root, flist)
        fixb2context(root)","def fixb2context(root):
    '''Removed bad AquaTk Button-2 (right) and Paste bindings.

    They prevent context menu access and seem to be gone in AquaTk8.6.
    See issue #24801.
    '''
    root.unbind_class('Text', '<B2>')
    root.unbind_class('Text', '<B2-Motion>')
    root.unbind_class('Text', '<<PasteSelection>>')

def addOpenEventSupport(root, flist):
    """"""
    This ensures that the application will respond to open AppleEvents, which
    makes is feasible to use IDLE as the default application for python files.
    """"""
    def doOpenFile(*args):
        for fn in args:
            flist.open(fn)

    # The command below is a hook in aquatk that is called whenever the app
    # receives a file open event. The callback can have multiple arguments,
    # one for every file that should be opened.
    root.createcommand(""::tk::mac::OpenDocument"", doOpenFile)

def isAquaTk():
    """"""
    Returns True if IDLE is using a native OS X Tk (Cocoa or Carbon).
    """"""
    if not _tk_type:
        _init_tk_type()
    return _tk_type == ""cocoa"" or _tk_type == ""carbon""

def overrideRootMenu(root, flist):
    """"""
    Replace the Tk root menu by something that is more appropriate for
    IDLE with an Aqua Tk.
    """"""
    # The menu that is attached to the Tk root (""."") is also used by AquaTk for
    # all windows that don't specify a menu of their own. The default menubar
    # contains a number of menus, none of which are appropriate for IDLE. The
    # Most annoying of those is an 'About Tck/Tk...' menu in the application
    # menu.
    #
    # This function replaces the default menubar by a mostly empty one, it
    # should only contain the correct application menu and the window menu.
    #
    # Due to a (mis-)feature of TkAqua the user will also see an empty Help
    # menu.
    from tkinter import Menu
    from idlelib import mainmenu
    from idlelib import window

    closeItem = mainmenu.menudefs[0][1][-2]

    # Remove the last 3 items of the file menu: a separator, close window and
    # quit. Close window will be reinserted just above the save item, where
    # it should be according to the HIG. Quit is in the application menu.
    del mainmenu.menudefs[0][1][-3:]
    mainmenu.menudefs[0][1].insert(6, closeItem)

    # Remove the 'About' entry from the help menu, it is in the application
    # menu
    del mainmenu.menudefs[-1][1][0:2]
    # Remove the 'Configure Idle' entry from the options menu, it is in the
    # application menu as 'Preferences'
    del mainmenu.menudefs[-3][1][0:2]
    menubar = Menu(root)
    root.configure(menu=menubar)
    menudict = {}

    menudict['window'] = menu = Menu(menubar, name='window', tearoff=0)
    menubar.add_cascade(label='Window', menu=menu, underline=0)

    def postwindowsmenu(menu=menu):
        end = menu.index('end')
        if end is None:
            end = -1

        if end > 0:
            menu.delete(0, end)
        window.add_windows_to_menu(menu)
    window.register_callback(postwindowsmenu)

    def about_dialog(event=None):
        ""Handle Help 'About IDLE' event.""
        # Synchronize with editor.EditorWindow.about_dialog.
        from idlelib import help_about
        help_about.AboutDialog(root)

    def config_dialog(event=None):
        ""Handle Options 'Configure IDLE' event.""
        # Synchronize with editor.EditorWindow.config_dialog.
        from idlelib import configdialog

        # Ensure that the root object has an instance_dict attribute,
        # mirrors code in EditorWindow (although that sets the attribute
        # on an EditorWindow instance that is then passed as the first
        # argument to ConfigDialog)
        root.instance_dict = flist.inversedict
        configdialog.ConfigDialog(root, 'Settings')

    def help_dialog(event=None):
        ""Handle Help 'IDLE Help' event.""
        # Synchronize with editor.EditorWindow.help_dialog.
        from idlelib import help
        help.show_idlehelp(root)

    root.bind('<<about-idle>>', about_dialog)
    root.bind('<<open-config-dialog>>', config_dialog)
    root.createcommand('::tk::mac::ShowPreferences', config_dialog)
    if flist:
        root.bind('<<close-all-windows>>', flist.close_all_callback)

        # The binding above doesn't reliably work on all versions of Tk
        # on macOS. Adding command definition below does seem to do the
        # right thing for now.
        root.createcommand('exit', flist.close_all_callback)

    if isCarbonTk():
        # for Carbon AquaTk, replace the default Tk apple menu
        menudict['application'] = menu = Menu(menubar, name='apple',
                                              tearoff=0)
        menubar.add_cascade(label='IDLE', menu=menu)
        mainmenu.menudefs.insert(0,
            ('application', [
                ('About IDLE', '<<about-idle>>'),
                    None,
                ]))
    if isCocoaTk():
        # replace default About dialog with About IDLE one
        root.createcommand('tkAboutDialog', about_dialog)
        # replace default ""Help"" item in Help menu
        root.createcommand('::tk::mac::ShowHelp', help_dialog)
        # remove redundant ""IDLE Help"" from menu
        del mainmenu.menudefs[-1][1][0]

def hideTkConsole(root):
    try:
        root.tk.call('console', 'hide')
    except tkinter.TclError:
        # Some versions of the Tk framework don't have a console object
        pass","def remove_bad_aquatk_bindings(root):
    '''Removed bad AquaTk Button-2 (right) and Paste bindings.

    They prevent context menu access and seem to be gone in AquaTk8.6.
    See issue #24801.
    '''
    root.unbind_class('Text', '<B2>')
    root.unbind_class('Text', '<B2-Motion>')
    root.unbind_class('Text', '<<PasteSelection>>')

def add_open_event_support(root, flist):
    """"""
    This ensures that the application will respond to open AppleEvents, which
    makes is feasible to use IDLE as the default application for python files.
    """"""
    def doOpenFile(*args):
        for fn in args:
            flist.open(fn)

    # The command below is a hook in aquatk that is called whenever the app
    # receives a file open event. The callback can have multiple arguments,
    # one for every file that should be opened.
    root.createcommand(""::tk::mac::OpenDocument"", doOpenFile)

def is_using_native_osx_tk():
    """"""
    Returns True if IDLE is using a native OS X Tk (Cocoa or Carbon).
    """"""
    if not _tk_type:
        _init_tk_type()
    return _tk_type == ""cocoa"" or _tk_type == ""carbon""

def replace_root_menu_for_idle(root, flist):
    """"""
    Replace the Tk root menu by something that is more appropriate for
    IDLE with an Aqua Tk.
    """"""
    # The menu that is attached to the Tk root (""."") is also used by AquaTk for
    # all windows that don't specify a menu of their own. The default menubar
    # contains a number of menus, none of which are appropriate for IDLE. The
    # Most annoying of those is an 'About Tck/Tk...' menu in the application
    # menu.
    #
    # This function replaces the default menubar by a mostly empty one, it
    # should only contain the correct application menu and the window menu.
    #
    # Due to a (mis-)feature of TkAqua the user will also see an empty Help
    # menu.
    from tkinter import Menu
    from idlelib import mainmenu
    from idlelib import window

    closeItem = mainmenu.menudefs[0][1][-2]

    # Remove the last 3 items of the file menu: a separator, close window and
    # quit. Close window will be reinserted just above the save item, where
    # it should be according to the HIG. Quit is in the application menu.
    del mainmenu.menudefs[0][1][-3:]
    mainmenu.menudefs[0][1].insert(6, closeItem)

    # Remove the 'About' entry from the help menu, it is in the application
    # menu
    del mainmenu.menudefs[-1][1][0:2]
    # Remove the 'Configure Idle' entry from the options menu, it is in the
    # application menu as 'Preferences'
    del mainmenu.menudefs[-3][1][0:2]
    menubar = Menu(root)
    root.configure(menu=menubar)
    menudict = {}

    menudict['window'] = menu = Menu(menubar, name='window', tearoff=0)
    menubar.add_cascade(label='Window', menu=menu, underline=0)

    def postwindowsmenu(menu=menu):
        end = menu.index('end')
        if end is None:
            end = -1

        if end > 0:
            menu.delete(0, end)
        window.add_windows_to_menu(menu)
    window.register_callback(postwindowsmenu)

    def about_dialog(event=None):
        ""Handle Help 'About IDLE' event.""
        # Synchronize with editor.EditorWindow.about_dialog.
        from idlelib import help_about
        help_about.AboutDialog(root)

    def config_dialog(event=None):
        ""Handle Options 'Configure IDLE' event.""
        # Synchronize with editor.EditorWindow.config_dialog.
        from idlelib import configdialog

        # Ensure that the root object has an instance_dict attribute,
        # mirrors code in EditorWindow (although that sets the attribute
        # on an EditorWindow instance that is then passed as the first
        # argument to ConfigDialog)
        root.instance_dict = flist.inversedict
        configdialog.ConfigDialog(root, 'Settings')

    def help_dialog(event=None):
        ""Handle Help 'IDLE Help' event.""
        # Synchronize with editor.EditorWindow.help_dialog.
        from idlelib import help
        help.show_idlehelp(root)

    root.bind('<<about-idle>>', about_dialog)
    root.bind('<<open-config-dialog>>', config_dialog)
    root.createcommand('::tk::mac::ShowPreferences', config_dialog)
    if flist:
        root.bind('<<close-all-windows>>', flist.close_all_callback)

        # The binding above doesn't reliably work on all versions of Tk
        # on macOS. Adding command definition below does seem to do the
        # right thing for now.
        root.createcommand('exit', flist.close_all_callback)

    if isCarbonTk():
        # for Carbon AquaTk, replace the default Tk apple menu
        menudict['application'] = menu = Menu(menubar, name='apple',
                                              tearoff=0)
        menubar.add_cascade(label='IDLE', menu=menu)
        mainmenu.menudefs.insert(0,
            ('application', [
                ('About IDLE', '<<about-idle>>'),
                    None,
                ]))
    if isCocoaTk():
        # replace default About dialog with About IDLE one
        root.createcommand('tkAboutDialog', about_dialog)
        # replace default ""Help"" item in Help menu
        root.createcommand('::tk::mac::ShowHelp', help_dialog)
        # remove redundant ""IDLE Help"" from menu
        del mainmenu.menudefs[-1][1][0]

def try_hide_tk_console(root):
    try:
        root.tk.call('console', 'hide')
    except tkinter.TclError:
        # Some versions of the Tk framework don't have a console object
        pass","def setup_app_perform_customizations(root, flist):
    """"""
    Perform initial OS X customizations if needed.
    Called from pyshell.main() after initial calls to Tk()

    There are currently three major versions of Tk in use on OS X:
        1. Aqua Cocoa Tk (native default since OS X 10.6)
        2. Aqua Carbon Tk (original native, 32-bit only, deprecated)
        3. X11 (supported by some third-party distributors, deprecated)
    There are various differences among the three that affect IDLE
    behavior, primarily with menus, mouse key events, and accelerators.
    Some one-time customizations are performed here.
    Others are dynamically tested throughout idlelib by calls to the
    is_using_native_osx_tk(), isCarbonTk(), isCocoaTk(), isXQuartz() functions which
    are initialized here as well.
    """"""
    if is_using_native_osx_tk():
        try_hide_tk_console(root)
        replace_root_menu_for_idle(root, flist)
        add_open_event_support(root, flist)
        remove_bad_aquatk_bindings(root)","{""fixb2context"": ""remove_bad_aquatk_bindings"", ""addOpenEventSupport"": ""add_open_event_support"", ""isAquaTk"": ""is_using_native_osx_tk"", ""overrideRootMenu"": ""replace_root_menu_for_idle"", ""hideTkConsole"": ""try_hide_tk_console""}","def 0(root):
    '''Removed bad AquaTk Button-2 (right) and Paste bindings.

    They prevent context menu access and seem to be gone in AquaTk8.6.
    See issue #24801.
    '''
    root.unbind_class('Text', '<B2>')
    root.unbind_class('Text', '<B2-Motion>')
    root.unbind_class('Text', '<<PasteSelection>>')

def 1(root, flist):
    """"""
    This ensures that the application will respond to open AppleEvents, which
    makes is feasible to use IDLE as the default application for python files.
    """"""
    def doOpenFile(*args):
        for fn in args:
            flist.open(fn)

    # The command below is a hook in aquatk that is called whenever the app
    # receives a file open event. The callback can have multiple arguments,
    # one for every file that should be opened.
    root.createcommand(""::tk::mac::OpenDocument"", doOpenFile)

def 3():
    """"""
    Returns True if IDLE is using a native OS X Tk (Cocoa or Carbon).
    """"""
    if not _tk_type:
        _init_tk_type()
    return _tk_type == ""cocoa"" or _tk_type == ""carbon""

def 4(root, flist):
    """"""
    Replace the Tk root menu by something that is more appropriate for
    IDLE with an Aqua Tk.
    """"""
    # The menu that is attached to the Tk root (""."") is also used by AquaTk for
    # all windows that don't specify a menu of their own. The default menubar
    # contains a number of menus, none of which are appropriate for IDLE. The
    # Most annoying of those is an 'About Tck/Tk...' menu in the application
    # menu.
    #
    # This function replaces the default menubar by a mostly empty one, it
    # should only contain the correct application menu and the window menu.
    #
    # Due to a (mis-)feature of TkAqua the user will also see an empty Help
    # menu.
    from tkinter import Menu
    from idlelib import mainmenu
    from idlelib import window

    closeItem = mainmenu.menudefs[0][1][-2]

    # Remove the last 3 items of the file menu: a separator, close window and
    # quit. Close window will be reinserted just above the save item, where
    # it should be according to the HIG. Quit is in the application menu.
    del mainmenu.menudefs[0][1][-3:]
    mainmenu.menudefs[0][1].insert(6, closeItem)

    # Remove the 'About' entry from the help menu, it is in the application
    # menu
    del mainmenu.menudefs[-1][1][0:2]
    # Remove the 'Configure Idle' entry from the options menu, it is in the
    # application menu as 'Preferences'
    del mainmenu.menudefs[-3][1][0:2]
    menubar = Menu(root)
    root.configure(menu=menubar)
    menudict = {}

    menudict['window'] = menu = Menu(menubar, name='window', tearoff=0)
    menubar.add_cascade(label='Window', menu=menu, underline=0)

    def postwindowsmenu(menu=menu):
        end = menu.index('end')
        if end is None:
            end = -1

        if end > 0:
            menu.delete(0, end)
        window.add_windows_to_menu(menu)
    window.register_callback(postwindowsmenu)

    def about_dialog(event=None):
        ""Handle Help 'About IDLE' event.""
        # Synchronize with editor.EditorWindow.about_dialog.
        from idlelib import help_about
        help_about.AboutDialog(root)

    def config_dialog(event=None):
        ""Handle Options 'Configure IDLE' event.""
        # Synchronize with editor.EditorWindow.config_dialog.
        from idlelib import configdialog

        # Ensure that the root object has an instance_dict attribute,
        # mirrors code in EditorWindow (although that sets the attribute
        # on an EditorWindow instance that is then passed as the first
        # argument to ConfigDialog)
        root.instance_dict = flist.inversedict
        configdialog.ConfigDialog(root, 'Settings')

    def help_dialog(event=None):
        ""Handle Help 'IDLE Help' event.""
        # Synchronize with editor.EditorWindow.help_dialog.
        from idlelib import help
        help.show_idlehelp(root)

    root.bind('<<about-idle>>', about_dialog)
    root.bind('<<open-config-dialog>>', config_dialog)
    root.createcommand('::tk::mac::ShowPreferences', config_dialog)
    if flist:
        root.bind('<<close-all-windows>>', flist.close_all_callback)

        # The binding above doesn't reliably work on all versions of Tk
        # on macOS. Adding command definition below does seem to do the
        # right thing for now.
        root.createcommand('exit', flist.close_all_callback)

    if isCarbonTk():
        # for Carbon AquaTk, replace the default Tk apple menu
        menudict['application'] = menu = Menu(menubar, name='apple',
                                              tearoff=0)
        menubar.add_cascade(label='IDLE', menu=menu)
        mainmenu.menudefs.insert(0,
            ('application', [
                ('About IDLE', '<<about-idle>>'),
                    None,
                ]))
    if isCocoaTk():
        # replace default About dialog with About IDLE one
        root.createcommand('tkAboutDialog', about_dialog)
        # replace default ""Help"" item in Help menu
        root.createcommand('::tk::mac::ShowHelp', help_dialog)
        # remove redundant ""IDLE Help"" from menu
        del mainmenu.menudefs[-1][1][0]

def 9(root):
    try:
        root.tk.call('console', 'hide')
    except tkinter.TclError:
        # Some versions of the Tk framework don't have a console object
        pass","def 10(root, flist):
    """"""
    Perform initial OS X customizations if needed.
    Called from pyshell.main() after initial calls to Tk()

    There are currently three major versions of Tk in use on OS X:
        1. Aqua Cocoa Tk (native default since OS X 10.6)
        2. Aqua Carbon Tk (original native, 32-bit only, deprecated)
        3. X11 (supported by some third-party distributors, deprecated)
    There are various differences among the three that affect IDLE
    behavior, primarily with menus, mouse key events, and accelerators.
    Some one-time customizations are performed here.
    Others are dynamically tested throughout idlelib by calls to the
    3(), isCarbonTk(), isCocoaTk(), isXQuartz() functions which
    are initialized here as well.
    """"""
    if 3():
        9(root)
        4(root, flist)
        1(root, flist)
        0(root)","{""fixb2context"": ""0"", ""addOpenEventSupport"": ""1"", ""isAquaTk"": ""3"", ""overrideRootMenu"": ""4"", ""hideTkConsole"": ""9""}"
600,600,"def wrap_info(info):
    ""replace info[2], a traceback instance, by its ID""
    if info is None:
        return None
    else:
        traceback = info[2]
        assert isinstance(traceback, types.TracebackType)
        traceback_id = id(traceback)
        tracebacktable[traceback_id] = traceback
        modified_info = (info[0], info[1], traceback_id)
        return modified_info","def id() -> str:
    """"""
    Return the distro ID of the current distribution, as a
    machine-readable string.

    For a number of OS distributions, the returned distro ID value is
    *reliable*, in the sense that it is documented and that it does not change
    across releases of the distribution.

    This package maintains the following reliable distro ID values:

    ==============  =========================================
    Distro ID       Distribution
    ==============  =========================================
    ""ubuntu""        Ubuntu
    ""debian""        Debian
    ""rhel""          RedHat Enterprise Linux
    ""centos""        CentOS
    ""fedora""        Fedora
    ""sles""          SUSE Linux Enterprise Server
    ""opensuse""      openSUSE
    ""amzn""          Amazon Linux
    ""arch""          Arch Linux
    ""cloudlinux""    CloudLinux OS
    ""exherbo""       Exherbo Linux
    ""gentoo""        GenToo Linux
    ""ibm_powerkvm""  IBM PowerKVM
    ""kvmibm""        KVM for IBM z Systems
    ""linuxmint""     Linux Mint
    ""mageia""        Mageia
    ""mandriva""      Mandriva Linux
    ""parallels""     Parallels
    ""pidora""        Pidora
    ""raspbian""      Raspbian
    ""oracle""        Oracle Linux (and Oracle Enterprise Linux)
    ""scientific""    Scientific Linux
    ""slackware""     Slackware
    ""xenserver""     XenServer
    ""openbsd""       OpenBSD
    ""netbsd""        NetBSD
    ""freebsd""       FreeBSD
    ""midnightbsd""   MidnightBSD
    ""rocky""         Rocky Linux
    ""aix""           AIX
    ==============  =========================================

    If you have a need to get distros for reliable IDs added into this set,
    or if you find that the :func:`distro.id` function returns a different
    distro ID for one of the listed distros, please create an issue in the
    `distro issue tracker`_.

    **Lookup hierarchy and transformations:**

    First, the ID is obtained from the following sources, in the specified
    order. The first available and non-empty value is used:

    * the value of the ""ID"" attribute of the os-release file,

    * the value of the ""Distributor ID"" attribute returned by the lsb_release
      command,

    * the first part of the file name of the distro release file,

    The so determined ID value then passes the following transformations,
    before it is returned by this method:

    * it is translated to lower case,

    * blanks (which should not be there anyway) are translated to underscores,

    * a normalization of the ID is performed, based upon
      `normalization tables`_. The purpose of this normalization is to ensure
      that the ID is as reliable as possible, even across incompatible changes
      in the OS distributions. A common reason for an incompatible change is
      the addition of an os-release file, or the addition of the lsb_release
      command, with ID values that differ from what was previously determined
      from the distro release file name.
    """"""
    return _distro.id()","def id() -> str:
    """"""
    Return the distro ID of the current distribution, as a
    machine-readable string.

    For a number of OS distributions, the returned distro ID value is
    *reliable*, in the sense that it is documented and that it does not change
    across releases of the distribution.

    This package maintains the following reliable distro ID values:

    ==============  =========================================
    Distro ID       Distribution
    ==============  =========================================
    ""ubuntu""        Ubuntu
    ""debian""        Debian
    ""rhel""          RedHat Enterprise Linux
    ""centos""        CentOS
    ""fedora""        Fedora
    ""sles""          SUSE Linux Enterprise Server
    ""opensuse""      openSUSE
    ""amzn""          Amazon Linux
    ""arch""          Arch Linux
    ""cloudlinux""    CloudLinux OS
    ""exherbo""       Exherbo Linux
    ""gentoo""        GenToo Linux
    ""ibm_powerkvm""  IBM PowerKVM
    ""kvmibm""        KVM for IBM z Systems
    ""linuxmint""     Linux Mint
    ""mageia""        Mageia
    ""mandriva""      Mandriva Linux
    ""parallels""     Parallels
    ""pidora""        Pidora
    ""raspbian""      Raspbian
    ""oracle""        Oracle Linux (and Oracle Enterprise Linux)
    ""scientific""    Scientific Linux
    ""slackware""     Slackware
    ""xenserver""     XenServer
    ""openbsd""       OpenBSD
    ""netbsd""        NetBSD
    ""freebsd""       FreeBSD
    ""midnightbsd""   MidnightBSD
    ""rocky""         Rocky Linux
    ""aix""           AIX
    ==============  =========================================

    If you have a need to get distros for reliable IDs added into this set,
    or if you find that the :func:`distro.id` function returns a different
    distro ID for one of the listed distros, please create an issue in the
    `distro issue tracker`_.

    **Lookup hierarchy and transformations:**

    First, the ID is obtained from the following sources, in the specified
    order. The first available and non-empty value is used:

    * the value of the ""ID"" attribute of the os-release file,

    * the value of the ""Distributor ID"" attribute returned by the lsb_release
      command,

    * the first part of the file name of the distro release file,

    The so determined ID value then passes the following transformations,
    before it is returned by this method:

    * it is translated to lower case,

    * blanks (which should not be there anyway) are translated to underscores,

    * a normalization of the ID is performed, based upon
      `normalization tables`_. The purpose of this normalization is to ensure
      that the ID is as reliable as possible, even across incompatible changes
      in the OS distributions. A common reason for an incompatible change is
      the addition of an os-release file, or the addition of the lsb_release
      command, with ID values that differ from what was previously determined
      from the distro release file name.
    """"""
    return _distro.id()","def wrap_info_replace_traceback_id(info):
    ""replace info[2], a traceback instance, by its ID""
    if info is None:
        return None
    else:
        traceback = info[2]
        assert isinstance(traceback, types.TracebackType)
        traceback_id = id(traceback)
        tracebacktable[traceback_id] = traceback
        modified_info = (info[0], info[1], traceback_id)
        return modified_info",{},"def id() -> str:
    """"""
    Return the distro ID of the current distribution, as a
    machine-readable string.

    For a number of OS distributions, the returned distro ID value is
    *reliable*, in the sense that it is documented and that it does not change
    across releases of the distribution.

    This package maintains the following reliable distro ID values:

    ==============  =========================================
    Distro ID       Distribution
    ==============  =========================================
    ""ubuntu""        Ubuntu
    ""debian""        Debian
    ""rhel""          RedHat Enterprise Linux
    ""centos""        CentOS
    ""fedora""        Fedora
    ""sles""          SUSE Linux Enterprise Server
    ""opensuse""      openSUSE
    ""amzn""          Amazon Linux
    ""arch""          Arch Linux
    ""cloudlinux""    CloudLinux OS
    ""exherbo""       Exherbo Linux
    ""gentoo""        GenToo Linux
    ""ibm_powerkvm""  IBM PowerKVM
    ""kvmibm""        KVM for IBM z Systems
    ""linuxmint""     Linux Mint
    ""mageia""        Mageia
    ""mandriva""      Mandriva Linux
    ""parallels""     Parallels
    ""pidora""        Pidora
    ""raspbian""      Raspbian
    ""oracle""        Oracle Linux (and Oracle Enterprise Linux)
    ""scientific""    Scientific Linux
    ""slackware""     Slackware
    ""xenserver""     XenServer
    ""openbsd""       OpenBSD
    ""netbsd""        NetBSD
    ""freebsd""       FreeBSD
    ""midnightbsd""   MidnightBSD
    ""rocky""         Rocky Linux
    ""aix""           AIX
    ==============  =========================================

    If you have a need to get distros for reliable IDs added into this set,
    or if you find that the :func:`distro.id` function returns a different
    distro ID for one of the listed distros, please create an issue in the
    `distro issue tracker`_.

    **Lookup hierarchy and transformations:**

    First, the ID is obtained from the following sources, in the specified
    order. The first available and non-empty value is used:

    * the value of the ""ID"" attribute of the os-release file,

    * the value of the ""Distributor ID"" attribute returned by the lsb_release
      command,

    * the first part of the file name of the distro release file,

    The so determined ID value then passes the following transformations,
    before it is returned by this method:

    * it is translated to lower case,

    * blanks (which should not be there anyway) are translated to underscores,

    * a normalization of the ID is performed, based upon
      `normalization tables`_. The purpose of this normalization is to ensure
      that the ID is as reliable as possible, even across incompatible changes
      in the OS distributions. A common reason for an incompatible change is
      the addition of an os-release file, or the addition of the lsb_release
      command, with ID values that differ from what was previously determined
      from the distro release file name.
    """"""
    return _distro.id()","def 10(info):
    ""replace info[2], a traceback instance, by its ID""
    if info is None:
        return None
    else:
        traceback = info[2]
        assert isinstance(traceback, types.TracebackType)
        traceback_id = id(traceback)
        tracebacktable[traceback_id] = traceback
        modified_info = (info[0], info[1], traceback_id)
        return modified_info",{}
601,601,"def close_remote_debugger(rpcclt):
    """"""Shut down subprocess debugger and Idle side of debugger RPC link

    Request that the RPCServer shut down the subprocess debugger and link.
    Unregister the GUIAdapter, which will cause a GC on the Idle process
    debugger and RPC link objects.  (The second reference to the debugger GUI
    is deleted in pyshell.close_remote_debugger().)

    """"""
    close_subprocess_debugger(rpcclt)
    rpcclt.unregister(gui_adap_oid)","def close_subprocess_debugger(rpcclt):
    rpcclt.remotecall(""exec"", ""stop_the_debugger"", (idb_adap_oid,), {})","def close_debugger_subprocess(rpcclt):
    rpcclt.remotecall(""exec"", ""stop_the_debugger"", (idb_adap_oid,), {})","def close_remote_debugger_shutdown_link(rpcclt):
    """"""Shut down subprocess debugger and Idle side of debugger RPC link

    Request that the RPCServer shut down the subprocess debugger and link.
    Unregister the GUIAdapter, which will cause a GC on the Idle process
    debugger and RPC link objects.  (The second reference to the debugger GUI
    is deleted in pyshell.close_remote_debugger_shutdown_link().)

    """"""
    close_debugger_subprocess(rpcclt)
    rpcclt.unregister(gui_adap_oid)","{""close_subprocess_debugger"": ""close_debugger_subprocess""}","def 0(rpcclt):
    rpcclt.remotecall(""exec"", ""stop_the_debugger"", (idb_adap_oid,), {})","def 1(rpcclt):
    """"""Shut down subprocess debugger and Idle side of debugger RPC link

    Request that the RPCServer shut down the subprocess debugger and link.
    Unregister the GUIAdapter, which will cause a GC on the Idle process
    debugger and RPC link objects.  (The second reference to the debugger GUI
    is deleted in pyshell.1().)

    """"""
    0(rpcclt)
    rpcclt.unregister(gui_adap_oid)","{""close_subprocess_debugger"": ""0""}"
602,602,"def format_selection(format_line):
    ""Apply a formatting function to all of the selected lines.""

    @wraps(format_line)
    def apply(self, event=None):
        head, tail, chars, lines = self.formatter.get_region()
        for pos in range(len(lines) - 1):
            line = lines[pos]
            lines[pos] = format_line(self, line)
        self.formatter.set_region(head, tail, chars, lines)
        return 'break'

    return apply","def wraps(wrapped,
          assigned = WRAPPER_ASSIGNMENTS,
          updated = WRAPPER_UPDATES):
    """"""Decorator factory to apply update_wrapper() to a wrapper function

       Returns a decorator that invokes update_wrapper() with the decorated
       function as the wrapper argument and the arguments to wraps() as the
       remaining arguments. Default arguments are as for update_wrapper().
       This is a convenience function to simplify applying partial() to
       update_wrapper().
    """"""
    return partial(update_wrapper, wrapped=wrapped,
                   assigned=assigned, updated=updated)","def create_decorator_with_update_wrapper(wrapped,
          assigned = WRAPPER_ASSIGNMENTS,
          updated = WRAPPER_UPDATES):
    """"""Decorator factory to apply update_wrapper() to a wrapper function

       Returns a decorator that invokes update_wrapper() with the decorated
       function as the wrapper argument and the arguments to create_decorator_with_update_wrapper() as the
       remaining arguments. Default arguments are as for update_wrapper().
       This is a convenience function to simplify applying partial() to
       update_wrapper().
    """"""
    return partial(update_wrapper, wrapped=wrapped,
                   assigned=assigned, updated=updated)","def format_selection_apply_format_line_to_selected_lines_format_line(format_line):
    ""Apply a formatting function to all of the selected lines.""

    @create_decorator_with_update_wrapper(format_line)
    def apply_format_line_to_selected_lines(self, event=None):
        head, tail, chars, lines = self.formatter.get_region()
        for pos in range(len(lines) - 1):
            line = lines[pos]
            lines[pos] = format_line(self, line)
        self.formatter.set_region(head, tail, chars, lines)
        return 'break'

    return apply_format_line_to_selected_lines","{""wraps"": ""create_decorator_with_update_wrapper""}","def 0(wrapped,
          assigned = WRAPPER_ASSIGNMENTS,
          updated = WRAPPER_UPDATES):
    """"""Decorator factory to apply update_wrapper() to a wrapper function

       Returns a decorator that invokes update_wrapper() with the decorated
       function as the wrapper argument and the arguments to 0() as the
       remaining arguments. Default arguments are as for update_wrapper().
       This is a convenience function to simplify applying partial() to
       update_wrapper().
    """"""
    return partial(update_wrapper, wrapped=wrapped,
                   assigned=assigned, updated=updated)","def 1(format_line):
    ""Apply a formatting function to all of the selected lines.""

    @0(format_line)
    def 2(self, event=None):
        head, tail, chars, lines = self.formatter.get_region()
        for pos in range(len(lines) - 1):
            line = lines[pos]
            lines[pos] = format_line(self, line)
        self.formatter.set_region(head, tail, chars, lines)
        return 'break'

    return apply","{""wraps"": ""0""}"
603,603,"def create_task(coro, *, name=None):
    """"""Schedule the execution of a coroutine object in a spawn task.

    Return a Task object.
    """"""
    loop = events.get_running_loop()
    task = loop.create_task(coro)
    _set_task_name(task, name)
    return task","def _set_task_name(task, name):
    if name is not None:
        try:
            set_name = task.set_name
        except AttributeError:
            pass
        else:
            set_name(name)","def set_task_name_if_not_none(task, name):
    if name is not None:
        try:
            set_name = task.set_name
        except AttributeError:
            pass
        else:
            set_name(name)","def create_task_schedule_coroutine(coro, *, name=None):
    """"""Schedule the execution of a coroutine object in a spawn task.

    Return a Task object.
    """"""
    loop = events.get_running_loop()
    task = loop.create_task_schedule_coroutine(coro)
    set_task_name_if_not_none(task, name)
    return task","{""_set_task_name"": ""set_task_name_if_not_none""}","def 0(task, name):
    if name is not None:
        try:
            set_name = task.set_name
        except AttributeError:
            pass
        else:
            set_name(name)","def 1(coro, *, name=None):
    """"""Schedule the execution of a coroutine object in a spawn task.

    Return a Task object.
    """"""
    loop = events.get_running_loop()
    task = loop.1(coro)
    0(task, name)
    return task","{""_set_task_name"": ""0""}"
604,604,"def ensure_future(coro_or_future, *, loop=None):
    """"""Wrap a coroutine or an awaitable in a future.

    If the argument is a Future, it is returned directly.
    """"""
    return _ensure_future(coro_or_future, loop=loop)","def _ensure_future(coro_or_future, *, loop=None):
    if futures.isfuture(coro_or_future):
        if loop is not None and loop is not futures._get_loop(coro_or_future):
            raise ValueError('The future belongs to a different loop than '
                            'the one specified as the loop argument')
        return coro_or_future
    called_wrap_awaitable = False
    if not coroutines.iscoroutine(coro_or_future):
        if inspect.isawaitable(coro_or_future):
            coro_or_future = _wrap_awaitable(coro_or_future)
            called_wrap_awaitable = True
        else:
            raise TypeError('An asyncio.Future, a coroutine or an awaitable '
                            'is required')

    if loop is None:
        loop = events._get_event_loop(stacklevel=4)
    try:
        return loop.create_task(coro_or_future)
    except RuntimeError: 
        if not called_wrap_awaitable:
            coro_or_future.close()
        raise","def ensure_future_internal(coro_or_future, *, loop=None):
    if futures.isfuture(coro_or_future):
        if loop is not None and loop is not futures._get_loop(coro_or_future):
            raise ValueError('The future belongs to a different loop than '
                            'the one specified as the loop argument')
        return coro_or_future
    called_wrap_awaitable = False
    if not coroutines.iscoroutine(coro_or_future):
        if inspect.isawaitable(coro_or_future):
            coro_or_future = _wrap_awaitable(coro_or_future)
            called_wrap_awaitable = True
        else:
            raise TypeError('An asyncio.Future, a coroutine or an awaitable '
                            'is required')

    if loop is None:
        loop = events._get_event_loop(stacklevel=4)
    try:
        return loop.create_task(coro_or_future)
    except RuntimeError: 
        if not called_wrap_awaitable:
            coro_or_future.close()
        raise","def ensure_future_wrap(coro_or_future, *, loop=None):
    """"""Wrap a coroutine or an awaitable in a future.

    If the argument is a Future, it is returned directly.
    """"""
    return ensure_future_wrap_internal(coro_or_future, loop=loop)","{""_ensure_future"": ""ensure_future_internal""}","def 0(coro_or_future, *, loop=None):
    if futures.isfuture(coro_or_future):
        if loop is not None and loop is not futures._get_loop(coro_or_future):
            raise ValueError('The future belongs to a different loop than '
                            'the one specified as the loop argument')
        return coro_or_future
    called_wrap_awaitable = False
    if not coroutines.iscoroutine(coro_or_future):
        if inspect.isawaitable(coro_or_future):
            coro_or_future = _wrap_awaitable(coro_or_future)
            called_wrap_awaitable = True
        else:
            raise TypeError('An asyncio.Future, a coroutine or an awaitable '
                            'is required')

    if loop is None:
        loop = events._get_event_loop(stacklevel=4)
    try:
        return loop.create_task(coro_or_future)
    except RuntimeError: 
        if not called_wrap_awaitable:
            coro_or_future.close()
        raise","def 1(coro_or_future, *, loop=None):
    """"""Wrap a coroutine or an awaitable in a future.

    If the argument is a Future, it is returned directly.
    """"""
    return 0(coro_or_future, loop=loop)","{""_ensure_future"": ""0""}"
605,605,"def run_coroutine_threadsafe(coro, loop):
    """"""Submit a coroutine object to a given event loop.

    Return a concurrent.futures.Future to access the result.
    """"""
    if not coroutines.iscoroutine(coro):
        raise TypeError('A coroutine object is required')
    future = concurrent.futures.Future()

    def callback():
        try:
            futures._chain_future(ensure_future(coro, loop=loop), future)
        except (SystemExit, KeyboardInterrupt):
            raise
        except BaseException as exc:
            if future.set_running_or_notify_cancel():
                future.set_exception(exc)
            raise

    loop.call_soon_threadsafe(callback)
    return future","def ensure_future(coro_or_future, *, loop=None):
    """"""Wrap a coroutine or an awaitable in a future.

    If the argument is a Future, it is returned directly.
    """"""
    return _ensure_future(coro_or_future, loop=loop)","def ensure_future_wrap(coro_or_future, *, loop=None):
    """"""Wrap a coroutine or an awaitable in a future.

    If the argument is a Future, it is returned directly.
    """"""
    return _ensure_future_wrap(coro_or_future, loop=loop)","def run_coroutine_threadsafe_submit_to_loop(coro, loop):
    """"""Submit a coroutine object to a given event loop.

    Return a concurrent.futures.Future to access the result.
    """"""
    if not coroutines.iscoroutine(coro):
        raise TypeError('A coroutine object is required')
    future = concurrent.futures.Future()

    def callback_that_processes_matched_groups():
        try:
            futures._chain_future(ensure_future_wrap(coro, loop=loop), future)
        except (SystemExit, KeyboardInterrupt):
            raise
        except BaseException as exc:
            if future.set_running_or_notify_cancel():
                future.set_exception(exc)
            raise

    loop.call_soon_threadsafe(callback_that_processes_matched_groups)
    return future","{""ensure_future"": ""ensure_future_wrap""}","def 0(coro_or_future, *, loop=None):
    """"""Wrap a coroutine or an awaitable in a future.

    If the argument is a Future, it is returned directly.
    """"""
    return _0(coro_or_future, loop=loop)","def 1(coro, loop):
    """"""Submit a coroutine object to a given event loop.

    Return a concurrent.futures.Future to access the result.
    """"""
    if not coroutines.iscoroutine(coro):
        raise TypeError('A coroutine object is required')
    future = concurrent.futures.Future()

    def 2():
        try:
            futures._chain_future(0(coro, loop=loop), future)
        except (SystemExit, KeyboardInterrupt):
            raise
        except BaseException as exc:
            if future.set_running_or_notify_cancel():
                future.set_exception(exc)
            raise

    loop.call_soon_threadsafe(callback)
    return future","{""ensure_future"": ""0""}"
606,606,"def get_running_loop():
    """"""Return the running event loop.  Raise a RuntimeError if there is none.

    This function is thread-specific.
    """"""
    # NOTE: this function is implemented in C (see _asynciomodule.c)
    loop = _get_running_loop()
    if loop is None:
        raise RuntimeError('no running event loop')
    return loop","def _get_running_loop():
    """"""Return the running event loop or None.

    This is a low-level function intended to be used by event loops.
    This function is thread-specific.
    """"""
    # NOTE: this function is implemented in C (see _asynciomodule.c)
    running_loop, pid = _running_loop.loop_pid
    if running_loop is not None and pid == os.getpid():
        return running_loop","def get_running_event_loop():
    """"""Return the running event loop or None.

    This is a low-level function intended to be used by event loops.
    This function is thread-specific.
    """"""
    # NOTE: this function is implemented in C (see _asynciomodule.c)
    running_loop, pid = _running_loop.loop_pid
    if running_loop is not None and pid == os.getpid():
        return running_loop","def get_running_loop_raise_runtime_error():
    """"""Return the running event loop.  Raise a RuntimeError if there is none.

    This function is thread-specific.
    """"""
    # NOTE: this function is implemented in C (see _asynciomodule.c)
    loop = get_running_event_loop()
    if loop is None:
        raise RuntimeError('no running event loop')
    return loop","{""_get_running_loop"": ""get_running_event_loop""}","def 0():
    """"""Return the running event loop or None.

    This is a low-level function intended to be used by event loops.
    This function is thread-specific.
    """"""
    # NOTE: this function is implemented in C (see _asynciomodule.c)
    running_loop, pid = _running_loop.loop_pid
    if running_loop is not None and pid == os.getpid():
        return running_loop","def 1():
    """"""Return the running event loop.  Raise a RuntimeError if there is none.

    This function is thread-specific.
    """"""
    # NOTE: this function is implemented in C (see _asynciomodule.c)
    loop = 0()
    if loop is None:
        raise RuntimeError('no running event loop')
    return loop","{""_get_running_loop"": ""0""}"
607,607,"def get_event_loop_policy():
    """"""Get the current event loop policy.""""""
    if _event_loop_policy is None:
        _init_event_loop_policy()
    return _event_loop_policy","def _init_event_loop_policy():
    global _event_loop_policy
    with _lock:
        if _event_loop_policy is None:  # pragma: no branch
            from . import DefaultEventLoopPolicy
            _event_loop_policy = DefaultEventLoopPolicy()","def initialize_global_event_loop_policy():
    global _event_loop_policy
    with _lock:
        if _event_loop_policy is None:  # pragma: no branch
            from . import DefaultEventLoopPolicy
            _event_loop_policy = DefaultEventLoopPolicy()","def get_current_event_loop_policy():
    """"""Get the current event loop policy.""""""
    if _event_loop_policy is None:
        initialize_global_event_loop_policy()
    return _event_loop_policy","{""_init_event_loop_policy"": ""initialize_global_event_loop_policy""}","def 0():
    global _event_loop_policy
    with _lock:
        if _event_loop_policy is None:  # pragma: no branch
            from . import DefaultEventLoopPolicy
            _event_loop_policy = DefaultEventLoopPolicy()","def 1():
    """"""Get the current event loop policy.""""""
    if _event_loop_policy is None:
        0()
    return _event_loop_policy","{""_init_event_loop_policy"": ""0""}"
608,608,"def set_event_loop(loop):
    """"""Equivalent to calling get_event_loop_policy().set_event_loop(loop).""""""
    get_event_loop_policy().set_event_loop(loop)","def get_event_loop_policy():
    """"""Get the current event loop policy.""""""
    if _event_loop_policy is None:
        _init_event_loop_policy()
    return _event_loop_policy","def get_current_event_loop_policy():
    """"""Get the current event loop policy.""""""
    if _event_loop_policy is None:
        _init_event_loop_policy()
    return _event_loop_policy","def set_event_loop_via_event_loop_policy(loop):
    """"""Equivalent to calling get_current_event_loop_policy().set_event_loop_via_event_loop_policy(loop).""""""
    get_current_event_loop_policy().set_event_loop_via_event_loop_policy(loop)","{""get_event_loop_policy"": ""get_current_event_loop_policy""}","def 0():
    """"""Get the current event loop policy.""""""
    if _event_loop_policy is None:
        _init_event_loop_policy()
    return _event_loop_policy","def 1(loop):
    """"""Equivalent to calling 0().1(loop).""""""
    0().1(loop)","{""get_event_loop_policy"": ""0""}"
609,609,"def new_event_loop():
    """"""Equivalent to calling get_event_loop_policy().new_event_loop().""""""
    return get_event_loop_policy().new_event_loop()","def get_event_loop_policy():
    """"""Get the current event loop policy.""""""
    if _event_loop_policy is None:
        _init_event_loop_policy()
    return _event_loop_policy","def get_current_event_loop_policy():
    """"""Get the current event loop policy.""""""
    if _event_loop_policy is None:
        _init_event_loop_policy()
    return _event_loop_policy","def new_event_loop_via_event_loop_policy():
    """"""Equivalent to calling get_current_event_loop_policy().new_event_loop_via_event_loop_policy().""""""
    return get_current_event_loop_policy().new_event_loop_via_event_loop_policy()","{""get_event_loop_policy"": ""get_current_event_loop_policy""}","def 0():
    """"""Get the current event loop policy.""""""
    if _event_loop_policy is None:
        _init_event_loop_policy()
    return _event_loop_policy","def 1():
    """"""Equivalent to calling 0().1().""""""
    return 0().1()","{""get_event_loop_policy"": ""0""}"
610,610,"def get_child_watcher():
    """"""Equivalent to calling get_event_loop_policy().get_child_watcher().""""""
    return get_event_loop_policy().get_child_watcher()","def get_event_loop_policy():
    """"""Get the current event loop policy.""""""
    if _event_loop_policy is None:
        _init_event_loop_policy()
    return _event_loop_policy","def get_current_event_loop_policy():
    """"""Get the current event loop policy.""""""
    if _event_loop_policy is None:
        _init_event_loop_policy()
    return _event_loop_policy","def get_child_watcher_via_event_loop_policy():
    """"""Equivalent to calling get_current_event_loop_policy().get_child_watcher_via_event_loop_policy().""""""
    return get_current_event_loop_policy().get_child_watcher_via_event_loop_policy()","{""get_event_loop_policy"": ""get_current_event_loop_policy""}","def 0():
    """"""Get the current event loop policy.""""""
    if _event_loop_policy is None:
        _init_event_loop_policy()
    return _event_loop_policy","def 1():
    """"""Equivalent to calling 0().1().""""""
    return 0().1()","{""get_event_loop_policy"": ""0""}"
611,611,"def set_child_watcher(watcher):
    """"""Equivalent to calling
    get_event_loop_policy().set_child_watcher(watcher).""""""
    return get_event_loop_policy().set_child_watcher(watcher)","def get_event_loop_policy():
    """"""Get the current event loop policy.""""""
    if _event_loop_policy is None:
        _init_event_loop_policy()
    return _event_loop_policy","def get_current_event_loop_policy():
    """"""Get the current event loop policy.""""""
    if _event_loop_policy is None:
        _init_event_loop_policy()
    return _event_loop_policy","def set_child_watcher_via_event_loop_policy(watcher):
    """"""Equivalent to calling
    get_current_event_loop_policy().set_child_watcher_via_event_loop_policy(watcher).""""""
    return get_current_event_loop_policy().set_child_watcher_via_event_loop_policy(watcher)","{""get_event_loop_policy"": ""get_current_event_loop_policy""}","def 0():
    """"""Get the current event loop policy.""""""
    if _event_loop_policy is None:
        _init_event_loop_policy()
    return _event_loop_policy","def 1(watcher):
    """"""Equivalent to calling
    0().1(watcher).""""""
    return 0().1(watcher)","{""get_event_loop_policy"": ""0""}"
612,612,"def _set_concurrent_future_state(concurrent, source):
    """"""Copy state from a future to a concurrent.futures.Future.""""""
    assert source.done()
    if source.cancelled():
        concurrent.cancel()
    if not concurrent.set_running_or_notify_cancel():
        return
    exception = source.exception()
    if exception is not None:
        concurrent.set_exception(_convert_future_exc(exception))
    else:
        result = source.result()
        concurrent.set_result(result)","def _convert_future_exc(exc):
    exc_class = type(exc)
    if exc_class is concurrent.futures.CancelledError:
        return exceptions.CancelledError(*exc.args)
    elif exc_class is concurrent.futures.TimeoutError:
        return exceptions.TimeoutError(*exc.args)
    elif exc_class is concurrent.futures.InvalidStateError:
        return exceptions.InvalidStateError(*exc.args)
    else:
        return exc","def convert_future_exception(exc):
    exc_class = type(exc)
    if exc_class is concurrent.futures.CancelledError:
        return exceptions.CancelledError(*exc.args)
    elif exc_class is concurrent.futures.TimeoutError:
        return exceptions.TimeoutError(*exc.args)
    elif exc_class is concurrent.futures.InvalidStateError:
        return exceptions.InvalidStateError(*exc.args)
    else:
        return exc","def set_concurrent_future_state_from_future(concurrent, source):
    """"""Copy state from a future to a concurrent.futures.Future.""""""
    assert source.done()
    if source.cancelled():
        concurrent.cancel()
    if not concurrent.set_running_or_notify_cancel():
        return
    exception = source.exception()
    if exception is not None:
        concurrent.set_exception(convert_future_exception(exception))
    else:
        result = source.result()
        concurrent.set_result(result)","{""_convert_future_exc"": ""convert_future_exception""}","def 0(exc):
    exc_class = type(exc)
    if exc_class is concurrent.futures.CancelledError:
        return exceptions.CancelledError(*exc.args)
    elif exc_class is concurrent.futures.TimeoutError:
        return exceptions.TimeoutError(*exc.args)
    elif exc_class is concurrent.futures.InvalidStateError:
        return exceptions.InvalidStateError(*exc.args)
    else:
        return exc","def 1(concurrent, source):
    """"""Copy state from a future to a concurrent.futures.Future.""""""
    assert source.done()
    if source.cancelled():
        concurrent.cancel()
    if not concurrent.set_running_or_notify_cancel():
        return
    exception = source.exception()
    if exception is not None:
        concurrent.set_exception(0(exception))
    else:
        result = source.result()
        concurrent.set_result(result)","{""_convert_future_exc"": ""0""}"
613,613,"def _copy_future_state(source, dest):
    """"""Internal helper to copy state from another Future.

    The other Future may be a concurrent.futures.Future.
    """"""
    assert source.done()
    if dest.cancelled():
        return
    assert not dest.done()
    if source.cancelled():
        dest.cancel()
    else:
        exception = source.exception()
        if exception is not None:
            dest.set_exception(_convert_future_exc(exception))
        else:
            result = source.result()
            dest.set_result(result)","def _convert_future_exc(exc):
    exc_class = type(exc)
    if exc_class is concurrent.futures.CancelledError:
        return exceptions.CancelledError(*exc.args)
    elif exc_class is concurrent.futures.TimeoutError:
        return exceptions.TimeoutError(*exc.args)
    elif exc_class is concurrent.futures.InvalidStateError:
        return exceptions.InvalidStateError(*exc.args)
    else:
        return exc","def convert_future_exception(exc):
    exc_class = type(exc)
    if exc_class is concurrent.futures.CancelledError:
        return exceptions.CancelledError(*exc.args)
    elif exc_class is concurrent.futures.TimeoutError:
        return exceptions.TimeoutError(*exc.args)
    elif exc_class is concurrent.futures.InvalidStateError:
        return exceptions.InvalidStateError(*exc.args)
    else:
        return exc","def copy_future_state_from_another_future(source, dest):
    """"""Internal helper to copy state from another Future.

    The other Future may be a concurrent.futures.Future.
    """"""
    assert source.done()
    if dest.cancelled():
        return
    assert not dest.done()
    if source.cancelled():
        dest.cancel()
    else:
        exception = source.exception()
        if exception is not None:
            dest.set_exception(convert_future_exception(exception))
        else:
            result = source.result()
            dest.set_result(result)","{""_convert_future_exc"": ""convert_future_exception""}","def 0(exc):
    exc_class = type(exc)
    if exc_class is concurrent.futures.CancelledError:
        return exceptions.CancelledError(*exc.args)
    elif exc_class is concurrent.futures.TimeoutError:
        return exceptions.TimeoutError(*exc.args)
    elif exc_class is concurrent.futures.InvalidStateError:
        return exceptions.InvalidStateError(*exc.args)
    else:
        return exc","def 1(source, dest):
    """"""Internal helper to copy state from another Future.

    The other Future may be a concurrent.futures.Future.
    """"""
    assert source.done()
    if dest.cancelled():
        return
    assert not dest.done()
    if source.cancelled():
        dest.cancel()
    else:
        exception = source.exception()
        if exception is not None:
            dest.set_exception(0(exception))
        else:
            result = source.result()
            dest.set_result(result)","{""_convert_future_exc"": ""0""}"
614,614,"def wrap_future(future, *, loop=None):
    """"""Wrap concurrent.futures.Future object.""""""
    if isfuture(future):
        return future
    assert isinstance(future, concurrent.futures.Future), \
        f'concurrent.futures.Future is expected, got {future!r}'
    if loop is None:
        loop = events._get_event_loop()
    new_future = loop.create_future()
    _chain_future(future, new_future)
    return new_future","def isfuture(obj):
    """"""Check for a Future.

    This returns True when obj is a Future instance or is advertising
    itself as duck-type compatible by setting _asyncio_future_blocking.
    See comment in Future for more details.
    """"""
    return (hasattr(obj.__class__, '_asyncio_future_blocking') and
            obj._asyncio_future_blocking is not None)

def _chain_future(source, destination):
    """"""Chain two futures so that when one completes, so does the other.

    The result (or exception) of source will be copied to destination.
    If destination is cancelled, source gets cancelled too.
    Compatible with both asyncio.Future and concurrent.futures.Future.
    """"""
    if not isfuture(source) and not isinstance(source,
                                               concurrent.futures.Future):
        raise TypeError('A future is required for source argument')
    if not isfuture(destination) and not isinstance(destination,
                                                    concurrent.futures.Future):
        raise TypeError('A future is required for destination argument')
    source_loop = _get_loop(source) if isfuture(source) else None
    dest_loop = _get_loop(destination) if isfuture(destination) else None

    def _set_state(future, other):
        if isfuture(future):
            _copy_future_state(other, future)
        else:
            _set_concurrent_future_state(future, other)

    def _call_check_cancel(destination):
        if destination.cancelled():
            if source_loop is None or source_loop is dest_loop:
                source.cancel()
            else:
                source_loop.call_soon_threadsafe(source.cancel)

    def _call_set_state(source):
        if (destination.cancelled() and
                dest_loop is not None and dest_loop.is_closed()):
            return
        if dest_loop is None or dest_loop is source_loop:
            _set_state(destination, source)
        else:
            dest_loop.call_soon_threadsafe(_set_state, destination, source)

    destination.add_done_callback(_call_check_cancel)
    source.add_done_callback(_call_set_state)","def is_future_instance(obj):
    """"""Check for a Future.

    This returns True when obj is a Future instance or is advertising
    itself as duck-type compatible by setting _asyncio_future_blocking.
    See comment in Future for more details.
    """"""
    return (hasattr(obj.__class__, '_asyncio_future_blocking') and
            obj._asyncio_future_blocking is not None)

def chain_two_futures(source, destination):
    """"""Chain two futures so that when one completes, so does the other.

    The result (or exception) of source will be copied to destination.
    If destination is cancelled, source gets cancelled too.
    Compatible with both asyncio.Future and concurrent.futures.Future.
    """"""
    if not is_future_instance(source) and not isinstance(source,
                                               concurrent.futures.Future):
        raise TypeError('A future is required for source argument')
    if not is_future_instance(destination) and not isinstance(destination,
                                                    concurrent.futures.Future):
        raise TypeError('A future is required for destination argument')
    source_loop = _get_loop(source) if is_future_instance(source) else None
    dest_loop = _get_loop(destination) if is_future_instance(destination) else None

    def _set_state(future, other):
        if is_future_instance(future):
            _copy_future_state(other, future)
        else:
            _set_concurrent_future_state(future, other)

    def _call_check_cancel(destination):
        if destination.cancelled():
            if source_loop is None or source_loop is dest_loop:
                source.cancel()
            else:
                source_loop.call_soon_threadsafe(source.cancel)

    def _call_set_state(source):
        if (destination.cancelled() and
                dest_loop is not None and dest_loop.is_closed()):
            return
        if dest_loop is None or dest_loop is source_loop:
            _set_state(destination, source)
        else:
            dest_loop.call_soon_threadsafe(_set_state, destination, source)

    destination.add_done_callback(_call_check_cancel)
    source.add_done_callback(_call_set_state)","def wrap_given_concurrent_futures_Future_object(future, *, loop=None):
    """"""Wrap concurrent.futures.Future object.""""""
    if is_future_instance(future):
        return future
    assert isinstance(future, concurrent.futures.Future), \
        f'concurrent.futures.Future is expected, got {future!r}'
    if loop is None:
        loop = events._get_event_loop()
    new_future = loop.create_future()
    chain_two_futures(future, new_future)
    return new_future","{""isfuture"": ""is_future_instance"", ""_chain_future"": ""chain_two_futures""}","def 0(obj):
    """"""Check for a Future.

    This returns True when obj is a Future instance or is advertising
    itself as duck-type compatible by setting _asyncio_future_blocking.
    See comment in Future for more details.
    """"""
    return (hasattr(obj.__class__, '_asyncio_future_blocking') and
            obj._asyncio_future_blocking is not None)

def 1(source, destination):
    """"""Chain two futures so that when one completes, so does the other.

    The result (or exception) of source will be copied to destination.
    If destination is cancelled, source gets cancelled too.
    Compatible with both asyncio.Future and concurrent.futures.Future.
    """"""
    if not 0(source) and not isinstance(source,
                                               concurrent.futures.Future):
        raise TypeError('A future is required for source argument')
    if not 0(destination) and not isinstance(destination,
                                                    concurrent.futures.Future):
        raise TypeError('A future is required for destination argument')
    source_loop = _get_loop(source) if 0(source) else None
    dest_loop = _get_loop(destination) if 0(destination) else None

    def _set_state(future, other):
        if 0(future):
            _copy_future_state(other, future)
        else:
            _set_concurrent_future_state(future, other)

    def _call_check_cancel(destination):
        if destination.cancelled():
            if source_loop is None or source_loop is dest_loop:
                source.cancel()
            else:
                source_loop.call_soon_threadsafe(source.cancel)

    def _call_set_state(source):
        if (destination.cancelled() and
                dest_loop is not None and dest_loop.is_closed()):
            return
        if dest_loop is None or dest_loop is source_loop:
            _set_state(destination, source)
        else:
            dest_loop.call_soon_threadsafe(_set_state, destination, source)

    destination.add_done_callback(_call_check_cancel)
    source.add_done_callback(_call_set_state)","def 5(future, *, loop=None):
    """"""Wrap concurrent.futures.Future object.""""""
    if 0(future):
        return future
    assert isinstance(future, concurrent.futures.Future), \
        f'concurrent.futures.Future is expected, got {future!r}'
    if loop is None:
        loop = events._get_event_loop()
    new_future = loop.create_future()
    1(future, new_future)
    return new_future","{""isfuture"": ""0"", ""_chain_future"": ""1""}"
615,615,"def _future_repr_info(future):
    # (Future) -> str
    """"""helper function for Future.__repr__""""""
    info = [future._state.lower()]
    if future._state == _FINISHED:
        if future._exception is not None:
            info.append(f'exception={future._exception!r}')
        else:
            key = id(future), get_ident()
            if key in _repr_running:
                result = '...'
            else:
                _repr_running.add(key)
                try:
                    # use reprlib to limit the length of the output, especially
                    # for very long strings
                    result = reprlib.repr(future._result)
                finally:
                    _repr_running.discard(key)
            info.append(f'result={result}')
    if future._callbacks:
        info.append(_format_callbacks(future._callbacks))
    if future._source_traceback:
        frame = future._source_traceback[-1]
        info.append(f'created at {frame[0]}:{frame[1]}')
    return info","def _format_callbacks(cb):
    """"""helper function for Future.__repr__""""""
    size = len(cb)
    if not size:
        cb = ''

    def format_cb(callback):
        return format_helpers._format_callback_source(callback, ())

    if size == 1:
        cb = format_cb(cb[0][0])
    elif size == 2:
        cb = '{}, {}'.format(format_cb(cb[0][0]), format_cb(cb[1][0]))
    elif size > 2:
        cb = '{}, <{} more>, {}'.format(format_cb(cb[0][0]),
                                        size - 2,
                                        format_cb(cb[-1][0]))
    return f'cb=[{cb}]'

def id() -> str:
    """"""
    Return the distro ID of the current distribution, as a
    machine-readable string.

    For a number of OS distributions, the returned distro ID value is
    *reliable*, in the sense that it is documented and that it does not change
    across releases of the distribution.

    This package maintains the following reliable distro ID values:

    ==============  =========================================
    Distro ID       Distribution
    ==============  =========================================
    ""ubuntu""        Ubuntu
    ""debian""        Debian
    ""rhel""          RedHat Enterprise Linux
    ""centos""        CentOS
    ""fedora""        Fedora
    ""sles""          SUSE Linux Enterprise Server
    ""opensuse""      openSUSE
    ""amzn""          Amazon Linux
    ""arch""          Arch Linux
    ""cloudlinux""    CloudLinux OS
    ""exherbo""       Exherbo Linux
    ""gentoo""        GenToo Linux
    ""ibm_powerkvm""  IBM PowerKVM
    ""kvmibm""        KVM for IBM z Systems
    ""linuxmint""     Linux Mint
    ""mageia""        Mageia
    ""mandriva""      Mandriva Linux
    ""parallels""     Parallels
    ""pidora""        Pidora
    ""raspbian""      Raspbian
    ""oracle""        Oracle Linux (and Oracle Enterprise Linux)
    ""scientific""    Scientific Linux
    ""slackware""     Slackware
    ""xenserver""     XenServer
    ""openbsd""       OpenBSD
    ""netbsd""        NetBSD
    ""freebsd""       FreeBSD
    ""midnightbsd""   MidnightBSD
    ""rocky""         Rocky Linux
    ""aix""           AIX
    ==============  =========================================

    If you have a need to get distros for reliable IDs added into this set,
    or if you find that the :func:`distro.id` function returns a different
    distro ID for one of the listed distros, please create an issue in the
    `distro issue tracker`_.

    **Lookup hierarchy and transformations:**

    First, the ID is obtained from the following sources, in the specified
    order. The first available and non-empty value is used:

    * the value of the ""ID"" attribute of the os-release file,

    * the value of the ""Distributor ID"" attribute returned by the lsb_release
      command,

    * the first part of the file name of the distro release file,

    The so determined ID value then passes the following transformations,
    before it is returned by this method:

    * it is translated to lower case,

    * blanks (which should not be there anyway) are translated to underscores,

    * a normalization of the ID is performed, based upon
      `normalization tables`_. The purpose of this normalization is to ensure
      that the ID is as reliable as possible, even across incompatible changes
      in the OS distributions. A common reason for an incompatible change is
      the addition of an os-release file, or the addition of the lsb_release
      command, with ID values that differ from what was previously determined
      from the distro release file name.
    """"""
    return _distro.id()","def format_callbacks_helper(cb):
    """"""helper function for Future.__repr__""""""
    size = len(cb)
    if not size:
        cb = ''

    def format_cb(callback):
        return format_helpers._format_callback_source(callback, ())

    if size == 1:
        cb = format_cb(cb[0][0])
    elif size == 2:
        cb = '{}, {}'.format(format_cb(cb[0][0]), format_cb(cb[1][0]))
    elif size > 2:
        cb = '{}, <{} more>, {}'.format(format_cb(cb[0][0]),
                                        size - 2,
                                        format_cb(cb[-1][0]))
    return f'cb=[{cb}]'

def id() -> str:
    """"""
    Return the distro ID of the current distribution, as a
    machine-readable string.

    For a number of OS distributions, the returned distro ID value is
    *reliable*, in the sense that it is documented and that it does not change
    across releases of the distribution.

    This package maintains the following reliable distro ID values:

    ==============  =========================================
    Distro ID       Distribution
    ==============  =========================================
    ""ubuntu""        Ubuntu
    ""debian""        Debian
    ""rhel""          RedHat Enterprise Linux
    ""centos""        CentOS
    ""fedora""        Fedora
    ""sles""          SUSE Linux Enterprise Server
    ""opensuse""      openSUSE
    ""amzn""          Amazon Linux
    ""arch""          Arch Linux
    ""cloudlinux""    CloudLinux OS
    ""exherbo""       Exherbo Linux
    ""gentoo""        GenToo Linux
    ""ibm_powerkvm""  IBM PowerKVM
    ""kvmibm""        KVM for IBM z Systems
    ""linuxmint""     Linux Mint
    ""mageia""        Mageia
    ""mandriva""      Mandriva Linux
    ""parallels""     Parallels
    ""pidora""        Pidora
    ""raspbian""      Raspbian
    ""oracle""        Oracle Linux (and Oracle Enterprise Linux)
    ""scientific""    Scientific Linux
    ""slackware""     Slackware
    ""xenserver""     XenServer
    ""openbsd""       OpenBSD
    ""netbsd""        NetBSD
    ""freebsd""       FreeBSD
    ""midnightbsd""   MidnightBSD
    ""rocky""         Rocky Linux
    ""aix""           AIX
    ==============  =========================================

    If you have a need to get distros for reliable IDs added into this set,
    or if you find that the :func:`distro.id` function returns a different
    distro ID for one of the listed distros, please create an issue in the
    `distro issue tracker`_.

    **Lookup hierarchy and transformations:**

    First, the ID is obtained from the following sources, in the specified
    order. The first available and non-empty value is used:

    * the value of the ""ID"" attribute of the os-release file,

    * the value of the ""Distributor ID"" attribute returned by the lsb_release
      command,

    * the first part of the file name of the distro release file,

    The so determined ID value then passes the following transformations,
    before it is returned by this method:

    * it is translated to lower case,

    * blanks (which should not be there anyway) are translated to underscores,

    * a normalization of the ID is performed, based upon
      `normalization tables`_. The purpose of this normalization is to ensure
      that the ID is as reliable as possible, even across incompatible changes
      in the OS distributions. A common reason for an incompatible change is
      the addition of an os-release file, or the addition of the lsb_release
      command, with ID values that differ from what was previously determined
      from the distro release file name.
    """"""
    return _distro.id()","def helper_function_for_Future_repr_with_exception_information_and_callbacks(future):
    # (Future) -> str
    """"""helper function for Future.__repr__""""""
    info = [future._state.lower()]
    if future._state == _FINISHED:
        if future._exception is not None:
            info.append(f'exception={future._exception!r}')
        else:
            key = id(future), get_ident()
            if key in _repr_running:
                result = '...'
            else:
                _repr_running.add(key)
                try:
                    # use reprlib to limit the length of the output, especially
                    # for very long strings
                    result = reprlib.repr(future._result)
                finally:
                    _repr_running.discard(key)
            info.append(f'result={result}')
    if future._callbacks:
        info.append(format_callbacks_helper(future._callbacks))
    if future._source_traceback:
        frame = future._source_traceback[-1]
        info.append(f'created at {frame[0]}:{frame[1]}')
    return info","{""_format_callbacks"": ""format_callbacks_helper""}","def 0(cb):
    """"""helper function for Future.__repr__""""""
    size = len(cb)
    if not size:
        cb = ''

    def format_cb(callback):
        return format_helpers._format_callback_source(callback, ())

    if size == 1:
        cb = format_cb(cb[0][0])
    elif size == 2:
        cb = '{}, {}'.format(format_cb(cb[0][0]), format_cb(cb[1][0]))
    elif size > 2:
        cb = '{}, <{} more>, {}'.format(format_cb(cb[0][0]),
                                        size - 2,
                                        format_cb(cb[-1][0]))
    return f'cb=[{cb}]'

def id() -> str:
    """"""
    Return the distro ID of the current distribution, as a
    machine-readable string.

    For a number of OS distributions, the returned distro ID value is
    *reliable*, in the sense that it is documented and that it does not change
    across releases of the distribution.

    This package maintains the following reliable distro ID values:

    ==============  =========================================
    Distro ID       Distribution
    ==============  =========================================
    ""ubuntu""        Ubuntu
    ""debian""        Debian
    ""rhel""          RedHat Enterprise Linux
    ""centos""        CentOS
    ""fedora""        Fedora
    ""sles""          SUSE Linux Enterprise Server
    ""opensuse""      openSUSE
    ""amzn""          Amazon Linux
    ""arch""          Arch Linux
    ""cloudlinux""    CloudLinux OS
    ""exherbo""       Exherbo Linux
    ""gentoo""        GenToo Linux
    ""ibm_powerkvm""  IBM PowerKVM
    ""kvmibm""        KVM for IBM z Systems
    ""linuxmint""     Linux Mint
    ""mageia""        Mageia
    ""mandriva""      Mandriva Linux
    ""parallels""     Parallels
    ""pidora""        Pidora
    ""raspbian""      Raspbian
    ""oracle""        Oracle Linux (and Oracle Enterprise Linux)
    ""scientific""    Scientific Linux
    ""slackware""     Slackware
    ""xenserver""     XenServer
    ""openbsd""       OpenBSD
    ""netbsd""        NetBSD
    ""freebsd""       FreeBSD
    ""midnightbsd""   MidnightBSD
    ""rocky""         Rocky Linux
    ""aix""           AIX
    ==============  =========================================

    If you have a need to get distros for reliable IDs added into this set,
    or if you find that the :func:`distro.id` function returns a different
    distro ID for one of the listed distros, please create an issue in the
    `distro issue tracker`_.

    **Lookup hierarchy and transformations:**

    First, the ID is obtained from the following sources, in the specified
    order. The first available and non-empty value is used:

    * the value of the ""ID"" attribute of the os-release file,

    * the value of the ""Distributor ID"" attribute returned by the lsb_release
      command,

    * the first part of the file name of the distro release file,

    The so determined ID value then passes the following transformations,
    before it is returned by this method:

    * it is translated to lower case,

    * blanks (which should not be there anyway) are translated to underscores,

    * a normalization of the ID is performed, based upon
      `normalization tables`_. The purpose of this normalization is to ensure
      that the ID is as reliable as possible, even across incompatible changes
      in the OS distributions. A common reason for an incompatible change is
      the addition of an os-release file, or the addition of the lsb_release
      command, with ID values that differ from what was previously determined
      from the distro release file name.
    """"""
    return _distro.id()","def 2(future):
    # (Future) -> str
    """"""helper function for Future.__repr__""""""
    info = [future._state.lower()]
    if future._state == _FINISHED:
        if future._exception is not None:
            info.append(f'exception={future._exception!r}')
        else:
            key = id(future), get_ident()
            if key in _repr_running:
                result = '...'
            else:
                _repr_running.add(key)
                try:
                    # use reprlib to limit the length of the output, especially
                    # for very long strings
                    result = reprlib.repr(future._result)
                finally:
                    _repr_running.discard(key)
            info.append(f'result={result}')
    if future._callbacks:
        info.append(0(future._callbacks))
    if future._source_traceback:
        frame = future._source_traceback[-1]
        info.append(f'created at {frame[0]}:{frame[1]}')
    return info","{""_format_callbacks"": ""0""}"
616,616,"def fileConfig(fname, defaults=None, disable_existing_loggers=True, encoding=None):
    """"""
    Read the logging configuration from a ConfigParser-format file.

    This can be called several times from an application, allowing an end user
    the ability to select from various pre-canned configurations (if the
    developer provides a mechanism to present the choices and load the chosen
    configuration).
    """"""
    import configparser

    if isinstance(fname, configparser.RawConfigParser):
        cp = fname
    else:
        cp = configparser.ConfigParser(defaults)
        if hasattr(fname, 'readline'):
            cp.read_file(fname)
        else:
            encoding = io.text_encoding(encoding)
            cp.read(fname, encoding=encoding)

    formatters = _create_formatters(cp)

    # critical section
    logging._acquireLock()
    try:
        _clearExistingHandlers()

        # Handlers add themselves to logging._handlers
        handlers = _install_handlers(cp, formatters)
        _install_loggers(cp, handlers, disable_existing_loggers)
    finally:
        logging._releaseLock()","def _create_formatters(cp):
    """"""Create and return formatters""""""
    flist = cp[""formatters""][""keys""]
    if not len(flist):
        return {}
    flist = flist.split("","")
    flist = _strip_spaces(flist)
    formatters = {}
    for form in flist:
        sectname = ""formatter_%s"" % form
        fs = cp.get(sectname, ""format"", raw=True, fallback=None)
        dfs = cp.get(sectname, ""datefmt"", raw=True, fallback=None)
        stl = cp.get(sectname, ""style"", raw=True, fallback='%')
        c = logging.Formatter
        class_name = cp[sectname].get(""class"")
        if class_name:
            c = _resolve(class_name)
        f = c(fs, dfs, stl)
        formatters[form] = f
    return formatters

def _install_handlers(cp, formatters):
    """"""Install and return handlers""""""
    hlist = cp[""handlers""][""keys""]
    if not len(hlist):
        return {}
    hlist = hlist.split("","")
    hlist = _strip_spaces(hlist)
    handlers = {}
    fixups = [] #for inter-handler references
    for hand in hlist:
        section = cp[""handler_%s"" % hand]
        klass = section[""class""]
        fmt = section.get(""formatter"", """")
        try:
            klass = eval(klass, vars(logging))
        except (AttributeError, NameError):
            klass = _resolve(klass)
        args = section.get(""args"", '()')
        args = eval(args, vars(logging))
        kwargs = section.get(""kwargs"", '{}')
        kwargs = eval(kwargs, vars(logging))
        h = klass(*args, **kwargs)
        h.name = hand
        if ""level"" in section:
            level = section[""level""]
            h.setLevel(level)
        if len(fmt):
            h.setFormatter(formatters[fmt])
        if issubclass(klass, logging.handlers.MemoryHandler):
            target = section.get(""target"", """")
            if len(target): #the target handler may not be loaded yet, so keep for later...
                fixups.append((h, target))
        handlers[hand] = h
    #now all handlers are loaded, fixup inter-handler references...
    for h, t in fixups:
        h.setTarget(handlers[t])
    return handlers

def _clearExistingHandlers():
    """"""Clear and close existing handlers""""""
    logging._handlers.clear()
    logging.shutdown(logging._handlerList[:])
    del logging._handlerList[:]

def _install_loggers(cp, handlers, disable_existing):
    """"""Create and install loggers""""""

    # configure the root first
    llist = cp[""loggers""][""keys""]
    llist = llist.split("","")
    llist = list(_strip_spaces(llist))
    llist.remove(""root"")
    section = cp[""logger_root""]
    root = logging.root
    log = root
    if ""level"" in section:
        level = section[""level""]
        log.setLevel(level)
    for h in root.handlers[:]:
        root.removeHandler(h)
    hlist = section[""handlers""]
    if len(hlist):
        hlist = hlist.split("","")
        hlist = _strip_spaces(hlist)
        for hand in hlist:
            log.addHandler(handlers[hand])

    #and now the others...
    #we don't want to lose the existing loggers,
    #since other threads may have pointers to them.
    #existing is set to contain all existing loggers,
    #and as we go through the new configuration we
    #remove any which are configured. At the end,
    #what's left in existing is the set of loggers
    #which were in the previous configuration but
    #which are not in the new configuration.
    existing = list(root.manager.loggerDict.keys())
    #The list needs to be sorted so that we can
    #avoid disabling child loggers of explicitly
    #named loggers. With a sorted list it is easier
    #to find the child loggers.
    existing.sort()
    #We'll keep the list of existing loggers
    #which are children of named loggers here...
    child_loggers = []
    #now set up the new ones...
    for log in llist:
        section = cp[""logger_%s"" % log]
        qn = section[""qualname""]
        propagate = section.getint(""propagate"", fallback=1)
        logger = logging.getLogger(qn)
        if qn in existing:
            i = existing.index(qn) + 1 # start with the entry after qn
            prefixed = qn + "".""
            pflen = len(prefixed)
            num_existing = len(existing)
            while i < num_existing:
                if existing[i][:pflen] == prefixed:
                    child_loggers.append(existing[i])
                i += 1
            existing.remove(qn)
        if ""level"" in section:
            level = section[""level""]
            logger.setLevel(level)
        for h in logger.handlers[:]:
            logger.removeHandler(h)
        logger.propagate = propagate
        logger.disabled = 0
        hlist = section[""handlers""]
        if len(hlist):
            hlist = hlist.split("","")
            hlist = _strip_spaces(hlist)
            for hand in hlist:
                logger.addHandler(handlers[hand])

    #Disable any old loggers. There's no point deleting
    #them as other threads may continue to hold references
    #and by disabling them, you stop them doing any logging.
    #However, don't disable children of named loggers, as that's
    #probably not what was intended by the user.
    #for log in existing:
    #    logger = root.manager.loggerDict[log]
    #    if log in child_loggers:
    #        logger.level = logging.NOTSET
    #        logger.handlers = []
    #        logger.propagate = 1
    #    elif disable_existing_loggers:
    #        logger.disabled = 1
    _handle_existing_loggers(existing, child_loggers, disable_existing)","def create_log_formatters(cp):
    """"""Create and return formatters""""""
    flist = cp[""formatters""][""keys""]
    if not len(flist):
        return {}
    flist = flist.split("","")
    flist = _strip_spaces(flist)
    formatters = {}
    for form in flist:
        sectname = ""formatter_%s"" % form
        fs = cp.get(sectname, ""format"", raw=True, fallback=None)
        dfs = cp.get(sectname, ""datefmt"", raw=True, fallback=None)
        stl = cp.get(sectname, ""style"", raw=True, fallback='%')
        c = logging.Formatter
        class_name = cp[sectname].get(""class"")
        if class_name:
            c = _resolve(class_name)
        f = c(fs, dfs, stl)
        formatters[form] = f
    return formatters

def install_handlers_from_config(cp, formatters):
    """"""Install and return handlers""""""
    hlist = cp[""handlers""][""keys""]
    if not len(hlist):
        return {}
    hlist = hlist.split("","")
    hlist = _strip_spaces(hlist)
    handlers = {}
    fixups = [] #for inter-handler references
    for hand in hlist:
        section = cp[""handler_%s"" % hand]
        klass = section[""class""]
        fmt = section.get(""formatter"", """")
        try:
            klass = eval(klass, vars(logging))
        except (AttributeError, NameError):
            klass = _resolve(klass)
        args = section.get(""args"", '()')
        args = eval(args, vars(logging))
        kwargs = section.get(""kwargs"", '{}')
        kwargs = eval(kwargs, vars(logging))
        h = klass(*args, **kwargs)
        h.name = hand
        if ""level"" in section:
            level = section[""level""]
            h.setLevel(level)
        if len(fmt):
            h.setFormatter(formatters[fmt])
        if issubclass(klass, logging.handlers.MemoryHandler):
            target = section.get(""target"", """")
            if len(target): #the target handler may not be loaded yet, so keep for later...
                fixups.append((h, target))
        handlers[hand] = h
    #now all handlers are loaded, fixup inter-handler references...
    for h, t in fixups:
        h.setTarget(handlers[t])
    return handlers

def clear_and_close_existing_handlers():
    """"""Clear and close existing handlers""""""
    logging._handlers.clear()
    logging.shutdown(logging._handlerList[:])
    del logging._handlerList[:]

def configure_and_install_loggers(cp, handlers, disable_existing):
    """"""Create and install loggers""""""

    # configure the root first
    llist = cp[""loggers""][""keys""]
    llist = llist.split("","")
    llist = list(_strip_spaces(llist))
    llist.remove(""root"")
    section = cp[""logger_root""]
    root = logging.root
    log = root
    if ""level"" in section:
        level = section[""level""]
        log.setLevel(level)
    for h in root.handlers[:]:
        root.removeHandler(h)
    hlist = section[""handlers""]
    if len(hlist):
        hlist = hlist.split("","")
        hlist = _strip_spaces(hlist)
        for hand in hlist:
            log.addHandler(handlers[hand])

    #and now the others...
    #we don't want to lose the existing loggers,
    #since other threads may have pointers to them.
    #existing is set to contain all existing loggers,
    #and as we go through the new configuration we
    #remove any which are configured. At the end,
    #what's left in existing is the set of loggers
    #which were in the previous configuration but
    #which are not in the new configuration.
    existing = list(root.manager.loggerDict.keys())
    #The list needs to be sorted so that we can
    #avoid disabling child loggers of explicitly
    #named loggers. With a sorted list it is easier
    #to find the child loggers.
    existing.sort()
    #We'll keep the list of existing loggers
    #which are children of named loggers here...
    child_loggers = []
    #now set up the new ones...
    for log in llist:
        section = cp[""logger_%s"" % log]
        qn = section[""qualname""]
        propagate = section.getint(""propagate"", fallback=1)
        logger = logging.getLogger(qn)
        if qn in existing:
            i = existing.index(qn) + 1 # start with the entry after qn
            prefixed = qn + "".""
            pflen = len(prefixed)
            num_existing = len(existing)
            while i < num_existing:
                if existing[i][:pflen] == prefixed:
                    child_loggers.append(existing[i])
                i += 1
            existing.remove(qn)
        if ""level"" in section:
            level = section[""level""]
            logger.setLevel(level)
        for h in logger.handlers[:]:
            logger.removeHandler(h)
        logger.propagate = propagate
        logger.disabled = 0
        hlist = section[""handlers""]
        if len(hlist):
            hlist = hlist.split("","")
            hlist = _strip_spaces(hlist)
            for hand in hlist:
                logger.addHandler(handlers[hand])

    #Disable any old loggers. There's no point deleting
    #them as other threads may continue to hold references
    #and by disabling them, you stop them doing any logging.
    #However, don't disable children of named loggers, as that's
    #probably not what was intended by the user.
    #for log in existing:
    #    logger = root.manager.loggerDict[log]
    #    if log in child_loggers:
    #        logger.level = logging.NOTSET
    #        logger.handlers = []
    #        logger.propagate = 1
    #    elif disable_existing_loggers:
    #        logger.disabled = 1
    _handle_existing_loggers(existing, child_loggers, disable_existing)","def read_logging_configuration_from_ConfigParser_format_file(fname, defaults=None, disable_existing_loggers=True, encoding=None):
    """"""
    Read the logging configuration from a ConfigParser-format file.

    This can be called several times from an application, allowing an end user
    the ability to select from various pre-canned configurations (if the
    developer provides a mechanism to present the choices and load the chosen
    configuration).
    """"""
    import configparser

    if isinstance(fname, configparser.RawConfigParser):
        cp = fname
    else:
        cp = configparser.ConfigParser(defaults)
        if hasattr(fname, 'readline'):
            cp.read_file(fname)
        else:
            encoding = io.text_encoding(encoding)
            cp.read(fname, encoding=encoding)

    formatters = create_log_formatters(cp)

    # critical section
    logging._acquireLock()
    try:
        clear_and_close_existing_handlers()

        # Handlers add themselves to logging._handlers
        handlers = install_handlers_from_config(cp, formatters)
        configure_and_install_loggers(cp, handlers, disable_existing_loggers)
    finally:
        logging._releaseLock()","{""_create_formatters"": ""create_log_formatters"", ""_install_handlers"": ""install_handlers_from_config"", ""_clearExistingHandlers"": ""clear_and_close_existing_handlers"", ""_install_loggers"": ""configure_and_install_loggers""}","def 0(cp):
    """"""Create and return formatters""""""
    flist = cp[""formatters""][""keys""]
    if not len(flist):
        return {}
    flist = flist.split("","")
    flist = _strip_spaces(flist)
    formatters = {}
    for form in flist:
        sectname = ""formatter_%s"" % form
        fs = cp.get(sectname, ""format"", raw=True, fallback=None)
        dfs = cp.get(sectname, ""datefmt"", raw=True, fallback=None)
        stl = cp.get(sectname, ""style"", raw=True, fallback='%')
        c = logging.Formatter
        class_name = cp[sectname].get(""class"")
        if class_name:
            c = _resolve(class_name)
        f = c(fs, dfs, stl)
        formatters[form] = f
    return formatters

def 1(cp, formatters):
    """"""Install and return handlers""""""
    hlist = cp[""handlers""][""keys""]
    if not len(hlist):
        return {}
    hlist = hlist.split("","")
    hlist = _strip_spaces(hlist)
    handlers = {}
    fixups = [] #for inter-handler references
    for hand in hlist:
        section = cp[""handler_%s"" % hand]
        klass = section[""class""]
        fmt = section.get(""formatter"", """")
        try:
            klass = eval(klass, vars(logging))
        except (AttributeError, NameError):
            klass = _resolve(klass)
        args = section.get(""args"", '()')
        args = eval(args, vars(logging))
        kwargs = section.get(""kwargs"", '{}')
        kwargs = eval(kwargs, vars(logging))
        h = klass(*args, **kwargs)
        h.name = hand
        if ""level"" in section:
            level = section[""level""]
            h.setLevel(level)
        if len(fmt):
            h.setFormatter(formatters[fmt])
        if issubclass(klass, logging.handlers.MemoryHandler):
            target = section.get(""target"", """")
            if len(target): #the target handler may not be loaded yet, so keep for later...
                fixups.append((h, target))
        handlers[hand] = h
    #now all handlers are loaded, fixup inter-handler references...
    for h, t in fixups:
        h.setTarget(handlers[t])
    return handlers

def 2():
    """"""Clear and close existing handlers""""""
    logging._handlers.clear()
    logging.shutdown(logging._handlerList[:])
    del logging._handlerList[:]

def 3(cp, handlers, disable_existing):
    """"""Create and install loggers""""""

    # configure the root first
    llist = cp[""loggers""][""keys""]
    llist = llist.split("","")
    llist = list(_strip_spaces(llist))
    llist.remove(""root"")
    section = cp[""logger_root""]
    root = logging.root
    log = root
    if ""level"" in section:
        level = section[""level""]
        log.setLevel(level)
    for h in root.handlers[:]:
        root.removeHandler(h)
    hlist = section[""handlers""]
    if len(hlist):
        hlist = hlist.split("","")
        hlist = _strip_spaces(hlist)
        for hand in hlist:
            log.addHandler(handlers[hand])

    #and now the others...
    #we don't want to lose the existing loggers,
    #since other threads may have pointers to them.
    #existing is set to contain all existing loggers,
    #and as we go through the new configuration we
    #remove any which are configured. At the end,
    #what's left in existing is the set of loggers
    #which were in the previous configuration but
    #which are not in the new configuration.
    existing = list(root.manager.loggerDict.keys())
    #The list needs to be sorted so that we can
    #avoid disabling child loggers of explicitly
    #named loggers. With a sorted list it is easier
    #to find the child loggers.
    existing.sort()
    #We'll keep the list of existing loggers
    #which are children of named loggers here...
    child_loggers = []
    #now set up the new ones...
    for log in llist:
        section = cp[""logger_%s"" % log]
        qn = section[""qualname""]
        propagate = section.getint(""propagate"", fallback=1)
        logger = logging.getLogger(qn)
        if qn in existing:
            i = existing.index(qn) + 1 # start with the entry after qn
            prefixed = qn + "".""
            pflen = len(prefixed)
            num_existing = len(existing)
            while i < num_existing:
                if existing[i][:pflen] == prefixed:
                    child_loggers.append(existing[i])
                i += 1
            existing.remove(qn)
        if ""level"" in section:
            level = section[""level""]
            logger.setLevel(level)
        for h in logger.handlers[:]:
            logger.removeHandler(h)
        logger.propagate = propagate
        logger.disabled = 0
        hlist = section[""handlers""]
        if len(hlist):
            hlist = hlist.split("","")
            hlist = _strip_spaces(hlist)
            for hand in hlist:
                logger.addHandler(handlers[hand])

    #Disable any old loggers. There's no point deleting
    #them as other threads may continue to hold references
    #and by disabling them, you stop them doing any logging.
    #However, don't disable children of named loggers, as that's
    #probably not what was intended by the user.
    #for log in existing:
    #    logger = root.manager.loggerDict[log]
    #    if log in child_loggers:
    #        logger.level = logging.NOTSET
    #        logger.handlers = []
    #        logger.propagate = 1
    #    elif disable_existing_loggers:
    #        logger.disabled = 1
    _handle_existing_loggers(existing, child_loggers, disable_existing)","def 4(fname, defaults=None, disable_existing_loggers=True, encoding=None):
    """"""
    Read the logging configuration from a ConfigParser-format file.

    This can be called several times from an application, allowing an end user
    the ability to select from various pre-canned configurations (if the
    developer provides a mechanism to present the choices and load the chosen
    configuration).
    """"""
    import configparser

    if isinstance(fname, configparser.RawConfigParser):
        cp = fname
    else:
        cp = configparser.ConfigParser(defaults)
        if hasattr(fname, 'readline'):
            cp.read_file(fname)
        else:
            encoding = io.text_encoding(encoding)
            cp.read(fname, encoding=encoding)

    formatters = 0(cp)

    # critical section
    logging._acquireLock()
    try:
        2()

        # Handlers add themselves to logging._handlers
        handlers = 1(cp, formatters)
        3(cp, handlers, disable_existing_loggers)
    finally:
        logging._releaseLock()","{""_create_formatters"": ""0"", ""_install_handlers"": ""1"", ""_clearExistingHandlers"": ""2"", ""_install_loggers"": ""3""}"
617,617,"def _resolve(name):
    """"""Resolve a dotted name to a global object.""""""
    name = name.split('.')
    used = name.pop(0)
    found = __import__(used)
    for n in name:
        used = used + '.' + n
        try:
            found = getattr(found, n)
        except AttributeError:
            __import__(used)
            found = getattr(found, n)
    return found","def __import__(name, globals=None, locals=None, fromlist=(), level=0):
    """"""Import a module.

    The 'globals' argument is used to infer where the import is occurring from
    to handle relative imports. The 'locals' argument is ignored. The
    'fromlist' argument specifies what should exist as attributes on the module
    being imported (e.g. ``from module import <fromlist>``).  The 'level'
    argument represents the package location to import from in a relative
    import (e.g. ``from ..pkg import mod`` would have a 'level' of 2).

    """"""
    if level == 0:
        module = _gcd_import(name)
    else:
        globals_ = globals if globals is not None else {}
        package = _calc___package__(globals_)
        module = _gcd_import(name, package, level)
    if not fromlist:
        # Return up to the first dot in 'name'. This is complicated by the fact
        # that 'name' may be relative.
        if level == 0:
            return _gcd_import(name.partition('.')[0])
        elif not name:
            return module
        else:
            # Figure out where to slice the module's name up to the first dot
            # in 'name'.
            cut_off = len(name) - len(name.partition('.')[0])
            # Slice end needs to be positive to alleviate need to special-case
            # when ``'.' not in name``.
            return sys.modules[module.__name__[:len(module.__name__)-cut_off]]
    elif hasattr(module, '__path__'):
        return _handle_fromlist(module, fromlist, _gcd_import)
    else:
        return module","def import_module(name, globals=None, locals=None, fromlist=(), level=0):
    """"""Import a module.

    The 'globals' argument is used to infer where the import is occurring from
    to handle relative imports. The 'locals' argument is ignored. The
    'fromlist' argument specifies what should exist as attributes on the module
    being imported (e.g. ``from module import <fromlist>``).  The 'level'
    argument represents the package location to import from in a relative
    import (e.g. ``from ..pkg import mod`` would have a 'level' of 2).

    """"""
    if level == 0:
        module = _gcd_import(name)
    else:
        globals_ = globals if globals is not None else {}
        package = _calc___package__(globals_)
        module = _gcd_import(name, package, level)
    if not fromlist:
        # Return up to the first dot in 'name'. This is complicated by the fact
        # that 'name' may be relative.
        if level == 0:
            return _gcd_import(name.partition('.')[0])
        elif not name:
            return module
        else:
            # Figure out where to slice the module's name up to the first dot
            # in 'name'.
            cut_off = len(name) - len(name.partition('.')[0])
            # Slice end needs to be positive to alleviate need to special-case
            # when ``'.' not in name``.
            return sys.modules[module.__name__[:len(module.__name__)-cut_off]]
    elif hasattr(module, '__path__'):
        return _handle_fromlist(module, fromlist, _gcd_import)
    else:
        return module","def resolve_dotted_name(name):
    """"""Resolve a dotted name to a global object.""""""
    name = name.split('.')
    used = name.pop(0)
    found = import_module(used)
    for n in name:
        used = used + '.' + n
        try:
            found = getattr(found, n)
        except AttributeError:
            import_module(used)
            found = getattr(found, n)
    return found","{""__import__"": ""import_module""}","def 0(name, globals=None, locals=None, fromlist=(), level=0):
    """"""Import a module.

    The 'globals' argument is used to infer where the import is occurring from
    to handle relative imports. The 'locals' argument is ignored. The
    'fromlist' argument specifies what should exist as attributes on the module
    being imported (e.g. ``from module import <fromlist>``).  The 'level'
    argument represents the package location to import from in a relative
    import (e.g. ``from ..pkg import mod`` would have a 'level' of 2).

    """"""
    if level == 0:
        module = _gcd_import(name)
    else:
        globals_ = globals if globals is not None else {}
        package = _calc___package__(globals_)
        module = _gcd_import(name, package, level)
    if not fromlist:
        # Return up to the first dot in 'name'. This is complicated by the fact
        # that 'name' may be relative.
        if level == 0:
            return _gcd_import(name.partition('.')[0])
        elif not name:
            return module
        else:
            # Figure out where to slice the module's name up to the first dot
            # in 'name'.
            cut_off = len(name) - len(name.partition('.')[0])
            # Slice end needs to be positive to alleviate need to special-case
            # when ``'.' not in name``.
            return sys.modules[module.__name__[:len(module.__name__)-cut_off]]
    elif hasattr(module, '__path__'):
        return _handle_fromlist(module, fromlist, _gcd_import)
    else:
        return module","def 1(name):
    """"""Resolve a dotted name to a global object.""""""
    name = name.split('.')
    used = name.pop(0)
    found = 0(used)
    for n in name:
        used = used + '.' + n
        try:
            found = getattr(found, n)
        except AttributeError:
            0(used)
            found = getattr(found, n)
    return found","{""__import__"": ""0""}"
618,618,"def _create_formatters(cp):
    """"""Create and return formatters""""""
    flist = cp[""formatters""][""keys""]
    if not len(flist):
        return {}
    flist = flist.split("","")
    flist = _strip_spaces(flist)
    formatters = {}
    for form in flist:
        sectname = ""formatter_%s"" % form
        fs = cp.get(sectname, ""format"", raw=True, fallback=None)
        dfs = cp.get(sectname, ""datefmt"", raw=True, fallback=None)
        stl = cp.get(sectname, ""style"", raw=True, fallback='%')
        c = logging.Formatter
        class_name = cp[sectname].get(""class"")
        if class_name:
            c = _resolve(class_name)
        f = c(fs, dfs, stl)
        formatters[form] = f
    return formatters","def _resolve(name):
    """"""Resolve a dotted name to a global object.""""""
    name = name.split('.')
    used = name.pop(0)
    found = __import__(used)
    for n in name:
        used = used + '.' + n
        try:
            found = getattr(found, n)
        except AttributeError:
            __import__(used)
            found = getattr(found, n)
    return found

def _strip_spaces(alist):
    return map(str.strip, alist)","def resolve_dotted_name(name):
    """"""Resolve a dotted name to a global object.""""""
    name = name.split('.')
    used = name.pop(0)
    found = __import__(used)
    for n in name:
        used = used + '.' + n
        try:
            found = getattr(found, n)
        except AttributeError:
            __import__(used)
            found = getattr(found, n)
    return found

def strip_spaces_from_list(alist):
    return map(str.strip, alist)","def create_log_formatters(cp):
    """"""Create and return formatters""""""
    flist = cp[""formatters""][""keys""]
    if not len(flist):
        return {}
    flist = flist.split("","")
    flist = strip_spaces_from_list(flist)
    formatters = {}
    for form in flist:
        sectname = ""formatter_%s"" % form
        fs = cp.get(sectname, ""format"", raw=True, fallback=None)
        dfs = cp.get(sectname, ""datefmt"", raw=True, fallback=None)
        stl = cp.get(sectname, ""style"", raw=True, fallback='%')
        c = logging.Formatter
        class_name = cp[sectname].get(""class"")
        if class_name:
            c = resolve_dotted_name(class_name)
        f = c(fs, dfs, stl)
        formatters[form] = f
    return formatters","{""_resolve"": ""resolve_dotted_name"", ""_strip_spaces"": ""strip_spaces_from_list""}","def 0(name):
    """"""Resolve a dotted name to a global object.""""""
    name = name.split('.')
    used = name.pop(0)
    found = __import__(used)
    for n in name:
        used = used + '.' + n
        try:
            found = getattr(found, n)
        except AttributeError:
            __import__(used)
            found = getattr(found, n)
    return found

def 1(alist):
    return map(str.strip, alist)","def 2(cp):
    """"""Create and return formatters""""""
    flist = cp[""formatters""][""keys""]
    if not len(flist):
        return {}
    flist = flist.split("","")
    flist = 1(flist)
    formatters = {}
    for form in flist:
        sectname = ""formatter_%s"" % form
        fs = cp.get(sectname, ""format"", raw=True, fallback=None)
        dfs = cp.get(sectname, ""datefmt"", raw=True, fallback=None)
        stl = cp.get(sectname, ""style"", raw=True, fallback='%')
        c = logging.Formatter
        class_name = cp[sectname].get(""class"")
        if class_name:
            c = 0(class_name)
        f = c(fs, dfs, stl)
        formatters[form] = f
    return formatters","{""_resolve"": ""0"", ""_strip_spaces"": ""1""}"
619,619,"def addLevelName(level, levelName):
    """"""
    Associate 'levelName' with 'level'.

    This is used when converting levels to text during message formatting.
    """"""
    _acquireLock()
    try:    #unlikely to cause an exception, but you never know...
        _levelToName[level] = levelName
        _nameToLevel[levelName] = level
    finally:
        _releaseLock()","def _acquireLock():
    """"""
    Acquire the module-level lock for serializing access to shared data.

    This should be released with _releaseLock().
    """"""
    if _lock:
        _lock.acquire()

def _releaseLock():
    """"""
    Release the module-level lock acquired by calling _acquireLock().
    """"""
    if _lock:
        _lock.release()","def acquire_module_level_lock():
    """"""
    Acquire the module-level lock for serializing access to shared data.

    This should be released with release_module_level_lock().
    """"""
    if _lock:
        _lock.acquire()

def release_module_level_lock():
    """"""
    Release the module-level lock acquired by calling acquire_module_level_lock().
    """"""
    if _lock:
        _lock.release()","def associate_levelName_with_level_for_message_formatting(level, levelName):
    """"""
    Associate 'levelName' with 'level'.

    This is used when converting levels to text during message formatting.
    """"""
    acquire_module_level_lock()
    try:    #unlikely to cause an exception, but you never know...
        _levelToName[level] = levelName
        _nameToLevel[levelName] = level
    finally:
        release_module_level_lock()","{""_acquireLock"": ""acquire_module_level_lock"", ""_releaseLock"": ""release_module_level_lock""}","def 0():
    """"""
    Acquire the module-level lock for serializing access to shared data.

    This should be released with 1().
    """"""
    if _lock:
        _lock.acquire()

def 1():
    """"""
    Release the module-level lock acquired by calling 0().
    """"""
    if _lock:
        _lock.release()","def 2(level, levelName):
    """"""
    Associate 'levelName' with 'level'.

    This is used when converting levels to text during message formatting.
    """"""
    0()
    try:    #unlikely to cause an exception, but you never know...
        _levelToName[level] = levelName
        _nameToLevel[levelName] = level
    finally:
        1()","{""_acquireLock"": ""0"", ""_releaseLock"": ""1""}"
620,620,"def _removeHandlerRef(wr):
    """"""
    Remove a handler reference from the internal cleanup list.
    """"""
    # This function can be called during module teardown, when globals are
    # set to None. It can also be called from another thread. So we need to
    # pre-emptively grab the necessary globals and check if they're None,
    # to prevent race conditions and failures during interpreter shutdown.
    acquire, release, handlers = _acquireLock, _releaseLock, _handlerList
    if acquire and release and handlers:
        acquire()
        try:
            if wr in handlers:
                handlers.remove(wr)
        finally:
            release()","def release():

    """""" Returns the system's release, e.g. '2.2.0' or 'NT'

        An empty string is returned if the value cannot be determined.

    """"""
    return uname().release","def get_system_release():

    """""" Returns the system's get_system_release, e.g. '2.2.0' or 'NT'

        An empty string is returned if the value cannot be determined.

    """"""
    return uname().get_system_release","def remove_handler_reference_from_internal_cleanup_list(wr):
    """"""
    Remove a handler reference from the internal cleanup list.
    """"""
    # This function can be called during module teardown, when globals are
    # set to None. It can also be called from another thread. So we need to
    # pre-emptively grab the necessary globals and check if they're None,
    # to prevent race conditions and failures during interpreter shutdown.
    acquire, get_system_release, handlers = _acquireLock, _get_system_releaseLock, _handlerList
    if acquire and get_system_release and handlers:
        acquire()
        try:
            if wr in handlers:
                handlers.remove(wr)
        finally:
            get_system_release()","{""release"": ""get_system_release""}","def 0():

    """""" Returns the system's release, e.g. '2.2.0' or 'NT'

        An empty string is returned if the value cannot be determined.

    """"""
    return uname().release","def 1(wr):
    """"""
    Remove a handler reference from the internal cleanup list.
    """"""
    # This function can be called during module teardown, when globals are
    # set to None. It can also be called from another thread. So we need to
    # pre-emptively grab the necessary globals and check if they're None,
    # to prevent race conditions and failures during interpreter shutdown.
    acquire, release, handlers = _acquireLock, _releaseLock, _handlerList
    if acquire and release and handlers:
        acquire()
        try:
            if wr in handlers:
                handlers.remove(wr)
        finally:
            0()","{""release"": ""0""}"
621,621,"def _addHandlerRef(handler):
    """"""
    Add a handler to the internal cleanup list using a weak reference.
    """"""
    _acquireLock()
    try:
        _handlerList.append(weakref.ref(handler, _removeHandlerRef))
    finally:
        _releaseLock()","def _acquireLock():
    """"""
    Acquire the module-level lock for serializing access to shared data.

    This should be released with _releaseLock().
    """"""
    if _lock:
        _lock.acquire()

def _releaseLock():
    """"""
    Release the module-level lock acquired by calling _acquireLock().
    """"""
    if _lock:
        _lock.release()","def acquire_module_level_lock():
    """"""
    Acquire the module-level lock for serializing access to shared data.

    This should be released with release_module_level_lock().
    """"""
    if _lock:
        _lock.acquire()

def release_module_level_lock():
    """"""
    Release the module-level lock acquired by calling acquire_module_level_lock().
    """"""
    if _lock:
        _lock.release()","def add_handler_to_internal_cleanup_list_using_weak_reference(handler):
    """"""
    Add a handler to the internal cleanup list using a weak reference.
    """"""
    acquire_module_level_lock()
    try:
        _handlerList.append(weakref.ref(handler, _removeHandlerRef))
    finally:
        release_module_level_lock()","{""_acquireLock"": ""acquire_module_level_lock"", ""_releaseLock"": ""release_module_level_lock""}","def 0():
    """"""
    Acquire the module-level lock for serializing access to shared data.

    This should be released with 1().
    """"""
    if _lock:
        _lock.acquire()

def 1():
    """"""
    Release the module-level lock acquired by calling 0().
    """"""
    if _lock:
        _lock.release()","def 2(handler):
    """"""
    Add a handler to the internal cleanup list using a weak reference.
    """"""
    0()
    try:
        _handlerList.append(weakref.ref(handler, _removeHandlerRef))
    finally:
        1()","{""_acquireLock"": ""0"", ""_releaseLock"": ""1""}"
622,622,"def critical(msg, *args, **kwargs):
    """"""
    Log a message with severity 'CRITICAL' on the root logger. If the logger
    has no handlers, call basicConfig() to add a console handler with a
    pre-defined format.
    """"""
    if len(root.handlers) == 0:
        basicConfig()
    root.critical(msg, *args, **kwargs)","def basicConfig(**kwargs):
    """"""
    Do basic configuration for the logging system.

    This function does nothing if the root logger already has handlers
    configured, unless the keyword argument *force* is set to ``True``.
    It is a convenience method intended for use by simple scripts
    to do one-shot configuration of the logging package.

    The default behaviour is to create a StreamHandler which writes to
    sys.stderr, set a formatter using the BASIC_FORMAT format string, and
    add the handler to the root logger.

    A number of optional keyword arguments may be specified, which can alter
    the default behaviour.

    filename  Specifies that a FileHandler be created, using the specified
              filename, rather than a StreamHandler.
    filemode  Specifies the mode to open the file, if filename is specified
              (if filemode is unspecified, it defaults to 'a').
    format    Use the specified format string for the handler.
    datefmt   Use the specified date/time format.
    style     If a format string is specified, use this to specify the
              type of format string (possible values '%', '{', '$', for
              %-formatting, :meth:`str.format` and :class:`string.Template`
              - defaults to '%').
    level     Set the root logger level to the specified level.
    stream    Use the specified stream to initialize the StreamHandler. Note
              that this argument is incompatible with 'filename' - if both
              are present, 'stream' is ignored.
    handlers  If specified, this should be an iterable of already created
              handlers, which will be added to the root handler. Any handler
              in the list which does not have a formatter assigned will be
              assigned the formatter created in this function.
    force     If this keyword  is specified as true, any existing handlers
              attached to the root logger are removed and closed, before
              carrying out the configuration as specified by the other
              arguments.
    encoding  If specified together with a filename, this encoding is passed to
              the created FileHandler, causing it to be used when the file is
              opened.
    errors    If specified together with a filename, this value is passed to the
              created FileHandler, causing it to be used when the file is
              opened in text mode. If not specified, the default value is
              `backslashreplace`.

    Note that you could specify a stream created using open(filename, mode)
    rather than passing the filename and mode in. However, it should be
    remembered that StreamHandler does not close its stream (since it may be
    using sys.stdout or sys.stderr), whereas FileHandler closes its stream
    when the handler is closed.

    .. versionchanged:: 3.2
       Added the ``style`` parameter.

    .. versionchanged:: 3.3
       Added the ``handlers`` parameter. A ``ValueError`` is now thrown for
       incompatible arguments (e.g. ``handlers`` specified together with
       ``filename``/``filemode``, or ``filename``/``filemode`` specified
       together with ``stream``, or ``handlers`` specified together with
       ``stream``.

    .. versionchanged:: 3.8
       Added the ``force`` parameter.

    .. versionchanged:: 3.9
       Added the ``encoding`` and ``errors`` parameters.
    """"""
    # Add thread safety in case someone mistakenly calls
    # basicConfig() from multiple threads
    _acquireLock()
    try:
        force = kwargs.pop('force', False)
        encoding = kwargs.pop('encoding', None)
        errors = kwargs.pop('errors', 'backslashreplace')
        if force:
            for h in root.handlers[:]:
                root.removeHandler(h)
                h.close()
        if len(root.handlers) == 0:
            handlers = kwargs.pop(""handlers"", None)
            if handlers is None:
                if ""stream"" in kwargs and ""filename"" in kwargs:
                    raise ValueError(""'stream' and 'filename' should not be ""
                                     ""specified together"")
            else:
                if ""stream"" in kwargs or ""filename"" in kwargs:
                    raise ValueError(""'stream' or 'filename' should not be ""
                                     ""specified together with 'handlers'"")
            if handlers is None:
                filename = kwargs.pop(""filename"", None)
                mode = kwargs.pop(""filemode"", 'a')
                if filename:
                    if 'b' in mode:
                        errors = None
                    else:
                        encoding = io.text_encoding(encoding)
                    h = FileHandler(filename, mode,
                                    encoding=encoding, errors=errors)
                else:
                    stream = kwargs.pop(""stream"", None)
                    h = StreamHandler(stream)
                handlers = [h]
            dfs = kwargs.pop(""datefmt"", None)
            style = kwargs.pop(""style"", '%')
            if style not in _STYLES:
                raise ValueError('Style must be one of: %s' % ','.join(
                                 _STYLES.keys()))
            fs = kwargs.pop(""format"", _STYLES[style][1])
            fmt = Formatter(fs, dfs, style)
            for h in handlers:
                if h.formatter is None:
                    h.setFormatter(fmt)
                root.addHandler(h)
            level = kwargs.pop(""level"", None)
            if level is not None:
                root.setLevel(level)
            if kwargs:
                keys = ', '.join(kwargs.keys())
                raise ValueError('Unrecognised argument(s): %s' % keys)
    finally:
        _releaseLock()","def do_basic_logging_configuration(**kwargs):
    """"""
    Do basic configuration for the logging system.

    This function does nothing if the root logger already has handlers
    configured, unless the keyword argument *force* is set to ``True``.
    It is a convenience method intended for use by simple scripts
    to do one-shot configuration of the logging package.

    The default behaviour is to create a StreamHandler which writes to
    sys.stderr, set a formatter using the BASIC_FORMAT format string, and
    add the handler to the root logger.

    A number of optional keyword arguments may be specified, which can alter
    the default behaviour.

    filename  Specifies that a FileHandler be created, using the specified
              filename, rather than a StreamHandler.
    filemode  Specifies the mode to open the file, if filename is specified
              (if filemode is unspecified, it defaults to 'a').
    format    Use the specified format string for the handler.
    datefmt   Use the specified date/time format.
    style     If a format string is specified, use this to specify the
              type of format string (possible values '%', '{', '$', for
              %-formatting, :meth:`str.format` and :class:`string.Template`
              - defaults to '%').
    level     Set the root logger level to the specified level.
    stream    Use the specified stream to initialize the StreamHandler. Note
              that this argument is incompatible with 'filename' - if both
              are present, 'stream' is ignored.
    handlers  If specified, this should be an iterable of already created
              handlers, which will be added to the root handler. Any handler
              in the list which does not have a formatter assigned will be
              assigned the formatter created in this function.
    force     If this keyword  is specified as true, any existing handlers
              attached to the root logger are removed and closed, before
              carrying out the configuration as specified by the other
              arguments.
    encoding  If specified together with a filename, this encoding is passed to
              the created FileHandler, causing it to be used when the file is
              opened.
    errors    If specified together with a filename, this value is passed to the
              created FileHandler, causing it to be used when the file is
              opened in text mode. If not specified, the default value is
              `backslashreplace`.

    Note that you could specify a stream created using open(filename, mode)
    rather than passing the filename and mode in. However, it should be
    remembered that StreamHandler does not close its stream (since it may be
    using sys.stdout or sys.stderr), whereas FileHandler closes its stream
    when the handler is closed.

    .. versionchanged:: 3.2
       Added the ``style`` parameter.

    .. versionchanged:: 3.3
       Added the ``handlers`` parameter. A ``ValueError`` is now thrown for
       incompatible arguments (e.g. ``handlers`` specified together with
       ``filename``/``filemode``, or ``filename``/``filemode`` specified
       together with ``stream``, or ``handlers`` specified together with
       ``stream``.

    .. versionchanged:: 3.8
       Added the ``force`` parameter.

    .. versionchanged:: 3.9
       Added the ``encoding`` and ``errors`` parameters.
    """"""
    # Add thread safety in case someone mistakenly calls
    # do_basic_logging_configuration() from multiple threads
    _acquireLock()
    try:
        force = kwargs.pop('force', False)
        encoding = kwargs.pop('encoding', None)
        errors = kwargs.pop('errors', 'backslashreplace')
        if force:
            for h in root.handlers[:]:
                root.removeHandler(h)
                h.close()
        if len(root.handlers) == 0:
            handlers = kwargs.pop(""handlers"", None)
            if handlers is None:
                if ""stream"" in kwargs and ""filename"" in kwargs:
                    raise ValueError(""'stream' and 'filename' should not be ""
                                     ""specified together"")
            else:
                if ""stream"" in kwargs or ""filename"" in kwargs:
                    raise ValueError(""'stream' or 'filename' should not be ""
                                     ""specified together with 'handlers'"")
            if handlers is None:
                filename = kwargs.pop(""filename"", None)
                mode = kwargs.pop(""filemode"", 'a')
                if filename:
                    if 'b' in mode:
                        errors = None
                    else:
                        encoding = io.text_encoding(encoding)
                    h = FileHandler(filename, mode,
                                    encoding=encoding, errors=errors)
                else:
                    stream = kwargs.pop(""stream"", None)
                    h = StreamHandler(stream)
                handlers = [h]
            dfs = kwargs.pop(""datefmt"", None)
            style = kwargs.pop(""style"", '%')
            if style not in _STYLES:
                raise ValueError('Style must be one of: %s' % ','.join(
                                 _STYLES.keys()))
            fs = kwargs.pop(""format"", _STYLES[style][1])
            fmt = Formatter(fs, dfs, style)
            for h in handlers:
                if h.formatter is None:
                    h.setFormatter(fmt)
                root.addHandler(h)
            level = kwargs.pop(""level"", None)
            if level is not None:
                root.setLevel(level)
            if kwargs:
                keys = ', '.join(kwargs.keys())
                raise ValueError('Unrecognised argument(s): %s' % keys)
    finally:
        _releaseLock()","def log_critical_message_root_logger(msg, *args, **kwargs):
    """"""
    Log a message with severity 'CRITICAL' on the root logger. If the logger
    has no handlers, call do_basic_logging_configuration() to add a console handler with a
    pre-defined format.
    """"""
    if len(root.handlers) == 0:
        do_basic_logging_configuration()
    root.log_critical_message_root_logger(msg, *args, **kwargs)","{""basicConfig"": ""do_basic_logging_configuration""}","def 0(**kwargs):
    """"""
    Do basic configuration for the logging system.

    This function does nothing if the root logger already has handlers
    configured, unless the keyword argument *force* is set to ``True``.
    It is a convenience method intended for use by simple scripts
    to do one-shot configuration of the logging package.

    The default behaviour is to create a StreamHandler which writes to
    sys.stderr, set a formatter using the BASIC_FORMAT format string, and
    add the handler to the root logger.

    A number of optional keyword arguments may be specified, which can alter
    the default behaviour.

    filename  Specifies that a FileHandler be created, using the specified
              filename, rather than a StreamHandler.
    filemode  Specifies the mode to open the file, if filename is specified
              (if filemode is unspecified, it defaults to 'a').
    format    Use the specified format string for the handler.
    datefmt   Use the specified date/time format.
    style     If a format string is specified, use this to specify the
              type of format string (possible values '%', '{', '$', for
              %-formatting, :meth:`str.format` and :class:`string.Template`
              - defaults to '%').
    level     Set the root logger level to the specified level.
    stream    Use the specified stream to initialize the StreamHandler. Note
              that this argument is incompatible with 'filename' - if both
              are present, 'stream' is ignored.
    handlers  If specified, this should be an iterable of already created
              handlers, which will be added to the root handler. Any handler
              in the list which does not have a formatter assigned will be
              assigned the formatter created in this function.
    force     If this keyword  is specified as true, any existing handlers
              attached to the root logger are removed and closed, before
              carrying out the configuration as specified by the other
              arguments.
    encoding  If specified together with a filename, this encoding is passed to
              the created FileHandler, causing it to be used when the file is
              opened.
    errors    If specified together with a filename, this value is passed to the
              created FileHandler, causing it to be used when the file is
              opened in text mode. If not specified, the default value is
              `backslashreplace`.

    Note that you could specify a stream created using open(filename, mode)
    rather than passing the filename and mode in. However, it should be
    remembered that StreamHandler does not close its stream (since it may be
    using sys.stdout or sys.stderr), whereas FileHandler closes its stream
    when the handler is closed.

    .. versionchanged:: 3.2
       Added the ``style`` parameter.

    .. versionchanged:: 3.3
       Added the ``handlers`` parameter. A ``ValueError`` is now thrown for
       incompatible arguments (e.g. ``handlers`` specified together with
       ``filename``/``filemode``, or ``filename``/``filemode`` specified
       together with ``stream``, or ``handlers`` specified together with
       ``stream``.

    .. versionchanged:: 3.8
       Added the ``force`` parameter.

    .. versionchanged:: 3.9
       Added the ``encoding`` and ``errors`` parameters.
    """"""
    # Add thread safety in case someone mistakenly calls
    # 0() from multiple threads
    _acquireLock()
    try:
        force = kwargs.pop('force', False)
        encoding = kwargs.pop('encoding', None)
        errors = kwargs.pop('errors', 'backslashreplace')
        if force:
            for h in root.handlers[:]:
                root.removeHandler(h)
                h.close()
        if len(root.handlers) == 0:
            handlers = kwargs.pop(""handlers"", None)
            if handlers is None:
                if ""stream"" in kwargs and ""filename"" in kwargs:
                    raise ValueError(""'stream' and 'filename' should not be ""
                                     ""specified together"")
            else:
                if ""stream"" in kwargs or ""filename"" in kwargs:
                    raise ValueError(""'stream' or 'filename' should not be ""
                                     ""specified together with 'handlers'"")
            if handlers is None:
                filename = kwargs.pop(""filename"", None)
                mode = kwargs.pop(""filemode"", 'a')
                if filename:
                    if 'b' in mode:
                        errors = None
                    else:
                        encoding = io.text_encoding(encoding)
                    h = FileHandler(filename, mode,
                                    encoding=encoding, errors=errors)
                else:
                    stream = kwargs.pop(""stream"", None)
                    h = StreamHandler(stream)
                handlers = [h]
            dfs = kwargs.pop(""datefmt"", None)
            style = kwargs.pop(""style"", '%')
            if style not in _STYLES:
                raise ValueError('Style must be one of: %s' % ','.join(
                                 _STYLES.keys()))
            fs = kwargs.pop(""format"", _STYLES[style][1])
            fmt = Formatter(fs, dfs, style)
            for h in handlers:
                if h.formatter is None:
                    h.setFormatter(fmt)
                root.addHandler(h)
            level = kwargs.pop(""level"", None)
            if level is not None:
                root.setLevel(level)
            if kwargs:
                keys = ', '.join(kwargs.keys())
                raise ValueError('Unrecognised argument(s): %s' % keys)
    finally:
        _releaseLock()","def 1(msg, *args, **kwargs):
    """"""
    Log a message with severity 'CRITICAL' on the root logger. If the logger
    has no handlers, call 0() to add a console handler with a
    pre-defined format.
    """"""
    if len(root.handlers) == 0:
        0()
    root.1(msg, *args, **kwargs)","{""basicConfig"": ""0""}"
623,623,"def fatal(msg, *args, **kwargs):
    """"""
    Don't use this function, use critical() instead.
    """"""
    critical(msg, *args, **kwargs)","def critical(msg, *args, **kwargs):
    """"""
    Log a message with severity 'CRITICAL' on the root logger. If the logger
    has no handlers, call basicConfig() to add a console handler with a
    pre-defined format.
    """"""
    if len(root.handlers) == 0:
        basicConfig()
    root.critical(msg, *args, **kwargs)","def log_critical_message_root_logger(msg, *args, **kwargs):
    """"""
    Log a message with severity 'CRITICAL' on the root logger. If the logger
    has no handlers, call basicConfig() to add a console handler with a
    pre-defined format.
    """"""
    if len(root.handlers) == 0:
        basicConfig()
    root.log_critical_message_root_logger(msg, *args, **kwargs)","def don_t_use_this_function_use_critical_instead(msg, *args, **kwargs):
    """"""
    Don't use this function, use log_critical_message_root_logger() instead.
    """"""
    log_critical_message_root_logger(msg, *args, **kwargs)","{""critical"": ""log_critical_message_root_logger""}","def 0(msg, *args, **kwargs):
    """"""
    Log a message with severity 'CRITICAL' on the root logger. If the logger
    has no handlers, call basicConfig() to add a console handler with a
    pre-defined format.
    """"""
    if len(root.handlers) == 0:
        basicConfig()
    root.0(msg, *args, **kwargs)","def 1(msg, *args, **kwargs):
    """"""
    Don't use this function, use 0() instead.
    """"""
    0(msg, *args, **kwargs)","{""critical"": ""0""}"
624,624,"def error(msg, *args, **kwargs):
    """"""
    Log a message with severity 'ERROR' on the root logger. If the logger has
    no handlers, call basicConfig() to add a console handler with a pre-defined
    format.
    """"""
    if len(root.handlers) == 0:
        basicConfig()
    root.error(msg, *args, **kwargs)","def basicConfig(**kwargs):
    """"""
    Do basic configuration for the logging system.

    This function does nothing if the root logger already has handlers
    configured, unless the keyword argument *force* is set to ``True``.
    It is a convenience method intended for use by simple scripts
    to do one-shot configuration of the logging package.

    The default behaviour is to create a StreamHandler which writes to
    sys.stderr, set a formatter using the BASIC_FORMAT format string, and
    add the handler to the root logger.

    A number of optional keyword arguments may be specified, which can alter
    the default behaviour.

    filename  Specifies that a FileHandler be created, using the specified
              filename, rather than a StreamHandler.
    filemode  Specifies the mode to open the file, if filename is specified
              (if filemode is unspecified, it defaults to 'a').
    format    Use the specified format string for the handler.
    datefmt   Use the specified date/time format.
    style     If a format string is specified, use this to specify the
              type of format string (possible values '%', '{', '$', for
              %-formatting, :meth:`str.format` and :class:`string.Template`
              - defaults to '%').
    level     Set the root logger level to the specified level.
    stream    Use the specified stream to initialize the StreamHandler. Note
              that this argument is incompatible with 'filename' - if both
              are present, 'stream' is ignored.
    handlers  If specified, this should be an iterable of already created
              handlers, which will be added to the root handler. Any handler
              in the list which does not have a formatter assigned will be
              assigned the formatter created in this function.
    force     If this keyword  is specified as true, any existing handlers
              attached to the root logger are removed and closed, before
              carrying out the configuration as specified by the other
              arguments.
    encoding  If specified together with a filename, this encoding is passed to
              the created FileHandler, causing it to be used when the file is
              opened.
    errors    If specified together with a filename, this value is passed to the
              created FileHandler, causing it to be used when the file is
              opened in text mode. If not specified, the default value is
              `backslashreplace`.

    Note that you could specify a stream created using open(filename, mode)
    rather than passing the filename and mode in. However, it should be
    remembered that StreamHandler does not close its stream (since it may be
    using sys.stdout or sys.stderr), whereas FileHandler closes its stream
    when the handler is closed.

    .. versionchanged:: 3.2
       Added the ``style`` parameter.

    .. versionchanged:: 3.3
       Added the ``handlers`` parameter. A ``ValueError`` is now thrown for
       incompatible arguments (e.g. ``handlers`` specified together with
       ``filename``/``filemode``, or ``filename``/``filemode`` specified
       together with ``stream``, or ``handlers`` specified together with
       ``stream``.

    .. versionchanged:: 3.8
       Added the ``force`` parameter.

    .. versionchanged:: 3.9
       Added the ``encoding`` and ``errors`` parameters.
    """"""
    # Add thread safety in case someone mistakenly calls
    # basicConfig() from multiple threads
    _acquireLock()
    try:
        force = kwargs.pop('force', False)
        encoding = kwargs.pop('encoding', None)
        errors = kwargs.pop('errors', 'backslashreplace')
        if force:
            for h in root.handlers[:]:
                root.removeHandler(h)
                h.close()
        if len(root.handlers) == 0:
            handlers = kwargs.pop(""handlers"", None)
            if handlers is None:
                if ""stream"" in kwargs and ""filename"" in kwargs:
                    raise ValueError(""'stream' and 'filename' should not be ""
                                     ""specified together"")
            else:
                if ""stream"" in kwargs or ""filename"" in kwargs:
                    raise ValueError(""'stream' or 'filename' should not be ""
                                     ""specified together with 'handlers'"")
            if handlers is None:
                filename = kwargs.pop(""filename"", None)
                mode = kwargs.pop(""filemode"", 'a')
                if filename:
                    if 'b' in mode:
                        errors = None
                    else:
                        encoding = io.text_encoding(encoding)
                    h = FileHandler(filename, mode,
                                    encoding=encoding, errors=errors)
                else:
                    stream = kwargs.pop(""stream"", None)
                    h = StreamHandler(stream)
                handlers = [h]
            dfs = kwargs.pop(""datefmt"", None)
            style = kwargs.pop(""style"", '%')
            if style not in _STYLES:
                raise ValueError('Style must be one of: %s' % ','.join(
                                 _STYLES.keys()))
            fs = kwargs.pop(""format"", _STYLES[style][1])
            fmt = Formatter(fs, dfs, style)
            for h in handlers:
                if h.formatter is None:
                    h.setFormatter(fmt)
                root.addHandler(h)
            level = kwargs.pop(""level"", None)
            if level is not None:
                root.setLevel(level)
            if kwargs:
                keys = ', '.join(kwargs.keys())
                raise ValueError('Unrecognised argument(s): %s' % keys)
    finally:
        _releaseLock()","def do_basic_logging_configuration(**kwargs):
    """"""
    Do basic configuration for the logging system.

    This function does nothing if the root logger already has handlers
    configured, unless the keyword argument *force* is set to ``True``.
    It is a convenience method intended for use by simple scripts
    to do one-shot configuration of the logging package.

    The default behaviour is to create a StreamHandler which writes to
    sys.stderr, set a formatter using the BASIC_FORMAT format string, and
    add the handler to the root logger.

    A number of optional keyword arguments may be specified, which can alter
    the default behaviour.

    filename  Specifies that a FileHandler be created, using the specified
              filename, rather than a StreamHandler.
    filemode  Specifies the mode to open the file, if filename is specified
              (if filemode is unspecified, it defaults to 'a').
    format    Use the specified format string for the handler.
    datefmt   Use the specified date/time format.
    style     If a format string is specified, use this to specify the
              type of format string (possible values '%', '{', '$', for
              %-formatting, :meth:`str.format` and :class:`string.Template`
              - defaults to '%').
    level     Set the root logger level to the specified level.
    stream    Use the specified stream to initialize the StreamHandler. Note
              that this argument is incompatible with 'filename' - if both
              are present, 'stream' is ignored.
    handlers  If specified, this should be an iterable of already created
              handlers, which will be added to the root handler. Any handler
              in the list which does not have a formatter assigned will be
              assigned the formatter created in this function.
    force     If this keyword  is specified as true, any existing handlers
              attached to the root logger are removed and closed, before
              carrying out the configuration as specified by the other
              arguments.
    encoding  If specified together with a filename, this encoding is passed to
              the created FileHandler, causing it to be used when the file is
              opened.
    errors    If specified together with a filename, this value is passed to the
              created FileHandler, causing it to be used when the file is
              opened in text mode. If not specified, the default value is
              `backslashreplace`.

    Note that you could specify a stream created using open(filename, mode)
    rather than passing the filename and mode in. However, it should be
    remembered that StreamHandler does not close its stream (since it may be
    using sys.stdout or sys.stderr), whereas FileHandler closes its stream
    when the handler is closed.

    .. versionchanged:: 3.2
       Added the ``style`` parameter.

    .. versionchanged:: 3.3
       Added the ``handlers`` parameter. A ``ValueError`` is now thrown for
       incompatible arguments (e.g. ``handlers`` specified together with
       ``filename``/``filemode``, or ``filename``/``filemode`` specified
       together with ``stream``, or ``handlers`` specified together with
       ``stream``.

    .. versionchanged:: 3.8
       Added the ``force`` parameter.

    .. versionchanged:: 3.9
       Added the ``encoding`` and ``errors`` parameters.
    """"""
    # Add thread safety in case someone mistakenly calls
    # do_basic_logging_configuration() from multiple threads
    _acquireLock()
    try:
        force = kwargs.pop('force', False)
        encoding = kwargs.pop('encoding', None)
        errors = kwargs.pop('errors', 'backslashreplace')
        if force:
            for h in root.handlers[:]:
                root.removeHandler(h)
                h.close()
        if len(root.handlers) == 0:
            handlers = kwargs.pop(""handlers"", None)
            if handlers is None:
                if ""stream"" in kwargs and ""filename"" in kwargs:
                    raise ValueError(""'stream' and 'filename' should not be ""
                                     ""specified together"")
            else:
                if ""stream"" in kwargs or ""filename"" in kwargs:
                    raise ValueError(""'stream' or 'filename' should not be ""
                                     ""specified together with 'handlers'"")
            if handlers is None:
                filename = kwargs.pop(""filename"", None)
                mode = kwargs.pop(""filemode"", 'a')
                if filename:
                    if 'b' in mode:
                        errors = None
                    else:
                        encoding = io.text_encoding(encoding)
                    h = FileHandler(filename, mode,
                                    encoding=encoding, errors=errors)
                else:
                    stream = kwargs.pop(""stream"", None)
                    h = StreamHandler(stream)
                handlers = [h]
            dfs = kwargs.pop(""datefmt"", None)
            style = kwargs.pop(""style"", '%')
            if style not in _STYLES:
                raise ValueError('Style must be one of: %s' % ','.join(
                                 _STYLES.keys()))
            fs = kwargs.pop(""format"", _STYLES[style][1])
            fmt = Formatter(fs, dfs, style)
            for h in handlers:
                if h.formatter is None:
                    h.setFormatter(fmt)
                root.addHandler(h)
            level = kwargs.pop(""level"", None)
            if level is not None:
                root.setLevel(level)
            if kwargs:
                keys = ', '.join(kwargs.keys())
                raise ValueError('Unrecognised argument(s): %s' % keys)
    finally:
        _releaseLock()","def log_error_message_root_logger(msg, *args, **kwargs):
    """"""
    Log a message with severity 'ERROR' on the root logger. If the logger has
    no handlers, call do_basic_logging_configuration() to add a console handler with a pre-defined
    format.
    """"""
    if len(root.handlers) == 0:
        do_basic_logging_configuration()
    root.log_error_message_root_logger(msg, *args, **kwargs)","{""basicConfig"": ""do_basic_logging_configuration""}","def 0(**kwargs):
    """"""
    Do basic configuration for the logging system.

    This function does nothing if the root logger already has handlers
    configured, unless the keyword argument *force* is set to ``True``.
    It is a convenience method intended for use by simple scripts
    to do one-shot configuration of the logging package.

    The default behaviour is to create a StreamHandler which writes to
    sys.stderr, set a formatter using the BASIC_FORMAT format string, and
    add the handler to the root logger.

    A number of optional keyword arguments may be specified, which can alter
    the default behaviour.

    filename  Specifies that a FileHandler be created, using the specified
              filename, rather than a StreamHandler.
    filemode  Specifies the mode to open the file, if filename is specified
              (if filemode is unspecified, it defaults to 'a').
    format    Use the specified format string for the handler.
    datefmt   Use the specified date/time format.
    style     If a format string is specified, use this to specify the
              type of format string (possible values '%', '{', '$', for
              %-formatting, :meth:`str.format` and :class:`string.Template`
              - defaults to '%').
    level     Set the root logger level to the specified level.
    stream    Use the specified stream to initialize the StreamHandler. Note
              that this argument is incompatible with 'filename' - if both
              are present, 'stream' is ignored.
    handlers  If specified, this should be an iterable of already created
              handlers, which will be added to the root handler. Any handler
              in the list which does not have a formatter assigned will be
              assigned the formatter created in this function.
    force     If this keyword  is specified as true, any existing handlers
              attached to the root logger are removed and closed, before
              carrying out the configuration as specified by the other
              arguments.
    encoding  If specified together with a filename, this encoding is passed to
              the created FileHandler, causing it to be used when the file is
              opened.
    errors    If specified together with a filename, this value is passed to the
              created FileHandler, causing it to be used when the file is
              opened in text mode. If not specified, the default value is
              `backslashreplace`.

    Note that you could specify a stream created using open(filename, mode)
    rather than passing the filename and mode in. However, it should be
    remembered that StreamHandler does not close its stream (since it may be
    using sys.stdout or sys.stderr), whereas FileHandler closes its stream
    when the handler is closed.

    .. versionchanged:: 3.2
       Added the ``style`` parameter.

    .. versionchanged:: 3.3
       Added the ``handlers`` parameter. A ``ValueError`` is now thrown for
       incompatible arguments (e.g. ``handlers`` specified together with
       ``filename``/``filemode``, or ``filename``/``filemode`` specified
       together with ``stream``, or ``handlers`` specified together with
       ``stream``.

    .. versionchanged:: 3.8
       Added the ``force`` parameter.

    .. versionchanged:: 3.9
       Added the ``encoding`` and ``errors`` parameters.
    """"""
    # Add thread safety in case someone mistakenly calls
    # 0() from multiple threads
    _acquireLock()
    try:
        force = kwargs.pop('force', False)
        encoding = kwargs.pop('encoding', None)
        errors = kwargs.pop('errors', 'backslashreplace')
        if force:
            for h in root.handlers[:]:
                root.removeHandler(h)
                h.close()
        if len(root.handlers) == 0:
            handlers = kwargs.pop(""handlers"", None)
            if handlers is None:
                if ""stream"" in kwargs and ""filename"" in kwargs:
                    raise ValueError(""'stream' and 'filename' should not be ""
                                     ""specified together"")
            else:
                if ""stream"" in kwargs or ""filename"" in kwargs:
                    raise ValueError(""'stream' or 'filename' should not be ""
                                     ""specified together with 'handlers'"")
            if handlers is None:
                filename = kwargs.pop(""filename"", None)
                mode = kwargs.pop(""filemode"", 'a')
                if filename:
                    if 'b' in mode:
                        errors = None
                    else:
                        encoding = io.text_encoding(encoding)
                    h = FileHandler(filename, mode,
                                    encoding=encoding, errors=errors)
                else:
                    stream = kwargs.pop(""stream"", None)
                    h = StreamHandler(stream)
                handlers = [h]
            dfs = kwargs.pop(""datefmt"", None)
            style = kwargs.pop(""style"", '%')
            if style not in _STYLES:
                raise ValueError('Style must be one of: %s' % ','.join(
                                 _STYLES.keys()))
            fs = kwargs.pop(""format"", _STYLES[style][1])
            fmt = Formatter(fs, dfs, style)
            for h in handlers:
                if h.formatter is None:
                    h.setFormatter(fmt)
                root.addHandler(h)
            level = kwargs.pop(""level"", None)
            if level is not None:
                root.setLevel(level)
            if kwargs:
                keys = ', '.join(kwargs.keys())
                raise ValueError('Unrecognised argument(s): %s' % keys)
    finally:
        _releaseLock()","def 1(msg, *args, **kwargs):
    """"""
    Log a message with severity 'ERROR' on the root logger. If the logger has
    no handlers, call 0() to add a console handler with a pre-defined
    format.
    """"""
    if len(root.handlers) == 0:
        0()
    root.1(msg, *args, **kwargs)","{""basicConfig"": ""0""}"
625,625,"def exception(msg, *args, exc_info=True, **kwargs):
    """"""
    Log a message with severity 'ERROR' on the root logger, with exception
    information. If the logger has no handlers, basicConfig() is called to add
    a console handler with a pre-defined format.
    """"""
    error(msg, *args, exc_info=exc_info, **kwargs)","def error(msg, *args, **kwargs):
    """"""
    Log a message with severity 'ERROR' on the root logger. If the logger has
    no handlers, call basicConfig() to add a console handler with a pre-defined
    format.
    """"""
    if len(root.handlers) == 0:
        basicConfig()
    root.error(msg, *args, **kwargs)","def log_error_message_root_logger(msg, *args, **kwargs):
    """"""
    Log a message with severity 'ERROR' on the root logger. If the logger has
    no handlers, call basicConfig() to add a console handler with a pre-defined
    format.
    """"""
    if len(root.handlers) == 0:
        basicConfig()
    root.log_error_message_root_logger(msg, *args, **kwargs)","def log_message_with_severity_ERROR_on_root_logger_with_exception_information(msg, *args, exc_info=True, **kwargs):
    """"""
    Log a message with severity 'ERROR' on the root logger, with log_message_with_severity_ERROR_on_root_logger_with_exception_information
    information. If the logger has no handlers, basicConfig() is called to add
    a console handler with a pre-defined format.
    """"""
    log_error_message_root_logger(msg, *args, exc_info=exc_info, **kwargs)","{""error"": ""log_error_message_root_logger""}","def 0(msg, *args, **kwargs):
    """"""
    Log a message with severity 'ERROR' on the root logger. If the logger has
    no handlers, call basicConfig() to add a console handler with a pre-defined
    format.
    """"""
    if len(root.handlers) == 0:
        basicConfig()
    root.0(msg, *args, **kwargs)","def 1(msg, *args, exc_info=True, **kwargs):
    """"""
    Log a message with severity 'ERROR' on the root logger, with exception
    information. If the logger has no handlers, basicConfig() is called to add
    a console handler with a pre-defined format.
    """"""
    0(msg, *args, exc_info=exc_info, **kwargs)","{""error"": ""0""}"
626,626,"def warning(msg, *args, **kwargs):
    """"""
    Log a message with severity 'WARNING' on the root logger. If the logger has
    no handlers, call basicConfig() to add a console handler with a pre-defined
    format.
    """"""
    if len(root.handlers) == 0:
        basicConfig()
    root.warning(msg, *args, **kwargs)","def basicConfig(**kwargs):
    """"""
    Do basic configuration for the logging system.

    This function does nothing if the root logger already has handlers
    configured, unless the keyword argument *force* is set to ``True``.
    It is a convenience method intended for use by simple scripts
    to do one-shot configuration of the logging package.

    The default behaviour is to create a StreamHandler which writes to
    sys.stderr, set a formatter using the BASIC_FORMAT format string, and
    add the handler to the root logger.

    A number of optional keyword arguments may be specified, which can alter
    the default behaviour.

    filename  Specifies that a FileHandler be created, using the specified
              filename, rather than a StreamHandler.
    filemode  Specifies the mode to open the file, if filename is specified
              (if filemode is unspecified, it defaults to 'a').
    format    Use the specified format string for the handler.
    datefmt   Use the specified date/time format.
    style     If a format string is specified, use this to specify the
              type of format string (possible values '%', '{', '$', for
              %-formatting, :meth:`str.format` and :class:`string.Template`
              - defaults to '%').
    level     Set the root logger level to the specified level.
    stream    Use the specified stream to initialize the StreamHandler. Note
              that this argument is incompatible with 'filename' - if both
              are present, 'stream' is ignored.
    handlers  If specified, this should be an iterable of already created
              handlers, which will be added to the root handler. Any handler
              in the list which does not have a formatter assigned will be
              assigned the formatter created in this function.
    force     If this keyword  is specified as true, any existing handlers
              attached to the root logger are removed and closed, before
              carrying out the configuration as specified by the other
              arguments.
    encoding  If specified together with a filename, this encoding is passed to
              the created FileHandler, causing it to be used when the file is
              opened.
    errors    If specified together with a filename, this value is passed to the
              created FileHandler, causing it to be used when the file is
              opened in text mode. If not specified, the default value is
              `backslashreplace`.

    Note that you could specify a stream created using open(filename, mode)
    rather than passing the filename and mode in. However, it should be
    remembered that StreamHandler does not close its stream (since it may be
    using sys.stdout or sys.stderr), whereas FileHandler closes its stream
    when the handler is closed.

    .. versionchanged:: 3.2
       Added the ``style`` parameter.

    .. versionchanged:: 3.3
       Added the ``handlers`` parameter. A ``ValueError`` is now thrown for
       incompatible arguments (e.g. ``handlers`` specified together with
       ``filename``/``filemode``, or ``filename``/``filemode`` specified
       together with ``stream``, or ``handlers`` specified together with
       ``stream``.

    .. versionchanged:: 3.8
       Added the ``force`` parameter.

    .. versionchanged:: 3.9
       Added the ``encoding`` and ``errors`` parameters.
    """"""
    # Add thread safety in case someone mistakenly calls
    # basicConfig() from multiple threads
    _acquireLock()
    try:
        force = kwargs.pop('force', False)
        encoding = kwargs.pop('encoding', None)
        errors = kwargs.pop('errors', 'backslashreplace')
        if force:
            for h in root.handlers[:]:
                root.removeHandler(h)
                h.close()
        if len(root.handlers) == 0:
            handlers = kwargs.pop(""handlers"", None)
            if handlers is None:
                if ""stream"" in kwargs and ""filename"" in kwargs:
                    raise ValueError(""'stream' and 'filename' should not be ""
                                     ""specified together"")
            else:
                if ""stream"" in kwargs or ""filename"" in kwargs:
                    raise ValueError(""'stream' or 'filename' should not be ""
                                     ""specified together with 'handlers'"")
            if handlers is None:
                filename = kwargs.pop(""filename"", None)
                mode = kwargs.pop(""filemode"", 'a')
                if filename:
                    if 'b' in mode:
                        errors = None
                    else:
                        encoding = io.text_encoding(encoding)
                    h = FileHandler(filename, mode,
                                    encoding=encoding, errors=errors)
                else:
                    stream = kwargs.pop(""stream"", None)
                    h = StreamHandler(stream)
                handlers = [h]
            dfs = kwargs.pop(""datefmt"", None)
            style = kwargs.pop(""style"", '%')
            if style not in _STYLES:
                raise ValueError('Style must be one of: %s' % ','.join(
                                 _STYLES.keys()))
            fs = kwargs.pop(""format"", _STYLES[style][1])
            fmt = Formatter(fs, dfs, style)
            for h in handlers:
                if h.formatter is None:
                    h.setFormatter(fmt)
                root.addHandler(h)
            level = kwargs.pop(""level"", None)
            if level is not None:
                root.setLevel(level)
            if kwargs:
                keys = ', '.join(kwargs.keys())
                raise ValueError('Unrecognised argument(s): %s' % keys)
    finally:
        _releaseLock()","def do_basic_logging_configuration(**kwargs):
    """"""
    Do basic configuration for the logging system.

    This function does nothing if the root logger already has handlers
    configured, unless the keyword argument *force* is set to ``True``.
    It is a convenience method intended for use by simple scripts
    to do one-shot configuration of the logging package.

    The default behaviour is to create a StreamHandler which writes to
    sys.stderr, set a formatter using the BASIC_FORMAT format string, and
    add the handler to the root logger.

    A number of optional keyword arguments may be specified, which can alter
    the default behaviour.

    filename  Specifies that a FileHandler be created, using the specified
              filename, rather than a StreamHandler.
    filemode  Specifies the mode to open the file, if filename is specified
              (if filemode is unspecified, it defaults to 'a').
    format    Use the specified format string for the handler.
    datefmt   Use the specified date/time format.
    style     If a format string is specified, use this to specify the
              type of format string (possible values '%', '{', '$', for
              %-formatting, :meth:`str.format` and :class:`string.Template`
              - defaults to '%').
    level     Set the root logger level to the specified level.
    stream    Use the specified stream to initialize the StreamHandler. Note
              that this argument is incompatible with 'filename' - if both
              are present, 'stream' is ignored.
    handlers  If specified, this should be an iterable of already created
              handlers, which will be added to the root handler. Any handler
              in the list which does not have a formatter assigned will be
              assigned the formatter created in this function.
    force     If this keyword  is specified as true, any existing handlers
              attached to the root logger are removed and closed, before
              carrying out the configuration as specified by the other
              arguments.
    encoding  If specified together with a filename, this encoding is passed to
              the created FileHandler, causing it to be used when the file is
              opened.
    errors    If specified together with a filename, this value is passed to the
              created FileHandler, causing it to be used when the file is
              opened in text mode. If not specified, the default value is
              `backslashreplace`.

    Note that you could specify a stream created using open(filename, mode)
    rather than passing the filename and mode in. However, it should be
    remembered that StreamHandler does not close its stream (since it may be
    using sys.stdout or sys.stderr), whereas FileHandler closes its stream
    when the handler is closed.

    .. versionchanged:: 3.2
       Added the ``style`` parameter.

    .. versionchanged:: 3.3
       Added the ``handlers`` parameter. A ``ValueError`` is now thrown for
       incompatible arguments (e.g. ``handlers`` specified together with
       ``filename``/``filemode``, or ``filename``/``filemode`` specified
       together with ``stream``, or ``handlers`` specified together with
       ``stream``.

    .. versionchanged:: 3.8
       Added the ``force`` parameter.

    .. versionchanged:: 3.9
       Added the ``encoding`` and ``errors`` parameters.
    """"""
    # Add thread safety in case someone mistakenly calls
    # do_basic_logging_configuration() from multiple threads
    _acquireLock()
    try:
        force = kwargs.pop('force', False)
        encoding = kwargs.pop('encoding', None)
        errors = kwargs.pop('errors', 'backslashreplace')
        if force:
            for h in root.handlers[:]:
                root.removeHandler(h)
                h.close()
        if len(root.handlers) == 0:
            handlers = kwargs.pop(""handlers"", None)
            if handlers is None:
                if ""stream"" in kwargs and ""filename"" in kwargs:
                    raise ValueError(""'stream' and 'filename' should not be ""
                                     ""specified together"")
            else:
                if ""stream"" in kwargs or ""filename"" in kwargs:
                    raise ValueError(""'stream' or 'filename' should not be ""
                                     ""specified together with 'handlers'"")
            if handlers is None:
                filename = kwargs.pop(""filename"", None)
                mode = kwargs.pop(""filemode"", 'a')
                if filename:
                    if 'b' in mode:
                        errors = None
                    else:
                        encoding = io.text_encoding(encoding)
                    h = FileHandler(filename, mode,
                                    encoding=encoding, errors=errors)
                else:
                    stream = kwargs.pop(""stream"", None)
                    h = StreamHandler(stream)
                handlers = [h]
            dfs = kwargs.pop(""datefmt"", None)
            style = kwargs.pop(""style"", '%')
            if style not in _STYLES:
                raise ValueError('Style must be one of: %s' % ','.join(
                                 _STYLES.keys()))
            fs = kwargs.pop(""format"", _STYLES[style][1])
            fmt = Formatter(fs, dfs, style)
            for h in handlers:
                if h.formatter is None:
                    h.setFormatter(fmt)
                root.addHandler(h)
            level = kwargs.pop(""level"", None)
            if level is not None:
                root.setLevel(level)
            if kwargs:
                keys = ', '.join(kwargs.keys())
                raise ValueError('Unrecognised argument(s): %s' % keys)
    finally:
        _releaseLock()","def log_message_with_severity_WARNING_on_root_logger(msg, *args, **kwargs):
    """"""
    Log a message with severity 'WARNING' on the root logger. If the logger has
    no handlers, call do_basic_logging_configuration() to add a console handler with a pre-defined
    format.
    """"""
    if len(root.handlers) == 0:
        do_basic_logging_configuration()
    root.log_message_with_severity_WARNING_on_root_logger(msg, *args, **kwargs)","{""basicConfig"": ""do_basic_logging_configuration""}","def 0(**kwargs):
    """"""
    Do basic configuration for the logging system.

    This function does nothing if the root logger already has handlers
    configured, unless the keyword argument *force* is set to ``True``.
    It is a convenience method intended for use by simple scripts
    to do one-shot configuration of the logging package.

    The default behaviour is to create a StreamHandler which writes to
    sys.stderr, set a formatter using the BASIC_FORMAT format string, and
    add the handler to the root logger.

    A number of optional keyword arguments may be specified, which can alter
    the default behaviour.

    filename  Specifies that a FileHandler be created, using the specified
              filename, rather than a StreamHandler.
    filemode  Specifies the mode to open the file, if filename is specified
              (if filemode is unspecified, it defaults to 'a').
    format    Use the specified format string for the handler.
    datefmt   Use the specified date/time format.
    style     If a format string is specified, use this to specify the
              type of format string (possible values '%', '{', '$', for
              %-formatting, :meth:`str.format` and :class:`string.Template`
              - defaults to '%').
    level     Set the root logger level to the specified level.
    stream    Use the specified stream to initialize the StreamHandler. Note
              that this argument is incompatible with 'filename' - if both
              are present, 'stream' is ignored.
    handlers  If specified, this should be an iterable of already created
              handlers, which will be added to the root handler. Any handler
              in the list which does not have a formatter assigned will be
              assigned the formatter created in this function.
    force     If this keyword  is specified as true, any existing handlers
              attached to the root logger are removed and closed, before
              carrying out the configuration as specified by the other
              arguments.
    encoding  If specified together with a filename, this encoding is passed to
              the created FileHandler, causing it to be used when the file is
              opened.
    errors    If specified together with a filename, this value is passed to the
              created FileHandler, causing it to be used when the file is
              opened in text mode. If not specified, the default value is
              `backslashreplace`.

    Note that you could specify a stream created using open(filename, mode)
    rather than passing the filename and mode in. However, it should be
    remembered that StreamHandler does not close its stream (since it may be
    using sys.stdout or sys.stderr), whereas FileHandler closes its stream
    when the handler is closed.

    .. versionchanged:: 3.2
       Added the ``style`` parameter.

    .. versionchanged:: 3.3
       Added the ``handlers`` parameter. A ``ValueError`` is now thrown for
       incompatible arguments (e.g. ``handlers`` specified together with
       ``filename``/``filemode``, or ``filename``/``filemode`` specified
       together with ``stream``, or ``handlers`` specified together with
       ``stream``.

    .. versionchanged:: 3.8
       Added the ``force`` parameter.

    .. versionchanged:: 3.9
       Added the ``encoding`` and ``errors`` parameters.
    """"""
    # Add thread safety in case someone mistakenly calls
    # 0() from multiple threads
    _acquireLock()
    try:
        force = kwargs.pop('force', False)
        encoding = kwargs.pop('encoding', None)
        errors = kwargs.pop('errors', 'backslashreplace')
        if force:
            for h in root.handlers[:]:
                root.removeHandler(h)
                h.close()
        if len(root.handlers) == 0:
            handlers = kwargs.pop(""handlers"", None)
            if handlers is None:
                if ""stream"" in kwargs and ""filename"" in kwargs:
                    raise ValueError(""'stream' and 'filename' should not be ""
                                     ""specified together"")
            else:
                if ""stream"" in kwargs or ""filename"" in kwargs:
                    raise ValueError(""'stream' or 'filename' should not be ""
                                     ""specified together with 'handlers'"")
            if handlers is None:
                filename = kwargs.pop(""filename"", None)
                mode = kwargs.pop(""filemode"", 'a')
                if filename:
                    if 'b' in mode:
                        errors = None
                    else:
                        encoding = io.text_encoding(encoding)
                    h = FileHandler(filename, mode,
                                    encoding=encoding, errors=errors)
                else:
                    stream = kwargs.pop(""stream"", None)
                    h = StreamHandler(stream)
                handlers = [h]
            dfs = kwargs.pop(""datefmt"", None)
            style = kwargs.pop(""style"", '%')
            if style not in _STYLES:
                raise ValueError('Style must be one of: %s' % ','.join(
                                 _STYLES.keys()))
            fs = kwargs.pop(""format"", _STYLES[style][1])
            fmt = Formatter(fs, dfs, style)
            for h in handlers:
                if h.formatter is None:
                    h.setFormatter(fmt)
                root.addHandler(h)
            level = kwargs.pop(""level"", None)
            if level is not None:
                root.setLevel(level)
            if kwargs:
                keys = ', '.join(kwargs.keys())
                raise ValueError('Unrecognised argument(s): %s' % keys)
    finally:
        _releaseLock()","def 1(msg, *args, **kwargs):
    """"""
    Log a message with severity 'WARNING' on the root logger. If the logger has
    no handlers, call 0() to add a console handler with a pre-defined
    format.
    """"""
    if len(root.handlers) == 0:
        0()
    root.1(msg, *args, **kwargs)","{""basicConfig"": ""0""}"
627,627,"def info(msg, *args, **kwargs):
    """"""
    Log a message with severity 'INFO' on the root logger. If the logger has
    no handlers, call basicConfig() to add a console handler with a pre-defined
    format.
    """"""
    if len(root.handlers) == 0:
        basicConfig()
    root.info(msg, *args, **kwargs)","def basicConfig(**kwargs):
    """"""
    Do basic configuration for the logging system.

    This function does nothing if the root logger already has handlers
    configured, unless the keyword argument *force* is set to ``True``.
    It is a convenience method intended for use by simple scripts
    to do one-shot configuration of the logging package.

    The default behaviour is to create a StreamHandler which writes to
    sys.stderr, set a formatter using the BASIC_FORMAT format string, and
    add the handler to the root logger.

    A number of optional keyword arguments may be specified, which can alter
    the default behaviour.

    filename  Specifies that a FileHandler be created, using the specified
              filename, rather than a StreamHandler.
    filemode  Specifies the mode to open the file, if filename is specified
              (if filemode is unspecified, it defaults to 'a').
    format    Use the specified format string for the handler.
    datefmt   Use the specified date/time format.
    style     If a format string is specified, use this to specify the
              type of format string (possible values '%', '{', '$', for
              %-formatting, :meth:`str.format` and :class:`string.Template`
              - defaults to '%').
    level     Set the root logger level to the specified level.
    stream    Use the specified stream to initialize the StreamHandler. Note
              that this argument is incompatible with 'filename' - if both
              are present, 'stream' is ignored.
    handlers  If specified, this should be an iterable of already created
              handlers, which will be added to the root handler. Any handler
              in the list which does not have a formatter assigned will be
              assigned the formatter created in this function.
    force     If this keyword  is specified as true, any existing handlers
              attached to the root logger are removed and closed, before
              carrying out the configuration as specified by the other
              arguments.
    encoding  If specified together with a filename, this encoding is passed to
              the created FileHandler, causing it to be used when the file is
              opened.
    errors    If specified together with a filename, this value is passed to the
              created FileHandler, causing it to be used when the file is
              opened in text mode. If not specified, the default value is
              `backslashreplace`.

    Note that you could specify a stream created using open(filename, mode)
    rather than passing the filename and mode in. However, it should be
    remembered that StreamHandler does not close its stream (since it may be
    using sys.stdout or sys.stderr), whereas FileHandler closes its stream
    when the handler is closed.

    .. versionchanged:: 3.2
       Added the ``style`` parameter.

    .. versionchanged:: 3.3
       Added the ``handlers`` parameter. A ``ValueError`` is now thrown for
       incompatible arguments (e.g. ``handlers`` specified together with
       ``filename``/``filemode``, or ``filename``/``filemode`` specified
       together with ``stream``, or ``handlers`` specified together with
       ``stream``.

    .. versionchanged:: 3.8
       Added the ``force`` parameter.

    .. versionchanged:: 3.9
       Added the ``encoding`` and ``errors`` parameters.
    """"""
    # Add thread safety in case someone mistakenly calls
    # basicConfig() from multiple threads
    _acquireLock()
    try:
        force = kwargs.pop('force', False)
        encoding = kwargs.pop('encoding', None)
        errors = kwargs.pop('errors', 'backslashreplace')
        if force:
            for h in root.handlers[:]:
                root.removeHandler(h)
                h.close()
        if len(root.handlers) == 0:
            handlers = kwargs.pop(""handlers"", None)
            if handlers is None:
                if ""stream"" in kwargs and ""filename"" in kwargs:
                    raise ValueError(""'stream' and 'filename' should not be ""
                                     ""specified together"")
            else:
                if ""stream"" in kwargs or ""filename"" in kwargs:
                    raise ValueError(""'stream' or 'filename' should not be ""
                                     ""specified together with 'handlers'"")
            if handlers is None:
                filename = kwargs.pop(""filename"", None)
                mode = kwargs.pop(""filemode"", 'a')
                if filename:
                    if 'b' in mode:
                        errors = None
                    else:
                        encoding = io.text_encoding(encoding)
                    h = FileHandler(filename, mode,
                                    encoding=encoding, errors=errors)
                else:
                    stream = kwargs.pop(""stream"", None)
                    h = StreamHandler(stream)
                handlers = [h]
            dfs = kwargs.pop(""datefmt"", None)
            style = kwargs.pop(""style"", '%')
            if style not in _STYLES:
                raise ValueError('Style must be one of: %s' % ','.join(
                                 _STYLES.keys()))
            fs = kwargs.pop(""format"", _STYLES[style][1])
            fmt = Formatter(fs, dfs, style)
            for h in handlers:
                if h.formatter is None:
                    h.setFormatter(fmt)
                root.addHandler(h)
            level = kwargs.pop(""level"", None)
            if level is not None:
                root.setLevel(level)
            if kwargs:
                keys = ', '.join(kwargs.keys())
                raise ValueError('Unrecognised argument(s): %s' % keys)
    finally:
        _releaseLock()","def do_basic_logging_configuration(**kwargs):
    """"""
    Do basic configuration for the logging system.

    This function does nothing if the root logger already has handlers
    configured, unless the keyword argument *force* is set to ``True``.
    It is a convenience method intended for use by simple scripts
    to do one-shot configuration of the logging package.

    The default behaviour is to create a StreamHandler which writes to
    sys.stderr, set a formatter using the BASIC_FORMAT format string, and
    add the handler to the root logger.

    A number of optional keyword arguments may be specified, which can alter
    the default behaviour.

    filename  Specifies that a FileHandler be created, using the specified
              filename, rather than a StreamHandler.
    filemode  Specifies the mode to open the file, if filename is specified
              (if filemode is unspecified, it defaults to 'a').
    format    Use the specified format string for the handler.
    datefmt   Use the specified date/time format.
    style     If a format string is specified, use this to specify the
              type of format string (possible values '%', '{', '$', for
              %-formatting, :meth:`str.format` and :class:`string.Template`
              - defaults to '%').
    level     Set the root logger level to the specified level.
    stream    Use the specified stream to initialize the StreamHandler. Note
              that this argument is incompatible with 'filename' - if both
              are present, 'stream' is ignored.
    handlers  If specified, this should be an iterable of already created
              handlers, which will be added to the root handler. Any handler
              in the list which does not have a formatter assigned will be
              assigned the formatter created in this function.
    force     If this keyword  is specified as true, any existing handlers
              attached to the root logger are removed and closed, before
              carrying out the configuration as specified by the other
              arguments.
    encoding  If specified together with a filename, this encoding is passed to
              the created FileHandler, causing it to be used when the file is
              opened.
    errors    If specified together with a filename, this value is passed to the
              created FileHandler, causing it to be used when the file is
              opened in text mode. If not specified, the default value is
              `backslashreplace`.

    Note that you could specify a stream created using open(filename, mode)
    rather than passing the filename and mode in. However, it should be
    remembered that StreamHandler does not close its stream (since it may be
    using sys.stdout or sys.stderr), whereas FileHandler closes its stream
    when the handler is closed.

    .. versionchanged:: 3.2
       Added the ``style`` parameter.

    .. versionchanged:: 3.3
       Added the ``handlers`` parameter. A ``ValueError`` is now thrown for
       incompatible arguments (e.g. ``handlers`` specified together with
       ``filename``/``filemode``, or ``filename``/``filemode`` specified
       together with ``stream``, or ``handlers`` specified together with
       ``stream``.

    .. versionchanged:: 3.8
       Added the ``force`` parameter.

    .. versionchanged:: 3.9
       Added the ``encoding`` and ``errors`` parameters.
    """"""
    # Add thread safety in case someone mistakenly calls
    # do_basic_logging_configuration() from multiple threads
    _acquireLock()
    try:
        force = kwargs.pop('force', False)
        encoding = kwargs.pop('encoding', None)
        errors = kwargs.pop('errors', 'backslashreplace')
        if force:
            for h in root.handlers[:]:
                root.removeHandler(h)
                h.close()
        if len(root.handlers) == 0:
            handlers = kwargs.pop(""handlers"", None)
            if handlers is None:
                if ""stream"" in kwargs and ""filename"" in kwargs:
                    raise ValueError(""'stream' and 'filename' should not be ""
                                     ""specified together"")
            else:
                if ""stream"" in kwargs or ""filename"" in kwargs:
                    raise ValueError(""'stream' or 'filename' should not be ""
                                     ""specified together with 'handlers'"")
            if handlers is None:
                filename = kwargs.pop(""filename"", None)
                mode = kwargs.pop(""filemode"", 'a')
                if filename:
                    if 'b' in mode:
                        errors = None
                    else:
                        encoding = io.text_encoding(encoding)
                    h = FileHandler(filename, mode,
                                    encoding=encoding, errors=errors)
                else:
                    stream = kwargs.pop(""stream"", None)
                    h = StreamHandler(stream)
                handlers = [h]
            dfs = kwargs.pop(""datefmt"", None)
            style = kwargs.pop(""style"", '%')
            if style not in _STYLES:
                raise ValueError('Style must be one of: %s' % ','.join(
                                 _STYLES.keys()))
            fs = kwargs.pop(""format"", _STYLES[style][1])
            fmt = Formatter(fs, dfs, style)
            for h in handlers:
                if h.formatter is None:
                    h.setFormatter(fmt)
                root.addHandler(h)
            level = kwargs.pop(""level"", None)
            if level is not None:
                root.setLevel(level)
            if kwargs:
                keys = ', '.join(kwargs.keys())
                raise ValueError('Unrecognised argument(s): %s' % keys)
    finally:
        _releaseLock()","def log_message_with_severity_INFO_on_root_logger(msg, *args, **kwargs):
    """"""
    Log a message with severity 'INFO' on the root logger. If the logger has
    no handlers, call do_basic_logging_configuration() to add a console handler with a pre-defined
    format.
    """"""
    if len(root.handlers) == 0:
        do_basic_logging_configuration()
    root.log_message_with_severity_INFO_on_root_logger(msg, *args, **kwargs)","{""basicConfig"": ""do_basic_logging_configuration""}","def 0(**kwargs):
    """"""
    Do basic configuration for the logging system.

    This function does nothing if the root logger already has handlers
    configured, unless the keyword argument *force* is set to ``True``.
    It is a convenience method intended for use by simple scripts
    to do one-shot configuration of the logging package.

    The default behaviour is to create a StreamHandler which writes to
    sys.stderr, set a formatter using the BASIC_FORMAT format string, and
    add the handler to the root logger.

    A number of optional keyword arguments may be specified, which can alter
    the default behaviour.

    filename  Specifies that a FileHandler be created, using the specified
              filename, rather than a StreamHandler.
    filemode  Specifies the mode to open the file, if filename is specified
              (if filemode is unspecified, it defaults to 'a').
    format    Use the specified format string for the handler.
    datefmt   Use the specified date/time format.
    style     If a format string is specified, use this to specify the
              type of format string (possible values '%', '{', '$', for
              %-formatting, :meth:`str.format` and :class:`string.Template`
              - defaults to '%').
    level     Set the root logger level to the specified level.
    stream    Use the specified stream to initialize the StreamHandler. Note
              that this argument is incompatible with 'filename' - if both
              are present, 'stream' is ignored.
    handlers  If specified, this should be an iterable of already created
              handlers, which will be added to the root handler. Any handler
              in the list which does not have a formatter assigned will be
              assigned the formatter created in this function.
    force     If this keyword  is specified as true, any existing handlers
              attached to the root logger are removed and closed, before
              carrying out the configuration as specified by the other
              arguments.
    encoding  If specified together with a filename, this encoding is passed to
              the created FileHandler, causing it to be used when the file is
              opened.
    errors    If specified together with a filename, this value is passed to the
              created FileHandler, causing it to be used when the file is
              opened in text mode. If not specified, the default value is
              `backslashreplace`.

    Note that you could specify a stream created using open(filename, mode)
    rather than passing the filename and mode in. However, it should be
    remembered that StreamHandler does not close its stream (since it may be
    using sys.stdout or sys.stderr), whereas FileHandler closes its stream
    when the handler is closed.

    .. versionchanged:: 3.2
       Added the ``style`` parameter.

    .. versionchanged:: 3.3
       Added the ``handlers`` parameter. A ``ValueError`` is now thrown for
       incompatible arguments (e.g. ``handlers`` specified together with
       ``filename``/``filemode``, or ``filename``/``filemode`` specified
       together with ``stream``, or ``handlers`` specified together with
       ``stream``.

    .. versionchanged:: 3.8
       Added the ``force`` parameter.

    .. versionchanged:: 3.9
       Added the ``encoding`` and ``errors`` parameters.
    """"""
    # Add thread safety in case someone mistakenly calls
    # 0() from multiple threads
    _acquireLock()
    try:
        force = kwargs.pop('force', False)
        encoding = kwargs.pop('encoding', None)
        errors = kwargs.pop('errors', 'backslashreplace')
        if force:
            for h in root.handlers[:]:
                root.removeHandler(h)
                h.close()
        if len(root.handlers) == 0:
            handlers = kwargs.pop(""handlers"", None)
            if handlers is None:
                if ""stream"" in kwargs and ""filename"" in kwargs:
                    raise ValueError(""'stream' and 'filename' should not be ""
                                     ""specified together"")
            else:
                if ""stream"" in kwargs or ""filename"" in kwargs:
                    raise ValueError(""'stream' or 'filename' should not be ""
                                     ""specified together with 'handlers'"")
            if handlers is None:
                filename = kwargs.pop(""filename"", None)
                mode = kwargs.pop(""filemode"", 'a')
                if filename:
                    if 'b' in mode:
                        errors = None
                    else:
                        encoding = io.text_encoding(encoding)
                    h = FileHandler(filename, mode,
                                    encoding=encoding, errors=errors)
                else:
                    stream = kwargs.pop(""stream"", None)
                    h = StreamHandler(stream)
                handlers = [h]
            dfs = kwargs.pop(""datefmt"", None)
            style = kwargs.pop(""style"", '%')
            if style not in _STYLES:
                raise ValueError('Style must be one of: %s' % ','.join(
                                 _STYLES.keys()))
            fs = kwargs.pop(""format"", _STYLES[style][1])
            fmt = Formatter(fs, dfs, style)
            for h in handlers:
                if h.formatter is None:
                    h.setFormatter(fmt)
                root.addHandler(h)
            level = kwargs.pop(""level"", None)
            if level is not None:
                root.setLevel(level)
            if kwargs:
                keys = ', '.join(kwargs.keys())
                raise ValueError('Unrecognised argument(s): %s' % keys)
    finally:
        _releaseLock()","def 1(msg, *args, **kwargs):
    """"""
    Log a message with severity 'INFO' on the root logger. If the logger has
    no handlers, call 0() to add a console handler with a pre-defined
    format.
    """"""
    if len(root.handlers) == 0:
        0()
    root.1(msg, *args, **kwargs)","{""basicConfig"": ""0""}"
628,628,"def debug(msg, *args, **kwargs):
    """"""
    Log a message with severity 'DEBUG' on the root logger. If the logger has
    no handlers, call basicConfig() to add a console handler with a pre-defined
    format.
    """"""
    if len(root.handlers) == 0:
        basicConfig()
    root.debug(msg, *args, **kwargs)","def basicConfig(**kwargs):
    """"""
    Do basic configuration for the logging system.

    This function does nothing if the root logger already has handlers
    configured, unless the keyword argument *force* is set to ``True``.
    It is a convenience method intended for use by simple scripts
    to do one-shot configuration of the logging package.

    The default behaviour is to create a StreamHandler which writes to
    sys.stderr, set a formatter using the BASIC_FORMAT format string, and
    add the handler to the root logger.

    A number of optional keyword arguments may be specified, which can alter
    the default behaviour.

    filename  Specifies that a FileHandler be created, using the specified
              filename, rather than a StreamHandler.
    filemode  Specifies the mode to open the file, if filename is specified
              (if filemode is unspecified, it defaults to 'a').
    format    Use the specified format string for the handler.
    datefmt   Use the specified date/time format.
    style     If a format string is specified, use this to specify the
              type of format string (possible values '%', '{', '$', for
              %-formatting, :meth:`str.format` and :class:`string.Template`
              - defaults to '%').
    level     Set the root logger level to the specified level.
    stream    Use the specified stream to initialize the StreamHandler. Note
              that this argument is incompatible with 'filename' - if both
              are present, 'stream' is ignored.
    handlers  If specified, this should be an iterable of already created
              handlers, which will be added to the root handler. Any handler
              in the list which does not have a formatter assigned will be
              assigned the formatter created in this function.
    force     If this keyword  is specified as true, any existing handlers
              attached to the root logger are removed and closed, before
              carrying out the configuration as specified by the other
              arguments.
    encoding  If specified together with a filename, this encoding is passed to
              the created FileHandler, causing it to be used when the file is
              opened.
    errors    If specified together with a filename, this value is passed to the
              created FileHandler, causing it to be used when the file is
              opened in text mode. If not specified, the default value is
              `backslashreplace`.

    Note that you could specify a stream created using open(filename, mode)
    rather than passing the filename and mode in. However, it should be
    remembered that StreamHandler does not close its stream (since it may be
    using sys.stdout or sys.stderr), whereas FileHandler closes its stream
    when the handler is closed.

    .. versionchanged:: 3.2
       Added the ``style`` parameter.

    .. versionchanged:: 3.3
       Added the ``handlers`` parameter. A ``ValueError`` is now thrown for
       incompatible arguments (e.g. ``handlers`` specified together with
       ``filename``/``filemode``, or ``filename``/``filemode`` specified
       together with ``stream``, or ``handlers`` specified together with
       ``stream``.

    .. versionchanged:: 3.8
       Added the ``force`` parameter.

    .. versionchanged:: 3.9
       Added the ``encoding`` and ``errors`` parameters.
    """"""
    # Add thread safety in case someone mistakenly calls
    # basicConfig() from multiple threads
    _acquireLock()
    try:
        force = kwargs.pop('force', False)
        encoding = kwargs.pop('encoding', None)
        errors = kwargs.pop('errors', 'backslashreplace')
        if force:
            for h in root.handlers[:]:
                root.removeHandler(h)
                h.close()
        if len(root.handlers) == 0:
            handlers = kwargs.pop(""handlers"", None)
            if handlers is None:
                if ""stream"" in kwargs and ""filename"" in kwargs:
                    raise ValueError(""'stream' and 'filename' should not be ""
                                     ""specified together"")
            else:
                if ""stream"" in kwargs or ""filename"" in kwargs:
                    raise ValueError(""'stream' or 'filename' should not be ""
                                     ""specified together with 'handlers'"")
            if handlers is None:
                filename = kwargs.pop(""filename"", None)
                mode = kwargs.pop(""filemode"", 'a')
                if filename:
                    if 'b' in mode:
                        errors = None
                    else:
                        encoding = io.text_encoding(encoding)
                    h = FileHandler(filename, mode,
                                    encoding=encoding, errors=errors)
                else:
                    stream = kwargs.pop(""stream"", None)
                    h = StreamHandler(stream)
                handlers = [h]
            dfs = kwargs.pop(""datefmt"", None)
            style = kwargs.pop(""style"", '%')
            if style not in _STYLES:
                raise ValueError('Style must be one of: %s' % ','.join(
                                 _STYLES.keys()))
            fs = kwargs.pop(""format"", _STYLES[style][1])
            fmt = Formatter(fs, dfs, style)
            for h in handlers:
                if h.formatter is None:
                    h.setFormatter(fmt)
                root.addHandler(h)
            level = kwargs.pop(""level"", None)
            if level is not None:
                root.setLevel(level)
            if kwargs:
                keys = ', '.join(kwargs.keys())
                raise ValueError('Unrecognised argument(s): %s' % keys)
    finally:
        _releaseLock()","def do_basic_logging_configuration(**kwargs):
    """"""
    Do basic configuration for the logging system.

    This function does nothing if the root logger already has handlers
    configured, unless the keyword argument *force* is set to ``True``.
    It is a convenience method intended for use by simple scripts
    to do one-shot configuration of the logging package.

    The default behaviour is to create a StreamHandler which writes to
    sys.stderr, set a formatter using the BASIC_FORMAT format string, and
    add the handler to the root logger.

    A number of optional keyword arguments may be specified, which can alter
    the default behaviour.

    filename  Specifies that a FileHandler be created, using the specified
              filename, rather than a StreamHandler.
    filemode  Specifies the mode to open the file, if filename is specified
              (if filemode is unspecified, it defaults to 'a').
    format    Use the specified format string for the handler.
    datefmt   Use the specified date/time format.
    style     If a format string is specified, use this to specify the
              type of format string (possible values '%', '{', '$', for
              %-formatting, :meth:`str.format` and :class:`string.Template`
              - defaults to '%').
    level     Set the root logger level to the specified level.
    stream    Use the specified stream to initialize the StreamHandler. Note
              that this argument is incompatible with 'filename' - if both
              are present, 'stream' is ignored.
    handlers  If specified, this should be an iterable of already created
              handlers, which will be added to the root handler. Any handler
              in the list which does not have a formatter assigned will be
              assigned the formatter created in this function.
    force     If this keyword  is specified as true, any existing handlers
              attached to the root logger are removed and closed, before
              carrying out the configuration as specified by the other
              arguments.
    encoding  If specified together with a filename, this encoding is passed to
              the created FileHandler, causing it to be used when the file is
              opened.
    errors    If specified together with a filename, this value is passed to the
              created FileHandler, causing it to be used when the file is
              opened in text mode. If not specified, the default value is
              `backslashreplace`.

    Note that you could specify a stream created using open(filename, mode)
    rather than passing the filename and mode in. However, it should be
    remembered that StreamHandler does not close its stream (since it may be
    using sys.stdout or sys.stderr), whereas FileHandler closes its stream
    when the handler is closed.

    .. versionchanged:: 3.2
       Added the ``style`` parameter.

    .. versionchanged:: 3.3
       Added the ``handlers`` parameter. A ``ValueError`` is now thrown for
       incompatible arguments (e.g. ``handlers`` specified together with
       ``filename``/``filemode``, or ``filename``/``filemode`` specified
       together with ``stream``, or ``handlers`` specified together with
       ``stream``.

    .. versionchanged:: 3.8
       Added the ``force`` parameter.

    .. versionchanged:: 3.9
       Added the ``encoding`` and ``errors`` parameters.
    """"""
    # Add thread safety in case someone mistakenly calls
    # do_basic_logging_configuration() from multiple threads
    _acquireLock()
    try:
        force = kwargs.pop('force', False)
        encoding = kwargs.pop('encoding', None)
        errors = kwargs.pop('errors', 'backslashreplace')
        if force:
            for h in root.handlers[:]:
                root.removeHandler(h)
                h.close()
        if len(root.handlers) == 0:
            handlers = kwargs.pop(""handlers"", None)
            if handlers is None:
                if ""stream"" in kwargs and ""filename"" in kwargs:
                    raise ValueError(""'stream' and 'filename' should not be ""
                                     ""specified together"")
            else:
                if ""stream"" in kwargs or ""filename"" in kwargs:
                    raise ValueError(""'stream' or 'filename' should not be ""
                                     ""specified together with 'handlers'"")
            if handlers is None:
                filename = kwargs.pop(""filename"", None)
                mode = kwargs.pop(""filemode"", 'a')
                if filename:
                    if 'b' in mode:
                        errors = None
                    else:
                        encoding = io.text_encoding(encoding)
                    h = FileHandler(filename, mode,
                                    encoding=encoding, errors=errors)
                else:
                    stream = kwargs.pop(""stream"", None)
                    h = StreamHandler(stream)
                handlers = [h]
            dfs = kwargs.pop(""datefmt"", None)
            style = kwargs.pop(""style"", '%')
            if style not in _STYLES:
                raise ValueError('Style must be one of: %s' % ','.join(
                                 _STYLES.keys()))
            fs = kwargs.pop(""format"", _STYLES[style][1])
            fmt = Formatter(fs, dfs, style)
            for h in handlers:
                if h.formatter is None:
                    h.setFormatter(fmt)
                root.addHandler(h)
            level = kwargs.pop(""level"", None)
            if level is not None:
                root.setLevel(level)
            if kwargs:
                keys = ', '.join(kwargs.keys())
                raise ValueError('Unrecognised argument(s): %s' % keys)
    finally:
        _releaseLock()","def log_message_with_severity_DEBUG_on_root_logger(msg, *args, **kwargs):
    """"""
    Log a message with severity 'DEBUG' on the root logger. If the logger has
    no handlers, call do_basic_logging_configuration() to add a console handler with a pre-defined
    format.
    """"""
    if len(root.handlers) == 0:
        do_basic_logging_configuration()
    root.log_message_with_severity_DEBUG_on_root_logger(msg, *args, **kwargs)","{""basicConfig"": ""do_basic_logging_configuration""}","def 0(**kwargs):
    """"""
    Do basic configuration for the logging system.

    This function does nothing if the root logger already has handlers
    configured, unless the keyword argument *force* is set to ``True``.
    It is a convenience method intended for use by simple scripts
    to do one-shot configuration of the logging package.

    The default behaviour is to create a StreamHandler which writes to
    sys.stderr, set a formatter using the BASIC_FORMAT format string, and
    add the handler to the root logger.

    A number of optional keyword arguments may be specified, which can alter
    the default behaviour.

    filename  Specifies that a FileHandler be created, using the specified
              filename, rather than a StreamHandler.
    filemode  Specifies the mode to open the file, if filename is specified
              (if filemode is unspecified, it defaults to 'a').
    format    Use the specified format string for the handler.
    datefmt   Use the specified date/time format.
    style     If a format string is specified, use this to specify the
              type of format string (possible values '%', '{', '$', for
              %-formatting, :meth:`str.format` and :class:`string.Template`
              - defaults to '%').
    level     Set the root logger level to the specified level.
    stream    Use the specified stream to initialize the StreamHandler. Note
              that this argument is incompatible with 'filename' - if both
              are present, 'stream' is ignored.
    handlers  If specified, this should be an iterable of already created
              handlers, which will be added to the root handler. Any handler
              in the list which does not have a formatter assigned will be
              assigned the formatter created in this function.
    force     If this keyword  is specified as true, any existing handlers
              attached to the root logger are removed and closed, before
              carrying out the configuration as specified by the other
              arguments.
    encoding  If specified together with a filename, this encoding is passed to
              the created FileHandler, causing it to be used when the file is
              opened.
    errors    If specified together with a filename, this value is passed to the
              created FileHandler, causing it to be used when the file is
              opened in text mode. If not specified, the default value is
              `backslashreplace`.

    Note that you could specify a stream created using open(filename, mode)
    rather than passing the filename and mode in. However, it should be
    remembered that StreamHandler does not close its stream (since it may be
    using sys.stdout or sys.stderr), whereas FileHandler closes its stream
    when the handler is closed.

    .. versionchanged:: 3.2
       Added the ``style`` parameter.

    .. versionchanged:: 3.3
       Added the ``handlers`` parameter. A ``ValueError`` is now thrown for
       incompatible arguments (e.g. ``handlers`` specified together with
       ``filename``/``filemode``, or ``filename``/``filemode`` specified
       together with ``stream``, or ``handlers`` specified together with
       ``stream``.

    .. versionchanged:: 3.8
       Added the ``force`` parameter.

    .. versionchanged:: 3.9
       Added the ``encoding`` and ``errors`` parameters.
    """"""
    # Add thread safety in case someone mistakenly calls
    # 0() from multiple threads
    _acquireLock()
    try:
        force = kwargs.pop('force', False)
        encoding = kwargs.pop('encoding', None)
        errors = kwargs.pop('errors', 'backslashreplace')
        if force:
            for h in root.handlers[:]:
                root.removeHandler(h)
                h.close()
        if len(root.handlers) == 0:
            handlers = kwargs.pop(""handlers"", None)
            if handlers is None:
                if ""stream"" in kwargs and ""filename"" in kwargs:
                    raise ValueError(""'stream' and 'filename' should not be ""
                                     ""specified together"")
            else:
                if ""stream"" in kwargs or ""filename"" in kwargs:
                    raise ValueError(""'stream' or 'filename' should not be ""
                                     ""specified together with 'handlers'"")
            if handlers is None:
                filename = kwargs.pop(""filename"", None)
                mode = kwargs.pop(""filemode"", 'a')
                if filename:
                    if 'b' in mode:
                        errors = None
                    else:
                        encoding = io.text_encoding(encoding)
                    h = FileHandler(filename, mode,
                                    encoding=encoding, errors=errors)
                else:
                    stream = kwargs.pop(""stream"", None)
                    h = StreamHandler(stream)
                handlers = [h]
            dfs = kwargs.pop(""datefmt"", None)
            style = kwargs.pop(""style"", '%')
            if style not in _STYLES:
                raise ValueError('Style must be one of: %s' % ','.join(
                                 _STYLES.keys()))
            fs = kwargs.pop(""format"", _STYLES[style][1])
            fmt = Formatter(fs, dfs, style)
            for h in handlers:
                if h.formatter is None:
                    h.setFormatter(fmt)
                root.addHandler(h)
            level = kwargs.pop(""level"", None)
            if level is not None:
                root.setLevel(level)
            if kwargs:
                keys = ', '.join(kwargs.keys())
                raise ValueError('Unrecognised argument(s): %s' % keys)
    finally:
        _releaseLock()","def 1(msg, *args, **kwargs):
    """"""
    Log a message with severity 'DEBUG' on the root logger. If the logger has
    no handlers, call 0() to add a console handler with a pre-defined
    format.
    """"""
    if len(root.handlers) == 0:
        0()
    root.1(msg, *args, **kwargs)","{""basicConfig"": ""0""}"
629,629,"def log(level, msg, *args, **kwargs):
    """"""
    Log 'msg % args' with the integer severity 'level' on the root logger. If
    the logger has no handlers, call basicConfig() to add a console handler
    with a pre-defined format.
    """"""
    if len(root.handlers) == 0:
        basicConfig()
    root.log(level, msg, *args, **kwargs)","def basicConfig(**kwargs):
    """"""
    Do basic configuration for the logging system.

    This function does nothing if the root logger already has handlers
    configured, unless the keyword argument *force* is set to ``True``.
    It is a convenience method intended for use by simple scripts
    to do one-shot configuration of the logging package.

    The default behaviour is to create a StreamHandler which writes to
    sys.stderr, set a formatter using the BASIC_FORMAT format string, and
    add the handler to the root logger.

    A number of optional keyword arguments may be specified, which can alter
    the default behaviour.

    filename  Specifies that a FileHandler be created, using the specified
              filename, rather than a StreamHandler.
    filemode  Specifies the mode to open the file, if filename is specified
              (if filemode is unspecified, it defaults to 'a').
    format    Use the specified format string for the handler.
    datefmt   Use the specified date/time format.
    style     If a format string is specified, use this to specify the
              type of format string (possible values '%', '{', '$', for
              %-formatting, :meth:`str.format` and :class:`string.Template`
              - defaults to '%').
    level     Set the root logger level to the specified level.
    stream    Use the specified stream to initialize the StreamHandler. Note
              that this argument is incompatible with 'filename' - if both
              are present, 'stream' is ignored.
    handlers  If specified, this should be an iterable of already created
              handlers, which will be added to the root handler. Any handler
              in the list which does not have a formatter assigned will be
              assigned the formatter created in this function.
    force     If this keyword  is specified as true, any existing handlers
              attached to the root logger are removed and closed, before
              carrying out the configuration as specified by the other
              arguments.
    encoding  If specified together with a filename, this encoding is passed to
              the created FileHandler, causing it to be used when the file is
              opened.
    errors    If specified together with a filename, this value is passed to the
              created FileHandler, causing it to be used when the file is
              opened in text mode. If not specified, the default value is
              `backslashreplace`.

    Note that you could specify a stream created using open(filename, mode)
    rather than passing the filename and mode in. However, it should be
    remembered that StreamHandler does not close its stream (since it may be
    using sys.stdout or sys.stderr), whereas FileHandler closes its stream
    when the handler is closed.

    .. versionchanged:: 3.2
       Added the ``style`` parameter.

    .. versionchanged:: 3.3
       Added the ``handlers`` parameter. A ``ValueError`` is now thrown for
       incompatible arguments (e.g. ``handlers`` specified together with
       ``filename``/``filemode``, or ``filename``/``filemode`` specified
       together with ``stream``, or ``handlers`` specified together with
       ``stream``.

    .. versionchanged:: 3.8
       Added the ``force`` parameter.

    .. versionchanged:: 3.9
       Added the ``encoding`` and ``errors`` parameters.
    """"""
    # Add thread safety in case someone mistakenly calls
    # basicConfig() from multiple threads
    _acquireLock()
    try:
        force = kwargs.pop('force', False)
        encoding = kwargs.pop('encoding', None)
        errors = kwargs.pop('errors', 'backslashreplace')
        if force:
            for h in root.handlers[:]:
                root.removeHandler(h)
                h.close()
        if len(root.handlers) == 0:
            handlers = kwargs.pop(""handlers"", None)
            if handlers is None:
                if ""stream"" in kwargs and ""filename"" in kwargs:
                    raise ValueError(""'stream' and 'filename' should not be ""
                                     ""specified together"")
            else:
                if ""stream"" in kwargs or ""filename"" in kwargs:
                    raise ValueError(""'stream' or 'filename' should not be ""
                                     ""specified together with 'handlers'"")
            if handlers is None:
                filename = kwargs.pop(""filename"", None)
                mode = kwargs.pop(""filemode"", 'a')
                if filename:
                    if 'b' in mode:
                        errors = None
                    else:
                        encoding = io.text_encoding(encoding)
                    h = FileHandler(filename, mode,
                                    encoding=encoding, errors=errors)
                else:
                    stream = kwargs.pop(""stream"", None)
                    h = StreamHandler(stream)
                handlers = [h]
            dfs = kwargs.pop(""datefmt"", None)
            style = kwargs.pop(""style"", '%')
            if style not in _STYLES:
                raise ValueError('Style must be one of: %s' % ','.join(
                                 _STYLES.keys()))
            fs = kwargs.pop(""format"", _STYLES[style][1])
            fmt = Formatter(fs, dfs, style)
            for h in handlers:
                if h.formatter is None:
                    h.setFormatter(fmt)
                root.addHandler(h)
            level = kwargs.pop(""level"", None)
            if level is not None:
                root.setLevel(level)
            if kwargs:
                keys = ', '.join(kwargs.keys())
                raise ValueError('Unrecognised argument(s): %s' % keys)
    finally:
        _releaseLock()","def do_basic_logging_configuration(**kwargs):
    """"""
    Do basic configuration for the logging system.

    This function does nothing if the root logger already has handlers
    configured, unless the keyword argument *force* is set to ``True``.
    It is a convenience method intended for use by simple scripts
    to do one-shot configuration of the logging package.

    The default behaviour is to create a StreamHandler which writes to
    sys.stderr, set a formatter using the BASIC_FORMAT format string, and
    add the handler to the root logger.

    A number of optional keyword arguments may be specified, which can alter
    the default behaviour.

    filename  Specifies that a FileHandler be created, using the specified
              filename, rather than a StreamHandler.
    filemode  Specifies the mode to open the file, if filename is specified
              (if filemode is unspecified, it defaults to 'a').
    format    Use the specified format string for the handler.
    datefmt   Use the specified date/time format.
    style     If a format string is specified, use this to specify the
              type of format string (possible values '%', '{', '$', for
              %-formatting, :meth:`str.format` and :class:`string.Template`
              - defaults to '%').
    level     Set the root logger level to the specified level.
    stream    Use the specified stream to initialize the StreamHandler. Note
              that this argument is incompatible with 'filename' - if both
              are present, 'stream' is ignored.
    handlers  If specified, this should be an iterable of already created
              handlers, which will be added to the root handler. Any handler
              in the list which does not have a formatter assigned will be
              assigned the formatter created in this function.
    force     If this keyword  is specified as true, any existing handlers
              attached to the root logger are removed and closed, before
              carrying out the configuration as specified by the other
              arguments.
    encoding  If specified together with a filename, this encoding is passed to
              the created FileHandler, causing it to be used when the file is
              opened.
    errors    If specified together with a filename, this value is passed to the
              created FileHandler, causing it to be used when the file is
              opened in text mode. If not specified, the default value is
              `backslashreplace`.

    Note that you could specify a stream created using open(filename, mode)
    rather than passing the filename and mode in. However, it should be
    remembered that StreamHandler does not close its stream (since it may be
    using sys.stdout or sys.stderr), whereas FileHandler closes its stream
    when the handler is closed.

    .. versionchanged:: 3.2
       Added the ``style`` parameter.

    .. versionchanged:: 3.3
       Added the ``handlers`` parameter. A ``ValueError`` is now thrown for
       incompatible arguments (e.g. ``handlers`` specified together with
       ``filename``/``filemode``, or ``filename``/``filemode`` specified
       together with ``stream``, or ``handlers`` specified together with
       ``stream``.

    .. versionchanged:: 3.8
       Added the ``force`` parameter.

    .. versionchanged:: 3.9
       Added the ``encoding`` and ``errors`` parameters.
    """"""
    # Add thread safety in case someone mistakenly calls
    # do_basic_logging_configuration() from multiple threads
    _acquireLock()
    try:
        force = kwargs.pop('force', False)
        encoding = kwargs.pop('encoding', None)
        errors = kwargs.pop('errors', 'backslashreplace')
        if force:
            for h in root.handlers[:]:
                root.removeHandler(h)
                h.close()
        if len(root.handlers) == 0:
            handlers = kwargs.pop(""handlers"", None)
            if handlers is None:
                if ""stream"" in kwargs and ""filename"" in kwargs:
                    raise ValueError(""'stream' and 'filename' should not be ""
                                     ""specified together"")
            else:
                if ""stream"" in kwargs or ""filename"" in kwargs:
                    raise ValueError(""'stream' or 'filename' should not be ""
                                     ""specified together with 'handlers'"")
            if handlers is None:
                filename = kwargs.pop(""filename"", None)
                mode = kwargs.pop(""filemode"", 'a')
                if filename:
                    if 'b' in mode:
                        errors = None
                    else:
                        encoding = io.text_encoding(encoding)
                    h = FileHandler(filename, mode,
                                    encoding=encoding, errors=errors)
                else:
                    stream = kwargs.pop(""stream"", None)
                    h = StreamHandler(stream)
                handlers = [h]
            dfs = kwargs.pop(""datefmt"", None)
            style = kwargs.pop(""style"", '%')
            if style not in _STYLES:
                raise ValueError('Style must be one of: %s' % ','.join(
                                 _STYLES.keys()))
            fs = kwargs.pop(""format"", _STYLES[style][1])
            fmt = Formatter(fs, dfs, style)
            for h in handlers:
                if h.formatter is None:
                    h.setFormatter(fmt)
                root.addHandler(h)
            level = kwargs.pop(""level"", None)
            if level is not None:
                root.setLevel(level)
            if kwargs:
                keys = ', '.join(kwargs.keys())
                raise ValueError('Unrecognised argument(s): %s' % keys)
    finally:
        _releaseLock()","def log_msg_with_integer_severity_level_on_root_logger(level, msg, *args, **kwargs):
    """"""
    Log 'msg % args' with the integer severity 'level' on the root log_msg_with_integer_severity_level_on_root_loggerger. If
    the log_msg_with_integer_severity_level_on_root_loggerger has no handlers, call do_basic_log_msg_with_integer_severity_level_on_root_loggerging_configuration() to add a console handler
    with a pre-defined format.
    """"""
    if len(root.handlers) == 0:
        do_basic_log_msg_with_integer_severity_level_on_root_loggerging_configuration()
    root.log_msg_with_integer_severity_level_on_root_logger(level, msg, *args, **kwargs)","{""basicConfig"": ""do_basic_logging_configuration""}","def 0(**kwargs):
    """"""
    Do basic configuration for the logging system.

    This function does nothing if the root logger already has handlers
    configured, unless the keyword argument *force* is set to ``True``.
    It is a convenience method intended for use by simple scripts
    to do one-shot configuration of the logging package.

    The default behaviour is to create a StreamHandler which writes to
    sys.stderr, set a formatter using the BASIC_FORMAT format string, and
    add the handler to the root logger.

    A number of optional keyword arguments may be specified, which can alter
    the default behaviour.

    filename  Specifies that a FileHandler be created, using the specified
              filename, rather than a StreamHandler.
    filemode  Specifies the mode to open the file, if filename is specified
              (if filemode is unspecified, it defaults to 'a').
    format    Use the specified format string for the handler.
    datefmt   Use the specified date/time format.
    style     If a format string is specified, use this to specify the
              type of format string (possible values '%', '{', '$', for
              %-formatting, :meth:`str.format` and :class:`string.Template`
              - defaults to '%').
    level     Set the root logger level to the specified level.
    stream    Use the specified stream to initialize the StreamHandler. Note
              that this argument is incompatible with 'filename' - if both
              are present, 'stream' is ignored.
    handlers  If specified, this should be an iterable of already created
              handlers, which will be added to the root handler. Any handler
              in the list which does not have a formatter assigned will be
              assigned the formatter created in this function.
    force     If this keyword  is specified as true, any existing handlers
              attached to the root logger are removed and closed, before
              carrying out the configuration as specified by the other
              arguments.
    encoding  If specified together with a filename, this encoding is passed to
              the created FileHandler, causing it to be used when the file is
              opened.
    errors    If specified together with a filename, this value is passed to the
              created FileHandler, causing it to be used when the file is
              opened in text mode. If not specified, the default value is
              `backslashreplace`.

    Note that you could specify a stream created using open(filename, mode)
    rather than passing the filename and mode in. However, it should be
    remembered that StreamHandler does not close its stream (since it may be
    using sys.stdout or sys.stderr), whereas FileHandler closes its stream
    when the handler is closed.

    .. versionchanged:: 3.2
       Added the ``style`` parameter.

    .. versionchanged:: 3.3
       Added the ``handlers`` parameter. A ``ValueError`` is now thrown for
       incompatible arguments (e.g. ``handlers`` specified together with
       ``filename``/``filemode``, or ``filename``/``filemode`` specified
       together with ``stream``, or ``handlers`` specified together with
       ``stream``.

    .. versionchanged:: 3.8
       Added the ``force`` parameter.

    .. versionchanged:: 3.9
       Added the ``encoding`` and ``errors`` parameters.
    """"""
    # Add thread safety in case someone mistakenly calls
    # 0() from multiple threads
    _acquireLock()
    try:
        force = kwargs.pop('force', False)
        encoding = kwargs.pop('encoding', None)
        errors = kwargs.pop('errors', 'backslashreplace')
        if force:
            for h in root.handlers[:]:
                root.removeHandler(h)
                h.close()
        if len(root.handlers) == 0:
            handlers = kwargs.pop(""handlers"", None)
            if handlers is None:
                if ""stream"" in kwargs and ""filename"" in kwargs:
                    raise ValueError(""'stream' and 'filename' should not be ""
                                     ""specified together"")
            else:
                if ""stream"" in kwargs or ""filename"" in kwargs:
                    raise ValueError(""'stream' or 'filename' should not be ""
                                     ""specified together with 'handlers'"")
            if handlers is None:
                filename = kwargs.pop(""filename"", None)
                mode = kwargs.pop(""filemode"", 'a')
                if filename:
                    if 'b' in mode:
                        errors = None
                    else:
                        encoding = io.text_encoding(encoding)
                    h = FileHandler(filename, mode,
                                    encoding=encoding, errors=errors)
                else:
                    stream = kwargs.pop(""stream"", None)
                    h = StreamHandler(stream)
                handlers = [h]
            dfs = kwargs.pop(""datefmt"", None)
            style = kwargs.pop(""style"", '%')
            if style not in _STYLES:
                raise ValueError('Style must be one of: %s' % ','.join(
                                 _STYLES.keys()))
            fs = kwargs.pop(""format"", _STYLES[style][1])
            fmt = Formatter(fs, dfs, style)
            for h in handlers:
                if h.formatter is None:
                    h.setFormatter(fmt)
                root.addHandler(h)
            level = kwargs.pop(""level"", None)
            if level is not None:
                root.setLevel(level)
            if kwargs:
                keys = ', '.join(kwargs.keys())
                raise ValueError('Unrecognised argument(s): %s' % keys)
    finally:
        _releaseLock()","def 1(level, msg, *args, **kwargs):
    """"""
    Log 'msg % args' with the integer severity 'level' on the root logger. If
    the logger has no handlers, call 0() to add a console handler
    with a pre-defined format.
    """"""
    if len(root.handlers) == 0:
        0()
    root.1(level, msg, *args, **kwargs)","{""basicConfig"": ""0""}"
630,630,"def get_qp_ctext(value):
    r""""""ctext = <printable ascii except \ ( )>

    This is not the RFC ctext, since we are handling nested comments in comment
    and unquoting quoted-pairs here.  We allow anything except the '()'
    characters, but if we find any ASCII other than the RFC defined printable
    ASCII, a NonPrintableDefect is added to the token's defects list.  Since
    quoted pairs are converted to their unquoted values, what is returned is
    a 'ptext' token.  In this case it is a WhiteSpaceTerminal, so it's value
    is ' '.

    """"""
    ptext, value, _ = _get_ptext_to_endchars(value, '()')
    ptext = WhiteSpaceTerminal(ptext, 'ptext')
    _validate_xtext(ptext)
    return ptext, value","def _validate_xtext(xtext):
    """"""If input token contains ASCII non-printables, register a defect.""""""

    non_printables = _non_printable_finder(xtext)
    if non_printables:
        xtext.defects.append(errors.NonPrintableDefect(non_printables))
    if utils._has_surrogates(xtext):
        xtext.defects.append(errors.UndecodableBytesDefect(
            ""Non-ASCII characters found in header token""))

def _get_ptext_to_endchars(value, endchars):
    """"""Scan printables/quoted-pairs until endchars and return unquoted ptext.

    This function turns a run of qcontent, ccontent-without-comments, or
    dtext-with-quoted-printables into a single string by unquoting any
    quoted printables.  It returns the string, the remaining value, and
    a flag that is True iff there were any quoted printables decoded.

    """"""
    fragment, *remainder = _wsp_splitter(value, 1)
    vchars = []
    escape = False
    had_qp = False
    for pos in range(len(fragment)):
        if fragment[pos] == '\\':
            if escape:
                escape = False
                had_qp = True
            else:
                escape = True
                continue
        if escape:
            escape = False
        elif fragment[pos] in endchars:
            break
        vchars.append(fragment[pos])
    else:
        pos = pos + 1
    return ''.join(vchars), ''.join([fragment[pos:]] + remainder), had_qp","def validate_xtext_input_token(xtext):
    """"""If input token contains ASCII non-printables, register a defect.""""""

    non_printables = _non_printable_finder(xtext)
    if non_printables:
        xtext.defects.append(errors.NonPrintableDefect(non_printables))
    if utils._has_surrogates(xtext):
        xtext.defects.append(errors.UndecodableBytesDefect(
            ""Non-ASCII characters found in header token""))

def get_ptext_until_endchars(value, endchars):
    """"""Scan printables/quoted-pairs until endchars and return unquoted ptext.

    This function turns a run of qcontent, ccontent-without-comments, or
    dtext-with-quoted-printables into a single string by unquoting any
    quoted printables.  It returns the string, the remaining value, and
    a flag that is True iff there were any quoted printables decoded.

    """"""
    fragment, *remainder = _wsp_splitter(value, 1)
    vchars = []
    escape = False
    had_qp = False
    for pos in range(len(fragment)):
        if fragment[pos] == '\\':
            if escape:
                escape = False
                had_qp = True
            else:
                escape = True
                continue
        if escape:
            escape = False
        elif fragment[pos] in endchars:
            break
        vchars.append(fragment[pos])
    else:
        pos = pos + 1
    return ''.join(vchars), ''.join([fragment[pos:]] + remainder), had_qp","def parse_qp_ctext_from_value(value):
    r""""""ctext = <printable ascii except \ ( )>

    This is not the RFC ctext, since we are handling nested comments in comment
    and unquoting quoted-pairs here.  We allow anything except the '()'
    characters, but if we find any ASCII other than the RFC defined printable
    ASCII, a NonPrintableDefect is added to the token's defects list.  Since
    quoted pairs are converted to their unquoted values, what is returned is
    a 'ptext' token.  In this case it is a WhiteSpaceTerminal, so it's value
    is ' '.

    """"""
    ptext, value, _ = get_ptext_until_endchars(value, '()')
    ptext = WhiteSpaceTerminal(ptext, 'ptext')
    validate_xtext_input_token(ptext)
    return ptext, value","{""_validate_xtext"": ""validate_xtext_input_token"", ""_get_ptext_to_endchars"": ""get_ptext_until_endchars""}","def 0(xtext):
    """"""If input token contains ASCII non-printables, register a defect.""""""

    non_printables = _non_printable_finder(xtext)
    if non_printables:
        xtext.defects.append(errors.NonPrintableDefect(non_printables))
    if utils._has_surrogates(xtext):
        xtext.defects.append(errors.UndecodableBytesDefect(
            ""Non-ASCII characters found in header token""))

def 1(value, endchars):
    """"""Scan printables/quoted-pairs until endchars and return unquoted ptext.

    This function turns a run of qcontent, ccontent-without-comments, or
    dtext-with-quoted-printables into a single string by unquoting any
    quoted printables.  It returns the string, the remaining value, and
    a flag that is True iff there were any quoted printables decoded.

    """"""
    fragment, *remainder = _wsp_splitter(value, 1)
    vchars = []
    escape = False
    had_qp = False
    for pos in range(len(fragment)):
        if fragment[pos] == '\\':
            if escape:
                escape = False
                had_qp = True
            else:
                escape = True
                continue
        if escape:
            escape = False
        elif fragment[pos] in endchars:
            break
        vchars.append(fragment[pos])
    else:
        pos = pos + 1
    return ''.join(vchars), ''.join([fragment[pos:]] + remainder), had_qp","def 2(value):
    r""""""ctext = <printable ascii except \ ( )>

    This is not the RFC ctext, since we are handling nested comments in comment
    and unquoting quoted-pairs here.  We allow anything except the '()'
    characters, but if we find any ASCII other than the RFC defined printable
    ASCII, a NonPrintableDefect is added to the token's defects list.  Since
    quoted pairs are converted to their unquoted values, what is returned is
    a 'ptext' token.  In this case it is a WhiteSpaceTerminal, so it's value
    is ' '.

    """"""
    ptext, value, _ = 1(value, '()')
    ptext = WhiteSpaceTerminal(ptext, 'ptext')
    0(ptext)
    return ptext, value","{""_validate_xtext"": ""0"", ""_get_ptext_to_endchars"": ""1""}"
631,631,"def get_qcontent(value):
    """"""qcontent = qtext / quoted-pair

    We allow anything except the DQUOTE character, but if we find any ASCII
    other than the RFC defined printable ASCII, a NonPrintableDefect is
    added to the token's defects list.  Any quoted pairs are converted to their
    unquoted values, so what is returned is a 'ptext' token.  In this case it
    is a ValueTerminal.

    """"""
    ptext, value, _ = _get_ptext_to_endchars(value, '""')
    ptext = ValueTerminal(ptext, 'ptext')
    _validate_xtext(ptext)
    return ptext, value","def _validate_xtext(xtext):
    """"""If input token contains ASCII non-printables, register a defect.""""""

    non_printables = _non_printable_finder(xtext)
    if non_printables:
        xtext.defects.append(errors.NonPrintableDefect(non_printables))
    if utils._has_surrogates(xtext):
        xtext.defects.append(errors.UndecodableBytesDefect(
            ""Non-ASCII characters found in header token""))

def _get_ptext_to_endchars(value, endchars):
    """"""Scan printables/quoted-pairs until endchars and return unquoted ptext.

    This function turns a run of qcontent, ccontent-without-comments, or
    dtext-with-quoted-printables into a single string by unquoting any
    quoted printables.  It returns the string, the remaining value, and
    a flag that is True iff there were any quoted printables decoded.

    """"""
    fragment, *remainder = _wsp_splitter(value, 1)
    vchars = []
    escape = False
    had_qp = False
    for pos in range(len(fragment)):
        if fragment[pos] == '\\':
            if escape:
                escape = False
                had_qp = True
            else:
                escape = True
                continue
        if escape:
            escape = False
        elif fragment[pos] in endchars:
            break
        vchars.append(fragment[pos])
    else:
        pos = pos + 1
    return ''.join(vchars), ''.join([fragment[pos:]] + remainder), had_qp","def validate_xtext_input_token(xtext):
    """"""If input token contains ASCII non-printables, register a defect.""""""

    non_printables = _non_printable_finder(xtext)
    if non_printables:
        xtext.defects.append(errors.NonPrintableDefect(non_printables))
    if utils._has_surrogates(xtext):
        xtext.defects.append(errors.UndecodableBytesDefect(
            ""Non-ASCII characters found in header token""))

def get_ptext_until_endchars(value, endchars):
    """"""Scan printables/quoted-pairs until endchars and return unquoted ptext.

    This function turns a run of qcontent, ccontent-without-comments, or
    dtext-with-quoted-printables into a single string by unquoting any
    quoted printables.  It returns the string, the remaining value, and
    a flag that is True iff there were any quoted printables decoded.

    """"""
    fragment, *remainder = _wsp_splitter(value, 1)
    vchars = []
    escape = False
    had_qp = False
    for pos in range(len(fragment)):
        if fragment[pos] == '\\':
            if escape:
                escape = False
                had_qp = True
            else:
                escape = True
                continue
        if escape:
            escape = False
        elif fragment[pos] in endchars:
            break
        vchars.append(fragment[pos])
    else:
        pos = pos + 1
    return ''.join(vchars), ''.join([fragment[pos:]] + remainder), had_qp","def qcontent_equals_qtext_or_quoted_pair(value):
    """"""qcontent = qtext / quoted-pair

    We allow anything except the DQUOTE character, but if we find any ASCII
    other than the RFC defined printable ASCII, a NonPrintableDefect is
    added to the token's defects list.  Any quoted pairs are converted to their
    unquoted values, so what is returned is a 'ptext' token.  In this case it
    is a ValueTerminal.

    """"""
    ptext, value, _ = get_ptext_until_endchars(value, '""')
    ptext = ValueTerminal(ptext, 'ptext')
    validate_xtext_input_token(ptext)
    return ptext, value","{""_validate_xtext"": ""validate_xtext_input_token"", ""_get_ptext_to_endchars"": ""get_ptext_until_endchars""}","def 0(xtext):
    """"""If input token contains ASCII non-printables, register a defect.""""""

    non_printables = _non_printable_finder(xtext)
    if non_printables:
        xtext.defects.append(errors.NonPrintableDefect(non_printables))
    if utils._has_surrogates(xtext):
        xtext.defects.append(errors.UndecodableBytesDefect(
            ""Non-ASCII characters found in header token""))

def 1(value, endchars):
    """"""Scan printables/quoted-pairs until endchars and return unquoted ptext.

    This function turns a run of qcontent, ccontent-without-comments, or
    dtext-with-quoted-printables into a single string by unquoting any
    quoted printables.  It returns the string, the remaining value, and
    a flag that is True iff there were any quoted printables decoded.

    """"""
    fragment, *remainder = _wsp_splitter(value, 1)
    vchars = []
    escape = False
    had_qp = False
    for pos in range(len(fragment)):
        if fragment[pos] == '\\':
            if escape:
                escape = False
                had_qp = True
            else:
                escape = True
                continue
        if escape:
            escape = False
        elif fragment[pos] in endchars:
            break
        vchars.append(fragment[pos])
    else:
        pos = pos + 1
    return ''.join(vchars), ''.join([fragment[pos:]] + remainder), had_qp","def 2(value):
    """"""qcontent = qtext / quoted-pair

    We allow anything except the DQUOTE character, but if we find any ASCII
    other than the RFC defined printable ASCII, a NonPrintableDefect is
    added to the token's defects list.  Any quoted pairs are converted to their
    unquoted values, so what is returned is a 'ptext' token.  In this case it
    is a ValueTerminal.

    """"""
    ptext, value, _ = 1(value, '""')
    ptext = ValueTerminal(ptext, 'ptext')
    0(ptext)
    return ptext, value","{""_validate_xtext"": ""0"", ""_get_ptext_to_endchars"": ""1""}"
632,632,"def get_atext(value):
    """"""atext = <matches _atext_matcher>

    We allow any non-ATOM_ENDS in atext, but add an InvalidATextDefect to
    the token's defects list if we find non-atext characters.
    """"""
    m = _non_atom_end_matcher(value)
    if not m:
        raise errors.HeaderParseError(
            ""expected atext but found '{}'"".format(value))
    atext = m.group()
    value = value[len(atext):]
    atext = ValueTerminal(atext, 'atext')
    _validate_xtext(atext)
    return atext, value","def _validate_xtext(xtext):
    """"""If input token contains ASCII non-printables, register a defect.""""""

    non_printables = _non_printable_finder(xtext)
    if non_printables:
        xtext.defects.append(errors.NonPrintableDefect(non_printables))
    if utils._has_surrogates(xtext):
        xtext.defects.append(errors.UndecodableBytesDefect(
            ""Non-ASCII characters found in header token""))","def validate_xtext_input_token(xtext):
    """"""If input token contains ASCII non-printables, register a defect.""""""

    non_printables = _non_printable_finder(xtext)
    if non_printables:
        xtext.defects.append(errors.NonPrintableDefect(non_printables))
    if utils._has_surrogates(xtext):
        xtext.defects.append(errors.UndecodableBytesDefect(
            ""Non-ASCII characters found in header token""))","def parse_atext_from_value(value):
    """"""atext = <matches _atext_matcher>

    We allow any non-ATOM_ENDS in atext, but add an InvalidATextDefect to
    the token's defects list if we find non-atext characters.
    """"""
    m = _non_atom_end_matcher(value)
    if not m:
        raise errors.HeaderParseError(
            ""expected atext but found '{}'"".format(value))
    atext = m.group()
    value = value[len(atext):]
    atext = ValueTerminal(atext, 'atext')
    validate_xtext_input_token(atext)
    return atext, value","{""_validate_xtext"": ""validate_xtext_input_token""}","def 0(xtext):
    """"""If input token contains ASCII non-printables, register a defect.""""""

    non_printables = _non_printable_finder(xtext)
    if non_printables:
        xtext.defects.append(errors.NonPrintableDefect(non_printables))
    if utils._has_surrogates(xtext):
        xtext.defects.append(errors.UndecodableBytesDefect(
            ""Non-ASCII characters found in header token""))","def 1(value):
    """"""atext = <matches _atext_matcher>

    We allow any non-ATOM_ENDS in atext, but add an InvalidATextDefect to
    the token's defects list if we find non-atext characters.
    """"""
    m = _non_atom_end_matcher(value)
    if not m:
        raise errors.HeaderParseError(
            ""expected atext but found '{}'"".format(value))
    atext = m.group()
    value = value[len(atext):]
    atext = ValueTerminal(atext, 'atext')
    0(atext)
    return atext, value","{""_validate_xtext"": ""0""}"
633,633,"def get_comment(value):
    """"""comment = ""("" *([FWS] ccontent) [FWS] "")""
       ccontent = ctext / quoted-pair / comment

    We handle nested comments here, and quoted-pair in our qp-ctext routine.
    """"""
    if value and value[0] != '(':
        raise errors.HeaderParseError(
            ""expected '(' but found '{}'"".format(value))
    comment = Comment()
    value = value[1:]
    while value and value[0] != "")"":
        if value[0] in WSP:
            token, value = get_fws(value)
        elif value[0] == '(':
            token, value = get_comment(value)
        else:
            token, value = get_qp_ctext(value)
        comment.append(token)
    if not value:
        comment.defects.append(errors.InvalidHeaderDefect(
            ""end of header inside comment""))
        return comment, value
    return comment, value[1:]","def get_comment(value):
    """"""comment = ""("" *([FWS] ccontent) [FWS] "")""
       ccontent = ctext / quoted-pair / comment

    We handle nested comments here, and quoted-pair in our qp-ctext routine.
    """"""
    if value and value[0] != '(':
        raise errors.HeaderParseError(
            ""expected '(' but found '{}'"".format(value))
    comment = Comment()
    value = value[1:]
    while value and value[0] != "")"":
        if value[0] in WSP:
            token, value = get_fws(value)
        elif value[0] == '(':
            token, value = get_comment(value)
        else:
            token, value = get_qp_ctext(value)
        comment.append(token)
    if not value:
        comment.defects.append(errors.InvalidHeaderDefect(
            ""end of header inside comment""))
        return comment, value
    return comment, value[1:]

def get_fws(value):
    """"""FWS = 1*WSP

    This isn't the RFC definition.  We're using fws to represent tokens where
    folding can be done, but when we are parsing the *un*folding has already
    been done so we don't need to watch out for CRLF.

    """"""
    newvalue = value.lstrip()
    fws = WhiteSpaceTerminal(value[:len(value)-len(newvalue)], 'fws')
    return fws, newvalue

def Comment(text=None):
    """"""Comment element factory.

    This function creates a special element which the standard serializer
    serializes as an XML comment.

    *text* is a string containing the comment string.

    """"""
    element = Element(Comment)
    element.text = text
    return element

def get_qp_ctext(value):
    r""""""ctext = <printable ascii except \ ( )>

    This is not the RFC ctext, since we are handling nested comments in comment
    and unquoting quoted-pairs here.  We allow anything except the '()'
    characters, but if we find any ASCII other than the RFC defined printable
    ASCII, a NonPrintableDefect is added to the token's defects list.  Since
    quoted pairs are converted to their unquoted values, what is returned is
    a 'ptext' token.  In this case it is a WhiteSpaceTerminal, so it's value
    is ' '.

    """"""
    ptext, value, _ = _get_ptext_to_endchars(value, '()')
    ptext = WhiteSpaceTerminal(ptext, 'ptext')
    _validate_xtext(ptext)
    return ptext, value","def parse_comment_from_value(value):
    """"""comment = ""("" *([FWS] ccontent) [FWS] "")""
       ccontent = ctext / quoted-pair / comment

    We handle nested comments here, and quoted-pair in our qp-ctext routine.
    """"""
    if value and value[0] != '(':
        raise errors.HeaderParseError(
            ""expected '(' but found '{}'"".format(value))
    comment = create_comment_element()
    value = value[1:]
    while value and value[0] != "")"":
        if value[0] in WSP:
            token, value = parse_fws_from_value(value)
        elif value[0] == '(':
            token, value = parse_comment_from_value(value)
        else:
            token, value = parse_qp_ctext_from_value(value)
        comment.append(token)
    if not value:
        comment.defects.append(errors.InvalidHeaderDefect(
            ""end of header inside comment""))
        return comment, value
    return comment, value[1:]

def parse_fws_from_value(value):
    """"""FWS = 1*WSP

    This isn't the RFC definition.  We're using fws to represent tokens where
    folding can be done, but when we are parsing the *un*folding has already
    been done so we don't need to watch out for CRLF.

    """"""
    newvalue = value.lstrip()
    fws = WhiteSpaceTerminal(value[:len(value)-len(newvalue)], 'fws')
    return fws, newvalue

def create_comment_element(text=None):
    """"""create_comment_element element factory.

    This function creates a special element which the standard serializer
    serializes as an XML comment.

    *text* is a string containing the comment string.

    """"""
    element = Element(create_comment_element)
    element.text = text
    return element

def parse_qp_ctext_from_value(value):
    r""""""ctext = <printable ascii except \ ( )>

    This is not the RFC ctext, since we are handling nested comments in comment
    and unquoting quoted-pairs here.  We allow anything except the '()'
    characters, but if we find any ASCII other than the RFC defined printable
    ASCII, a NonPrintableDefect is added to the token's defects list.  Since
    quoted pairs are converted to their unquoted values, what is returned is
    a 'ptext' token.  In this case it is a WhiteSpaceTerminal, so it's value
    is ' '.

    """"""
    ptext, value, _ = _get_ptext_to_endchars(value, '()')
    ptext = WhiteSpaceTerminal(ptext, 'ptext')
    _validate_xtext(ptext)
    return ptext, value","def parse_comment_from_value(value):
    """"""comment = ""("" *([FWS] ccontent) [FWS] "")""
       ccontent = ctext / quoted-pair / comment

    We handle nested comments here, and quoted-pair in our qp-ctext routine.
    """"""
    if value and value[0] != '(':
        raise errors.HeaderParseError(
            ""expected '(' but found '{}'"".format(value))
    comment = create_comment_element()
    value = value[1:]
    while value and value[0] != "")"":
        if value[0] in WSP:
            token, value = parse_fws_from_value(value)
        elif value[0] == '(':
            token, value = parse_comment_from_value(value)
        else:
            token, value = parse_qp_ctext_from_value(value)
        comment.append(token)
    if not value:
        comment.defects.append(errors.InvalidHeaderDefect(
            ""end of header inside comment""))
        return comment, value
    return comment, value[1:]","{""get_comment"": ""parse_comment_from_value"", ""get_fws"": ""parse_fws_from_value"", ""Comment"": ""create_comment_element"", ""get_qp_ctext"": ""parse_qp_ctext_from_value""}","def 0(value):
    """"""comment = ""("" *([FWS] ccontent) [FWS] "")""
       ccontent = ctext / quoted-pair / comment

    We handle nested comments here, and quoted-pair in our qp-ctext routine.
    """"""
    if value and value[0] != '(':
        raise errors.HeaderParseError(
            ""expected '(' but found '{}'"".format(value))
    comment = 2()
    value = value[1:]
    while value and value[0] != "")"":
        if value[0] in WSP:
            token, value = 1(value)
        elif value[0] == '(':
            token, value = 0(value)
        else:
            token, value = 3(value)
        comment.append(token)
    if not value:
        comment.defects.append(errors.InvalidHeaderDefect(
            ""end of header inside comment""))
        return comment, value
    return comment, value[1:]

def 1(value):
    """"""FWS = 1*WSP

    This isn't the RFC definition.  We're using fws to represent tokens where
    folding can be done, but when we are parsing the *un*folding has already
    been done so we don't need to watch out for CRLF.

    """"""
    newvalue = value.lstrip()
    fws = WhiteSpaceTerminal(value[:len(value)-len(newvalue)], 'fws')
    return fws, newvalue

def 2(text=None):
    """"""Comment element factory.

    This function creates a special element which the standard serializer
    serializes as an XML comment.

    *text* is a string containing the comment string.

    """"""
    element = Element(Comment)
    element.text = text
    return element

def 3(value):
    r""""""ctext = <printable ascii except \ ( )>

    This is not the RFC ctext, since we are handling nested comments in comment
    and unquoting quoted-pairs here.  We allow anything except the '()'
    characters, but if we find any ASCII other than the RFC defined printable
    ASCII, a NonPrintableDefect is added to the token's defects list.  Since
    quoted pairs are converted to their unquoted values, what is returned is
    a 'ptext' token.  In this case it is a WhiteSpaceTerminal, so it's value
    is ' '.

    """"""
    ptext, value, _ = _get_ptext_to_endchars(value, '()')
    ptext = WhiteSpaceTerminal(ptext, 'ptext')
    _validate_xtext(ptext)
    return ptext, value","def 0(value):
    """"""comment = ""("" *([FWS] ccontent) [FWS] "")""
       ccontent = ctext / quoted-pair / comment

    We handle nested comments here, and quoted-pair in our qp-ctext routine.
    """"""
    if value and value[0] != '(':
        raise errors.HeaderParseError(
            ""expected '(' but found '{}'"".format(value))
    comment = 2()
    value = value[1:]
    while value and value[0] != "")"":
        if value[0] in WSP:
            token, value = 1(value)
        elif value[0] == '(':
            token, value = 0(value)
        else:
            token, value = 3(value)
        comment.append(token)
    if not value:
        comment.defects.append(errors.InvalidHeaderDefect(
            ""end of header inside comment""))
        return comment, value
    return comment, value[1:]","{""get_comment"": ""0"", ""get_fws"": ""1"", ""Comment"": ""2"", ""get_qp_ctext"": ""3""}"
634,634,"def get_cfws(value):
    """"""CFWS = (1*([FWS] comment) [FWS]) / FWS

    """"""
    cfws = CFWSList()
    while value and value[0] in CFWS_LEADER:
        if value[0] in WSP:
            token, value = get_fws(value)
        else:
            token, value = get_comment(value)
        cfws.append(token)
    return cfws, value","def get_comment(value):
    """"""comment = ""("" *([FWS] ccontent) [FWS] "")""
       ccontent = ctext / quoted-pair / comment

    We handle nested comments here, and quoted-pair in our qp-ctext routine.
    """"""
    if value and value[0] != '(':
        raise errors.HeaderParseError(
            ""expected '(' but found '{}'"".format(value))
    comment = Comment()
    value = value[1:]
    while value and value[0] != "")"":
        if value[0] in WSP:
            token, value = get_fws(value)
        elif value[0] == '(':
            token, value = get_comment(value)
        else:
            token, value = get_qp_ctext(value)
        comment.append(token)
    if not value:
        comment.defects.append(errors.InvalidHeaderDefect(
            ""end of header inside comment""))
        return comment, value
    return comment, value[1:]

def get_fws(value):
    """"""FWS = 1*WSP

    This isn't the RFC definition.  We're using fws to represent tokens where
    folding can be done, but when we are parsing the *un*folding has already
    been done so we don't need to watch out for CRLF.

    """"""
    newvalue = value.lstrip()
    fws = WhiteSpaceTerminal(value[:len(value)-len(newvalue)], 'fws')
    return fws, newvalue","def parse_comment_from_value(value):
    """"""comment = ""("" *([FWS] ccontent) [FWS] "")""
       ccontent = ctext / quoted-pair / comment

    We handle nested comments here, and quoted-pair in our qp-ctext routine.
    """"""
    if value and value[0] != '(':
        raise errors.HeaderParseError(
            ""expected '(' but found '{}'"".format(value))
    comment = Comment()
    value = value[1:]
    while value and value[0] != "")"":
        if value[0] in WSP:
            token, value = parse_fws_from_value(value)
        elif value[0] == '(':
            token, value = parse_comment_from_value(value)
        else:
            token, value = get_qp_ctext(value)
        comment.append(token)
    if not value:
        comment.defects.append(errors.InvalidHeaderDefect(
            ""end of header inside comment""))
        return comment, value
    return comment, value[1:]

def parse_fws_from_value(value):
    """"""FWS = 1*WSP

    This isn't the RFC definition.  We're using fws to represent tokens where
    folding can be done, but when we are parsing the *un*folding has already
    been done so we don't need to watch out for CRLF.

    """"""
    newvalue = value.lstrip()
    fws = WhiteSpaceTerminal(value[:len(value)-len(newvalue)], 'fws')
    return fws, newvalue","def parse_cfws_from_value(value):
    """"""CFWS = (1*([FWS] comment) [FWS]) / FWS

    """"""
    cfws = CFWSList()
    while value and value[0] in CFWS_LEADER:
        if value[0] in WSP:
            token, value = parse_fws_from_value(value)
        else:
            token, value = parse_comment_from_value(value)
        cfws.append(token)
    return cfws, value","{""get_comment"": ""parse_comment_from_value"", ""get_fws"": ""parse_fws_from_value""}","def 0(value):
    """"""comment = ""("" *([FWS] ccontent) [FWS] "")""
       ccontent = ctext / quoted-pair / comment

    We handle nested comments here, and quoted-pair in our qp-ctext routine.
    """"""
    if value and value[0] != '(':
        raise errors.HeaderParseError(
            ""expected '(' but found '{}'"".format(value))
    comment = Comment()
    value = value[1:]
    while value and value[0] != "")"":
        if value[0] in WSP:
            token, value = 1(value)
        elif value[0] == '(':
            token, value = 0(value)
        else:
            token, value = get_qp_ctext(value)
        comment.append(token)
    if not value:
        comment.defects.append(errors.InvalidHeaderDefect(
            ""end of header inside comment""))
        return comment, value
    return comment, value[1:]

def 1(value):
    """"""FWS = 1*WSP

    This isn't the RFC definition.  We're using fws to represent tokens where
    folding can be done, but when we are parsing the *un*folding has already
    been done so we don't need to watch out for CRLF.

    """"""
    newvalue = value.lstrip()
    fws = WhiteSpaceTerminal(value[:len(value)-len(newvalue)], 'fws')
    return fws, newvalue","def 2(value):
    """"""CFWS = (1*([FWS] comment) [FWS]) / FWS

    """"""
    cfws = CFWSList()
    while value and value[0] in CFWS_LEADER:
        if value[0] in WSP:
            token, value = 1(value)
        else:
            token, value = 0(value)
        cfws.append(token)
    return cfws, value","{""get_comment"": ""0"", ""get_fws"": ""1""}"
635,635,"def get_quoted_string(value):
    """"""quoted-string = [CFWS] <bare-quoted-string> [CFWS]

    'bare-quoted-string' is an intermediate class defined by this
    parser and not by the RFC grammar.  It is the quoted string
    without any attached CFWS.
    """"""
    quoted_string = QuotedString()
    if value and value[0] in CFWS_LEADER:
        token, value = get_cfws(value)
        quoted_string.append(token)
    token, value = get_bare_quoted_string(value)
    quoted_string.append(token)
    if value and value[0] in CFWS_LEADER:
        token, value = get_cfws(value)
        quoted_string.append(token)
    return quoted_string, value","def get_cfws(value):
    """"""CFWS = (1*([FWS] comment) [FWS]) / FWS

    """"""
    cfws = CFWSList()
    while value and value[0] in CFWS_LEADER:
        if value[0] in WSP:
            token, value = get_fws(value)
        else:
            token, value = get_comment(value)
        cfws.append(token)
    return cfws, value

def get_bare_quoted_string(value):
    """"""bare-quoted-string = DQUOTE *([FWS] qcontent) [FWS] DQUOTE

    A quoted-string without the leading or trailing white space.  Its
    value is the text between the quote marks, with whitespace
    preserved and quoted pairs decoded.
    """"""
    if value[0] != '""':
        raise errors.HeaderParseError(
            ""expected '\""' but found '{}'"".format(value))
    bare_quoted_string = BareQuotedString()
    value = value[1:]
    if value and value[0] == '""':
        token, value = get_qcontent(value)
        bare_quoted_string.append(token)
    while value and value[0] != '""':
        if value[0] in WSP:
            token, value = get_fws(value)
        elif value[:2] == '=?':
            valid_ew = False
            try:
                token, value = get_encoded_word(value)
                bare_quoted_string.defects.append(errors.InvalidHeaderDefect(
                    ""encoded word inside quoted string""))
                valid_ew = True
            except errors.HeaderParseError:
                token, value = get_qcontent(value)
            # Collapse the whitespace between two encoded words that occur in a
            # bare-quoted-string.
            if valid_ew and len(bare_quoted_string) > 1:
                if (bare_quoted_string[-1].token_type == 'fws' and
                        bare_quoted_string[-2].token_type == 'encoded-word'):
                    bare_quoted_string[-1] = EWWhiteSpaceTerminal(
                        bare_quoted_string[-1], 'fws')
        else:
            token, value = get_qcontent(value)
        bare_quoted_string.append(token)
    if not value:
        bare_quoted_string.defects.append(errors.InvalidHeaderDefect(
            ""end of header inside quoted string""))
        return bare_quoted_string, value
    return bare_quoted_string, value[1:]","def parse_cfws_from_value(value):
    """"""CFWS = (1*([FWS] comment) [FWS]) / FWS

    """"""
    cfws = CFWSList()
    while value and value[0] in CFWS_LEADER:
        if value[0] in WSP:
            token, value = get_fws(value)
        else:
            token, value = get_comment(value)
        cfws.append(token)
    return cfws, value

def parse_bare_quoted_string(value):
    """"""bare-quoted-string = DQUOTE *([FWS] qcontent) [FWS] DQUOTE

    A quoted-string without the leading or trailing white space.  Its
    value is the text between the quote marks, with whitespace
    preserved and quoted pairs decoded.
    """"""
    if value[0] != '""':
        raise errors.HeaderParseError(
            ""expected '\""' but found '{}'"".format(value))
    bare_quoted_string = BareQuotedString()
    value = value[1:]
    if value and value[0] == '""':
        token, value = get_qcontent(value)
        bare_quoted_string.append(token)
    while value and value[0] != '""':
        if value[0] in WSP:
            token, value = get_fws(value)
        elif value[:2] == '=?':
            valid_ew = False
            try:
                token, value = get_encoded_word(value)
                bare_quoted_string.defects.append(errors.InvalidHeaderDefect(
                    ""encoded word inside quoted string""))
                valid_ew = True
            except errors.HeaderParseError:
                token, value = get_qcontent(value)
            # Collapse the whitespace between two encoded words that occur in a
            # bare-quoted-string.
            if valid_ew and len(bare_quoted_string) > 1:
                if (bare_quoted_string[-1].token_type == 'fws' and
                        bare_quoted_string[-2].token_type == 'encoded-word'):
                    bare_quoted_string[-1] = EWWhiteSpaceTerminal(
                        bare_quoted_string[-1], 'fws')
        else:
            token, value = get_qcontent(value)
        bare_quoted_string.append(token)
    if not value:
        bare_quoted_string.defects.append(errors.InvalidHeaderDefect(
            ""end of header inside quoted string""))
        return bare_quoted_string, value
    return bare_quoted_string, value[1:]","def parse_quoted_string_from_value(value):
    """"""quoted-string = [CFWS] <bare-quoted-string> [CFWS]

    'bare-quoted-string' is an intermediate class defined by this
    parser and not by the RFC grammar.  It is the quoted string
    without any attached CFWS.
    """"""
    quoted_string = QuotedString()
    if value and value[0] in CFWS_LEADER:
        token, value = parse_cfws_from_value(value)
        quoted_string.append(token)
    token, value = parse_bare_quoted_string(value)
    quoted_string.append(token)
    if value and value[0] in CFWS_LEADER:
        token, value = parse_cfws_from_value(value)
        quoted_string.append(token)
    return quoted_string, value","{""get_cfws"": ""parse_cfws_from_value"", ""get_bare_quoted_string"": ""parse_bare_quoted_string""}","def 0(value):
    """"""CFWS = (1*([FWS] comment) [FWS]) / FWS

    """"""
    cfws = CFWSList()
    while value and value[0] in CFWS_LEADER:
        if value[0] in WSP:
            token, value = get_fws(value)
        else:
            token, value = get_comment(value)
        cfws.append(token)
    return cfws, value

def 1(value):
    """"""bare-quoted-string = DQUOTE *([FWS] qcontent) [FWS] DQUOTE

    A quoted-string without the leading or trailing white space.  Its
    value is the text between the quote marks, with whitespace
    preserved and quoted pairs decoded.
    """"""
    if value[0] != '""':
        raise errors.HeaderParseError(
            ""expected '\""' but found '{}'"".format(value))
    bare_quoted_string = BareQuotedString()
    value = value[1:]
    if value and value[0] == '""':
        token, value = get_qcontent(value)
        bare_quoted_string.append(token)
    while value and value[0] != '""':
        if value[0] in WSP:
            token, value = get_fws(value)
        elif value[:2] == '=?':
            valid_ew = False
            try:
                token, value = get_encoded_word(value)
                bare_quoted_string.defects.append(errors.InvalidHeaderDefect(
                    ""encoded word inside quoted string""))
                valid_ew = True
            except errors.HeaderParseError:
                token, value = get_qcontent(value)
            # Collapse the whitespace between two encoded words that occur in a
            # bare-quoted-string.
            if valid_ew and len(bare_quoted_string) > 1:
                if (bare_quoted_string[-1].token_type == 'fws' and
                        bare_quoted_string[-2].token_type == 'encoded-word'):
                    bare_quoted_string[-1] = EWWhiteSpaceTerminal(
                        bare_quoted_string[-1], 'fws')
        else:
            token, value = get_qcontent(value)
        bare_quoted_string.append(token)
    if not value:
        bare_quoted_string.defects.append(errors.InvalidHeaderDefect(
            ""end of header inside quoted string""))
        return bare_quoted_string, value
    return bare_quoted_string, value[1:]","def 2(value):
    """"""quoted-string = [CFWS] <bare-quoted-string> [CFWS]

    'bare-quoted-string' is an intermediate class defined by this
    parser and not by the RFC grammar.  It is the quoted string
    without any attached CFWS.
    """"""
    quoted_string = QuotedString()
    if value and value[0] in CFWS_LEADER:
        token, value = 0(value)
        quoted_string.append(token)
    token, value = 1(value)
    quoted_string.append(token)
    if value and value[0] in CFWS_LEADER:
        token, value = 0(value)
        quoted_string.append(token)
    return quoted_string, value","{""get_cfws"": ""0"", ""get_bare_quoted_string"": ""1""}"
636,636,"def get_atom(value):
    """"""atom = [CFWS] 1*atext [CFWS]

    An atom could be an rfc2047 encoded word.
    """"""
    atom = Atom()
    if value and value[0] in CFWS_LEADER:
        token, value = get_cfws(value)
        atom.append(token)
    if value and value[0] in ATOM_ENDS:
        raise errors.HeaderParseError(
            ""expected atom but found '{}'"".format(value))
    if value.startswith('=?'):
        try:
            token, value = get_encoded_word(value)
        except errors.HeaderParseError:
            # XXX: need to figure out how to register defects when
            # appropriate here.
            token, value = get_atext(value)
    else:
        token, value = get_atext(value)
    atom.append(token)
    if value and value[0] in CFWS_LEADER:
        token, value = get_cfws(value)
        atom.append(token)
    return atom, value","def get_cfws(value):
    """"""CFWS = (1*([FWS] comment) [FWS]) / FWS

    """"""
    cfws = CFWSList()
    while value and value[0] in CFWS_LEADER:
        if value[0] in WSP:
            token, value = get_fws(value)
        else:
            token, value = get_comment(value)
        cfws.append(token)
    return cfws, value

def get_encoded_word(value):
    """""" encoded-word = ""=?"" charset ""?"" encoding ""?"" encoded-text ""?=""

    """"""
    ew = EncodedWord()
    if not value.startswith('=?'):
        raise errors.HeaderParseError(
            ""expected encoded word but found {}"".format(value))
    tok, *remainder = value[2:].split('?=', 1)
    if tok == value[2:]:
        raise errors.HeaderParseError(
            ""expected encoded word but found {}"".format(value))
    remstr = ''.join(remainder)
    if (len(remstr) > 1 and
        remstr[0] in hexdigits and
        remstr[1] in hexdigits and
        tok.count('?') < 2):
        # The ? after the CTE was followed by an encoded word escape (=XX).
        rest, *remainder = remstr.split('?=', 1)
        tok = tok + '?=' + rest
    if len(tok.split()) > 1:
        ew.defects.append(errors.InvalidHeaderDefect(
            ""whitespace inside encoded word""))
    ew.cte = value
    value = ''.join(remainder)
    try:
        text, charset, lang, defects = _ew.decode('=?' + tok + '?=')
    except (ValueError, KeyError):
        raise _InvalidEwError(
            ""encoded word format invalid: '{}'"".format(ew.cte))
    ew.charset = charset
    ew.lang = lang
    ew.defects.extend(defects)
    while text:
        if text[0] in WSP:
            token, text = get_fws(text)
            ew.append(token)
            continue
        chars, *remainder = _wsp_splitter(text, 1)
        vtext = ValueTerminal(chars, 'vtext')
        _validate_xtext(vtext)
        ew.append(vtext)
        text = ''.join(remainder)
    # Encoded words should be followed by a WS
    if value and value[0] not in WSP:
        ew.defects.append(errors.InvalidHeaderDefect(
            ""missing trailing whitespace after encoded-word""))
    return ew, value

def get_atext(value):
    """"""atext = <matches _atext_matcher>

    We allow any non-ATOM_ENDS in atext, but add an InvalidATextDefect to
    the token's defects list if we find non-atext characters.
    """"""
    m = _non_atom_end_matcher(value)
    if not m:
        raise errors.HeaderParseError(
            ""expected atext but found '{}'"".format(value))
    atext = m.group()
    value = value[len(atext):]
    atext = ValueTerminal(atext, 'atext')
    _validate_xtext(atext)
    return atext, value","def parse_cfws_from_value(value):
    """"""CFWS = (1*([FWS] comment) [FWS]) / FWS

    """"""
    cfws = CFWSList()
    while value and value[0] in CFWS_LEADER:
        if value[0] in WSP:
            token, value = get_fws(value)
        else:
            token, value = get_comment(value)
        cfws.append(token)
    return cfws, value

def parse_encoded_word_from_value(value):
    """""" encoded-word = ""=?"" charset ""?"" encoding ""?"" encoded-text ""?=""

    """"""
    ew = EncodedWord()
    if not value.startswith('=?'):
        raise errors.HeaderParseError(
            ""expected encoded word but found {}"".format(value))
    tok, *remainder = value[2:].split('?=', 1)
    if tok == value[2:]:
        raise errors.HeaderParseError(
            ""expected encoded word but found {}"".format(value))
    remstr = ''.join(remainder)
    if (len(remstr) > 1 and
        remstr[0] in hexdigits and
        remstr[1] in hexdigits and
        tok.count('?') < 2):
        # The ? after the CTE was followed by an encoded word escape (=XX).
        rest, *remainder = remstr.split('?=', 1)
        tok = tok + '?=' + rest
    if len(tok.split()) > 1:
        ew.defects.append(errors.InvalidHeaderDefect(
            ""whitespace inside encoded word""))
    ew.cte = value
    value = ''.join(remainder)
    try:
        text, charset, lang, defects = _ew.decode('=?' + tok + '?=')
    except (ValueError, KeyError):
        raise _InvalidEwError(
            ""encoded word format invalid: '{}'"".format(ew.cte))
    ew.charset = charset
    ew.lang = lang
    ew.defects.extend(defects)
    while text:
        if text[0] in WSP:
            token, text = get_fws(text)
            ew.append(token)
            continue
        chars, *remainder = _wsp_splitter(text, 1)
        vtext = ValueTerminal(chars, 'vtext')
        _validate_xtext(vtext)
        ew.append(vtext)
        text = ''.join(remainder)
    # Encoded words should be followed by a WS
    if value and value[0] not in WSP:
        ew.defects.append(errors.InvalidHeaderDefect(
            ""missing trailing whitespace after encoded-word""))
    return ew, value

def parse_atext_from_value(value):
    """"""atext = <matches _atext_matcher>

    We allow any non-ATOM_ENDS in atext, but add an InvalidATextDefect to
    the token's defects list if we find non-atext characters.
    """"""
    m = _non_atom_end_matcher(value)
    if not m:
        raise errors.HeaderParseError(
            ""expected atext but found '{}'"".format(value))
    atext = m.group()
    value = value[len(atext):]
    atext = ValueTerminal(atext, 'atext')
    _validate_xtext(atext)
    return atext, value","def atom_equals_optional_CFWS_one_or_more_atext_optional_CFWS(value):
    """"""atom = [CFWS] 1*atext [CFWS]

    An atom could be an rfc2047 encoded word.
    """"""
    atom = Atom()
    if value and value[0] in CFWS_LEADER:
        token, value = parse_cfws_from_value(value)
        atom.append(token)
    if value and value[0] in ATOM_ENDS:
        raise errors.HeaderParseError(
            ""expected atom but found '{}'"".format(value))
    if value.startswith('=?'):
        try:
            token, value = parse_encoded_word_from_value(value)
        except errors.HeaderParseError:
            # XXX: need to figure out how to register defects when
            # appropriate here.
            token, value = parse_atext_from_value(value)
    else:
        token, value = parse_atext_from_value(value)
    atom.append(token)
    if value and value[0] in CFWS_LEADER:
        token, value = parse_cfws_from_value(value)
        atom.append(token)
    return atom, value","{""get_cfws"": ""parse_cfws_from_value"", ""get_encoded_word"": ""parse_encoded_word_from_value"", ""get_atext"": ""parse_atext_from_value""}","def 0(value):
    """"""CFWS = (1*([FWS] comment) [FWS]) / FWS

    """"""
    cfws = CFWSList()
    while value and value[0] in CFWS_LEADER:
        if value[0] in WSP:
            token, value = get_fws(value)
        else:
            token, value = get_comment(value)
        cfws.append(token)
    return cfws, value

def 1(value):
    """""" encoded-word = ""=?"" charset ""?"" encoding ""?"" encoded-text ""?=""

    """"""
    ew = EncodedWord()
    if not value.startswith('=?'):
        raise errors.HeaderParseError(
            ""expected encoded word but found {}"".format(value))
    tok, *remainder = value[2:].split('?=', 1)
    if tok == value[2:]:
        raise errors.HeaderParseError(
            ""expected encoded word but found {}"".format(value))
    remstr = ''.join(remainder)
    if (len(remstr) > 1 and
        remstr[0] in hexdigits and
        remstr[1] in hexdigits and
        tok.count('?') < 2):
        # The ? after the CTE was followed by an encoded word escape (=XX).
        rest, *remainder = remstr.split('?=', 1)
        tok = tok + '?=' + rest
    if len(tok.split()) > 1:
        ew.defects.append(errors.InvalidHeaderDefect(
            ""whitespace inside encoded word""))
    ew.cte = value
    value = ''.join(remainder)
    try:
        text, charset, lang, defects = _ew.decode('=?' + tok + '?=')
    except (ValueError, KeyError):
        raise _InvalidEwError(
            ""encoded word format invalid: '{}'"".format(ew.cte))
    ew.charset = charset
    ew.lang = lang
    ew.defects.extend(defects)
    while text:
        if text[0] in WSP:
            token, text = get_fws(text)
            ew.append(token)
            continue
        chars, *remainder = _wsp_splitter(text, 1)
        vtext = ValueTerminal(chars, 'vtext')
        _validate_xtext(vtext)
        ew.append(vtext)
        text = ''.join(remainder)
    # Encoded words should be followed by a WS
    if value and value[0] not in WSP:
        ew.defects.append(errors.InvalidHeaderDefect(
            ""missing trailing whitespace after encoded-word""))
    return ew, value

def 2(value):
    """"""atext = <matches _atext_matcher>

    We allow any non-ATOM_ENDS in atext, but add an InvalidATextDefect to
    the token's defects list if we find non-atext characters.
    """"""
    m = _non_atom_end_matcher(value)
    if not m:
        raise errors.HeaderParseError(
            ""expected atext but found '{}'"".format(value))
    atext = m.group()
    value = value[len(atext):]
    atext = ValueTerminal(atext, 'atext')
    _validate_xtext(atext)
    return atext, value","def 3(value):
    """"""atom = [CFWS] 1*atext [CFWS]

    An atom could be an rfc2047 encoded word.
    """"""
    atom = Atom()
    if value and value[0] in CFWS_LEADER:
        token, value = 0(value)
        atom.append(token)
    if value and value[0] in ATOM_ENDS:
        raise errors.HeaderParseError(
            ""expected atom but found '{}'"".format(value))
    if value.startswith('=?'):
        try:
            token, value = 1(value)
        except errors.HeaderParseError:
            # XXX: need to figure out how to register defects when
            # appropriate here.
            token, value = 2(value)
    else:
        token, value = 2(value)
    atom.append(token)
    if value and value[0] in CFWS_LEADER:
        token, value = 0(value)
        atom.append(token)
    return atom, value","{""get_cfws"": ""0"", ""get_encoded_word"": ""1"", ""get_atext"": ""2""}"
637,637,"def get_dot_atom_text(value):
    """""" dot-text = 1*atext *(""."" 1*atext)

    """"""
    dot_atom_text = DotAtomText()
    if not value or value[0] in ATOM_ENDS:
        raise errors.HeaderParseError(""expected atom at a start of ""
            ""dot-atom-text but found '{}'"".format(value))
    while value and value[0] not in ATOM_ENDS:
        token, value = get_atext(value)
        dot_atom_text.append(token)
        if value and value[0] == '.':
            dot_atom_text.append(DOT)
            value = value[1:]
    if dot_atom_text[-1] is DOT:
        raise errors.HeaderParseError(""expected atom at end of dot-atom-text ""
            ""but found '{}'"".format('.'+value))
    return dot_atom_text, value","def get_atext(value):
    """"""atext = <matches _atext_matcher>

    We allow any non-ATOM_ENDS in atext, but add an InvalidATextDefect to
    the token's defects list if we find non-atext characters.
    """"""
    m = _non_atom_end_matcher(value)
    if not m:
        raise errors.HeaderParseError(
            ""expected atext but found '{}'"".format(value))
    atext = m.group()
    value = value[len(atext):]
    atext = ValueTerminal(atext, 'atext')
    _validate_xtext(atext)
    return atext, value","def parse_atext_from_value(value):
    """"""atext = <matches _atext_matcher>

    We allow any non-ATOM_ENDS in atext, but add an InvalidATextDefect to
    the token's defects list if we find non-atext characters.
    """"""
    m = _non_atom_end_matcher(value)
    if not m:
        raise errors.HeaderParseError(
            ""expected atext but found '{}'"".format(value))
    atext = m.group()
    value = value[len(atext):]
    atext = ValueTerminal(atext, 'atext')
    _validate_xtext(atext)
    return atext, value","def parse_dot_atom_text(value):
    """""" dot-text = 1*atext *(""."" 1*atext)

    """"""
    dot_atom_text = DotAtomText()
    if not value or value[0] in ATOM_ENDS:
        raise errors.HeaderParseError(""expected atom at a start of ""
            ""dot-atom-text but found '{}'"".format(value))
    while value and value[0] not in ATOM_ENDS:
        token, value = parse_atext_from_value(value)
        dot_atom_text.append(token)
        if value and value[0] == '.':
            dot_atom_text.append(DOT)
            value = value[1:]
    if dot_atom_text[-1] is DOT:
        raise errors.HeaderParseError(""expected atom at end of dot-atom-text ""
            ""but found '{}'"".format('.'+value))
    return dot_atom_text, value","{""get_atext"": ""parse_atext_from_value""}","def 0(value):
    """"""atext = <matches _atext_matcher>

    We allow any non-ATOM_ENDS in atext, but add an InvalidATextDefect to
    the token's defects list if we find non-atext characters.
    """"""
    m = _non_atom_end_matcher(value)
    if not m:
        raise errors.HeaderParseError(
            ""expected atext but found '{}'"".format(value))
    atext = m.group()
    value = value[len(atext):]
    atext = ValueTerminal(atext, 'atext')
    _validate_xtext(atext)
    return atext, value","def 1(value):
    """""" dot-text = 1*atext *(""."" 1*atext)

    """"""
    dot_atom_text = DotAtomText()
    if not value or value[0] in ATOM_ENDS:
        raise errors.HeaderParseError(""expected atom at a start of ""
            ""dot-atom-text but found '{}'"".format(value))
    while value and value[0] not in ATOM_ENDS:
        token, value = 0(value)
        dot_atom_text.append(token)
        if value and value[0] == '.':
            dot_atom_text.append(DOT)
            value = value[1:]
    if dot_atom_text[-1] is DOT:
        raise errors.HeaderParseError(""expected atom at end of dot-atom-text ""
            ""but found '{}'"".format('.'+value))
    return dot_atom_text, value","{""get_atext"": ""0""}"
638,638,"def get_dot_atom(value):
    """""" dot-atom = [CFWS] dot-atom-text [CFWS]

    Any place we can have a dot atom, we could instead have an rfc2047 encoded
    word.
    """"""
    dot_atom = DotAtom()
    if value[0] in CFWS_LEADER:
        token, value = get_cfws(value)
        dot_atom.append(token)
    if value.startswith('=?'):
        try:
            token, value = get_encoded_word(value)
        except errors.HeaderParseError:
            # XXX: need to figure out how to register defects when
            # appropriate here.
            token, value = get_dot_atom_text(value)
    else:
        token, value = get_dot_atom_text(value)
    dot_atom.append(token)
    if value and value[0] in CFWS_LEADER:
        token, value = get_cfws(value)
        dot_atom.append(token)
    return dot_atom, value","def get_dot_atom_text(value):
    """""" dot-text = 1*atext *(""."" 1*atext)

    """"""
    dot_atom_text = DotAtomText()
    if not value or value[0] in ATOM_ENDS:
        raise errors.HeaderParseError(""expected atom at a start of ""
            ""dot-atom-text but found '{}'"".format(value))
    while value and value[0] not in ATOM_ENDS:
        token, value = get_atext(value)
        dot_atom_text.append(token)
        if value and value[0] == '.':
            dot_atom_text.append(DOT)
            value = value[1:]
    if dot_atom_text[-1] is DOT:
        raise errors.HeaderParseError(""expected atom at end of dot-atom-text ""
            ""but found '{}'"".format('.'+value))
    return dot_atom_text, value

def get_cfws(value):
    """"""CFWS = (1*([FWS] comment) [FWS]) / FWS

    """"""
    cfws = CFWSList()
    while value and value[0] in CFWS_LEADER:
        if value[0] in WSP:
            token, value = get_fws(value)
        else:
            token, value = get_comment(value)
        cfws.append(token)
    return cfws, value

def get_encoded_word(value):
    """""" encoded-word = ""=?"" charset ""?"" encoding ""?"" encoded-text ""?=""

    """"""
    ew = EncodedWord()
    if not value.startswith('=?'):
        raise errors.HeaderParseError(
            ""expected encoded word but found {}"".format(value))
    tok, *remainder = value[2:].split('?=', 1)
    if tok == value[2:]:
        raise errors.HeaderParseError(
            ""expected encoded word but found {}"".format(value))
    remstr = ''.join(remainder)
    if (len(remstr) > 1 and
        remstr[0] in hexdigits and
        remstr[1] in hexdigits and
        tok.count('?') < 2):
        # The ? after the CTE was followed by an encoded word escape (=XX).
        rest, *remainder = remstr.split('?=', 1)
        tok = tok + '?=' + rest
    if len(tok.split()) > 1:
        ew.defects.append(errors.InvalidHeaderDefect(
            ""whitespace inside encoded word""))
    ew.cte = value
    value = ''.join(remainder)
    try:
        text, charset, lang, defects = _ew.decode('=?' + tok + '?=')
    except (ValueError, KeyError):
        raise _InvalidEwError(
            ""encoded word format invalid: '{}'"".format(ew.cte))
    ew.charset = charset
    ew.lang = lang
    ew.defects.extend(defects)
    while text:
        if text[0] in WSP:
            token, text = get_fws(text)
            ew.append(token)
            continue
        chars, *remainder = _wsp_splitter(text, 1)
        vtext = ValueTerminal(chars, 'vtext')
        _validate_xtext(vtext)
        ew.append(vtext)
        text = ''.join(remainder)
    # Encoded words should be followed by a WS
    if value and value[0] not in WSP:
        ew.defects.append(errors.InvalidHeaderDefect(
            ""missing trailing whitespace after encoded-word""))
    return ew, value","def parse_dot_atom_text(value):
    """""" dot-text = 1*atext *(""."" 1*atext)

    """"""
    dot_atom_text = DotAtomText()
    if not value or value[0] in ATOM_ENDS:
        raise errors.HeaderParseError(""expected atom at a start of ""
            ""dot-atom-text but found '{}'"".format(value))
    while value and value[0] not in ATOM_ENDS:
        token, value = get_atext(value)
        dot_atom_text.append(token)
        if value and value[0] == '.':
            dot_atom_text.append(DOT)
            value = value[1:]
    if dot_atom_text[-1] is DOT:
        raise errors.HeaderParseError(""expected atom at end of dot-atom-text ""
            ""but found '{}'"".format('.'+value))
    return dot_atom_text, value

def parse_cfws_from_value(value):
    """"""CFWS = (1*([FWS] comment) [FWS]) / FWS

    """"""
    cfws = CFWSList()
    while value and value[0] in CFWS_LEADER:
        if value[0] in WSP:
            token, value = get_fws(value)
        else:
            token, value = get_comment(value)
        cfws.append(token)
    return cfws, value

def parse_encoded_word_from_value(value):
    """""" encoded-word = ""=?"" charset ""?"" encoding ""?"" encoded-text ""?=""

    """"""
    ew = EncodedWord()
    if not value.startswith('=?'):
        raise errors.HeaderParseError(
            ""expected encoded word but found {}"".format(value))
    tok, *remainder = value[2:].split('?=', 1)
    if tok == value[2:]:
        raise errors.HeaderParseError(
            ""expected encoded word but found {}"".format(value))
    remstr = ''.join(remainder)
    if (len(remstr) > 1 and
        remstr[0] in hexdigits and
        remstr[1] in hexdigits and
        tok.count('?') < 2):
        # The ? after the CTE was followed by an encoded word escape (=XX).
        rest, *remainder = remstr.split('?=', 1)
        tok = tok + '?=' + rest
    if len(tok.split()) > 1:
        ew.defects.append(errors.InvalidHeaderDefect(
            ""whitespace inside encoded word""))
    ew.cte = value
    value = ''.join(remainder)
    try:
        text, charset, lang, defects = _ew.decode('=?' + tok + '?=')
    except (ValueError, KeyError):
        raise _InvalidEwError(
            ""encoded word format invalid: '{}'"".format(ew.cte))
    ew.charset = charset
    ew.lang = lang
    ew.defects.extend(defects)
    while text:
        if text[0] in WSP:
            token, text = get_fws(text)
            ew.append(token)
            continue
        chars, *remainder = _wsp_splitter(text, 1)
        vtext = ValueTerminal(chars, 'vtext')
        _validate_xtext(vtext)
        ew.append(vtext)
        text = ''.join(remainder)
    # Encoded words should be followed by a WS
    if value and value[0] not in WSP:
        ew.defects.append(errors.InvalidHeaderDefect(
            ""missing trailing whitespace after encoded-word""))
    return ew, value","def dot_atom_equals_optional_CFWS_dot_atom_text_optional_CFWS(value):
    """""" dot-atom = [CFWS] dot-atom-text [CFWS]

    Any place we can have a dot atom, we could instead have an rfc2047 encoded
    word.
    """"""
    dot_atom = DotAtom()
    if value[0] in CFWS_LEADER:
        token, value = parse_cfws_from_value(value)
        dot_atom.append(token)
    if value.startswith('=?'):
        try:
            token, value = parse_encoded_word_from_value(value)
        except errors.HeaderParseError:
            # XXX: need to figure out how to register defects when
            # appropriate here.
            token, value = parse_dot_atom_text(value)
    else:
        token, value = parse_dot_atom_text(value)
    dot_atom.append(token)
    if value and value[0] in CFWS_LEADER:
        token, value = parse_cfws_from_value(value)
        dot_atom.append(token)
    return dot_atom, value","{""get_dot_atom_text"": ""parse_dot_atom_text"", ""get_cfws"": ""parse_cfws_from_value"", ""get_encoded_word"": ""parse_encoded_word_from_value""}","def 0(value):
    """""" dot-text = 1*atext *(""."" 1*atext)

    """"""
    dot_atom_text = DotAtomText()
    if not value or value[0] in ATOM_ENDS:
        raise errors.HeaderParseError(""expected atom at a start of ""
            ""dot-atom-text but found '{}'"".format(value))
    while value and value[0] not in ATOM_ENDS:
        token, value = get_atext(value)
        dot_atom_text.append(token)
        if value and value[0] == '.':
            dot_atom_text.append(DOT)
            value = value[1:]
    if dot_atom_text[-1] is DOT:
        raise errors.HeaderParseError(""expected atom at end of dot-atom-text ""
            ""but found '{}'"".format('.'+value))
    return dot_atom_text, value

def 1(value):
    """"""CFWS = (1*([FWS] comment) [FWS]) / FWS

    """"""
    cfws = CFWSList()
    while value and value[0] in CFWS_LEADER:
        if value[0] in WSP:
            token, value = get_fws(value)
        else:
            token, value = get_comment(value)
        cfws.append(token)
    return cfws, value

def 2(value):
    """""" encoded-word = ""=?"" charset ""?"" encoding ""?"" encoded-text ""?=""

    """"""
    ew = EncodedWord()
    if not value.startswith('=?'):
        raise errors.HeaderParseError(
            ""expected encoded word but found {}"".format(value))
    tok, *remainder = value[2:].split('?=', 1)
    if tok == value[2:]:
        raise errors.HeaderParseError(
            ""expected encoded word but found {}"".format(value))
    remstr = ''.join(remainder)
    if (len(remstr) > 1 and
        remstr[0] in hexdigits and
        remstr[1] in hexdigits and
        tok.count('?') < 2):
        # The ? after the CTE was followed by an encoded word escape (=XX).
        rest, *remainder = remstr.split('?=', 1)
        tok = tok + '?=' + rest
    if len(tok.split()) > 1:
        ew.defects.append(errors.InvalidHeaderDefect(
            ""whitespace inside encoded word""))
    ew.cte = value
    value = ''.join(remainder)
    try:
        text, charset, lang, defects = _ew.decode('=?' + tok + '?=')
    except (ValueError, KeyError):
        raise _InvalidEwError(
            ""encoded word format invalid: '{}'"".format(ew.cte))
    ew.charset = charset
    ew.lang = lang
    ew.defects.extend(defects)
    while text:
        if text[0] in WSP:
            token, text = get_fws(text)
            ew.append(token)
            continue
        chars, *remainder = _wsp_splitter(text, 1)
        vtext = ValueTerminal(chars, 'vtext')
        _validate_xtext(vtext)
        ew.append(vtext)
        text = ''.join(remainder)
    # Encoded words should be followed by a WS
    if value and value[0] not in WSP:
        ew.defects.append(errors.InvalidHeaderDefect(
            ""missing trailing whitespace after encoded-word""))
    return ew, value","def 3(value):
    """""" dot-atom = [CFWS] dot-atom-text [CFWS]

    Any place we can have a dot atom, we could instead have an rfc2047 encoded
    word.
    """"""
    dot_atom = DotAtom()
    if value[0] in CFWS_LEADER:
        token, value = 1(value)
        dot_atom.append(token)
    if value.startswith('=?'):
        try:
            token, value = 2(value)
        except errors.HeaderParseError:
            # XXX: need to figure out how to register defects when
            # appropriate here.
            token, value = 0(value)
    else:
        token, value = 0(value)
    dot_atom.append(token)
    if value and value[0] in CFWS_LEADER:
        token, value = 1(value)
        dot_atom.append(token)
    return dot_atom, value","{""get_dot_atom_text"": ""0"", ""get_cfws"": ""1"", ""get_encoded_word"": ""2""}"
639,639,"def get_dtext(value):
    r"""""" dtext = <printable ascii except \ [ ]> / obs-dtext
        obs-dtext = obs-NO-WS-CTL / quoted-pair

    We allow anything except the excluded characters, but if we find any
    ASCII other than the RFC defined printable ASCII, a NonPrintableDefect is
    added to the token's defects list.  Quoted pairs are converted to their
    unquoted values, so what is returned is a ptext token, in this case a
    ValueTerminal.  If there were quoted-printables, an ObsoleteHeaderDefect is
    added to the returned token's defect list.

    """"""
    ptext, value, had_qp = _get_ptext_to_endchars(value, '[]')
    ptext = ValueTerminal(ptext, 'ptext')
    if had_qp:
        ptext.defects.append(errors.ObsoleteHeaderDefect(
            ""quoted printable found in domain-literal""))
    _validate_xtext(ptext)
    return ptext, value","def _validate_xtext(xtext):
    """"""If input token contains ASCII non-printables, register a defect.""""""

    non_printables = _non_printable_finder(xtext)
    if non_printables:
        xtext.defects.append(errors.NonPrintableDefect(non_printables))
    if utils._has_surrogates(xtext):
        xtext.defects.append(errors.UndecodableBytesDefect(
            ""Non-ASCII characters found in header token""))

def _get_ptext_to_endchars(value, endchars):
    """"""Scan printables/quoted-pairs until endchars and return unquoted ptext.

    This function turns a run of qcontent, ccontent-without-comments, or
    dtext-with-quoted-printables into a single string by unquoting any
    quoted printables.  It returns the string, the remaining value, and
    a flag that is True iff there were any quoted printables decoded.

    """"""
    fragment, *remainder = _wsp_splitter(value, 1)
    vchars = []
    escape = False
    had_qp = False
    for pos in range(len(fragment)):
        if fragment[pos] == '\\':
            if escape:
                escape = False
                had_qp = True
            else:
                escape = True
                continue
        if escape:
            escape = False
        elif fragment[pos] in endchars:
            break
        vchars.append(fragment[pos])
    else:
        pos = pos + 1
    return ''.join(vchars), ''.join([fragment[pos:]] + remainder), had_qp","def validate_xtext_input_token(xtext):
    """"""If input token contains ASCII non-printables, register a defect.""""""

    non_printables = _non_printable_finder(xtext)
    if non_printables:
        xtext.defects.append(errors.NonPrintableDefect(non_printables))
    if utils._has_surrogates(xtext):
        xtext.defects.append(errors.UndecodableBytesDefect(
            ""Non-ASCII characters found in header token""))

def get_ptext_until_endchars(value, endchars):
    """"""Scan printables/quoted-pairs until endchars and return unquoted ptext.

    This function turns a run of qcontent, ccontent-without-comments, or
    dtext-with-quoted-printables into a single string by unquoting any
    quoted printables.  It returns the string, the remaining value, and
    a flag that is True iff there were any quoted printables decoded.

    """"""
    fragment, *remainder = _wsp_splitter(value, 1)
    vchars = []
    escape = False
    had_qp = False
    for pos in range(len(fragment)):
        if fragment[pos] == '\\':
            if escape:
                escape = False
                had_qp = True
            else:
                escape = True
                continue
        if escape:
            escape = False
        elif fragment[pos] in endchars:
            break
        vchars.append(fragment[pos])
    else:
        pos = pos + 1
    return ''.join(vchars), ''.join([fragment[pos:]] + remainder), had_qp","def parse_dtext_from_value(value):
    r"""""" dtext = <printable ascii except \ [ ]> / obs-dtext
        obs-dtext = obs-NO-WS-CTL / quoted-pair

    We allow anything except the excluded characters, but if we find any
    ASCII other than the RFC defined printable ASCII, a NonPrintableDefect is
    added to the token's defects list.  Quoted pairs are converted to their
    unquoted values, so what is returned is a ptext token, in this case a
    ValueTerminal.  If there were quoted-printables, an ObsoleteHeaderDefect is
    added to the returned token's defect list.

    """"""
    ptext, value, had_qp = get_ptext_until_endchars(value, '[]')
    ptext = ValueTerminal(ptext, 'ptext')
    if had_qp:
        ptext.defects.append(errors.ObsoleteHeaderDefect(
            ""quoted printable found in domain-literal""))
    validate_xtext_input_token(ptext)
    return ptext, value","{""_validate_xtext"": ""validate_xtext_input_token"", ""_get_ptext_to_endchars"": ""get_ptext_until_endchars""}","def 0(xtext):
    """"""If input token contains ASCII non-printables, register a defect.""""""

    non_printables = _non_printable_finder(xtext)
    if non_printables:
        xtext.defects.append(errors.NonPrintableDefect(non_printables))
    if utils._has_surrogates(xtext):
        xtext.defects.append(errors.UndecodableBytesDefect(
            ""Non-ASCII characters found in header token""))

def 1(value, endchars):
    """"""Scan printables/quoted-pairs until endchars and return unquoted ptext.

    This function turns a run of qcontent, ccontent-without-comments, or
    dtext-with-quoted-printables into a single string by unquoting any
    quoted printables.  It returns the string, the remaining value, and
    a flag that is True iff there were any quoted printables decoded.

    """"""
    fragment, *remainder = _wsp_splitter(value, 1)
    vchars = []
    escape = False
    had_qp = False
    for pos in range(len(fragment)):
        if fragment[pos] == '\\':
            if escape:
                escape = False
                had_qp = True
            else:
                escape = True
                continue
        if escape:
            escape = False
        elif fragment[pos] in endchars:
            break
        vchars.append(fragment[pos])
    else:
        pos = pos + 1
    return ''.join(vchars), ''.join([fragment[pos:]] + remainder), had_qp","def 2(value):
    r"""""" dtext = <printable ascii except \ [ ]> / obs-dtext
        obs-dtext = obs-NO-WS-CTL / quoted-pair

    We allow anything except the excluded characters, but if we find any
    ASCII other than the RFC defined printable ASCII, a NonPrintableDefect is
    added to the token's defects list.  Quoted pairs are converted to their
    unquoted values, so what is returned is a ptext token, in this case a
    ValueTerminal.  If there were quoted-printables, an ObsoleteHeaderDefect is
    added to the returned token's defect list.

    """"""
    ptext, value, had_qp = 1(value, '[]')
    ptext = ValueTerminal(ptext, 'ptext')
    if had_qp:
        ptext.defects.append(errors.ObsoleteHeaderDefect(
            ""quoted printable found in domain-literal""))
    0(ptext)
    return ptext, value","{""_validate_xtext"": ""0"", ""_get_ptext_to_endchars"": ""1""}"
640,640,"def get_addr_spec(value):
    """""" addr-spec = local-part ""@"" domain

    """"""
    addr_spec = AddrSpec()
    token, value = get_local_part(value)
    addr_spec.append(token)
    if not value or value[0] != '@':
        addr_spec.defects.append(errors.InvalidHeaderDefect(
            ""addr-spec local part with no domain""))
        return addr_spec, value
    addr_spec.append(ValueTerminal('@', 'address-at-symbol'))
    token, value = get_domain(value[1:])
    addr_spec.append(token)
    return addr_spec, value","def get_local_part(value):
    """""" local-part = dot-atom / quoted-string / obs-local-part

    """"""
    local_part = LocalPart()
    leader = None
    if value[0] in CFWS_LEADER:
        leader, value = get_cfws(value)
    if not value:
        raise errors.HeaderParseError(
            ""expected local-part but found '{}'"".format(value))
    try:
        token, value = get_dot_atom(value)
    except errors.HeaderParseError:
        try:
            token, value = get_word(value)
        except errors.HeaderParseError:
            if value[0] != '\\' and value[0] in PHRASE_ENDS:
                raise
            token = TokenList()
    if leader is not None:
        token[:0] = [leader]
    local_part.append(token)
    if value and (value[0]=='\\' or value[0] not in PHRASE_ENDS):
        obs_local_part, value = get_obs_local_part(str(local_part) + value)
        if obs_local_part.token_type == 'invalid-obs-local-part':
            local_part.defects.append(errors.InvalidHeaderDefect(
                ""local-part is not dot-atom, quoted-string, or obs-local-part""))
        else:
            local_part.defects.append(errors.ObsoleteHeaderDefect(
                ""local-part is not a dot-atom (contains CFWS)""))
        local_part[0] = obs_local_part
    try:
        local_part.value.encode('ascii')
    except UnicodeEncodeError:
        local_part.defects.append(errors.NonASCIILocalPartDefect(
                ""local-part contains non-ASCII characters)""))
    return local_part, value

def get_domain(value):
    """""" domain = dot-atom / domain-literal / obs-domain
        obs-domain = atom *(""."" atom))

    """"""
    domain = Domain()
    leader = None
    if value[0] in CFWS_LEADER:
        leader, value = get_cfws(value)
    if not value:
        raise errors.HeaderParseError(
            ""expected domain but found '{}'"".format(value))
    if value[0] == '[':
        token, value = get_domain_literal(value)
        if leader is not None:
            token[:0] = [leader]
        domain.append(token)
        return domain, value
    try:
        token, value = get_dot_atom(value)
    except errors.HeaderParseError:
        token, value = get_atom(value)
    if value and value[0] == '@':
        raise errors.HeaderParseError('Invalid Domain')
    if leader is not None:
        token[:0] = [leader]
    domain.append(token)
    if value and value[0] == '.':
        domain.defects.append(errors.ObsoleteHeaderDefect(
            ""domain is not a dot-atom (contains CFWS)""))
        if domain[0].token_type == 'dot-atom':
            domain[:] = domain[0]
        while value and value[0] == '.':
            domain.append(DOT)
            token, value = get_atom(value[1:])
            domain.append(token)
    return domain, value","def parse_local_part_from_value(value):
    """""" local-part = dot-atom / quoted-string / obs-local-part

    """"""
    local_part = LocalPart()
    leader = None
    if value[0] in CFWS_LEADER:
        leader, value = get_cfws(value)
    if not value:
        raise errors.HeaderParseError(
            ""expected local-part but found '{}'"".format(value))
    try:
        token, value = get_dot_atom(value)
    except errors.HeaderParseError:
        try:
            token, value = get_word(value)
        except errors.HeaderParseError:
            if value[0] != '\\' and value[0] in PHRASE_ENDS:
                raise
            token = TokenList()
    if leader is not None:
        token[:0] = [leader]
    local_part.append(token)
    if value and (value[0]=='\\' or value[0] not in PHRASE_ENDS):
        obs_local_part, value = get_obs_local_part(str(local_part) + value)
        if obs_local_part.token_type == 'invalid-obs-local-part':
            local_part.defects.append(errors.InvalidHeaderDefect(
                ""local-part is not dot-atom, quoted-string, or obs-local-part""))
        else:
            local_part.defects.append(errors.ObsoleteHeaderDefect(
                ""local-part is not a dot-atom (contains CFWS)""))
        local_part[0] = obs_local_part
    try:
        local_part.value.encode('ascii')
    except UnicodeEncodeError:
        local_part.defects.append(errors.NonASCIILocalPartDefect(
                ""local-part contains non-ASCII characters)""))
    return local_part, value

def parse_domain(value):
    """""" domain = dot-atom / domain-literal / obs-domain
        obs-domain = atom *(""."" atom))

    """"""
    domain = Domain()
    leader = None
    if value[0] in CFWS_LEADER:
        leader, value = get_cfws(value)
    if not value:
        raise errors.HeaderParseError(
            ""expected domain but found '{}'"".format(value))
    if value[0] == '[':
        token, value = parse_domain_literal(value)
        if leader is not None:
            token[:0] = [leader]
        domain.append(token)
        return domain, value
    try:
        token, value = get_dot_atom(value)
    except errors.HeaderParseError:
        token, value = get_atom(value)
    if value and value[0] == '@':
        raise errors.HeaderParseError('Invalid Domain')
    if leader is not None:
        token[:0] = [leader]
    domain.append(token)
    if value and value[0] == '.':
        domain.defects.append(errors.ObsoleteHeaderDefect(
            ""domain is not a dot-atom (contains CFWS)""))
        if domain[0].token_type == 'dot-atom':
            domain[:] = domain[0]
        while value and value[0] == '.':
            domain.append(DOT)
            token, value = get_atom(value[1:])
            domain.append(token)
    return domain, value","def addr_spec_equals_local_part_at_domain(value):
    """""" addr-spec = local-part ""@"" domain

    """"""
    addr_spec = AddrSpec()
    token, value = parse_local_part_from_value(value)
    addr_spec.append(token)
    if not value or value[0] != '@':
        addr_spec.defects.append(errors.InvalidHeaderDefect(
            ""addr-spec local part with no domain""))
        return addr_spec, value
    addr_spec.append(ValueTerminal('@', 'address-at-symbol'))
    token, value = parse_domain(value[1:])
    addr_spec.append(token)
    return addr_spec, value","{""get_local_part"": ""parse_local_part_from_value"", ""get_domain"": ""parse_domain""}","def 0(value):
    """""" local-part = dot-atom / quoted-string / obs-local-part

    """"""
    local_part = LocalPart()
    leader = None
    if value[0] in CFWS_LEADER:
        leader, value = get_cfws(value)
    if not value:
        raise errors.HeaderParseError(
            ""expected local-part but found '{}'"".format(value))
    try:
        token, value = get_dot_atom(value)
    except errors.HeaderParseError:
        try:
            token, value = get_word(value)
        except errors.HeaderParseError:
            if value[0] != '\\' and value[0] in PHRASE_ENDS:
                raise
            token = TokenList()
    if leader is not None:
        token[:0] = [leader]
    local_part.append(token)
    if value and (value[0]=='\\' or value[0] not in PHRASE_ENDS):
        obs_local_part, value = get_obs_local_part(str(local_part) + value)
        if obs_local_part.token_type == 'invalid-obs-local-part':
            local_part.defects.append(errors.InvalidHeaderDefect(
                ""local-part is not dot-atom, quoted-string, or obs-local-part""))
        else:
            local_part.defects.append(errors.ObsoleteHeaderDefect(
                ""local-part is not a dot-atom (contains CFWS)""))
        local_part[0] = obs_local_part
    try:
        local_part.value.encode('ascii')
    except UnicodeEncodeError:
        local_part.defects.append(errors.NonASCIILocalPartDefect(
                ""local-part contains non-ASCII characters)""))
    return local_part, value

def 1(value):
    """""" domain = dot-atom / domain-literal / obs-domain
        obs-domain = atom *(""."" atom))

    """"""
    domain = Domain()
    leader = None
    if value[0] in CFWS_LEADER:
        leader, value = get_cfws(value)
    if not value:
        raise errors.HeaderParseError(
            ""expected domain but found '{}'"".format(value))
    if value[0] == '[':
        token, value = get_domain_literal(value)
        if leader is not None:
            token[:0] = [leader]
        domain.append(token)
        return domain, value
    try:
        token, value = get_dot_atom(value)
    except errors.HeaderParseError:
        token, value = get_atom(value)
    if value and value[0] == '@':
        raise errors.HeaderParseError('Invalid Domain')
    if leader is not None:
        token[:0] = [leader]
    domain.append(token)
    if value and value[0] == '.':
        domain.defects.append(errors.ObsoleteHeaderDefect(
            ""domain is not a dot-atom (contains CFWS)""))
        if domain[0].token_type == 'dot-atom':
            domain[:] = domain[0]
        while value and value[0] == '.':
            domain.append(DOT)
            token, value = get_atom(value[1:])
            domain.append(token)
    return domain, value","def 2(value):
    """""" addr-spec = local-part ""@"" domain

    """"""
    addr_spec = AddrSpec()
    token, value = 0(value)
    addr_spec.append(token)
    if not value or value[0] != '@':
        addr_spec.defects.append(errors.InvalidHeaderDefect(
            ""addr-spec local part with no domain""))
        return addr_spec, value
    addr_spec.append(ValueTerminal('@', 'address-at-symbol'))
    token, value = 1(value[1:])
    addr_spec.append(token)
    return addr_spec, value","{""get_local_part"": ""0"", ""get_domain"": ""1""}"
641,641,"def get_display_name(value):
    """""" display-name = phrase

    Because this is simply a name-rule, we don't return a display-name
    token containing a phrase, but rather a display-name token with
    the content of the phrase.

    """"""
    display_name = DisplayName()
    token, value = get_phrase(value)
    display_name.extend(token[:])
    display_name.defects = token.defects[:]
    return display_name, value","def get_phrase(value):
    """""" phrase = 1*word / obs-phrase
        obs-phrase = word *(word / ""."" / CFWS)

    This means a phrase can be a sequence of words, periods, and CFWS in any
    order as long as it starts with at least one word.  If anything other than
    words is detected, an ObsoleteHeaderDefect is added to the token's defect
    list.  We also accept a phrase that starts with CFWS followed by a dot;
    this is registered as an InvalidHeaderDefect, since it is not supported by
    even the obsolete grammar.

    """"""
    phrase = Phrase()
    try:
        token, value = get_word(value)
        phrase.append(token)
    except errors.HeaderParseError:
        phrase.defects.append(errors.InvalidHeaderDefect(
            ""phrase does not start with word""))
    while value and value[0] not in PHRASE_ENDS:
        if value[0]=='.':
            phrase.append(DOT)
            phrase.defects.append(errors.ObsoleteHeaderDefect(
                ""period in 'phrase'""))
            value = value[1:]
        else:
            try:
                token, value = get_word(value)
            except errors.HeaderParseError:
                if value[0] in CFWS_LEADER:
                    token, value = get_cfws(value)
                    phrase.defects.append(errors.ObsoleteHeaderDefect(
                        ""comment found without atom""))
                else:
                    raise
            phrase.append(token)
    return phrase, value","def parse_phrase_from_value(value):
    """""" phrase = 1*word / obs-phrase
        obs-phrase = word *(word / ""."" / CFWS)

    This means a phrase can be a sequence of words, periods, and CFWS in any
    order as long as it starts with at least one word.  If anything other than
    words is detected, an ObsoleteHeaderDefect is added to the token's defect
    list.  We also accept a phrase that starts with CFWS followed by a dot;
    this is registered as an InvalidHeaderDefect, since it is not supported by
    even the obsolete grammar.

    """"""
    phrase = Phrase()
    try:
        token, value = get_word(value)
        phrase.append(token)
    except errors.HeaderParseError:
        phrase.defects.append(errors.InvalidHeaderDefect(
            ""phrase does not start with word""))
    while value and value[0] not in PHRASE_ENDS:
        if value[0]=='.':
            phrase.append(DOT)
            phrase.defects.append(errors.ObsoleteHeaderDefect(
                ""period in 'phrase'""))
            value = value[1:]
        else:
            try:
                token, value = get_word(value)
            except errors.HeaderParseError:
                if value[0] in CFWS_LEADER:
                    token, value = get_cfws(value)
                    phrase.defects.append(errors.ObsoleteHeaderDefect(
                        ""comment found without atom""))
                else:
                    raise
            phrase.append(token)
    return phrase, value","def parse_display_name(value):
    """""" display-name = phrase

    Because this is simply a name-rule, we don't return a display-name
    token containing a phrase, but rather a display-name token with
    the content of the phrase.

    """"""
    display_name = DisplayName()
    token, value = parse_phrase_from_value(value)
    display_name.extend(token[:])
    display_name.defects = token.defects[:]
    return display_name, value","{""get_phrase"": ""parse_phrase_from_value""}","def 0(value):
    """""" phrase = 1*word / obs-phrase
        obs-phrase = word *(word / ""."" / CFWS)

    This means a phrase can be a sequence of words, periods, and CFWS in any
    order as long as it starts with at least one word.  If anything other than
    words is detected, an ObsoleteHeaderDefect is added to the token's defect
    list.  We also accept a phrase that starts with CFWS followed by a dot;
    this is registered as an InvalidHeaderDefect, since it is not supported by
    even the obsolete grammar.

    """"""
    phrase = Phrase()
    try:
        token, value = get_word(value)
        phrase.append(token)
    except errors.HeaderParseError:
        phrase.defects.append(errors.InvalidHeaderDefect(
            ""phrase does not start with word""))
    while value and value[0] not in PHRASE_ENDS:
        if value[0]=='.':
            phrase.append(DOT)
            phrase.defects.append(errors.ObsoleteHeaderDefect(
                ""period in 'phrase'""))
            value = value[1:]
        else:
            try:
                token, value = get_word(value)
            except errors.HeaderParseError:
                if value[0] in CFWS_LEADER:
                    token, value = get_cfws(value)
                    phrase.defects.append(errors.ObsoleteHeaderDefect(
                        ""comment found without atom""))
                else:
                    raise
            phrase.append(token)
    return phrase, value","def 1(value):
    """""" display-name = phrase

    Because this is simply a name-rule, we don't return a display-name
    token containing a phrase, but rather a display-name token with
    the content of the phrase.

    """"""
    display_name = DisplayName()
    token, value = 0(value)
    display_name.extend(token[:])
    display_name.defects = token.defects[:]
    return display_name, value","{""get_phrase"": ""0""}"
642,642,"def get_name_addr(value):
    """""" name-addr = [display-name] angle-addr

    """"""
    name_addr = NameAddr()
    # Both the optional display name and the angle-addr can start with cfws.
    leader = None
    if value[0] in CFWS_LEADER:
        leader, value = get_cfws(value)
        if not value:
            raise errors.HeaderParseError(
                ""expected name-addr but found '{}'"".format(leader))
    if value[0] != '<':
        if value[0] in PHRASE_ENDS:
            raise errors.HeaderParseError(
                ""expected name-addr but found '{}'"".format(value))
        token, value = get_display_name(value)
        if not value:
            raise errors.HeaderParseError(
                ""expected name-addr but found '{}'"".format(token))
        if leader is not None:
            token[0][:0] = [leader]
            leader = None
        name_addr.append(token)
    token, value = get_angle_addr(value)
    if leader is not None:
        token[:0] = [leader]
    name_addr.append(token)
    return name_addr, value","def get_display_name(value):
    """""" display-name = phrase

    Because this is simply a name-rule, we don't return a display-name
    token containing a phrase, but rather a display-name token with
    the content of the phrase.

    """"""
    display_name = DisplayName()
    token, value = get_phrase(value)
    display_name.extend(token[:])
    display_name.defects = token.defects[:]
    return display_name, value

def get_cfws(value):
    """"""CFWS = (1*([FWS] comment) [FWS]) / FWS

    """"""
    cfws = CFWSList()
    while value and value[0] in CFWS_LEADER:
        if value[0] in WSP:
            token, value = get_fws(value)
        else:
            token, value = get_comment(value)
        cfws.append(token)
    return cfws, value

def get_angle_addr(value):
    """""" angle-addr = [CFWS] ""<"" addr-spec "">"" [CFWS] / obs-angle-addr
        obs-angle-addr = [CFWS] ""<"" obs-route addr-spec "">"" [CFWS]

    """"""
    angle_addr = AngleAddr()
    if value[0] in CFWS_LEADER:
        token, value = get_cfws(value)
        angle_addr.append(token)
    if not value or value[0] != '<':
        raise errors.HeaderParseError(
            ""expected angle-addr but found '{}'"".format(value))
    angle_addr.append(ValueTerminal('<', 'angle-addr-start'))
    value = value[1:]
    # Although it is not legal per RFC5322, SMTP uses '<>' in certain
    # circumstances.
    if value[0] == '>':
        angle_addr.append(ValueTerminal('>', 'angle-addr-end'))
        angle_addr.defects.append(errors.InvalidHeaderDefect(
            ""null addr-spec in angle-addr""))
        value = value[1:]
        return angle_addr, value
    try:
        token, value = get_addr_spec(value)
    except errors.HeaderParseError:
        try:
            token, value = get_obs_route(value)
            angle_addr.defects.append(errors.ObsoleteHeaderDefect(
                ""obsolete route specification in angle-addr""))
        except errors.HeaderParseError:
            raise errors.HeaderParseError(
                ""expected addr-spec or obs-route but found '{}'"".format(value))
        angle_addr.append(token)
        token, value = get_addr_spec(value)
    angle_addr.append(token)
    if value and value[0] == '>':
        value = value[1:]
    else:
        angle_addr.defects.append(errors.InvalidHeaderDefect(
            ""missing trailing '>' on angle-addr""))
    angle_addr.append(ValueTerminal('>', 'angle-addr-end'))
    if value and value[0] in CFWS_LEADER:
        token, value = get_cfws(value)
        angle_addr.append(token)
    return angle_addr, value","def parse_display_name(value):
    """""" display-name = phrase

    Because this is simply a name-rule, we don't return a display-name
    token containing a phrase, but rather a display-name token with
    the content of the phrase.

    """"""
    display_name = DisplayName()
    token, value = get_phrase(value)
    display_name.extend(token[:])
    display_name.defects = token.defects[:]
    return display_name, value

def parse_cfws_from_value(value):
    """"""CFWS = (1*([FWS] comment) [FWS]) / FWS

    """"""
    cfws = CFWSList()
    while value and value[0] in CFWS_LEADER:
        if value[0] in WSP:
            token, value = get_fws(value)
        else:
            token, value = get_comment(value)
        cfws.append(token)
    return cfws, value

def parse_angle_addr_from_value(value):
    """""" angle-addr = [CFWS] ""<"" addr-spec "">"" [CFWS] / obs-angle-addr
        obs-angle-addr = [CFWS] ""<"" obs-route addr-spec "">"" [CFWS]

    """"""
    angle_addr = AngleAddr()
    if value[0] in CFWS_LEADER:
        token, value = parse_cfws_from_value(value)
        angle_addr.append(token)
    if not value or value[0] != '<':
        raise errors.HeaderParseError(
            ""expected angle-addr but found '{}'"".format(value))
    angle_addr.append(ValueTerminal('<', 'angle-addr-start'))
    value = value[1:]
    # Although it is not legal per RFC5322, SMTP uses '<>' in certain
    # circumstances.
    if value[0] == '>':
        angle_addr.append(ValueTerminal('>', 'angle-addr-end'))
        angle_addr.defects.append(errors.InvalidHeaderDefect(
            ""null addr-spec in angle-addr""))
        value = value[1:]
        return angle_addr, value
    try:
        token, value = get_addr_spec(value)
    except errors.HeaderParseError:
        try:
            token, value = get_obs_route(value)
            angle_addr.defects.append(errors.ObsoleteHeaderDefect(
                ""obsolete route specification in angle-addr""))
        except errors.HeaderParseError:
            raise errors.HeaderParseError(
                ""expected addr-spec or obs-route but found '{}'"".format(value))
        angle_addr.append(token)
        token, value = get_addr_spec(value)
    angle_addr.append(token)
    if value and value[0] == '>':
        value = value[1:]
    else:
        angle_addr.defects.append(errors.InvalidHeaderDefect(
            ""missing trailing '>' on angle-addr""))
    angle_addr.append(ValueTerminal('>', 'angle-addr-end'))
    if value and value[0] in CFWS_LEADER:
        token, value = parse_cfws_from_value(value)
        angle_addr.append(token)
    return angle_addr, value","def name_addr_equals_optional_display_name_angle_addr(value):
    """""" name-addr = [display-name] angle-addr

    """"""
    name_addr = NameAddr()
    # Both the optional display name and the angle-addr can start with cfws.
    leader = None
    if value[0] in CFWS_LEADER:
        leader, value = parse_cfws_from_value(value)
        if not value:
            raise errors.HeaderParseError(
                ""expected name-addr but found '{}'"".format(leader))
    if value[0] != '<':
        if value[0] in PHRASE_ENDS:
            raise errors.HeaderParseError(
                ""expected name-addr but found '{}'"".format(value))
        token, value = parse_display_name(value)
        if not value:
            raise errors.HeaderParseError(
                ""expected name-addr but found '{}'"".format(token))
        if leader is not None:
            token[0][:0] = [leader]
            leader = None
        name_addr.append(token)
    token, value = parse_angle_addr_from_value(value)
    if leader is not None:
        token[:0] = [leader]
    name_addr.append(token)
    return name_addr, value","{""get_display_name"": ""parse_display_name"", ""get_cfws"": ""parse_cfws_from_value"", ""get_angle_addr"": ""parse_angle_addr_from_value""}","def 0(value):
    """""" display-name = phrase

    Because this is simply a name-rule, we don't return a display-name
    token containing a phrase, but rather a display-name token with
    the content of the phrase.

    """"""
    display_name = DisplayName()
    token, value = get_phrase(value)
    display_name.extend(token[:])
    display_name.defects = token.defects[:]
    return display_name, value

def 1(value):
    """"""CFWS = (1*([FWS] comment) [FWS]) / FWS

    """"""
    cfws = CFWSList()
    while value and value[0] in CFWS_LEADER:
        if value[0] in WSP:
            token, value = get_fws(value)
        else:
            token, value = get_comment(value)
        cfws.append(token)
    return cfws, value

def 2(value):
    """""" angle-addr = [CFWS] ""<"" addr-spec "">"" [CFWS] / obs-angle-addr
        obs-angle-addr = [CFWS] ""<"" obs-route addr-spec "">"" [CFWS]

    """"""
    angle_addr = AngleAddr()
    if value[0] in CFWS_LEADER:
        token, value = 1(value)
        angle_addr.append(token)
    if not value or value[0] != '<':
        raise errors.HeaderParseError(
            ""expected angle-addr but found '{}'"".format(value))
    angle_addr.append(ValueTerminal('<', 'angle-addr-start'))
    value = value[1:]
    # Although it is not legal per RFC5322, SMTP uses '<>' in certain
    # circumstances.
    if value[0] == '>':
        angle_addr.append(ValueTerminal('>', 'angle-addr-end'))
        angle_addr.defects.append(errors.InvalidHeaderDefect(
            ""null addr-spec in angle-addr""))
        value = value[1:]
        return angle_addr, value
    try:
        token, value = get_addr_spec(value)
    except errors.HeaderParseError:
        try:
            token, value = get_obs_route(value)
            angle_addr.defects.append(errors.ObsoleteHeaderDefect(
                ""obsolete route specification in angle-addr""))
        except errors.HeaderParseError:
            raise errors.HeaderParseError(
                ""expected addr-spec or obs-route but found '{}'"".format(value))
        angle_addr.append(token)
        token, value = get_addr_spec(value)
    angle_addr.append(token)
    if value and value[0] == '>':
        value = value[1:]
    else:
        angle_addr.defects.append(errors.InvalidHeaderDefect(
            ""missing trailing '>' on angle-addr""))
    angle_addr.append(ValueTerminal('>', 'angle-addr-end'))
    if value and value[0] in CFWS_LEADER:
        token, value = 1(value)
        angle_addr.append(token)
    return angle_addr, value","def 3(value):
    """""" name-addr = [display-name] angle-addr

    """"""
    name_addr = NameAddr()
    # Both the optional display name and the angle-addr can start with cfws.
    leader = None
    if value[0] in CFWS_LEADER:
        leader, value = 1(value)
        if not value:
            raise errors.HeaderParseError(
                ""expected name-addr but found '{}'"".format(leader))
    if value[0] != '<':
        if value[0] in PHRASE_ENDS:
            raise errors.HeaderParseError(
                ""expected name-addr but found '{}'"".format(value))
        token, value = 0(value)
        if not value:
            raise errors.HeaderParseError(
                ""expected name-addr but found '{}'"".format(token))
        if leader is not None:
            token[0][:0] = [leader]
            leader = None
        name_addr.append(token)
    token, value = 2(value)
    if leader is not None:
        token[:0] = [leader]
    name_addr.append(token)
    return name_addr, value","{""get_display_name"": ""0"", ""get_cfws"": ""1"", ""get_angle_addr"": ""2""}"
643,643,"def get_invalid_mailbox(value, endchars):
    """""" Read everything up to one of the chars in endchars.

    This is outside the formal grammar.  The InvalidMailbox TokenList that is
    returned acts like a Mailbox, but the data attributes are None.

    """"""
    invalid_mailbox = InvalidMailbox()
    while value and value[0] not in endchars:
        if value[0] in PHRASE_ENDS:
            invalid_mailbox.append(ValueTerminal(value[0],
                                                 'misplaced-special'))
            value = value[1:]
        else:
            token, value = get_phrase(value)
            invalid_mailbox.append(token)
    return invalid_mailbox, value","def get_phrase(value):
    """""" phrase = 1*word / obs-phrase
        obs-phrase = word *(word / ""."" / CFWS)

    This means a phrase can be a sequence of words, periods, and CFWS in any
    order as long as it starts with at least one word.  If anything other than
    words is detected, an ObsoleteHeaderDefect is added to the token's defect
    list.  We also accept a phrase that starts with CFWS followed by a dot;
    this is registered as an InvalidHeaderDefect, since it is not supported by
    even the obsolete grammar.

    """"""
    phrase = Phrase()
    try:
        token, value = get_word(value)
        phrase.append(token)
    except errors.HeaderParseError:
        phrase.defects.append(errors.InvalidHeaderDefect(
            ""phrase does not start with word""))
    while value and value[0] not in PHRASE_ENDS:
        if value[0]=='.':
            phrase.append(DOT)
            phrase.defects.append(errors.ObsoleteHeaderDefect(
                ""period in 'phrase'""))
            value = value[1:]
        else:
            try:
                token, value = get_word(value)
            except errors.HeaderParseError:
                if value[0] in CFWS_LEADER:
                    token, value = get_cfws(value)
                    phrase.defects.append(errors.ObsoleteHeaderDefect(
                        ""comment found without atom""))
                else:
                    raise
            phrase.append(token)
    return phrase, value","def parse_phrase_from_value(value):
    """""" phrase = 1*word / obs-phrase
        obs-phrase = word *(word / ""."" / CFWS)

    This means a phrase can be a sequence of words, periods, and CFWS in any
    order as long as it starts with at least one word.  If anything other than
    words is detected, an ObsoleteHeaderDefect is added to the token's defect
    list.  We also accept a phrase that starts with CFWS followed by a dot;
    this is registered as an InvalidHeaderDefect, since it is not supported by
    even the obsolete grammar.

    """"""
    phrase = Phrase()
    try:
        token, value = get_word(value)
        phrase.append(token)
    except errors.HeaderParseError:
        phrase.defects.append(errors.InvalidHeaderDefect(
            ""phrase does not start with word""))
    while value and value[0] not in PHRASE_ENDS:
        if value[0]=='.':
            phrase.append(DOT)
            phrase.defects.append(errors.ObsoleteHeaderDefect(
                ""period in 'phrase'""))
            value = value[1:]
        else:
            try:
                token, value = get_word(value)
            except errors.HeaderParseError:
                if value[0] in CFWS_LEADER:
                    token, value = get_cfws(value)
                    phrase.defects.append(errors.ObsoleteHeaderDefect(
                        ""comment found without atom""))
                else:
                    raise
            phrase.append(token)
    return phrase, value","def read_everything_up_to_one_of_the_chars_in_endchars_outside_formal_grammar(value, endchars):
    """""" Read everything up to one of the chars in endchars.

    This is outside the formal grammar.  The InvalidMailbox TokenList that is
    returned acts like a Mailbox, but the data attributes are None.

    """"""
    invalid_mailbox = InvalidMailbox()
    while value and value[0] not in endchars:
        if value[0] in PHRASE_ENDS:
            invalid_mailbox.append(ValueTerminal(value[0],
                                                 'misplaced-special'))
            value = value[1:]
        else:
            token, value = parse_phrase_from_value(value)
            invalid_mailbox.append(token)
    return invalid_mailbox, value","{""get_phrase"": ""parse_phrase_from_value""}","def 0(value):
    """""" phrase = 1*word / obs-phrase
        obs-phrase = word *(word / ""."" / CFWS)

    This means a phrase can be a sequence of words, periods, and CFWS in any
    order as long as it starts with at least one word.  If anything other than
    words is detected, an ObsoleteHeaderDefect is added to the token's defect
    list.  We also accept a phrase that starts with CFWS followed by a dot;
    this is registered as an InvalidHeaderDefect, since it is not supported by
    even the obsolete grammar.

    """"""
    phrase = Phrase()
    try:
        token, value = get_word(value)
        phrase.append(token)
    except errors.HeaderParseError:
        phrase.defects.append(errors.InvalidHeaderDefect(
            ""phrase does not start with word""))
    while value and value[0] not in PHRASE_ENDS:
        if value[0]=='.':
            phrase.append(DOT)
            phrase.defects.append(errors.ObsoleteHeaderDefect(
                ""period in 'phrase'""))
            value = value[1:]
        else:
            try:
                token, value = get_word(value)
            except errors.HeaderParseError:
                if value[0] in CFWS_LEADER:
                    token, value = get_cfws(value)
                    phrase.defects.append(errors.ObsoleteHeaderDefect(
                        ""comment found without atom""))
                else:
                    raise
            phrase.append(token)
    return phrase, value","def 1(value, endchars):
    """""" Read everything up to one of the chars in endchars.

    This is outside the formal grammar.  The InvalidMailbox TokenList that is
    returned acts like a Mailbox, but the data attributes are None.

    """"""
    invalid_mailbox = InvalidMailbox()
    while value and value[0] not in endchars:
        if value[0] in PHRASE_ENDS:
            invalid_mailbox.append(ValueTerminal(value[0],
                                                 'misplaced-special'))
            value = value[1:]
        else:
            token, value = 0(value)
            invalid_mailbox.append(token)
    return invalid_mailbox, value","{""get_phrase"": ""0""}"
644,644,"def get_no_fold_literal(value):
    """""" no-fold-literal = ""["" *dtext ""]""
    """"""
    no_fold_literal = NoFoldLiteral()
    if not value:
        raise errors.HeaderParseError(
            ""expected no-fold-literal but found '{}'"".format(value))
    if value[0] != '[':
        raise errors.HeaderParseError(
            ""expected '[' at the start of no-fold-literal ""
            ""but found '{}'"".format(value))
    no_fold_literal.append(ValueTerminal('[', 'no-fold-literal-start'))
    value = value[1:]
    token, value = get_dtext(value)
    no_fold_literal.append(token)
    if not value or value[0] != ']':
        raise errors.HeaderParseError(
            ""expected ']' at the end of no-fold-literal ""
            ""but found '{}'"".format(value))
    no_fold_literal.append(ValueTerminal(']', 'no-fold-literal-end'))
    return no_fold_literal, value[1:]","def get_dtext(value):
    r"""""" dtext = <printable ascii except \ [ ]> / obs-dtext
        obs-dtext = obs-NO-WS-CTL / quoted-pair

    We allow anything except the excluded characters, but if we find any
    ASCII other than the RFC defined printable ASCII, a NonPrintableDefect is
    added to the token's defects list.  Quoted pairs are converted to their
    unquoted values, so what is returned is a ptext token, in this case a
    ValueTerminal.  If there were quoted-printables, an ObsoleteHeaderDefect is
    added to the returned token's defect list.

    """"""
    ptext, value, had_qp = _get_ptext_to_endchars(value, '[]')
    ptext = ValueTerminal(ptext, 'ptext')
    if had_qp:
        ptext.defects.append(errors.ObsoleteHeaderDefect(
            ""quoted printable found in domain-literal""))
    _validate_xtext(ptext)
    return ptext, value","def parse_dtext_from_value(value):
    r"""""" dtext = <printable ascii except \ [ ]> / obs-dtext
        obs-dtext = obs-NO-WS-CTL / quoted-pair

    We allow anything except the excluded characters, but if we find any
    ASCII other than the RFC defined printable ASCII, a NonPrintableDefect is
    added to the token's defects list.  Quoted pairs are converted to their
    unquoted values, so what is returned is a ptext token, in this case a
    ValueTerminal.  If there were quoted-printables, an ObsoleteHeaderDefect is
    added to the returned token's defect list.

    """"""
    ptext, value, had_qp = _get_ptext_to_endchars(value, '[]')
    ptext = ValueTerminal(ptext, 'ptext')
    if had_qp:
        ptext.defects.append(errors.ObsoleteHeaderDefect(
            ""quoted printable found in domain-literal""))
    _validate_xtext(ptext)
    return ptext, value","def no_fold_literal_equals_bracket_zero_or_more_dtext_bracket(value):
    """""" no-fold-literal = ""["" *dtext ""]""
    """"""
    no_fold_literal = NoFoldLiteral()
    if not value:
        raise errors.HeaderParseError(
            ""expected no-fold-literal but found '{}'"".format(value))
    if value[0] != '[':
        raise errors.HeaderParseError(
            ""expected '[' at the start of no-fold-literal ""
            ""but found '{}'"".format(value))
    no_fold_literal.append(ValueTerminal('[', 'no-fold-literal-start'))
    value = value[1:]
    token, value = parse_dtext_from_value(value)
    no_fold_literal.append(token)
    if not value or value[0] != ']':
        raise errors.HeaderParseError(
            ""expected ']' at the end of no-fold-literal ""
            ""but found '{}'"".format(value))
    no_fold_literal.append(ValueTerminal(']', 'no-fold-literal-end'))
    return no_fold_literal, value[1:]","{""get_dtext"": ""parse_dtext_from_value""}","def 0(value):
    r"""""" dtext = <printable ascii except \ [ ]> / obs-dtext
        obs-dtext = obs-NO-WS-CTL / quoted-pair

    We allow anything except the excluded characters, but if we find any
    ASCII other than the RFC defined printable ASCII, a NonPrintableDefect is
    added to the token's defects list.  Quoted pairs are converted to their
    unquoted values, so what is returned is a ptext token, in this case a
    ValueTerminal.  If there were quoted-printables, an ObsoleteHeaderDefect is
    added to the returned token's defect list.

    """"""
    ptext, value, had_qp = _get_ptext_to_endchars(value, '[]')
    ptext = ValueTerminal(ptext, 'ptext')
    if had_qp:
        ptext.defects.append(errors.ObsoleteHeaderDefect(
            ""quoted printable found in domain-literal""))
    _validate_xtext(ptext)
    return ptext, value","def 1(value):
    """""" no-fold-literal = ""["" *dtext ""]""
    """"""
    no_fold_literal = NoFoldLiteral()
    if not value:
        raise errors.HeaderParseError(
            ""expected no-fold-literal but found '{}'"".format(value))
    if value[0] != '[':
        raise errors.HeaderParseError(
            ""expected '[' at the start of no-fold-literal ""
            ""but found '{}'"".format(value))
    no_fold_literal.append(ValueTerminal('[', 'no-fold-literal-start'))
    value = value[1:]
    token, value = 0(value)
    no_fold_literal.append(token)
    if not value or value[0] != ']':
        raise errors.HeaderParseError(
            ""expected ']' at the end of no-fold-literal ""
            ""but found '{}'"".format(value))
    no_fold_literal.append(ValueTerminal(']', 'no-fold-literal-end'))
    return no_fold_literal, value[1:]","{""get_dtext"": ""0""}"
645,645,"def parse_message_id(value):
    """"""message-id      =   ""Message-ID:"" msg-id CRLF
    """"""
    message_id = MessageID()
    try:
        token, value = get_msg_id(value)
        message_id.append(token)
    except errors.HeaderParseError as ex:
        token = get_unstructured(value)
        message_id = InvalidMessageID(token)
        message_id.defects.append(
            errors.InvalidHeaderDefect(""Invalid msg-id: {!r}"".format(ex)))
    else:
        # Value after parsing a valid msg_id should be None.
        if value:
            message_id.defects.append(errors.InvalidHeaderDefect(
                ""Unexpected {!r}"".format(value)))

    return message_id","def get_msg_id(value):
    """"""msg-id = [CFWS] ""<"" id-left '@' id-right  "">"" [CFWS]
       id-left = dot-atom-text / obs-id-left
       id-right = dot-atom-text / no-fold-literal / obs-id-right
       no-fold-literal = ""["" *dtext ""]""
    """"""
    msg_id = MsgID()
    if value and value[0] in CFWS_LEADER:
        token, value = get_cfws(value)
        msg_id.append(token)
    if not value or value[0] != '<':
        raise errors.HeaderParseError(
            ""expected msg-id but found '{}'"".format(value))
    msg_id.append(ValueTerminal('<', 'msg-id-start'))
    value = value[1:]
    # Parse id-left.
    try:
        token, value = get_dot_atom_text(value)
    except errors.HeaderParseError:
        try:
            # obs-id-left is same as local-part of add-spec.
            token, value = get_obs_local_part(value)
            msg_id.defects.append(errors.ObsoleteHeaderDefect(
                ""obsolete id-left in msg-id""))
        except errors.HeaderParseError:
            raise errors.HeaderParseError(
                ""expected dot-atom-text or obs-id-left""
                "" but found '{}'"".format(value))
    msg_id.append(token)
    if not value or value[0] != '@':
        msg_id.defects.append(errors.InvalidHeaderDefect(
            ""msg-id with no id-right""))
        # Even though there is no id-right, if the local part
        # ends with `>` let's just parse it too and return
        # along with the defect.
        if value and value[0] == '>':
            msg_id.append(ValueTerminal('>', 'msg-id-end'))
            value = value[1:]
        return msg_id, value
    msg_id.append(ValueTerminal('@', 'address-at-symbol'))
    value = value[1:]
    # Parse id-right.
    try:
        token, value = get_dot_atom_text(value)
    except errors.HeaderParseError:
        try:
            token, value = get_no_fold_literal(value)
        except errors.HeaderParseError as e:
            try:
                token, value = get_domain(value)
                msg_id.defects.append(errors.ObsoleteHeaderDefect(
                    ""obsolete id-right in msg-id""))
            except errors.HeaderParseError:
                raise errors.HeaderParseError(
                    ""expected dot-atom-text, no-fold-literal or obs-id-right""
                    "" but found '{}'"".format(value))
    msg_id.append(token)
    if value and value[0] == '>':
        value = value[1:]
    else:
        msg_id.defects.append(errors.InvalidHeaderDefect(
            ""missing trailing '>' on msg-id""))
    msg_id.append(ValueTerminal('>', 'msg-id-end'))
    if value and value[0] in CFWS_LEADER:
        token, value = get_cfws(value)
        msg_id.append(token)
    return msg_id, value

def get_unstructured(value):
    """"""unstructured = (*([FWS] vchar) *WSP) / obs-unstruct
       obs-unstruct = *((*LF *CR *(obs-utext) *LF *CR)) / FWS)
       obs-utext = %d0 / obs-NO-WS-CTL / LF / CR

       obs-NO-WS-CTL is control characters except WSP/CR/LF.

    So, basically, we have printable runs, plus control characters or nulls in
    the obsolete syntax, separated by whitespace.  Since RFC 2047 uses the
    obsolete syntax in its specification, but requires whitespace on either
    side of the encoded words, I can see no reason to need to separate the
    non-printable-non-whitespace from the printable runs if they occur, so we
    parse this into xtext tokens separated by WSP tokens.

    Because an 'unstructured' value must by definition constitute the entire
    value, this 'get' routine does not return a remaining value, only the
    parsed TokenList.

    """"""
    # XXX: but what about bare CR and LF?  They might signal the start or
    # end of an encoded word.  YAGNI for now, since our current parsers
    # will never send us strings with bare CR or LF.

    unstructured = UnstructuredTokenList()
    while value:
        if value[0] in WSP:
            token, value = get_fws(value)
            unstructured.append(token)
            continue
        valid_ew = True
        if value.startswith('=?'):
            try:
                token, value = get_encoded_word(value)
            except _InvalidEwError:
                valid_ew = False
            except errors.HeaderParseError:
                # XXX: Need to figure out how to register defects when
                # appropriate here.
                pass
            else:
                have_ws = True
                if len(unstructured) > 0:
                    if unstructured[-1].token_type != 'fws':
                        unstructured.defects.append(errors.InvalidHeaderDefect(
                            ""missing whitespace before encoded word""))
                        have_ws = False
                if have_ws and len(unstructured) > 1:
                    if unstructured[-2].token_type == 'encoded-word':
                        unstructured[-1] = EWWhiteSpaceTerminal(
                            unstructured[-1], 'fws')
                unstructured.append(token)
                continue
        tok, *remainder = _wsp_splitter(value, 1)
        # Split in the middle of an atom if there is a rfc2047 encoded word
        # which does not have WSP on both sides. The defect will be registered
        # the next time through the loop.
        # This needs to only be performed when the encoded word is valid;
        # otherwise, performing it on an invalid encoded word can cause
        # the parser to go in an infinite loop.
        if valid_ew and rfc2047_matcher.search(tok):
            tok, *remainder = value.partition('=?')
        vtext = ValueTerminal(tok, 'vtext')
        _validate_xtext(vtext)
        unstructured.append(vtext)
        value = ''.join(remainder)
    return unstructured","def parse_msg_id_from_value(value):
    """"""msg-id = [CFWS] ""<"" id-left '@' id-right  "">"" [CFWS]
       id-left = dot-atom-text / obs-id-left
       id-right = dot-atom-text / no-fold-literal / obs-id-right
       no-fold-literal = ""["" *dtext ""]""
    """"""
    msg_id = MsgID()
    if value and value[0] in CFWS_LEADER:
        token, value = get_cfws(value)
        msg_id.append(token)
    if not value or value[0] != '<':
        raise errors.HeaderParseError(
            ""expected msg-id but found '{}'"".format(value))
    msg_id.append(ValueTerminal('<', 'msg-id-start'))
    value = value[1:]
    # Parse id-left.
    try:
        token, value = get_dot_atom_text(value)
    except errors.HeaderParseError:
        try:
            # obs-id-left is same as local-part of add-spec.
            token, value = get_obs_local_part(value)
            msg_id.defects.append(errors.ObsoleteHeaderDefect(
                ""obsolete id-left in msg-id""))
        except errors.HeaderParseError:
            raise errors.HeaderParseError(
                ""expected dot-atom-text or obs-id-left""
                "" but found '{}'"".format(value))
    msg_id.append(token)
    if not value or value[0] != '@':
        msg_id.defects.append(errors.InvalidHeaderDefect(
            ""msg-id with no id-right""))
        # Even though there is no id-right, if the local part
        # ends with `>` let's just parse it too and return
        # along with the defect.
        if value and value[0] == '>':
            msg_id.append(ValueTerminal('>', 'msg-id-end'))
            value = value[1:]
        return msg_id, value
    msg_id.append(ValueTerminal('@', 'address-at-symbol'))
    value = value[1:]
    # Parse id-right.
    try:
        token, value = get_dot_atom_text(value)
    except errors.HeaderParseError:
        try:
            token, value = get_no_fold_literal(value)
        except errors.HeaderParseError as e:
            try:
                token, value = get_domain(value)
                msg_id.defects.append(errors.ObsoleteHeaderDefect(
                    ""obsolete id-right in msg-id""))
            except errors.HeaderParseError:
                raise errors.HeaderParseError(
                    ""expected dot-atom-text, no-fold-literal or obs-id-right""
                    "" but found '{}'"".format(value))
    msg_id.append(token)
    if value and value[0] == '>':
        value = value[1:]
    else:
        msg_id.defects.append(errors.InvalidHeaderDefect(
            ""missing trailing '>' on msg-id""))
    msg_id.append(ValueTerminal('>', 'msg-id-end'))
    if value and value[0] in CFWS_LEADER:
        token, value = get_cfws(value)
        msg_id.append(token)
    return msg_id, value

def parse_unstructured_value(value):
    """"""unstructured = (*([FWS] vchar) *WSP) / obs-unstruct
       obs-unstruct = *((*LF *CR *(obs-utext) *LF *CR)) / FWS)
       obs-utext = %d0 / obs-NO-WS-CTL / LF / CR

       obs-NO-WS-CTL is control characters except WSP/CR/LF.

    So, basically, we have printable runs, plus control characters or nulls in
    the obsolete syntax, separated by whitespace.  Since RFC 2047 uses the
    obsolete syntax in its specification, but requires whitespace on either
    side of the encoded words, I can see no reason to need to separate the
    non-printable-non-whitespace from the printable runs if they occur, so we
    parse this into xtext tokens separated by WSP tokens.

    Because an 'unstructured' value must by definition constitute the entire
    value, this 'get' routine does not return a remaining value, only the
    parsed TokenList.

    """"""
    # XXX: but what about bare CR and LF?  They might signal the start or
    # end of an encoded word.  YAGNI for now, since our current parsers
    # will never send us strings with bare CR or LF.

    unstructured = UnstructuredTokenList()
    while value:
        if value[0] in WSP:
            token, value = get_fws(value)
            unstructured.append(token)
            continue
        valid_ew = True
        if value.startswith('=?'):
            try:
                token, value = get_encoded_word(value)
            except _InvalidEwError:
                valid_ew = False
            except errors.HeaderParseError:
                # XXX: Need to figure out how to register defects when
                # appropriate here.
                pass
            else:
                have_ws = True
                if len(unstructured) > 0:
                    if unstructured[-1].token_type != 'fws':
                        unstructured.defects.append(errors.InvalidHeaderDefect(
                            ""missing whitespace before encoded word""))
                        have_ws = False
                if have_ws and len(unstructured) > 1:
                    if unstructured[-2].token_type == 'encoded-word':
                        unstructured[-1] = EWWhiteSpaceTerminal(
                            unstructured[-1], 'fws')
                unstructured.append(token)
                continue
        tok, *remainder = _wsp_splitter(value, 1)
        # Split in the middle of an atom if there is a rfc2047 encoded word
        # which does not have WSP on both sides. The defect will be registered
        # the next time through the loop.
        # This needs to only be performed when the encoded word is valid;
        # otherwise, performing it on an invalid encoded word can cause
        # the parser to go in an infinite loop.
        if valid_ew and rfc2047_matcher.search(tok):
            tok, *remainder = value.partition('=?')
        vtext = ValueTerminal(tok, 'vtext')
        _validate_xtext(vtext)
        unstructured.append(vtext)
        value = ''.join(remainder)
    return unstructured","def message_id_equals_Message_ID_msg_id_CRLF(value):
    """"""message-id      =   ""Message-ID:"" msg-id CRLF
    """"""
    message_id = MessageID()
    try:
        token, value = parse_msg_id_from_value(value)
        message_id.append(token)
    except errors.HeaderParseError as ex:
        token = parse_unstructured_value(value)
        message_id = InvalidMessageID(token)
        message_id.defects.append(
            errors.InvalidHeaderDefect(""Invalid msg-id: {!r}"".format(ex)))
    else:
        # Value after parsing a valid msg_id should be None.
        if value:
            message_id.defects.append(errors.InvalidHeaderDefect(
                ""Unexpected {!r}"".format(value)))

    return message_id","{""get_msg_id"": ""parse_msg_id_from_value"", ""get_unstructured"": ""parse_unstructured_value""}","def 0(value):
    """"""msg-id = [CFWS] ""<"" id-left '@' id-right  "">"" [CFWS]
       id-left = dot-atom-text / obs-id-left
       id-right = dot-atom-text / no-fold-literal / obs-id-right
       no-fold-literal = ""["" *dtext ""]""
    """"""
    msg_id = MsgID()
    if value and value[0] in CFWS_LEADER:
        token, value = get_cfws(value)
        msg_id.append(token)
    if not value or value[0] != '<':
        raise errors.HeaderParseError(
            ""expected msg-id but found '{}'"".format(value))
    msg_id.append(ValueTerminal('<', 'msg-id-start'))
    value = value[1:]
    # Parse id-left.
    try:
        token, value = get_dot_atom_text(value)
    except errors.HeaderParseError:
        try:
            # obs-id-left is same as local-part of add-spec.
            token, value = get_obs_local_part(value)
            msg_id.defects.append(errors.ObsoleteHeaderDefect(
                ""obsolete id-left in msg-id""))
        except errors.HeaderParseError:
            raise errors.HeaderParseError(
                ""expected dot-atom-text or obs-id-left""
                "" but found '{}'"".format(value))
    msg_id.append(token)
    if not value or value[0] != '@':
        msg_id.defects.append(errors.InvalidHeaderDefect(
            ""msg-id with no id-right""))
        # Even though there is no id-right, if the local part
        # ends with `>` let's just parse it too and return
        # along with the defect.
        if value and value[0] == '>':
            msg_id.append(ValueTerminal('>', 'msg-id-end'))
            value = value[1:]
        return msg_id, value
    msg_id.append(ValueTerminal('@', 'address-at-symbol'))
    value = value[1:]
    # Parse id-right.
    try:
        token, value = get_dot_atom_text(value)
    except errors.HeaderParseError:
        try:
            token, value = get_no_fold_literal(value)
        except errors.HeaderParseError as e:
            try:
                token, value = get_domain(value)
                msg_id.defects.append(errors.ObsoleteHeaderDefect(
                    ""obsolete id-right in msg-id""))
            except errors.HeaderParseError:
                raise errors.HeaderParseError(
                    ""expected dot-atom-text, no-fold-literal or obs-id-right""
                    "" but found '{}'"".format(value))
    msg_id.append(token)
    if value and value[0] == '>':
        value = value[1:]
    else:
        msg_id.defects.append(errors.InvalidHeaderDefect(
            ""missing trailing '>' on msg-id""))
    msg_id.append(ValueTerminal('>', 'msg-id-end'))
    if value and value[0] in CFWS_LEADER:
        token, value = get_cfws(value)
        msg_id.append(token)
    return msg_id, value

def 1(value):
    """"""unstructured = (*([FWS] vchar) *WSP) / obs-unstruct
       obs-unstruct = *((*LF *CR *(obs-utext) *LF *CR)) / FWS)
       obs-utext = %d0 / obs-NO-WS-CTL / LF / CR

       obs-NO-WS-CTL is control characters except WSP/CR/LF.

    So, basically, we have printable runs, plus control characters or nulls in
    the obsolete syntax, separated by whitespace.  Since RFC 2047 uses the
    obsolete syntax in its specification, but requires whitespace on either
    side of the encoded words, I can see no reason to need to separate the
    non-printable-non-whitespace from the printable runs if they occur, so we
    parse this into xtext tokens separated by WSP tokens.

    Because an 'unstructured' value must by definition constitute the entire
    value, this 'get' routine does not return a remaining value, only the
    parsed TokenList.

    """"""
    # XXX: but what about bare CR and LF?  They might signal the start or
    # end of an encoded word.  YAGNI for now, since our current parsers
    # will never send us strings with bare CR or LF.

    unstructured = UnstructuredTokenList()
    while value:
        if value[0] in WSP:
            token, value = get_fws(value)
            unstructured.append(token)
            continue
        valid_ew = True
        if value.startswith('=?'):
            try:
                token, value = get_encoded_word(value)
            except _InvalidEwError:
                valid_ew = False
            except errors.HeaderParseError:
                # XXX: Need to figure out how to register defects when
                # appropriate here.
                pass
            else:
                have_ws = True
                if len(unstructured) > 0:
                    if unstructured[-1].token_type != 'fws':
                        unstructured.defects.append(errors.InvalidHeaderDefect(
                            ""missing whitespace before encoded word""))
                        have_ws = False
                if have_ws and len(unstructured) > 1:
                    if unstructured[-2].token_type == 'encoded-word':
                        unstructured[-1] = EWWhiteSpaceTerminal(
                            unstructured[-1], 'fws')
                unstructured.append(token)
                continue
        tok, *remainder = _wsp_splitter(value, 1)
        # Split in the middle of an atom if there is a rfc2047 encoded word
        # which does not have WSP on both sides. The defect will be registered
        # the next time through the loop.
        # This needs to only be performed when the encoded word is valid;
        # otherwise, performing it on an invalid encoded word can cause
        # the parser to go in an infinite loop.
        if valid_ew and rfc2047_matcher.search(tok):
            tok, *remainder = value.partition('=?')
        vtext = ValueTerminal(tok, 'vtext')
        _validate_xtext(vtext)
        unstructured.append(vtext)
        value = ''.join(remainder)
    return unstructured","def 2(value):
    """"""message-id      =   ""Message-ID:"" msg-id CRLF
    """"""
    message_id = MessageID()
    try:
        token, value = 0(value)
        message_id.append(token)
    except errors.HeaderParseError as ex:
        token = 1(value)
        message_id = InvalidMessageID(token)
        message_id.defects.append(
            errors.InvalidHeaderDefect(""Invalid msg-id: {!r}"".format(ex)))
    else:
        # Value after parsing a valid msg_id should be None.
        if value:
            message_id.defects.append(errors.InvalidHeaderDefect(
                ""Unexpected {!r}"".format(value)))

    return message_id","{""get_msg_id"": ""0"", ""get_unstructured"": ""1""}"
646,646,"def get_invalid_parameter(value):
    """""" Read everything up to the next ';'.

    This is outside the formal grammar.  The InvalidParameter TokenList that is
    returned acts like a Parameter, but the data attributes are None.

    """"""
    invalid_parameter = InvalidParameter()
    while value and value[0] != ';':
        if value[0] in PHRASE_ENDS:
            invalid_parameter.append(ValueTerminal(value[0],
                                                   'misplaced-special'))
            value = value[1:]
        else:
            token, value = get_phrase(value)
            invalid_parameter.append(token)
    return invalid_parameter, value","def get_phrase(value):
    """""" phrase = 1*word / obs-phrase
        obs-phrase = word *(word / ""."" / CFWS)

    This means a phrase can be a sequence of words, periods, and CFWS in any
    order as long as it starts with at least one word.  If anything other than
    words is detected, an ObsoleteHeaderDefect is added to the token's defect
    list.  We also accept a phrase that starts with CFWS followed by a dot;
    this is registered as an InvalidHeaderDefect, since it is not supported by
    even the obsolete grammar.

    """"""
    phrase = Phrase()
    try:
        token, value = get_word(value)
        phrase.append(token)
    except errors.HeaderParseError:
        phrase.defects.append(errors.InvalidHeaderDefect(
            ""phrase does not start with word""))
    while value and value[0] not in PHRASE_ENDS:
        if value[0]=='.':
            phrase.append(DOT)
            phrase.defects.append(errors.ObsoleteHeaderDefect(
                ""period in 'phrase'""))
            value = value[1:]
        else:
            try:
                token, value = get_word(value)
            except errors.HeaderParseError:
                if value[0] in CFWS_LEADER:
                    token, value = get_cfws(value)
                    phrase.defects.append(errors.ObsoleteHeaderDefect(
                        ""comment found without atom""))
                else:
                    raise
            phrase.append(token)
    return phrase, value","def parse_phrase_from_value(value):
    """""" phrase = 1*word / obs-phrase
        obs-phrase = word *(word / ""."" / CFWS)

    This means a phrase can be a sequence of words, periods, and CFWS in any
    order as long as it starts with at least one word.  If anything other than
    words is detected, an ObsoleteHeaderDefect is added to the token's defect
    list.  We also accept a phrase that starts with CFWS followed by a dot;
    this is registered as an InvalidHeaderDefect, since it is not supported by
    even the obsolete grammar.

    """"""
    phrase = Phrase()
    try:
        token, value = get_word(value)
        phrase.append(token)
    except errors.HeaderParseError:
        phrase.defects.append(errors.InvalidHeaderDefect(
            ""phrase does not start with word""))
    while value and value[0] not in PHRASE_ENDS:
        if value[0]=='.':
            phrase.append(DOT)
            phrase.defects.append(errors.ObsoleteHeaderDefect(
                ""period in 'phrase'""))
            value = value[1:]
        else:
            try:
                token, value = get_word(value)
            except errors.HeaderParseError:
                if value[0] in CFWS_LEADER:
                    token, value = get_cfws(value)
                    phrase.defects.append(errors.ObsoleteHeaderDefect(
                        ""comment found without atom""))
                else:
                    raise
            phrase.append(token)
    return phrase, value","def read_everything_up_to_next_semicolon_outside_formal_grammar(value):
    """""" Read everything up to the next ';'.

    This is outside the formal grammar.  The InvalidParameter TokenList that is
    returned acts like a Parameter, but the data attributes are None.

    """"""
    invalid_parameter = InvalidParameter()
    while value and value[0] != ';':
        if value[0] in PHRASE_ENDS:
            invalid_parameter.append(ValueTerminal(value[0],
                                                   'misplaced-special'))
            value = value[1:]
        else:
            token, value = parse_phrase_from_value(value)
            invalid_parameter.append(token)
    return invalid_parameter, value","{""get_phrase"": ""parse_phrase_from_value""}","def 0(value):
    """""" phrase = 1*word / obs-phrase
        obs-phrase = word *(word / ""."" / CFWS)

    This means a phrase can be a sequence of words, periods, and CFWS in any
    order as long as it starts with at least one word.  If anything other than
    words is detected, an ObsoleteHeaderDefect is added to the token's defect
    list.  We also accept a phrase that starts with CFWS followed by a dot;
    this is registered as an InvalidHeaderDefect, since it is not supported by
    even the obsolete grammar.

    """"""
    phrase = Phrase()
    try:
        token, value = get_word(value)
        phrase.append(token)
    except errors.HeaderParseError:
        phrase.defects.append(errors.InvalidHeaderDefect(
            ""phrase does not start with word""))
    while value and value[0] not in PHRASE_ENDS:
        if value[0]=='.':
            phrase.append(DOT)
            phrase.defects.append(errors.ObsoleteHeaderDefect(
                ""period in 'phrase'""))
            value = value[1:]
        else:
            try:
                token, value = get_word(value)
            except errors.HeaderParseError:
                if value[0] in CFWS_LEADER:
                    token, value = get_cfws(value)
                    phrase.defects.append(errors.ObsoleteHeaderDefect(
                        ""comment found without atom""))
                else:
                    raise
            phrase.append(token)
    return phrase, value","def 1(value):
    """""" Read everything up to the next ';'.

    This is outside the formal grammar.  The InvalidParameter TokenList that is
    returned acts like a Parameter, but the data attributes are None.

    """"""
    invalid_parameter = InvalidParameter()
    while value and value[0] != ';':
        if value[0] in PHRASE_ENDS:
            invalid_parameter.append(ValueTerminal(value[0],
                                                   'misplaced-special'))
            value = value[1:]
        else:
            token, value = 0(value)
            invalid_parameter.append(token)
    return invalid_parameter, value","{""get_phrase"": ""0""}"
647,647,"def get_ttext(value):
    """"""ttext = <matches _ttext_matcher>

    We allow any non-TOKEN_ENDS in ttext, but add defects to the token's
    defects list if we find non-ttext characters.  We also register defects for
    *any* non-printables even though the RFC doesn't exclude all of them,
    because we follow the spirit of RFC 5322.

    """"""
    m = _non_token_end_matcher(value)
    if not m:
        raise errors.HeaderParseError(
            ""expected ttext but found '{}'"".format(value))
    ttext = m.group()
    value = value[len(ttext):]
    ttext = ValueTerminal(ttext, 'ttext')
    _validate_xtext(ttext)
    return ttext, value","def _validate_xtext(xtext):
    """"""If input token contains ASCII non-printables, register a defect.""""""

    non_printables = _non_printable_finder(xtext)
    if non_printables:
        xtext.defects.append(errors.NonPrintableDefect(non_printables))
    if utils._has_surrogates(xtext):
        xtext.defects.append(errors.UndecodableBytesDefect(
            ""Non-ASCII characters found in header token""))","def validate_xtext_input_token(xtext):
    """"""If input token contains ASCII non-printables, register a defect.""""""

    non_printables = _non_printable_finder(xtext)
    if non_printables:
        xtext.defects.append(errors.NonPrintableDefect(non_printables))
    if utils._has_surrogates(xtext):
        xtext.defects.append(errors.UndecodableBytesDefect(
            ""Non-ASCII characters found in header token""))","def parse_ttext_from_value(value):
    """"""ttext = <matches _ttext_matcher>

    We allow any non-TOKEN_ENDS in ttext, but add defects to the token's
    defects list if we find non-ttext characters.  We also register defects for
    *any* non-printables even though the RFC doesn't exclude all of them,
    because we follow the spirit of RFC 5322.

    """"""
    m = _non_token_end_matcher(value)
    if not m:
        raise errors.HeaderParseError(
            ""expected ttext but found '{}'"".format(value))
    ttext = m.group()
    value = value[len(ttext):]
    ttext = ValueTerminal(ttext, 'ttext')
    validate_xtext_input_token(ttext)
    return ttext, value","{""_validate_xtext"": ""validate_xtext_input_token""}","def 0(xtext):
    """"""If input token contains ASCII non-printables, register a defect.""""""

    non_printables = _non_printable_finder(xtext)
    if non_printables:
        xtext.defects.append(errors.NonPrintableDefect(non_printables))
    if utils._has_surrogates(xtext):
        xtext.defects.append(errors.UndecodableBytesDefect(
            ""Non-ASCII characters found in header token""))","def 1(value):
    """"""ttext = <matches _ttext_matcher>

    We allow any non-TOKEN_ENDS in ttext, but add defects to the token's
    defects list if we find non-ttext characters.  We also register defects for
    *any* non-printables even though the RFC doesn't exclude all of them,
    because we follow the spirit of RFC 5322.

    """"""
    m = _non_token_end_matcher(value)
    if not m:
        raise errors.HeaderParseError(
            ""expected ttext but found '{}'"".format(value))
    ttext = m.group()
    value = value[len(ttext):]
    ttext = ValueTerminal(ttext, 'ttext')
    0(ttext)
    return ttext, value","{""_validate_xtext"": ""0""}"
648,648,"def get_token(value):
    """"""token = [CFWS] 1*ttext [CFWS]

    The RFC equivalent of ttext is any US-ASCII chars except space, ctls, or
    tspecials.  We also exclude tabs even though the RFC doesn't.

    The RFC implies the CFWS but is not explicit about it in the BNF.

    """"""
    mtoken = Token()
    if value and value[0] in CFWS_LEADER:
        token, value = get_cfws(value)
        mtoken.append(token)
    if value and value[0] in TOKEN_ENDS:
        raise errors.HeaderParseError(
            ""expected token but found '{}'"".format(value))
    token, value = get_ttext(value)
    mtoken.append(token)
    if value and value[0] in CFWS_LEADER:
        token, value = get_cfws(value)
        mtoken.append(token)
    return mtoken, value","def get_cfws(value):
    """"""CFWS = (1*([FWS] comment) [FWS]) / FWS

    """"""
    cfws = CFWSList()
    while value and value[0] in CFWS_LEADER:
        if value[0] in WSP:
            token, value = get_fws(value)
        else:
            token, value = get_comment(value)
        cfws.append(token)
    return cfws, value

def get_ttext(value):
    """"""ttext = <matches _ttext_matcher>

    We allow any non-TOKEN_ENDS in ttext, but add defects to the token's
    defects list if we find non-ttext characters.  We also register defects for
    *any* non-printables even though the RFC doesn't exclude all of them,
    because we follow the spirit of RFC 5322.

    """"""
    m = _non_token_end_matcher(value)
    if not m:
        raise errors.HeaderParseError(
            ""expected ttext but found '{}'"".format(value))
    ttext = m.group()
    value = value[len(ttext):]
    ttext = ValueTerminal(ttext, 'ttext')
    _validate_xtext(ttext)
    return ttext, value","def parse_cfws_from_value(value):
    """"""CFWS = (1*([FWS] comment) [FWS]) / FWS

    """"""
    cfws = CFWSList()
    while value and value[0] in CFWS_LEADER:
        if value[0] in WSP:
            token, value = get_fws(value)
        else:
            token, value = get_comment(value)
        cfws.append(token)
    return cfws, value

def parse_ttext_from_value(value):
    """"""ttext = <matches _ttext_matcher>

    We allow any non-TOKEN_ENDS in ttext, but add defects to the token's
    defects list if we find non-ttext characters.  We also register defects for
    *any* non-printables even though the RFC doesn't exclude all of them,
    because we follow the spirit of RFC 5322.

    """"""
    m = _non_token_end_matcher(value)
    if not m:
        raise errors.HeaderParseError(
            ""expected ttext but found '{}'"".format(value))
    ttext = m.group()
    value = value[len(ttext):]
    ttext = ValueTerminal(ttext, 'ttext')
    _validate_xtext(ttext)
    return ttext, value","def token_equals_optional_CFWS_one_ttext_optional_CFWS(value):
    """"""token = [CFWS] 1*ttext [CFWS]

    The RFC equivalent of ttext is any US-ASCII chars except space, ctls, or
    tspecials.  We also exclude tabs even though the RFC doesn't.

    The RFC implies the CFWS but is not explicit about it in the BNF.

    """"""
    mtoken = Token()
    if value and value[0] in CFWS_LEADER:
        token, value = parse_cfws_from_value(value)
        mtoken.append(token)
    if value and value[0] in TOKEN_ENDS:
        raise errors.HeaderParseError(
            ""expected token but found '{}'"".format(value))
    token, value = parse_ttext_from_value(value)
    mtoken.append(token)
    if value and value[0] in CFWS_LEADER:
        token, value = parse_cfws_from_value(value)
        mtoken.append(token)
    return mtoken, value","{""get_cfws"": ""parse_cfws_from_value"", ""get_ttext"": ""parse_ttext_from_value""}","def 0(value):
    """"""CFWS = (1*([FWS] comment) [FWS]) / FWS

    """"""
    cfws = CFWSList()
    while value and value[0] in CFWS_LEADER:
        if value[0] in WSP:
            token, value = get_fws(value)
        else:
            token, value = get_comment(value)
        cfws.append(token)
    return cfws, value

def 1(value):
    """"""ttext = <matches _ttext_matcher>

    We allow any non-TOKEN_ENDS in ttext, but add defects to the token's
    defects list if we find non-ttext characters.  We also register defects for
    *any* non-printables even though the RFC doesn't exclude all of them,
    because we follow the spirit of RFC 5322.

    """"""
    m = _non_token_end_matcher(value)
    if not m:
        raise errors.HeaderParseError(
            ""expected ttext but found '{}'"".format(value))
    ttext = m.group()
    value = value[len(ttext):]
    ttext = ValueTerminal(ttext, 'ttext')
    _validate_xtext(ttext)
    return ttext, value","def 2(value):
    """"""token = [CFWS] 1*ttext [CFWS]

    The RFC equivalent of ttext is any US-ASCII chars except space, ctls, or
    tspecials.  We also exclude tabs even though the RFC doesn't.

    The RFC implies the CFWS but is not explicit about it in the BNF.

    """"""
    mtoken = Token()
    if value and value[0] in CFWS_LEADER:
        token, value = 0(value)
        mtoken.append(token)
    if value and value[0] in TOKEN_ENDS:
        raise errors.HeaderParseError(
            ""expected token but found '{}'"".format(value))
    token, value = 1(value)
    mtoken.append(token)
    if value and value[0] in CFWS_LEADER:
        token, value = 0(value)
        mtoken.append(token)
    return mtoken, value","{""get_cfws"": ""0"", ""get_ttext"": ""1""}"
649,649,"def get_attrtext(value):
    """"""attrtext = 1*(any non-ATTRIBUTE_ENDS character)

    We allow any non-ATTRIBUTE_ENDS in attrtext, but add defects to the
    token's defects list if we find non-attrtext characters.  We also register
    defects for *any* non-printables even though the RFC doesn't exclude all of
    them, because we follow the spirit of RFC 5322.

    """"""
    m = _non_attribute_end_matcher(value)
    if not m:
        raise errors.HeaderParseError(
            ""expected attrtext but found {!r}"".format(value))
    attrtext = m.group()
    value = value[len(attrtext):]
    attrtext = ValueTerminal(attrtext, 'attrtext')
    _validate_xtext(attrtext)
    return attrtext, value","def _validate_xtext(xtext):
    """"""If input token contains ASCII non-printables, register a defect.""""""

    non_printables = _non_printable_finder(xtext)
    if non_printables:
        xtext.defects.append(errors.NonPrintableDefect(non_printables))
    if utils._has_surrogates(xtext):
        xtext.defects.append(errors.UndecodableBytesDefect(
            ""Non-ASCII characters found in header token""))","def validate_xtext_input_token(xtext):
    """"""If input token contains ASCII non-printables, register a defect.""""""

    non_printables = _non_printable_finder(xtext)
    if non_printables:
        xtext.defects.append(errors.NonPrintableDefect(non_printables))
    if utils._has_surrogates(xtext):
        xtext.defects.append(errors.UndecodableBytesDefect(
            ""Non-ASCII characters found in header token""))","def parse_attrtext_from_value(value):
    """"""attrtext = 1*(any non-ATTRIBUTE_ENDS character)

    We allow any non-ATTRIBUTE_ENDS in attrtext, but add defects to the
    token's defects list if we find non-attrtext characters.  We also register
    defects for *any* non-printables even though the RFC doesn't exclude all of
    them, because we follow the spirit of RFC 5322.

    """"""
    m = _non_attribute_end_matcher(value)
    if not m:
        raise errors.HeaderParseError(
            ""expected attrtext but found {!r}"".format(value))
    attrtext = m.group()
    value = value[len(attrtext):]
    attrtext = ValueTerminal(attrtext, 'attrtext')
    validate_xtext_input_token(attrtext)
    return attrtext, value","{""_validate_xtext"": ""validate_xtext_input_token""}","def 0(xtext):
    """"""If input token contains ASCII non-printables, register a defect.""""""

    non_printables = _non_printable_finder(xtext)
    if non_printables:
        xtext.defects.append(errors.NonPrintableDefect(non_printables))
    if utils._has_surrogates(xtext):
        xtext.defects.append(errors.UndecodableBytesDefect(
            ""Non-ASCII characters found in header token""))","def 1(value):
    """"""attrtext = 1*(any non-ATTRIBUTE_ENDS character)

    We allow any non-ATTRIBUTE_ENDS in attrtext, but add defects to the
    token's defects list if we find non-attrtext characters.  We also register
    defects for *any* non-printables even though the RFC doesn't exclude all of
    them, because we follow the spirit of RFC 5322.

    """"""
    m = _non_attribute_end_matcher(value)
    if not m:
        raise errors.HeaderParseError(
            ""expected attrtext but found {!r}"".format(value))
    attrtext = m.group()
    value = value[len(attrtext):]
    attrtext = ValueTerminal(attrtext, 'attrtext')
    0(attrtext)
    return attrtext, value","{""_validate_xtext"": ""0""}"
650,650,"def get_attribute(value):
    """""" [CFWS] 1*attrtext [CFWS]

    This version of the BNF makes the CFWS explicit, and as usual we use a
    value terminal for the actual run of characters.  The RFC equivalent of
    attrtext is the token characters, with the subtraction of '*', ""'"", and '%'.
    We include tab in the excluded set just as we do for token.

    """"""
    attribute = Attribute()
    if value and value[0] in CFWS_LEADER:
        token, value = get_cfws(value)
        attribute.append(token)
    if value and value[0] in ATTRIBUTE_ENDS:
        raise errors.HeaderParseError(
            ""expected token but found '{}'"".format(value))
    token, value = get_attrtext(value)
    attribute.append(token)
    if value and value[0] in CFWS_LEADER:
        token, value = get_cfws(value)
        attribute.append(token)
    return attribute, value","def get_cfws(value):
    """"""CFWS = (1*([FWS] comment) [FWS]) / FWS

    """"""
    cfws = CFWSList()
    while value and value[0] in CFWS_LEADER:
        if value[0] in WSP:
            token, value = get_fws(value)
        else:
            token, value = get_comment(value)
        cfws.append(token)
    return cfws, value

def get_attrtext(value):
    """"""attrtext = 1*(any non-ATTRIBUTE_ENDS character)

    We allow any non-ATTRIBUTE_ENDS in attrtext, but add defects to the
    token's defects list if we find non-attrtext characters.  We also register
    defects for *any* non-printables even though the RFC doesn't exclude all of
    them, because we follow the spirit of RFC 5322.

    """"""
    m = _non_attribute_end_matcher(value)
    if not m:
        raise errors.HeaderParseError(
            ""expected attrtext but found {!r}"".format(value))
    attrtext = m.group()
    value = value[len(attrtext):]
    attrtext = ValueTerminal(attrtext, 'attrtext')
    _validate_xtext(attrtext)
    return attrtext, value","def parse_cfws_from_value(value):
    """"""CFWS = (1*([FWS] comment) [FWS]) / FWS

    """"""
    cfws = CFWSList()
    while value and value[0] in CFWS_LEADER:
        if value[0] in WSP:
            token, value = get_fws(value)
        else:
            token, value = get_comment(value)
        cfws.append(token)
    return cfws, value

def parse_attrtext_from_value(value):
    """"""attrtext = 1*(any non-ATTRIBUTE_ENDS character)

    We allow any non-ATTRIBUTE_ENDS in attrtext, but add defects to the
    token's defects list if we find non-attrtext characters.  We also register
    defects for *any* non-printables even though the RFC doesn't exclude all of
    them, because we follow the spirit of RFC 5322.

    """"""
    m = _non_attribute_end_matcher(value)
    if not m:
        raise errors.HeaderParseError(
            ""expected attrtext but found {!r}"".format(value))
    attrtext = m.group()
    value = value[len(attrtext):]
    attrtext = ValueTerminal(attrtext, 'attrtext')
    _validate_xtext(attrtext)
    return attrtext, value","def optional_CFWS_one_or_more_attrtext_optional_CFWS(value):
    """""" [CFWS] 1*attrtext [CFWS]

    This version of the BNF makes the CFWS explicit, and as usual we use a
    value terminal for the actual run of characters.  The RFC equivalent of
    attrtext is the token characters, with the subtraction of '*', ""'"", and '%'.
    We include tab in the excluded set just as we do for token.

    """"""
    attribute = Attribute()
    if value and value[0] in CFWS_LEADER:
        token, value = parse_cfws_from_value(value)
        attribute.append(token)
    if value and value[0] in ATTRIBUTE_ENDS:
        raise errors.HeaderParseError(
            ""expected token but found '{}'"".format(value))
    token, value = parse_attrtext_from_value(value)
    attribute.append(token)
    if value and value[0] in CFWS_LEADER:
        token, value = parse_cfws_from_value(value)
        attribute.append(token)
    return attribute, value","{""get_cfws"": ""parse_cfws_from_value"", ""get_attrtext"": ""parse_attrtext_from_value""}","def 0(value):
    """"""CFWS = (1*([FWS] comment) [FWS]) / FWS

    """"""
    cfws = CFWSList()
    while value and value[0] in CFWS_LEADER:
        if value[0] in WSP:
            token, value = get_fws(value)
        else:
            token, value = get_comment(value)
        cfws.append(token)
    return cfws, value

def 1(value):
    """"""attrtext = 1*(any non-ATTRIBUTE_ENDS character)

    We allow any non-ATTRIBUTE_ENDS in attrtext, but add defects to the
    token's defects list if we find non-attrtext characters.  We also register
    defects for *any* non-printables even though the RFC doesn't exclude all of
    them, because we follow the spirit of RFC 5322.

    """"""
    m = _non_attribute_end_matcher(value)
    if not m:
        raise errors.HeaderParseError(
            ""expected attrtext but found {!r}"".format(value))
    attrtext = m.group()
    value = value[len(attrtext):]
    attrtext = ValueTerminal(attrtext, 'attrtext')
    _validate_xtext(attrtext)
    return attrtext, value","def 2(value):
    """""" [CFWS] 1*attrtext [CFWS]

    This version of the BNF makes the CFWS explicit, and as usual we use a
    value terminal for the actual run of characters.  The RFC equivalent of
    attrtext is the token characters, with the subtraction of '*', ""'"", and '%'.
    We include tab in the excluded set just as we do for token.

    """"""
    attribute = Attribute()
    if value and value[0] in CFWS_LEADER:
        token, value = 0(value)
        attribute.append(token)
    if value and value[0] in ATTRIBUTE_ENDS:
        raise errors.HeaderParseError(
            ""expected token but found '{}'"".format(value))
    token, value = 1(value)
    attribute.append(token)
    if value and value[0] in CFWS_LEADER:
        token, value = 0(value)
        attribute.append(token)
    return attribute, value","{""get_cfws"": ""0"", ""get_attrtext"": ""1""}"
651,651,"def get_extended_attrtext(value):
    """"""attrtext = 1*(any non-ATTRIBUTE_ENDS character plus '%')

    This is a special parsing routine so that we get a value that
    includes % escapes as a single string (which we decode as a single
    string later).

    """"""
    m = _non_extended_attribute_end_matcher(value)
    if not m:
        raise errors.HeaderParseError(
            ""expected extended attrtext but found {!r}"".format(value))
    attrtext = m.group()
    value = value[len(attrtext):]
    attrtext = ValueTerminal(attrtext, 'extended-attrtext')
    _validate_xtext(attrtext)
    return attrtext, value","def _validate_xtext(xtext):
    """"""If input token contains ASCII non-printables, register a defect.""""""

    non_printables = _non_printable_finder(xtext)
    if non_printables:
        xtext.defects.append(errors.NonPrintableDefect(non_printables))
    if utils._has_surrogates(xtext):
        xtext.defects.append(errors.UndecodableBytesDefect(
            ""Non-ASCII characters found in header token""))","def validate_xtext_input_token(xtext):
    """"""If input token contains ASCII non-printables, register a defect.""""""

    non_printables = _non_printable_finder(xtext)
    if non_printables:
        xtext.defects.append(errors.NonPrintableDefect(non_printables))
    if utils._has_surrogates(xtext):
        xtext.defects.append(errors.UndecodableBytesDefect(
            ""Non-ASCII characters found in header token""))","def parse_extended_attrtext(value):
    """"""attrtext = 1*(any non-ATTRIBUTE_ENDS character plus '%')

    This is a special parsing routine so that we get a value that
    includes % escapes as a single string (which we decode as a single
    string later).

    """"""
    m = _non_extended_attribute_end_matcher(value)
    if not m:
        raise errors.HeaderParseError(
            ""expected extended attrtext but found {!r}"".format(value))
    attrtext = m.group()
    value = value[len(attrtext):]
    attrtext = ValueTerminal(attrtext, 'extended-attrtext')
    validate_xtext_input_token(attrtext)
    return attrtext, value","{""_validate_xtext"": ""validate_xtext_input_token""}","def 0(xtext):
    """"""If input token contains ASCII non-printables, register a defect.""""""

    non_printables = _non_printable_finder(xtext)
    if non_printables:
        xtext.defects.append(errors.NonPrintableDefect(non_printables))
    if utils._has_surrogates(xtext):
        xtext.defects.append(errors.UndecodableBytesDefect(
            ""Non-ASCII characters found in header token""))","def 1(value):
    """"""attrtext = 1*(any non-ATTRIBUTE_ENDS character plus '%')

    This is a special parsing routine so that we get a value that
    includes % escapes as a single string (which we decode as a single
    string later).

    """"""
    m = _non_extended_attribute_end_matcher(value)
    if not m:
        raise errors.HeaderParseError(
            ""expected extended attrtext but found {!r}"".format(value))
    attrtext = m.group()
    value = value[len(attrtext):]
    attrtext = ValueTerminal(attrtext, 'extended-attrtext')
    0(attrtext)
    return attrtext, value","{""_validate_xtext"": ""0""}"
652,652,"def get_extended_attribute(value):
    """""" [CFWS] 1*extended_attrtext [CFWS]

    This is like the non-extended version except we allow % characters, so that
    we can pick up an encoded value as a single string.

    """"""
    # XXX: should we have an ExtendedAttribute TokenList?
    attribute = Attribute()
    if value and value[0] in CFWS_LEADER:
        token, value = get_cfws(value)
        attribute.append(token)
    if value and value[0] in EXTENDED_ATTRIBUTE_ENDS:
        raise errors.HeaderParseError(
            ""expected token but found '{}'"".format(value))
    token, value = get_extended_attrtext(value)
    attribute.append(token)
    if value and value[0] in CFWS_LEADER:
        token, value = get_cfws(value)
        attribute.append(token)
    return attribute, value","def get_cfws(value):
    """"""CFWS = (1*([FWS] comment) [FWS]) / FWS

    """"""
    cfws = CFWSList()
    while value and value[0] in CFWS_LEADER:
        if value[0] in WSP:
            token, value = get_fws(value)
        else:
            token, value = get_comment(value)
        cfws.append(token)
    return cfws, value

def get_extended_attrtext(value):
    """"""attrtext = 1*(any non-ATTRIBUTE_ENDS character plus '%')

    This is a special parsing routine so that we get a value that
    includes % escapes as a single string (which we decode as a single
    string later).

    """"""
    m = _non_extended_attribute_end_matcher(value)
    if not m:
        raise errors.HeaderParseError(
            ""expected extended attrtext but found {!r}"".format(value))
    attrtext = m.group()
    value = value[len(attrtext):]
    attrtext = ValueTerminal(attrtext, 'extended-attrtext')
    _validate_xtext(attrtext)
    return attrtext, value","def parse_cfws_from_value(value):
    """"""CFWS = (1*([FWS] comment) [FWS]) / FWS

    """"""
    cfws = CFWSList()
    while value and value[0] in CFWS_LEADER:
        if value[0] in WSP:
            token, value = get_fws(value)
        else:
            token, value = get_comment(value)
        cfws.append(token)
    return cfws, value

def parse_extended_attrtext(value):
    """"""attrtext = 1*(any non-ATTRIBUTE_ENDS character plus '%')

    This is a special parsing routine so that we get a value that
    includes % escapes as a single string (which we decode as a single
    string later).

    """"""
    m = _non_extended_attribute_end_matcher(value)
    if not m:
        raise errors.HeaderParseError(
            ""expected extended attrtext but found {!r}"".format(value))
    attrtext = m.group()
    value = value[len(attrtext):]
    attrtext = ValueTerminal(attrtext, 'extended-attrtext')
    _validate_xtext(attrtext)
    return attrtext, value","def parse_extended_attribute(value):
    """""" [CFWS] 1*extended_attrtext [CFWS]

    This is like the non-extended version except we allow % characters, so that
    we can pick up an encoded value as a single string.

    """"""
    # XXX: should we have an ExtendedAttribute TokenList?
    attribute = Attribute()
    if value and value[0] in CFWS_LEADER:
        token, value = parse_cfws_from_value(value)
        attribute.append(token)
    if value and value[0] in EXTENDED_ATTRIBUTE_ENDS:
        raise errors.HeaderParseError(
            ""expected token but found '{}'"".format(value))
    token, value = parse_extended_attrtext(value)
    attribute.append(token)
    if value and value[0] in CFWS_LEADER:
        token, value = parse_cfws_from_value(value)
        attribute.append(token)
    return attribute, value","{""get_cfws"": ""parse_cfws_from_value"", ""get_extended_attrtext"": ""parse_extended_attrtext""}","def 0(value):
    """"""CFWS = (1*([FWS] comment) [FWS]) / FWS

    """"""
    cfws = CFWSList()
    while value and value[0] in CFWS_LEADER:
        if value[0] in WSP:
            token, value = get_fws(value)
        else:
            token, value = get_comment(value)
        cfws.append(token)
    return cfws, value

def 1(value):
    """"""attrtext = 1*(any non-ATTRIBUTE_ENDS character plus '%')

    This is a special parsing routine so that we get a value that
    includes % escapes as a single string (which we decode as a single
    string later).

    """"""
    m = _non_extended_attribute_end_matcher(value)
    if not m:
        raise errors.HeaderParseError(
            ""expected extended attrtext but found {!r}"".format(value))
    attrtext = m.group()
    value = value[len(attrtext):]
    attrtext = ValueTerminal(attrtext, 'extended-attrtext')
    _validate_xtext(attrtext)
    return attrtext, value","def 2(value):
    """""" [CFWS] 1*extended_attrtext [CFWS]

    This is like the non-extended version except we allow % characters, so that
    we can pick up an encoded value as a single string.

    """"""
    # XXX: should we have an ExtendedAttribute TokenList?
    attribute = Attribute()
    if value and value[0] in CFWS_LEADER:
        token, value = 0(value)
        attribute.append(token)
    if value and value[0] in EXTENDED_ATTRIBUTE_ENDS:
        raise errors.HeaderParseError(
            ""expected token but found '{}'"".format(value))
    token, value = 1(value)
    attribute.append(token)
    if value and value[0] in CFWS_LEADER:
        token, value = 0(value)
        attribute.append(token)
    return attribute, value","{""get_cfws"": ""0"", ""get_extended_attrtext"": ""1""}"
653,653,"def get_value(value):
    """""" quoted-string / attribute

    """"""
    v = Value()
    if not value:
        raise errors.HeaderParseError(""Expected value but found end of string"")
    leader = None
    if value[0] in CFWS_LEADER:
        leader, value = get_cfws(value)
    if not value:
        raise errors.HeaderParseError(""Expected value but found ""
                                      ""only {}"".format(leader))
    if value[0] == '""':
        token, value = get_quoted_string(value)
    else:
        token, value = get_extended_attribute(value)
    if leader is not None:
        token[:0] = [leader]
    v.append(token)
    return v, value","def get_extended_attribute(value):
    """""" [CFWS] 1*extended_attrtext [CFWS]

    This is like the non-extended version except we allow % characters, so that
    we can pick up an encoded value as a single string.

    """"""
    # XXX: should we have an ExtendedAttribute TokenList?
    attribute = Attribute()
    if value and value[0] in CFWS_LEADER:
        token, value = get_cfws(value)
        attribute.append(token)
    if value and value[0] in EXTENDED_ATTRIBUTE_ENDS:
        raise errors.HeaderParseError(
            ""expected token but found '{}'"".format(value))
    token, value = get_extended_attrtext(value)
    attribute.append(token)
    if value and value[0] in CFWS_LEADER:
        token, value = get_cfws(value)
        attribute.append(token)
    return attribute, value

def get_quoted_string(value):
    """"""quoted-string = [CFWS] <bare-quoted-string> [CFWS]

    'bare-quoted-string' is an intermediate class defined by this
    parser and not by the RFC grammar.  It is the quoted string
    without any attached CFWS.
    """"""
    quoted_string = QuotedString()
    if value and value[0] in CFWS_LEADER:
        token, value = get_cfws(value)
        quoted_string.append(token)
    token, value = get_bare_quoted_string(value)
    quoted_string.append(token)
    if value and value[0] in CFWS_LEADER:
        token, value = get_cfws(value)
        quoted_string.append(token)
    return quoted_string, value

def get_cfws(value):
    """"""CFWS = (1*([FWS] comment) [FWS]) / FWS

    """"""
    cfws = CFWSList()
    while value and value[0] in CFWS_LEADER:
        if value[0] in WSP:
            token, value = get_fws(value)
        else:
            token, value = get_comment(value)
        cfws.append(token)
    return cfws, value

def Value(typecode_or_type, *args, lock=True, ctx=None):
    '''
    Return a synchronization wrapper for a Value
    '''
    obj = RawValue(typecode_or_type, *args)
    if lock is False:
        return obj
    if lock in (True, None):
        ctx = ctx or get_context()
        lock = ctx.RLock()
    if not hasattr(lock, 'acquire'):
        raise AttributeError(""%r has no method 'acquire'"" % lock)
    return synchronized(obj, lock, ctx=ctx)","def parse_extended_attribute(value):
    """""" [CFWS] 1*extended_attrtext [CFWS]

    This is like the non-extended version except we allow % characters, so that
    we can pick up an encoded value as a single string.

    """"""
    # XXX: should we have an ExtendedAttribute TokenList?
    attribute = Attribute()
    if value and value[0] in CFWS_LEADER:
        token, value = parse_cfws_from_value(value)
        attribute.append(token)
    if value and value[0] in EXTENDED_ATTRIBUTE_ENDS:
        raise errors.HeaderParseError(
            ""expected token but found '{}'"".format(value))
    token, value = get_extended_attrtext(value)
    attribute.append(token)
    if value and value[0] in CFWS_LEADER:
        token, value = parse_cfws_from_value(value)
        attribute.append(token)
    return attribute, value

def parse_quoted_string_from_value(value):
    """"""quoted-string = [CFWS] <bare-quoted-string> [CFWS]

    'bare-quoted-string' is an intermediate class defined by this
    parser and not by the RFC grammar.  It is the quoted string
    without any attached CFWS.
    """"""
    quoted_string = QuotedString()
    if value and value[0] in CFWS_LEADER:
        token, value = parse_cfws_from_value(value)
        quoted_string.append(token)
    token, value = get_bare_quoted_string(value)
    quoted_string.append(token)
    if value and value[0] in CFWS_LEADER:
        token, value = parse_cfws_from_value(value)
        quoted_string.append(token)
    return quoted_string, value

def parse_cfws_from_value(value):
    """"""CFWS = (1*([FWS] comment) [FWS]) / FWS

    """"""
    cfws = CFWSList()
    while value and value[0] in CFWS_LEADER:
        if value[0] in WSP:
            token, value = get_fws(value)
        else:
            token, value = get_comment(value)
        cfws.append(token)
    return cfws, value

def synchronized_value(typecode_or_type, *args, lock=True, ctx=None):
    '''
    Return a synchronization wrapper for a synchronized_value
    '''
    obj = Rawsynchronized_value(typecode_or_type, *args)
    if lock is False:
        return obj
    if lock in (True, None):
        ctx = ctx or get_context()
        lock = ctx.RLock()
    if not hasattr(lock, 'acquire'):
        raise AttributeError(""%r has no method 'acquire'"" % lock)
    return synchronized(obj, lock, ctx=ctx)","def quoted_string_or_attribute(value):
    """""" quoted-string / attribute

    """"""
    v = synchronized_value()
    if not value:
        raise errors.HeaderParseError(""Expected value but found end of string"")
    leader = None
    if value[0] in CFWS_LEADER:
        leader, value = parse_cfws_from_value(value)
    if not value:
        raise errors.HeaderParseError(""Expected value but found ""
                                      ""only {}"".format(leader))
    if value[0] == '""':
        token, value = parse_quoted_string_from_value(value)
    else:
        token, value = parse_extended_attribute(value)
    if leader is not None:
        token[:0] = [leader]
    v.append(token)
    return v, value","{""get_extended_attribute"": ""parse_extended_attribute"", ""get_quoted_string"": ""parse_quoted_string_from_value"", ""get_cfws"": ""parse_cfws_from_value"", ""Value"": ""synchronized_value""}","def 0(value):
    """""" [CFWS] 1*extended_attrtext [CFWS]

    This is like the non-extended version except we allow % characters, so that
    we can pick up an encoded value as a single string.

    """"""
    # XXX: should we have an ExtendedAttribute TokenList?
    attribute = Attribute()
    if value and value[0] in CFWS_LEADER:
        token, value = 2(value)
        attribute.append(token)
    if value and value[0] in EXTENDED_ATTRIBUTE_ENDS:
        raise errors.HeaderParseError(
            ""expected token but found '{}'"".format(value))
    token, value = get_extended_attrtext(value)
    attribute.append(token)
    if value and value[0] in CFWS_LEADER:
        token, value = 2(value)
        attribute.append(token)
    return attribute, value

def 1(value):
    """"""quoted-string = [CFWS] <bare-quoted-string> [CFWS]

    'bare-quoted-string' is an intermediate class defined by this
    parser and not by the RFC grammar.  It is the quoted string
    without any attached CFWS.
    """"""
    quoted_string = QuotedString()
    if value and value[0] in CFWS_LEADER:
        token, value = 2(value)
        quoted_string.append(token)
    token, value = get_bare_quoted_string(value)
    quoted_string.append(token)
    if value and value[0] in CFWS_LEADER:
        token, value = 2(value)
        quoted_string.append(token)
    return quoted_string, value

def 2(value):
    """"""CFWS = (1*([FWS] comment) [FWS]) / FWS

    """"""
    cfws = CFWSList()
    while value and value[0] in CFWS_LEADER:
        if value[0] in WSP:
            token, value = get_fws(value)
        else:
            token, value = get_comment(value)
        cfws.append(token)
    return cfws, value

def 3(typecode_or_type, *args, lock=True, ctx=None):
    '''
    Return a synchronization wrapper for a Value
    '''
    obj = Raw3(typecode_or_type, *args)
    if lock is False:
        return obj
    if lock in (True, None):
        ctx = ctx or get_context()
        lock = ctx.RLock()
    if not hasattr(lock, 'acquire'):
        raise AttributeError(""%r has no method 'acquire'"" % lock)
    return synchronized(obj, lock, ctx=ctx)","def 4(value):
    """""" quoted-string / attribute

    """"""
    v = 3()
    if not value:
        raise errors.HeaderParseError(""Expected value but found end of string"")
    leader = None
    if value[0] in CFWS_LEADER:
        leader, value = 2(value)
    if not value:
        raise errors.HeaderParseError(""Expected value but found ""
                                      ""only {}"".format(leader))
    if value[0] == '""':
        token, value = 1(value)
    else:
        token, value = 0(value)
    if leader is not None:
        token[:0] = [leader]
    v.append(token)
    return v, value","{""get_extended_attribute"": ""0"", ""get_quoted_string"": ""1"", ""get_cfws"": ""2"", ""Value"": ""3""}"
654,654,"def _find_mime_parameters(tokenlist, value):
    """"""Do our best to find the parameters in an invalid MIME header

    """"""
    while value and value[0] != ';':
        if value[0] in PHRASE_ENDS:
            tokenlist.append(ValueTerminal(value[0], 'misplaced-special'))
            value = value[1:]
        else:
            token, value = get_phrase(value)
            tokenlist.append(token)
    if not value:
        return
    tokenlist.append(ValueTerminal(';', 'parameter-separator'))
    tokenlist.append(parse_mime_parameters(value[1:]))","def get_phrase(value):
    """""" phrase = 1*word / obs-phrase
        obs-phrase = word *(word / ""."" / CFWS)

    This means a phrase can be a sequence of words, periods, and CFWS in any
    order as long as it starts with at least one word.  If anything other than
    words is detected, an ObsoleteHeaderDefect is added to the token's defect
    list.  We also accept a phrase that starts with CFWS followed by a dot;
    this is registered as an InvalidHeaderDefect, since it is not supported by
    even the obsolete grammar.

    """"""
    phrase = Phrase()
    try:
        token, value = get_word(value)
        phrase.append(token)
    except errors.HeaderParseError:
        phrase.defects.append(errors.InvalidHeaderDefect(
            ""phrase does not start with word""))
    while value and value[0] not in PHRASE_ENDS:
        if value[0]=='.':
            phrase.append(DOT)
            phrase.defects.append(errors.ObsoleteHeaderDefect(
                ""period in 'phrase'""))
            value = value[1:]
        else:
            try:
                token, value = get_word(value)
            except errors.HeaderParseError:
                if value[0] in CFWS_LEADER:
                    token, value = get_cfws(value)
                    phrase.defects.append(errors.ObsoleteHeaderDefect(
                        ""comment found without atom""))
                else:
                    raise
            phrase.append(token)
    return phrase, value

def parse_mime_parameters(value):
    """""" parameter *( "";"" parameter )

    That BNF is meant to indicate this routine should only be called after
    finding and handling the leading ';'.  There is no corresponding rule in
    the formal RFC grammar, but it is more convenient for us for the set of
    parameters to be treated as its own TokenList.

    This is 'parse' routine because it consumes the remaining value, but it
    would never be called to parse a full header.  Instead it is called to
    parse everything after the non-parameter value of a specific MIME header.

    """"""
    mime_parameters = MimeParameters()
    while value:
        try:
            token, value = get_parameter(value)
            mime_parameters.append(token)
        except errors.HeaderParseError as err:
            leader = None
            if value[0] in CFWS_LEADER:
                leader, value = get_cfws(value)
            if not value:
                mime_parameters.append(leader)
                return mime_parameters
            if value[0] == ';':
                if leader is not None:
                    mime_parameters.append(leader)
                mime_parameters.defects.append(errors.InvalidHeaderDefect(
                    ""parameter entry with no content""))
            else:
                token, value = get_invalid_parameter(value)
                if leader:
                    token[:0] = [leader]
                mime_parameters.append(token)
                mime_parameters.defects.append(errors.InvalidHeaderDefect(
                    ""invalid parameter {!r}"".format(token)))
        if value and value[0] != ';':
            # Junk after the otherwise valid parameter.  Mark it as
            # invalid, but it will have a value.
            param = mime_parameters[-1]
            param.token_type = 'invalid-parameter'
            token, value = get_invalid_parameter(value)
            param.extend(token)
            mime_parameters.defects.append(errors.InvalidHeaderDefect(
                ""parameter with invalid trailing text {!r}"".format(token)))
        if value:
            # Must be a ';' at this point.
            mime_parameters.append(ValueTerminal(';', 'parameter-separator'))
            value = value[1:]
    return mime_parameters","def parse_phrase_from_value(value):
    """""" phrase = 1*word / obs-phrase
        obs-phrase = word *(word / ""."" / CFWS)

    This means a phrase can be a sequence of words, periods, and CFWS in any
    order as long as it starts with at least one word.  If anything other than
    words is detected, an ObsoleteHeaderDefect is added to the token's defect
    list.  We also accept a phrase that starts with CFWS followed by a dot;
    this is registered as an InvalidHeaderDefect, since it is not supported by
    even the obsolete grammar.

    """"""
    phrase = Phrase()
    try:
        token, value = get_word(value)
        phrase.append(token)
    except errors.HeaderParseError:
        phrase.defects.append(errors.InvalidHeaderDefect(
            ""phrase does not start with word""))
    while value and value[0] not in PHRASE_ENDS:
        if value[0]=='.':
            phrase.append(DOT)
            phrase.defects.append(errors.ObsoleteHeaderDefect(
                ""period in 'phrase'""))
            value = value[1:]
        else:
            try:
                token, value = get_word(value)
            except errors.HeaderParseError:
                if value[0] in CFWS_LEADER:
                    token, value = get_cfws(value)
                    phrase.defects.append(errors.ObsoleteHeaderDefect(
                        ""comment found without atom""))
                else:
                    raise
            phrase.append(token)
    return phrase, value

def parse_mime_parameters_from_value(value):
    """""" parameter *( "";"" parameter )

    That BNF is meant to indicate this routine should only be called after
    finding and handling the leading ';'.  There is no corresponding rule in
    the formal RFC grammar, but it is more convenient for us for the set of
    parameters to be treated as its own TokenList.

    This is 'parse' routine because it consumes the remaining value, but it
    would never be called to parse a full header.  Instead it is called to
    parse everything after the non-parameter value of a specific MIME header.

    """"""
    mime_parameters = MimeParameters()
    while value:
        try:
            token, value = get_parameter(value)
            mime_parameters.append(token)
        except errors.HeaderParseError as err:
            leader = None
            if value[0] in CFWS_LEADER:
                leader, value = get_cfws(value)
            if not value:
                mime_parameters.append(leader)
                return mime_parameters
            if value[0] == ';':
                if leader is not None:
                    mime_parameters.append(leader)
                mime_parameters.defects.append(errors.InvalidHeaderDefect(
                    ""parameter entry with no content""))
            else:
                token, value = get_invalid_parameter(value)
                if leader:
                    token[:0] = [leader]
                mime_parameters.append(token)
                mime_parameters.defects.append(errors.InvalidHeaderDefect(
                    ""invalid parameter {!r}"".format(token)))
        if value and value[0] != ';':
            # Junk after the otherwise valid parameter.  Mark it as
            # invalid, but it will have a value.
            param = mime_parameters[-1]
            param.token_type = 'invalid-parameter'
            token, value = get_invalid_parameter(value)
            param.extend(token)
            mime_parameters.defects.append(errors.InvalidHeaderDefect(
                ""parameter with invalid trailing text {!r}"".format(token)))
        if value:
            # Must be a ';' at this point.
            mime_parameters.append(ValueTerminal(';', 'parameter-separator'))
            value = value[1:]
    return mime_parameters","def do_our_best_to_find_parameters_in_invalid_MIME_header(tokenlist, value):
    """"""Do our best to find the parameters in an invalid MIME header

    """"""
    while value and value[0] != ';':
        if value[0] in PHRASE_ENDS:
            tokenlist.append(ValueTerminal(value[0], 'misplaced-special'))
            value = value[1:]
        else:
            token, value = parse_phrase_from_value(value)
            tokenlist.append(token)
    if not value:
        return
    tokenlist.append(ValueTerminal(';', 'parameter-separator'))
    tokenlist.append(parse_mime_parameters_from_value(value[1:]))","{""get_phrase"": ""parse_phrase_from_value"", ""parse_mime_parameters"": ""parse_mime_parameters_from_value""}","def 0(value):
    """""" phrase = 1*word / obs-phrase
        obs-phrase = word *(word / ""."" / CFWS)

    This means a phrase can be a sequence of words, periods, and CFWS in any
    order as long as it starts with at least one word.  If anything other than
    words is detected, an ObsoleteHeaderDefect is added to the token's defect
    list.  We also accept a phrase that starts with CFWS followed by a dot;
    this is registered as an InvalidHeaderDefect, since it is not supported by
    even the obsolete grammar.

    """"""
    phrase = Phrase()
    try:
        token, value = get_word(value)
        phrase.append(token)
    except errors.HeaderParseError:
        phrase.defects.append(errors.InvalidHeaderDefect(
            ""phrase does not start with word""))
    while value and value[0] not in PHRASE_ENDS:
        if value[0]=='.':
            phrase.append(DOT)
            phrase.defects.append(errors.ObsoleteHeaderDefect(
                ""period in 'phrase'""))
            value = value[1:]
        else:
            try:
                token, value = get_word(value)
            except errors.HeaderParseError:
                if value[0] in CFWS_LEADER:
                    token, value = get_cfws(value)
                    phrase.defects.append(errors.ObsoleteHeaderDefect(
                        ""comment found without atom""))
                else:
                    raise
            phrase.append(token)
    return phrase, value

def 1(value):
    """""" parameter *( "";"" parameter )

    That BNF is meant to indicate this routine should only be called after
    finding and handling the leading ';'.  There is no corresponding rule in
    the formal RFC grammar, but it is more convenient for us for the set of
    parameters to be treated as its own TokenList.

    This is 'parse' routine because it consumes the remaining value, but it
    would never be called to parse a full header.  Instead it is called to
    parse everything after the non-parameter value of a specific MIME header.

    """"""
    mime_parameters = MimeParameters()
    while value:
        try:
            token, value = get_parameter(value)
            mime_parameters.append(token)
        except errors.HeaderParseError as err:
            leader = None
            if value[0] in CFWS_LEADER:
                leader, value = get_cfws(value)
            if not value:
                mime_parameters.append(leader)
                return mime_parameters
            if value[0] == ';':
                if leader is not None:
                    mime_parameters.append(leader)
                mime_parameters.defects.append(errors.InvalidHeaderDefect(
                    ""parameter entry with no content""))
            else:
                token, value = get_invalid_parameter(value)
                if leader:
                    token[:0] = [leader]
                mime_parameters.append(token)
                mime_parameters.defects.append(errors.InvalidHeaderDefect(
                    ""invalid parameter {!r}"".format(token)))
        if value and value[0] != ';':
            # Junk after the otherwise valid parameter.  Mark it as
            # invalid, but it will have a value.
            param = mime_parameters[-1]
            param.token_type = 'invalid-parameter'
            token, value = get_invalid_parameter(value)
            param.extend(token)
            mime_parameters.defects.append(errors.InvalidHeaderDefect(
                ""parameter with invalid trailing text {!r}"".format(token)))
        if value:
            # Must be a ';' at this point.
            mime_parameters.append(ValueTerminal(';', 'parameter-separator'))
            value = value[1:]
    return mime_parameters","def 2(tokenlist, value):
    """"""Do our best to find the parameters in an invalid MIME header

    """"""
    while value and value[0] != ';':
        if value[0] in PHRASE_ENDS:
            tokenlist.append(ValueTerminal(value[0], 'misplaced-special'))
            value = value[1:]
        else:
            token, value = 0(value)
            tokenlist.append(token)
    if not value:
        return
    tokenlist.append(ValueTerminal(';', 'parameter-separator'))
    tokenlist.append(1(value[1:]))","{""get_phrase"": ""0"", ""parse_mime_parameters"": ""1""}"
655,655,"def encode_base64(msg):
    """"""Encode the message's payload in Base64.

    Also, add an appropriate Content-Transfer-Encoding header.
    """"""
    orig = msg.get_payload(decode=True)
    encdata = str(_bencode(orig), 'ascii')
    msg.set_payload(encdata)
    msg['Content-Transfer-Encoding'] = 'base64'","def str(val):
    """"""Convert float to string, taking the locale into account.""""""
    return _format(""%.12g"", val)","def locale_aware_float_to_string(val):
    """"""Convert float to locale_aware_float_to_stringing, taking the locale into account.""""""
    return _format(""%.12g"", val)","def encode_message_payload_in_Base64_and_add_Content_Transfer_Encoding_header(msg):
    """"""Encode the message's payload in Base64.

    Also, add an appropriate Content-Transfer-Encoding header.
    """"""
    orig = msg.get_payload(decode=True)
    encdata = locale_aware_float_to_string(_bencode(orig), 'ascii')
    msg.set_payload(encdata)
    msg['Content-Transfer-Encoding'] = 'base64'","{""str"": ""locale_aware_float_to_string""}","def 0(val):
    """"""Convert float to string, taking the locale into account.""""""
    return _format(""%.12g"", val)","def 1(msg):
    """"""Encode the message's payload in Base64.

    Also, add an appropriate Content-Transfer-Encoding header.
    """"""
    orig = msg.get_payload(decode=True)
    encdata = 0(_bencode(orig), 'ascii')
    msg.set_payload(encdata)
    msg['Content-Transfer-Encoding'] = 'base64'","{""str"": ""0""}"
656,656,"def encode_quopri(msg):
    """"""Encode the message's payload in quoted-printable.

    Also, add an appropriate Content-Transfer-Encoding header.
    """"""
    orig = msg.get_payload(decode=True)
    encdata = _qencode(orig)
    msg.set_payload(encdata)
    msg['Content-Transfer-Encoding'] = 'quoted-printable'","def _qencode(s):
    enc = _encodestring(s, quotetabs=True)
    # Must encode spaces, which quopri.encodestring() doesn't do
    return enc.replace(b' ', b'=20')","def quopri_encode(s):
    enc = _encodestring(s, quotetabs=True)
    # Must encode spaces, which quopri.encodestring() doesn't do
    return enc.replace(b' ', b'=20')","def encode_message_payload_in_quoted_printable_and_add_Content_Transfer_Encoding_header(msg):
    """"""Encode the message's payload in quoted-printable.

    Also, add an appropriate Content-Transfer-Encoding header.
    """"""
    orig = msg.get_payload(decode=True)
    encdata = quopri_encode(orig)
    msg.set_payload(encdata)
    msg['Content-Transfer-Encoding'] = 'quoted-printable'","{""_qencode"": ""quopri_encode""}","def 0(s):
    enc = _encodestring(s, quotetabs=True)
    # Must encode spaces, which quopri.encodestring() doesn't do
    return enc.replace(b' ', b'=20')","def 1(msg):
    """"""Encode the message's payload in quoted-printable.

    Also, add an appropriate Content-Transfer-Encoding header.
    """"""
    orig = msg.get_payload(decode=True)
    encdata = 0(orig)
    msg.set_payload(encdata)
    msg['Content-Transfer-Encoding'] = 'quoted-printable'","{""_qencode"": ""0""}"
657,657,"def getaddresses(fieldvalues):
    """"""Return a list of (REALNAME, EMAIL) for each fieldvalue.""""""
    all = COMMASPACE.join(str(v) for v in fieldvalues)
    a = _AddressList(all)
    return a.addresslist","def str(val):
    """"""Convert float to string, taking the locale into account.""""""
    return _format(""%.12g"", val)","def locale_aware_float_to_string(val):
    """"""Convert float to locale_aware_float_to_stringing, taking the locale into account.""""""
    return _format(""%.12g"", val)","def return_list_of_REALNAME_EMAIL_for_each_fieldvalue(fieldvalues):
    """"""Return a list of (REALNAME, EMAIL) for each fieldvalue.""""""
    all = COMMASPACE.join(locale_aware_float_to_string(v) for v in fieldvalues)
    a = _AddressList(all)
    return a.addresslist","{""str"": ""locale_aware_float_to_string""}","def 0(val):
    """"""Convert float to string, taking the locale into account.""""""
    return _format(""%.12g"", val)","def 1(fieldvalues):
    """"""Return a list of (REALNAME, EMAIL) for each fieldvalue.""""""
    all = COMMASPACE.join(0(v) for v in fieldvalues)
    a = _AddressList(all)
    return a.addresslist","{""str"": ""0""}"
658,658,"def format_datetime(dt, usegmt=False):
    """"""Turn a datetime into a date string as specified in RFC 2822.

    If usegmt is True, dt must be an aware datetime with an offset of zero.  In
    this case 'GMT' will be rendered instead of the normal +0000 required by
    RFC2822.  This is to support HTTP headers involving date stamps.
    """"""
    now = dt.timetuple()
    if usegmt:
        if dt.tzinfo is None or dt.tzinfo != datetime.timezone.utc:
            raise ValueError(""usegmt option requires a UTC datetime"")
        zone = 'GMT'
    elif dt.tzinfo is None:
        zone = '-0000'
    else:
        zone = dt.strftime(""%z"")
    return _format_timetuple_and_zone(now, zone)","def _format_timetuple_and_zone(timetuple, zone):
    return '%s, %02d %s %04d %02d:%02d:%02d %s' % (
        ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'][timetuple[6]],
        timetuple[2],
        ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',
         'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'][timetuple[1] - 1],
        timetuple[0], timetuple[3], timetuple[4], timetuple[5],
        zone)","def format_timetuple_and_zone(timetuple, zone):
    return '%s, %02d %s %04d %02d:%02d:%02d %s' % (
        ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'][timetuple[6]],
        timetuple[2],
        ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',
         'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'][timetuple[1] - 1],
        timetuple[0], timetuple[3], timetuple[4], timetuple[5],
        zone)","def turn_datetime_into_date_string_as_specified_in_RFC_2822(dt, usegmt=False):
    """"""Turn a datetime into a date string as specified in RFC 2822.

    If usegmt is True, dt must be an aware datetime with an offset of zero.  In
    this case 'GMT' will be rendered instead of the normal +0000 required by
    RFC2822.  This is to support HTTP headers involving date stamps.
    """"""
    now = dt.timetuple()
    if usegmt:
        if dt.tzinfo is None or dt.tzinfo != datetime.timezone.utc:
            raise ValueError(""usegmt option requires a UTC datetime"")
        zone = 'GMT'
    elif dt.tzinfo is None:
        zone = '-0000'
    else:
        zone = dt.strftime(""%z"")
    return format_timetuple_and_zone(now, zone)","{""_format_timetuple_and_zone"": ""format_timetuple_and_zone""}","def 0(timetuple, zone):
    return '%s, %02d %s %04d %02d:%02d:%02d %s' % (
        ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'][timetuple[6]],
        timetuple[2],
        ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',
         'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'][timetuple[1] - 1],
        timetuple[0], timetuple[3], timetuple[4], timetuple[5],
        zone)","def 1(dt, usegmt=False):
    """"""Turn a datetime into a date string as specified in RFC 2822.

    If usegmt is True, dt must be an aware datetime with an offset of zero.  In
    this case 'GMT' will be rendered instead of the normal +0000 required by
    RFC2822.  This is to support HTTP headers involving date stamps.
    """"""
    now = dt.timetuple()
    if usegmt:
        if dt.tzinfo is None or dt.tzinfo != datetime.timezone.utc:
            raise ValueError(""usegmt option requires a UTC datetime"")
        zone = 'GMT'
    elif dt.tzinfo is None:
        zone = '-0000'
    else:
        zone = dt.strftime(""%z"")
    return 0(now, zone)","{""_format_timetuple_and_zone"": ""0""}"
659,659,"def parsedate_tz(data):
    """"""Convert a date string to a time tuple.

    Accounts for military timezones.
    """"""
    res = _parsedate_tz(data)
    if not res:
        return
    if res[9] is None:
        res[9] = 0
    return tuple(res)","def _parsedate_tz(data):
    """"""Convert date to extended time tuple.

    The last (additional) element is the time zone offset in seconds, except if
    the timezone was specified as -0000.  In that case the last element is
    None.  This indicates a UTC timestamp that explicitly declaims knowledge of
    the source timezone, as opposed to a +0000 timestamp that indicates the
    source timezone really was UTC.

    """"""
    if not data:
        return None
    data = data.split()
    if not data:  # This happens for whitespace-only input.
        return None
    # The FWS after the comma after the day-of-week is optional, so search and
    # adjust for this.
    if data[0].endswith(',') or data[0].lower() in _daynames:
        # There's a dayname here. Skip it
        del data[0]
    else:
        i = data[0].rfind(',')
        if i >= 0:
            data[0] = data[0][i+1:]
    if len(data) == 3: # RFC 850 date, deprecated
        stuff = data[0].split('-')
        if len(stuff) == 3:
            data = stuff + data[1:]
    if len(data) == 4:
        s = data[3]
        i = s.find('+')
        if i == -1:
            i = s.find('-')
        if i > 0:
            data[3:] = [s[:i], s[i:]]
        else:
            data.append('') # Dummy tz
    if len(data) < 5:
        return None
    data = data[:5]
    [dd, mm, yy, tm, tz] = data
    mm = mm.lower()
    if mm not in _monthnames:
        dd, mm = mm, dd.lower()
        if mm not in _monthnames:
            return None
    mm = _monthnames.index(mm) + 1
    if mm > 12:
        mm -= 12
    if dd[-1] == ',':
        dd = dd[:-1]
    i = yy.find(':')
    if i > 0:
        yy, tm = tm, yy
    if yy[-1] == ',':
        yy = yy[:-1]
    if not yy[0].isdigit():
        yy, tz = tz, yy
    if tm[-1] == ',':
        tm = tm[:-1]
    tm = tm.split(':')
    if len(tm) == 2:
        [thh, tmm] = tm
        tss = '0'
    elif len(tm) == 3:
        [thh, tmm, tss] = tm
    elif len(tm) == 1 and '.' in tm[0]:
        # Some non-compliant MUAs use '.' to separate time elements.
        tm = tm[0].split('.')
        if len(tm) == 2:
            [thh, tmm] = tm
            tss = 0
        elif len(tm) == 3:
            [thh, tmm, tss] = tm
        else:
            return None
    else:
        return None
    try:
        yy = int(yy)
        dd = int(dd)
        thh = int(thh)
        tmm = int(tmm)
        tss = int(tss)
    except ValueError:
        return None
    # Check for a yy specified in two-digit format, then convert it to the
    # appropriate four-digit format, according to the POSIX standard. RFC 822
    # calls for a two-digit yy, but RFC 2822 (which obsoletes RFC 822)
    # mandates a 4-digit yy. For more information, see the documentation for
    # the time module.
    if yy < 100:
        # The year is between 1969 and 1999 (inclusive).
        if yy > 68:
            yy += 1900
        # The year is between 2000 and 2068 (inclusive).
        else:
            yy += 2000
    tzoffset = None
    tz = tz.upper()
    if tz in _timezones:
        tzoffset = _timezones[tz]
    else:
        try:
            tzoffset = int(tz)
        except ValueError:
            pass
        if tzoffset==0 and tz.startswith('-'):
            tzoffset = None
    # Convert a timezone offset into seconds ; -0500 -> -18000
    if tzoffset:
        if tzoffset < 0:
            tzsign = -1
            tzoffset = -tzoffset
        else:
            tzsign = 1
        tzoffset = tzsign * ( (tzoffset//100)*3600 + (tzoffset % 100)*60)
    # Daylight Saving Time flag is set to -1, since DST is unknown.
    return [yy, mm, dd, thh, tmm, tss, 0, 1, -1, tzoffset]","def parse_date_with_timezone(data):
    """"""Convert date to extended time tuple.

    The last (additional) element is the time zone offset in seconds, except if
    the timezone was specified as -0000.  In that case the last element is
    None.  This indicates a UTC timestamp that explicitly declaims knowledge of
    the source timezone, as opposed to a +0000 timestamp that indicates the
    source timezone really was UTC.

    """"""
    if not data:
        return None
    data = data.split()
    if not data:  # This happens for whitespace-only input.
        return None
    # The FWS after the comma after the day-of-week is optional, so search and
    # adjust for this.
    if data[0].endswith(',') or data[0].lower() in _daynames:
        # There's a dayname here. Skip it
        del data[0]
    else:
        i = data[0].rfind(',')
        if i >= 0:
            data[0] = data[0][i+1:]
    if len(data) == 3: # RFC 850 date, deprecated
        stuff = data[0].split('-')
        if len(stuff) == 3:
            data = stuff + data[1:]
    if len(data) == 4:
        s = data[3]
        i = s.find('+')
        if i == -1:
            i = s.find('-')
        if i > 0:
            data[3:] = [s[:i], s[i:]]
        else:
            data.append('') # Dummy tz
    if len(data) < 5:
        return None
    data = data[:5]
    [dd, mm, yy, tm, tz] = data
    mm = mm.lower()
    if mm not in _monthnames:
        dd, mm = mm, dd.lower()
        if mm not in _monthnames:
            return None
    mm = _monthnames.index(mm) + 1
    if mm > 12:
        mm -= 12
    if dd[-1] == ',':
        dd = dd[:-1]
    i = yy.find(':')
    if i > 0:
        yy, tm = tm, yy
    if yy[-1] == ',':
        yy = yy[:-1]
    if not yy[0].isdigit():
        yy, tz = tz, yy
    if tm[-1] == ',':
        tm = tm[:-1]
    tm = tm.split(':')
    if len(tm) == 2:
        [thh, tmm] = tm
        tss = '0'
    elif len(tm) == 3:
        [thh, tmm, tss] = tm
    elif len(tm) == 1 and '.' in tm[0]:
        # Some non-compliant MUAs use '.' to separate time elements.
        tm = tm[0].split('.')
        if len(tm) == 2:
            [thh, tmm] = tm
            tss = 0
        elif len(tm) == 3:
            [thh, tmm, tss] = tm
        else:
            return None
    else:
        return None
    try:
        yy = int(yy)
        dd = int(dd)
        thh = int(thh)
        tmm = int(tmm)
        tss = int(tss)
    except ValueError:
        return None
    # Check for a yy specified in two-digit format, then convert it to the
    # appropriate four-digit format, according to the POSIX standard. RFC 822
    # calls for a two-digit yy, but RFC 2822 (which obsoletes RFC 822)
    # mandates a 4-digit yy. For more information, see the documentation for
    # the time module.
    if yy < 100:
        # The year is between 1969 and 1999 (inclusive).
        if yy > 68:
            yy += 1900
        # The year is between 2000 and 2068 (inclusive).
        else:
            yy += 2000
    tzoffset = None
    tz = tz.upper()
    if tz in _timezones:
        tzoffset = _timezones[tz]
    else:
        try:
            tzoffset = int(tz)
        except ValueError:
            pass
        if tzoffset==0 and tz.startswith('-'):
            tzoffset = None
    # Convert a timezone offset into seconds ; -0500 -> -18000
    if tzoffset:
        if tzoffset < 0:
            tzsign = -1
            tzoffset = -tzoffset
        else:
            tzsign = 1
        tzoffset = tzsign * ( (tzoffset//100)*3600 + (tzoffset % 100)*60)
    # Daylight Saving Time flag is set to -1, since DST is unknown.
    return [yy, mm, dd, thh, tmm, tss, 0, 1, -1, tzoffset]","def _parsedate_tz(data):
    """"""Convert a date string to a time tuple.

    Accounts for military timezones.
    """"""
    res = parse_date_with_timezone(data)
    if not res:
        return
    if res[9] is None:
        res[9] = 0
    return tuple(res)","{""_parsedate_tz"": ""parse_date_with_timezone""}","def 0(data):
    """"""Convert date to extended time tuple.

    The last (additional) element is the time zone offset in seconds, except if
    the timezone was specified as -0000.  In that case the last element is
    None.  This indicates a UTC timestamp that explicitly declaims knowledge of
    the source timezone, as opposed to a +0000 timestamp that indicates the
    source timezone really was UTC.

    """"""
    if not data:
        return None
    data = data.split()
    if not data:  # This happens for whitespace-only input.
        return None
    # The FWS after the comma after the day-of-week is optional, so search and
    # adjust for this.
    if data[0].endswith(',') or data[0].lower() in _daynames:
        # There's a dayname here. Skip it
        del data[0]
    else:
        i = data[0].rfind(',')
        if i >= 0:
            data[0] = data[0][i+1:]
    if len(data) == 3: # RFC 850 date, deprecated
        stuff = data[0].split('-')
        if len(stuff) == 3:
            data = stuff + data[1:]
    if len(data) == 4:
        s = data[3]
        i = s.find('+')
        if i == -1:
            i = s.find('-')
        if i > 0:
            data[3:] = [s[:i], s[i:]]
        else:
            data.append('') # Dummy tz
    if len(data) < 5:
        return None
    data = data[:5]
    [dd, mm, yy, tm, tz] = data
    mm = mm.lower()
    if mm not in _monthnames:
        dd, mm = mm, dd.lower()
        if mm not in _monthnames:
            return None
    mm = _monthnames.index(mm) + 1
    if mm > 12:
        mm -= 12
    if dd[-1] == ',':
        dd = dd[:-1]
    i = yy.find(':')
    if i > 0:
        yy, tm = tm, yy
    if yy[-1] == ',':
        yy = yy[:-1]
    if not yy[0].isdigit():
        yy, tz = tz, yy
    if tm[-1] == ',':
        tm = tm[:-1]
    tm = tm.split(':')
    if len(tm) == 2:
        [thh, tmm] = tm
        tss = '0'
    elif len(tm) == 3:
        [thh, tmm, tss] = tm
    elif len(tm) == 1 and '.' in tm[0]:
        # Some non-compliant MUAs use '.' to separate time elements.
        tm = tm[0].split('.')
        if len(tm) == 2:
            [thh, tmm] = tm
            tss = 0
        elif len(tm) == 3:
            [thh, tmm, tss] = tm
        else:
            return None
    else:
        return None
    try:
        yy = int(yy)
        dd = int(dd)
        thh = int(thh)
        tmm = int(tmm)
        tss = int(tss)
    except ValueError:
        return None
    # Check for a yy specified in two-digit format, then convert it to the
    # appropriate four-digit format, according to the POSIX standard. RFC 822
    # calls for a two-digit yy, but RFC 2822 (which obsoletes RFC 822)
    # mandates a 4-digit yy. For more information, see the documentation for
    # the time module.
    if yy < 100:
        # The year is between 1969 and 1999 (inclusive).
        if yy > 68:
            yy += 1900
        # The year is between 2000 and 2068 (inclusive).
        else:
            yy += 2000
    tzoffset = None
    tz = tz.upper()
    if tz in _timezones:
        tzoffset = _timezones[tz]
    else:
        try:
            tzoffset = int(tz)
        except ValueError:
            pass
        if tzoffset==0 and tz.startswith('-'):
            tzoffset = None
    # Convert a timezone offset into seconds ; -0500 -> -18000
    if tzoffset:
        if tzoffset < 0:
            tzsign = -1
            tzoffset = -tzoffset
        else:
            tzsign = 1
        tzoffset = tzsign * ( (tzoffset//100)*3600 + (tzoffset % 100)*60)
    # Daylight Saving Time flag is set to -1, since DST is unknown.
    return [yy, mm, dd, thh, tmm, tss, 0, 1, -1, tzoffset]","def 1(data):
    """"""Convert a date string to a time tuple.

    Accounts for military timezones.
    """"""
    res = 0(data)
    if not res:
        return
    if res[9] is None:
        res[9] = 0
    return tuple(res)","{""_parsedate_tz"": ""0""}"
660,660,"def parsedate(data):
    """"""Convert a time string to a time tuple.""""""
    t = parsedate_tz(data)
    if isinstance(t, tuple):
        return t[:9]
    else:
        return t","def parsedate_tz(data):
    """"""Convert a date string to a time tuple.

    Accounts for military timezones.
    """"""
    res = _parsedate_tz(data)
    if not res:
        return
    if res[9] is None:
        res[9] = 0
    return tuple(res)","def _parsedate_tz(data):
    """"""Convert a date string to a time tuple.

    Accounts for military timezones.
    """"""
    res = __parsedate_tz(data)
    if not res:
        return
    if res[9] is None:
        res[9] = 0
    return tuple(res)","def convert_time_string_to_time_tuple(data):
    """"""Convert a time string to a time tuple.""""""
    t = _convert_time_string_to_time_tuple_tz(data)
    if isinstance(t, tuple):
        return t[:9]
    else:
        return t","{""parsedate_tz"": ""_parsedate_tz""}","def 0(data):
    """"""Convert a date string to a time tuple.

    Accounts for military timezones.
    """"""
    res = _0(data)
    if not res:
        return
    if res[9] is None:
        res[9] = 0
    return tuple(res)","def 1(data):
    """"""Convert a time string to a time tuple.""""""
    t = 0(data)
    if isinstance(t, tuple):
        return t[:9]
    else:
        return t","{""parsedate_tz"": ""0""}"
661,661,"def header_encode(header_bytes, charset='iso-8859-1'):
    """"""Encode a single header line with Base64 encoding in a given charset.

    charset names the character set to use to encode the header.  It defaults
    to iso-8859-1.  Base64 encoding is defined in RFC 2045.
    """"""
    if not header_bytes:
        return """"
    if isinstance(header_bytes, str):
        header_bytes = header_bytes.encode(charset)
    encoded = b64encode(header_bytes).decode(""ascii"")
    return '=?%s?b?%s?=' % (charset, encoded)","def b64encode(s, altchars=None):
    """"""Encode the bytes-like object s using Base64 and return a bytes object.

    Optional altchars should be a byte string of length 2 which specifies an
    alternative alphabet for the '+' and '/' characters.  This allows an
    application to e.g. generate url or filesystem safe Base64 strings.
    """"""
    encoded = binascii.b2a_base64(s, newline=False)
    if altchars is not None:
        assert len(altchars) == 2, repr(altchars)
        return encoded.translate(bytes.maketrans(b'+/', altchars))
    return encoded","def encode_base64(s, altchars=None):
    """"""Encode the bytes-like object s using Base64 and return a bytes object.

    Optional altchars should be a byte string of length 2 which specifies an
    alternative alphabet for the '+' and '/' characters.  This allows an
    application to e.g. generate url or filesystem safe Base64 strings.
    """"""
    encoded = binascii.b2a_base64(s, newline=False)
    if altchars is not None:
        assert len(altchars) == 2, repr(altchars)
        return encoded.translate(bytes.maketrans(b'+/', altchars))
    return encoded","def encode_single_header_with_base64_in_given_charset(header_bytes, charset='iso-8859-1'):
    """"""Encode a single header line with Base64 encoding in a given charset.

    charset names the character set to use to encode the header.  It defaults
    to iso-8859-1.  Base64 encoding is defined in RFC 2045.
    """"""
    if not header_bytes:
        return """"
    if isinstance(header_bytes, str):
        header_bytes = header_bytes.encode(charset)
    encoded = encode_base64(header_bytes).decode(""ascii"")
    return '=?%s?b?%s?=' % (charset, encoded)","{""b64encode"": ""encode_base64""}","def 0(s, altchars=None):
    """"""Encode the bytes-like object s using Base64 and return a bytes object.

    Optional altchars should be a byte string of length 2 which specifies an
    alternative alphabet for the '+' and '/' characters.  This allows an
    application to e.g. generate url or filesystem safe Base64 strings.
    """"""
    encoded = binascii.b2a_base64(s, newline=False)
    if altchars is not None:
        assert len(altchars) == 2, repr(altchars)
        return encoded.translate(bytes.maketrans(b'+/', altchars))
    return encoded","def 1(header_bytes, charset='iso-8859-1'):
    """"""Encode a single header line with Base64 encoding in a given charset.

    charset names the character set to use to encode the header.  It defaults
    to iso-8859-1.  Base64 encoding is defined in RFC 2045.
    """"""
    if not header_bytes:
        return """"
    if isinstance(header_bytes, str):
        header_bytes = header_bytes.encode(charset)
    encoded = 0(header_bytes).decode(""ascii"")
    return '=?%s?b?%s?=' % (charset, encoded)","{""b64encode"": ""0""}"
662,662,"def translate(cnf, uniquify=False):
    'Translate a symbolic cnf to a numbered cnf and return a reverse mapping'
    # DIMACS CNF file format:
    # http://people.sc.fsu.edu/~jburkardt/data/cnf/cnf.html
    if uniquify:
        cnf = list(dict.fromkeys(cnf))
    lit2num, num2var = make_translate(cnf)
    numbered_cnf = [tuple([lit2num[lit] for lit in clause]) for clause in cnf]
    return numbered_cnf, num2var","def make_translate(cnf):
    """"""Make translator from symbolic CNF to PycoSat's numbered clauses.
       Return a literal to number dictionary and reverse lookup dict

        >>> make_translate([['~a', 'b', '~c'], ['a', '~c']])
        ({'a': 1, 'c': 3, 'b': 2, '~a': -1, '~b': -2, '~c': -3},
         {1: 'a', 2: 'b', 3: 'c', -1: '~a', -3: '~c', -2: '~b'})
    """"""
    lit2num = {}
    for clause in cnf:
        for literal in clause:
            if literal not in lit2num:
                var = literal[1:] if literal[0] == '~' else literal
                num = len(lit2num) // 2 + 1
                lit2num[intern(var)] = num
                lit2num[intern('~' + var)] = -num
    num2var = {num:lit for lit, num in lit2num.items()}
    return lit2num, num2var","def make_translate(cnf):
    """"""Make translator from symbolic CNF to PycoSat's numbered clauses.
       Return a literal to number dictionary and reverse lookup dict

        >>> make_translate([['~a', 'b', '~c'], ['a', '~c']])
        ({'a': 1, 'c': 3, 'b': 2, '~a': -1, '~b': -2, '~c': -3},
         {1: 'a', 2: 'b', 3: 'c', -1: '~a', -3: '~c', -2: '~b'})
    """"""
    lit2num = {}
    for clause in cnf:
        for literal in clause:
            if literal not in lit2num:
                var = literal[1:] if literal[0] == '~' else literal
                num = len(lit2num) // 2 + 1
                lit2num[intern(var)] = num
                lit2num[intern('~' + var)] = -num
    num2var = {num:lit for lit, num in lit2num.items()}
    return lit2num, num2var","def translate_symbolic_cnf_to_numbered_cnf_and_return_reverse_mapping(cnf, uniquify=False):
    'Translate a symbolic cnf to a numbered cnf and return a reverse mapping'
    # DIMACS CNF file format:
    # http://people.sc.fsu.edu/~jburkardt/data/cnf/cnf.html
    if uniquify:
        cnf = list(dict.fromkeys(cnf))
    lit2num, num2var = make_translate_symbolic_cnf_to_numbered_cnf_and_return_reverse_mapping(cnf)
    numbered_cnf = [tuple([lit2num[lit] for lit in clause]) for clause in cnf]
    return numbered_cnf, num2var","{""make_translate"": ""make_translate""}","def 0(cnf):
    """"""Make translator from symbolic CNF to PycoSat's numbered clauses.
       Return a literal to number dictionary and reverse lookup dict

        >>> 0([['~a', 'b', '~c'], ['a', '~c']])
        ({'a': 1, 'c': 3, 'b': 2, '~a': -1, '~b': -2, '~c': -3},
         {1: 'a', 2: 'b', 3: 'c', -1: '~a', -3: '~c', -2: '~b'})
    """"""
    lit2num = {}
    for clause in cnf:
        for literal in clause:
            if literal not in lit2num:
                var = literal[1:] if literal[0] == '~' else literal
                num = len(lit2num) // 2 + 1
                lit2num[intern(var)] = num
                lit2num[intern('~' + var)] = -num
    num2var = {num:lit for lit, num in lit2num.items()}
    return lit2num, num2var","def 1(cnf, uniquify=False):
    'Translate a symbolic cnf to a numbered cnf and return a reverse mapping'
    # DIMACS CNF file format:
    # http://people.sc.fsu.edu/~jburkardt/data/cnf/cnf.html
    if uniquify:
        cnf = list(dict.fromkeys(cnf))
    lit2num, num2var = 0(cnf)
    numbered_cnf = [tuple([lit2num[lit] for lit in clause]) for clause in cnf]
    return numbered_cnf, num2var","{""make_translate"": ""0""}"
663,663,"def from_dnf(groups) -> 'cnf':
    'Convert from or-of-ands to and-of-ors'
    cnf = {frozenset()}
    for group in groups:
        nl = {frozenset([literal]) : neg(literal) for literal in group}
        # The ""clause | literal"" prevents dup lits: {x, x, y} -> {x, y}
        # The nl check skips over identities: {x, ~x, y} -> True
        cnf = {clause | literal for literal in nl for clause in cnf
              if nl[literal] not in clause}
        # The sc check removes clauses with superfluous terms:
        #     {{x}, {x, z}, {y, z}} -> {{x}, {y, z}}
        # Should this be left until the end?
        sc = min(cnf, key=len)          # XXX not deterministic
        cnf -= {clause for clause in cnf if clause > sc}
    return list(map(tuple, cnf))","def neg(a):
    ""Same as -a.""
    return -a","def neg(a):
    ""Same as -a.""
    return -a","def convert_from_or_of_ands_to_and_of_ors_in_cnf(groups) -> 'cnf':
    'Convert from or-of-ands to and-of-ors'
    cnf = {frozenset()}
    for group in groups:
        nl = {frozenset([literal]) : neg(literal) for literal in group}
        # The ""clause | literal"" prevents dup lits: {x, x, y} -> {x, y}
        # The nl check skips over identities: {x, ~x, y} -> True
        cnf = {clause | literal for literal in nl for clause in cnf
              if nl[literal] not in clause}
        # The sc check removes clauses with superfluous terms:
        #     {{x}, {x, z}, {y, z}} -> {{x}, {y, z}}
        # Should this be left until the end?
        sc = min(cnf, key=len)          # XXX not deterministic
        cnf -= {clause for clause in cnf if clause > sc}
    return list(map(tuple, cnf))","{""neg"": ""neg""}","def 0(a):
    ""Same as -a.""
    return -a","def 1(groups) -> 'cnf':
    'Convert from or-of-ands to and-of-ors'
    cnf = {frozenset()}
    for group in groups:
        nl = {frozenset([literal]) : 0(literal) for literal in group}
        # The ""clause | literal"" prevents dup lits: {x, x, y} -> {x, y}
        # The nl check skips over identities: {x, ~x, y} -> True
        cnf = {clause | literal for literal in nl for clause in cnf
              if nl[literal] not in clause}
        # The sc check removes clauses with superfluous terms:
        #     {{x}, {x, z}, {y, z}} -> {{x}, {y, z}}
        # Should this be left until the end?
        sc = min(cnf, key=len)          # XXX not deterministic
        cnf -= {clause for clause in cnf if clause > sc}
    return list(map(tuple, cnf))","{""neg"": ""0""}"
664,664,"def parse_grid(grid):
    """"""Convert grid to a dict of possible values, {square: digits}, or
    return False if a contradiction is detected.""""""
    ## To start, every square can be any digit; then assign values from the grid.
    values = dict((s, digits) for s in squares)
    for s,d in grid_values(grid).items():
        if d in digits and not assign(values, s, d):
            return False ## (Fail if we can't assign d to square s.)
    return values","def grid_values(grid):
    ""Convert grid into a dict of {square: char} with '0' or '.' for empties.""
    chars = [c for c in grid if c in digits or c in '0.']
    assert len(chars) == 81
    return dict(zip(squares, chars))

def assign(values, s, d):
    """"""Eliminate all the other values (except d) from values[s] and propagate.
    Return values, except return False if a contradiction is detected.""""""
    other_values = values[s].replace(d, '')
    if all(eliminate(values, s, d2) for d2 in other_values):
        return values
    else:
        return False","def grid_values(grid):
    ""Convert grid into a dict of {square: char} with '0' or '.' for empties.""
    chars = [c for c in grid if c in digits or c in '0.']
    assert len(chars) == 81
    return dict(zip(squares, chars))

def assign(values, s, d):
    """"""Eliminate all the other values (except d) from values[s] and propagate.
    Return values, except return False if a contradiction is detected.""""""
    other_values = values[s].replace(d, '')
    if all(eliminate(values, s, d2) for d2 in other_values):
        return values
    else:
        return False","def convert_grid_to_dict_of_possible_values_or_return_false_if_contradiction_detected(grid):
    """"""Convert grid to a dict of possible values, {square: digits}, or
    return False if a contradiction is detected.""""""
    ## To start, every square can be any digit; then assign values from the grid.
    values = dict((s, digits) for s in squares)
    for s,d in grid_values(grid).items():
        if d in digits and not assign(values, s, d):
            return False ## (Fail if we can't assign d to square s.)
    return values","{""grid_values"": ""grid_values"", ""assign"": ""assign""}","def 0(grid):
    ""Convert grid into a dict of {square: char} with '0' or '.' for empties.""
    chars = [c for c in grid if c in digits or c in '0.']
    assert len(chars) == 81
    return dict(zip(squares, chars))

def 1(values, s, d):
    """"""Eliminate all the other values (except d) from values[s] and propagate.
    Return values, except return False if a contradiction is detected.""""""
    other_values = values[s].replace(d, '')
    if all(eliminate(values, s, d2) for d2 in other_values):
        return values
    else:
        return False","def 2(grid):
    """"""Convert grid to a dict of possible values, {square: digits}, or
    return False if a contradiction is detected.""""""
    ## To start, every square can be any digit; then assign values from the grid.
    values = dict((s, digits) for s in squares)
    for s,d in 0(grid).items():
        if d in digits and not 1(values, s, d):
            return False ## (Fail if we can't assign d to square s.)
    return values","{""grid_values"": ""0"", ""assign"": ""1""}"
665,665,"def assign(values, s, d):
    """"""Eliminate all the other values (except d) from values[s] and propagate.
    Return values, except return False if a contradiction is detected.""""""
    other_values = values[s].replace(d, '')
    if all(eliminate(values, s, d2) for d2 in other_values):
        return values
    else:
        return False","def eliminate(values, s, d):
    """"""Eliminate d from values[s]; propagate when values or places <= 2.
    Return values, except return False if a contradiction is detected.""""""
    if d not in values[s]:
        return values ## Already eliminated
    values[s] = values[s].replace(d,'')
    ## (1) If a square s is reduced to one value d2, then eliminate d2 from the peers.
    if len(values[s]) == 0:
        return False ## Contradiction: removed last value
    elif len(values[s]) == 1:
        d2 = values[s]
        if not all(eliminate(values, s2, d2) for s2 in peers[s]):
            return False
    ## (2) If a unit u is reduced to only one place for a value d, then put it there.
    for u in units[s]:
        dplaces = [s for s in u if d in values[s]]
        if len(dplaces) == 0:
            return False ## Contradiction: no place for this value
        elif len(dplaces) == 1:
            # d can only be in one place in unit; assign it there
            if not assign(values, dplaces[0], d):
                return False
    return values","def eliminate(values, s, d):
    """"""Eliminate d from values[s]; propagate when values or places <= 2.
    Return values, except return False if a contradiction is detected.""""""
    if d not in values[s]:
        return values ## Already eliminated
    values[s] = values[s].replace(d,'')
    ## (1) If a square s is reduced to one value d2, then eliminate d2 from the peers.
    if len(values[s]) == 0:
        return False ## Contradiction: removed last value
    elif len(values[s]) == 1:
        d2 = values[s]
        if not all(eliminate(values, s2, d2) for s2 in peers[s]):
            return False
    ## (2) If a unit u is reduced to only one place for a value d, then put it there.
    for u in units[s]:
        dplaces = [s for s in u if d in values[s]]
        if len(dplaces) == 0:
            return False ## Contradiction: no place for this value
        elif len(dplaces) == 1:
            # d can only be in one place in unit; assign it there
            if not assign(values, dplaces[0], d):
                return False
    return values","def assign(values, s, d):
    """"""Eliminate all the other values (except d) from values[s] and propagate.
    Return values, except return False if a contradiction is detected.""""""
    other_values = values[s].replace(d, '')
    if all(eliminate(values, s, d2) for d2 in other_values):
        return values
    else:
        return False","{""eliminate"": ""eliminate""}","def 0(values, s, d):
    """"""Eliminate d from values[s]; propagate when values or places <= 2.
    Return values, except return False if a contradiction is detected.""""""
    if d not in values[s]:
        return values ## Already eliminated
    values[s] = values[s].replace(d,'')
    ## (1) If a square s is reduced to one value d2, then eliminate d2 from the peers.
    if len(values[s]) == 0:
        return False ## Contradiction: removed last value
    elif len(values[s]) == 1:
        d2 = values[s]
        if not all(0(values, s2, d2) for s2 in peers[s]):
            return False
    ## (2) If a unit u is reduced to only one place for a value d, then put it there.
    for u in units[s]:
        dplaces = [s for s in u if d in values[s]]
        if len(dplaces) == 0:
            return False ## Contradiction: no place for this value
        elif len(dplaces) == 1:
            # d can only be in one place in unit; assign it there
            if not assign(values, dplaces[0], d):
                return False
    return values","def 1(values, s, d):
    """"""Eliminate all the other values (except d) from values[s] and propagate.
    Return values, except return False if a contradiction is detected.""""""
    other_values = values[s].replace(d, '')
    if all(0(values, s, d2) for d2 in other_values):
        return values
    else:
        return False","{""eliminate"": ""0""}"
666,666,"def random_puzzle(N=17):
    """"""Make a random puzzle with N or more assignments. Restart on contradictions.
    Note the resulting puzzle is not guaranteed to be solvable, but empirically
    about 99.8% of them are solvable. Some have multiple solutions.""""""
    values = dict((s, digits) for s in squares)
    for s in shuffled(squares):
        if not assign(values, s, random.choice(values[s])):
            break
        ds = [values[s] for s in squares if len(values[s]) == 1]
        if len(ds) >= N and len(set(ds)) >= 8:
            return ''.join(values[s] if len(values[s])==1 else '.' for s in squares)
    return random_puzzle(N) ## Give up and make a new puzzle","def shuffled(seq):
    ""Return a randomly shuffled copy of the input sequence.""
    seq = list(seq)
    random.shuffle(seq)
    return seq

def assign(values, s, d):
    """"""Eliminate all the other values (except d) from values[s] and propagate.
    Return values, except return False if a contradiction is detected.""""""
    other_values = values[s].replace(d, '')
    if all(eliminate(values, s, d2) for d2 in other_values):
        return values
    else:
        return False

def random_puzzle(N=17):
    """"""Make a random puzzle with N or more assignments. Restart on contradictions.
    Note the resulting puzzle is not guaranteed to be solvable, but empirically
    about 99.8% of them are solvable. Some have multiple solutions.""""""
    values = dict((s, digits) for s in squares)
    for s in shuffled(squares):
        if not assign(values, s, random.choice(values[s])):
            break
        ds = [values[s] for s in squares if len(values[s]) == 1]
        if len(ds) >= N and len(set(ds)) >= 8:
            return ''.join(values[s] if len(values[s])==1 else '.' for s in squares)
    return random_puzzle(N) ## Give up and make a new puzzle","def shuffled(seq):
    ""Return a randomly shuffled copy of the input sequence.""
    seq = list(seq)
    random.shuffle(seq)
    return seq

def assign(values, s, d):
    """"""Eliminate all the other values (except d) from values[s] and propagate.
    Return values, except return False if a contradiction is detected.""""""
    other_values = values[s].replace(d, '')
    if all(eliminate(values, s, d2) for d2 in other_values):
        return values
    else:
        return False

def random_puzzle(N=17):
    """"""Make a random puzzle with N or more assignments. Restart on contradictions.
    Note the resulting puzzle is not guaranteed to be solvable, but empirically
    about 99.8% of them are solvable. Some have multiple solutions.""""""
    values = dict((s, digits) for s in squares)
    for s in shuffled(squares):
        if not assign(values, s, random.choice(values[s])):
            break
        ds = [values[s] for s in squares if len(values[s]) == 1]
        if len(ds) >= N and len(set(ds)) >= 8:
            return ''.join(values[s] if len(values[s])==1 else '.' for s in squares)
    return random_puzzle(N) ## Give up and make a new puzzle","def random_puzzle(N=17):
    """"""Make a random puzzle with N or more assignments. Restart on contradictions.
    Note the resulting puzzle is not guaranteed to be solvable, but empirically
    about 99.8% of them are solvable. Some have multiple solutions.""""""
    values = dict((s, digits) for s in squares)
    for s in shuffled(squares):
        if not assign(values, s, random.choice(values[s])):
            break
        ds = [values[s] for s in squares if len(values[s]) == 1]
        if len(ds) >= N and len(set(ds)) >= 8:
            return ''.join(values[s] if len(values[s])==1 else '.' for s in squares)
    return random_puzzle(N) ## Give up and make a new puzzle","{""shuffled"": ""shuffled"", ""assign"": ""assign"", ""random_puzzle"": ""random_puzzle""}","def 0(seq):
    ""Return a randomly shuffled copy of the input sequence.""
    seq = list(seq)
    random.shuffle(seq)
    return seq

def 1(values, s, d):
    """"""Eliminate all the other values (except d) from values[s] and propagate.
    Return values, except return False if a contradiction is detected.""""""
    other_values = values[s].replace(d, '')
    if all(eliminate(values, s, d2) for d2 in other_values):
        return values
    else:
        return False

def 2(N=17):
    """"""Make a random puzzle with N or more assignments. Restart on contradictions.
    Note the resulting puzzle is not guaranteed to be solvable, but empirically
    about 99.8% of them are solvable. Some have multiple solutions.""""""
    values = dict((s, digits) for s in squares)
    for s in 0(squares):
        if not 1(values, s, random.choice(values[s])):
            break
        ds = [values[s] for s in squares if len(values[s]) == 1]
        if len(ds) >= N and len(set(ds)) >= 8:
            return ''.join(values[s] if len(values[s])==1 else '.' for s in squares)
    return 2(N) ## Give up and make a new puzzle","def 2(N=17):
    """"""Make a random puzzle with N or more assignments. Restart on contradictions.
    Note the resulting puzzle is not guaranteed to be solvable, but empirically
    about 99.8% of them are solvable. Some have multiple solutions.""""""
    values = dict((s, digits) for s in squares)
    for s in 0(squares):
        if not 1(values, s, random.choice(values[s])):
            break
        ds = [values[s] for s in squares if len(values[s]) == 1]
        if len(ds) >= N and len(set(ds)) >= 8:
            return ''.join(values[s] if len(values[s])==1 else '.' for s in squares)
    return 2(N) ## Give up and make a new puzzle","{""shuffled"": ""0"", ""assign"": ""1"", ""random_puzzle"": ""2""}"
