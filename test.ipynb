{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sashafedorova/jetBrainsIntern/venv/lib/python3.9/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2Model"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load the pretrained GPT2 model and tokenizer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "model_name = \"gpt2\"\n",
    "model = GPT2Model.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Input sequence"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "text = \"Hello, how are you?\"\n",
    "input_ids = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "attention_mask = torch.ones_like(input_ids)\n",
    "with torch.no_grad():\n",
    "    original_outputs = model(input_ids, attention_mask=attention_mask, output_attentions=True)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Print the original attention values for the first layer and first head\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Attention:\n",
      "tensor([[[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00],\n",
      "         [9.7093e-01, 2.9067e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00],\n",
      "         [7.3162e-01, 1.4214e-01, 1.2624e-01, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00],\n",
      "         [6.9679e-01, 9.4215e-02, 1.3803e-01, 7.0965e-02, 0.0000e+00,\n",
      "          0.0000e+00],\n",
      "         [6.3546e-01, 8.1361e-02, 1.7181e-01, 4.5568e-02, 6.5797e-02,\n",
      "          0.0000e+00],\n",
      "         [5.8499e-01, 3.4026e-02, 1.9450e-01, 3.3753e-02, 1.0432e-01,\n",
      "          4.8413e-02]],\n",
      "\n",
      "        [[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00],\n",
      "         [6.7076e-03, 9.9329e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00],\n",
      "         [1.2836e-03, 1.2506e-03, 9.9747e-01, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00],\n",
      "         [1.0240e-03, 1.0826e-02, 2.3372e-03, 9.8581e-01, 0.0000e+00,\n",
      "          0.0000e+00],\n",
      "         [1.3698e-02, 6.0359e-03, 8.2985e-03, 8.6038e-03, 9.6336e-01,\n",
      "          0.0000e+00],\n",
      "         [2.3530e-03, 2.0410e-02, 2.0370e-03, 1.7075e-03, 2.7388e-03,\n",
      "          9.7075e-01]],\n",
      "\n",
      "        [[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00],\n",
      "         [4.9785e-01, 5.0215e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00],\n",
      "         [6.2820e-01, 2.7376e-01, 9.8038e-02, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00],\n",
      "         [4.0632e-01, 1.8909e-01, 1.8624e-01, 2.1835e-01, 0.0000e+00,\n",
      "          0.0000e+00],\n",
      "         [3.0933e-01, 1.5809e-01, 1.5616e-01, 2.7411e-01, 1.0230e-01,\n",
      "          0.0000e+00],\n",
      "         [2.8540e-01, 1.3336e-01, 1.0848e-01, 2.3649e-01, 1.2887e-01,\n",
      "          1.0740e-01]],\n",
      "\n",
      "        [[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00],\n",
      "         [7.8983e-01, 2.1017e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00],\n",
      "         [7.0086e-02, 2.1500e-02, 9.0841e-01, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00],\n",
      "         [6.4048e-02, 4.8539e-02, 2.3343e-01, 6.5398e-01, 0.0000e+00,\n",
      "          0.0000e+00],\n",
      "         [4.3618e-02, 1.0148e-02, 5.4998e-02, 6.6257e-02, 8.2498e-01,\n",
      "          0.0000e+00],\n",
      "         [3.2198e-02, 6.4209e-03, 6.9542e-02, 3.2105e-02, 1.4558e-01,\n",
      "          7.1416e-01]],\n",
      "\n",
      "        [[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00],\n",
      "         [9.6888e-01, 3.1119e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00],\n",
      "         [7.0819e-01, 1.2864e-01, 1.6317e-01, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00],\n",
      "         [3.9644e-01, 1.3408e-01, 1.6590e-01, 3.0358e-01, 0.0000e+00,\n",
      "          0.0000e+00],\n",
      "         [3.6372e-01, 6.5735e-02, 1.5129e-01, 2.1399e-01, 2.0527e-01,\n",
      "          0.0000e+00],\n",
      "         [2.7468e-01, 7.3579e-02, 1.3575e-01, 1.2353e-01, 1.7342e-01,\n",
      "          2.1904e-01]],\n",
      "\n",
      "        [[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00],\n",
      "         [1.5098e-01, 8.4902e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00],\n",
      "         [5.1504e-02, 1.8193e-03, 9.4668e-01, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00],\n",
      "         [1.6987e-02, 8.4396e-03, 4.5007e-03, 9.7007e-01, 0.0000e+00,\n",
      "          0.0000e+00],\n",
      "         [6.8611e-02, 5.3320e-03, 3.7356e-03, 1.7665e-03, 9.2055e-01,\n",
      "          0.0000e+00],\n",
      "         [4.1268e-02, 4.0280e-03, 2.6427e-03, 9.4975e-04, 4.7821e-04,\n",
      "          9.5063e-01]],\n",
      "\n",
      "        [[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00],\n",
      "         [9.9311e-01, 6.8905e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00],\n",
      "         [7.9982e-01, 5.6453e-02, 1.4372e-01, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00],\n",
      "         [7.4578e-01, 4.7446e-02, 1.4981e-01, 5.6967e-02, 0.0000e+00,\n",
      "          0.0000e+00],\n",
      "         [8.2917e-01, 4.1794e-02, 7.7230e-02, 3.6007e-02, 1.5798e-02,\n",
      "          0.0000e+00],\n",
      "         [7.3318e-01, 5.9228e-02, 9.3622e-02, 4.6901e-02, 2.5968e-02,\n",
      "          4.1099e-02]],\n",
      "\n",
      "        [[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00],\n",
      "         [9.3998e-01, 6.0017e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00],\n",
      "         [5.3734e-01, 3.4313e-01, 1.1953e-01, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00],\n",
      "         [3.0869e-01, 1.9028e-01, 3.5105e-01, 1.4998e-01, 0.0000e+00,\n",
      "          0.0000e+00],\n",
      "         [2.2122e-01, 1.4071e-01, 1.9654e-01, 2.3257e-01, 2.0896e-01,\n",
      "          0.0000e+00],\n",
      "         [1.6009e-01, 7.3825e-02, 1.3440e-01, 1.2449e-01, 2.4033e-01,\n",
      "          2.6686e-01]],\n",
      "\n",
      "        [[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00],\n",
      "         [3.0257e-01, 6.9743e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00],\n",
      "         [2.8490e-01, 6.1052e-01, 1.0458e-01, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00],\n",
      "         [1.0988e-01, 4.0396e-01, 8.0273e-02, 4.0588e-01, 0.0000e+00,\n",
      "          0.0000e+00],\n",
      "         [1.2871e-01, 2.2231e-01, 5.7849e-02, 1.8632e-01, 4.0481e-01,\n",
      "          0.0000e+00],\n",
      "         [8.7686e-02, 4.5512e-01, 5.4900e-02, 1.5282e-01, 1.0686e-01,\n",
      "          1.4261e-01]],\n",
      "\n",
      "        [[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00],\n",
      "         [6.5280e-01, 3.4720e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00],\n",
      "         [4.8872e-01, 2.9628e-01, 2.1500e-01, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00],\n",
      "         [3.4001e-01, 2.4511e-01, 2.2883e-01, 1.8605e-01, 0.0000e+00,\n",
      "          0.0000e+00],\n",
      "         [3.2307e-01, 2.0955e-01, 1.8823e-01, 1.5602e-01, 1.2313e-01,\n",
      "          0.0000e+00],\n",
      "         [2.3624e-01, 1.8921e-01, 1.6386e-01, 1.2922e-01, 1.4839e-01,\n",
      "          1.3308e-01]],\n",
      "\n",
      "        [[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00],\n",
      "         [6.7875e-01, 3.2125e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00],\n",
      "         [3.5939e-01, 2.3090e-01, 4.0970e-01, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00],\n",
      "         [3.2138e-01, 2.3713e-01, 1.4635e-01, 2.9514e-01, 0.0000e+00,\n",
      "          0.0000e+00],\n",
      "         [2.7044e-01, 1.7202e-01, 1.4378e-01, 9.7727e-02, 3.1603e-01,\n",
      "          0.0000e+00],\n",
      "         [2.4077e-01, 1.7062e-01, 1.4985e-01, 1.1211e-01, 9.9447e-02,\n",
      "          2.2720e-01]],\n",
      "\n",
      "        [[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00],\n",
      "         [5.8466e-01, 4.1534e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00],\n",
      "         [4.9338e-01, 2.7068e-01, 2.3594e-01, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00],\n",
      "         [3.6027e-01, 2.5608e-01, 1.7478e-01, 2.0887e-01, 0.0000e+00,\n",
      "          0.0000e+00],\n",
      "         [3.5122e-01, 1.6815e-01, 1.8155e-01, 1.7265e-01, 1.2644e-01,\n",
      "          0.0000e+00],\n",
      "         [2.6518e-01, 1.3262e-01, 1.4199e-01, 9.3936e-02, 7.5510e-02,\n",
      "          2.9076e-01]]])\n"
     ]
    }
   ],
   "source": [
    "original_attention = original_outputs.attentions[0][0]\n",
    "print(\"Original Attention:\")\n",
    "print(original_attention)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Modify the GPT2 model with MeanGPT2Attention"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Dimension specified as -2 but tensor has no dimensions",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 5\u001B[0m\n\u001B[1;32m      3\u001B[0m modified_model \u001B[38;5;241m=\u001B[39m GPT2Wrapper(model)\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m----> 5\u001B[0m     modified_outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodified_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      7\u001B[0m modified_attention \u001B[38;5;241m=\u001B[39m modified_outputs\u001B[38;5;241m.\u001B[39mattentions[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mModified Attention:\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/jetBrainsIntern/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/jetBrainsIntern/gpt2_attentions_aproximations.py:99\u001B[0m, in \u001B[0;36mGPT2Wrapper.forward\u001B[0;34m(self, input_ids, attention_mask)\u001B[0m\n\u001B[1;32m     97\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, input_ids, attention_mask\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m     98\u001B[0m     \u001B[38;5;66;03m# Forward the inputs through the modified GPT2 model\u001B[39;00m\n\u001B[0;32m---> 99\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgpt2_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/jetBrainsIntern/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/jetBrainsIntern/venv/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:803\u001B[0m, in \u001B[0;36mGPT2Model.forward\u001B[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m    801\u001B[0m     past_key_values \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mtuple\u001B[39m([\u001B[38;5;28;01mNone\u001B[39;00m] \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mh))\n\u001B[1;32m    802\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 803\u001B[0m     past_length \u001B[38;5;241m=\u001B[39m \u001B[43mpast_key_values\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msize\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    804\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m position_ids \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    805\u001B[0m     position_ids \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39marange(past_length, input_shape[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m+\u001B[39m past_length, dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mlong, device\u001B[38;5;241m=\u001B[39mdevice)\n",
      "\u001B[0;31mIndexError\u001B[0m: Dimension specified as -2 but tensor has no dimensions"
     ]
    }
   ],
   "source": [
    "from gpt2_attentions_aproximations import GPT2Wrapper\n",
    "\n",
    "modified_model = GPT2Wrapper(model)\n",
    "with torch.no_grad():\n",
    "    modified_outputs = modified_model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "modified_attention = modified_outputs.attentions[0][0]\n",
    "print(\"Modified Attention:\")\n",
    "print(modified_attention)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Original Attention:\n",
      "tensor([[0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667]])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'modified_attention' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[8], line 7\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28mprint\u001B[39m(mean_original_attention)\n\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# Check if the modified attention values match the mean of the original attention values\u001B[39;00m\n\u001B[0;32m----> 7\u001B[0m mean_modified_attention \u001B[38;5;241m=\u001B[39m \u001B[43mmodified_attention\u001B[49m\u001B[38;5;241m.\u001B[39mmean(dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m      8\u001B[0m mean_equal \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mallclose(mean_modified_attention, mean_original_attention)\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMean Attention Equal:\u001B[39m\u001B[38;5;124m\"\u001B[39m, mean_equal)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'modified_attention' is not defined"
     ]
    }
   ],
   "source": [
    "# Calculate the mean of the original attention values\n",
    "mean_original_attention = original_attention.mean(dim=-1)\n",
    "print(\"Mean Original Attention:\")\n",
    "print(mean_original_attention)\n",
    "\n",
    "# Check if the modified attention values match the mean of the original attention values\n",
    "mean_modified_attention = modified_attention.mean(dim=-1)\n",
    "mean_equal = torch.allclose(mean_modified_attention, mean_original_attention)\n",
    "print(\"Mean Attention Equal:\", mean_equal)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
