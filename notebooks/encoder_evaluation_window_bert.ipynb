{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a850946e",
   "metadata": {},
   "source": [
    "## Import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0341042b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/shapkin/Projects/effective-inference\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84306765",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModel, GPT2LMHeadModel\n",
    "from utils.attention_patterns.bert_modules import BertWrapper, WindowBert2Attention\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n",
    "from sklearn.metrics import classification_report\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "import torch\n",
    "#from progressbar import progressbar\n",
    "from tqdm.auto import tqdm\n",
    "from utils.prepare_dataset import load_datasets, cut_datasets\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157f45f0",
   "metadata": {},
   "source": [
    "## Define hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49bf79c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Define datasets\n",
    "#['mrpc', 'sst2', 'cola', 'rte', 'qnli']\n",
    "glue_classification = {'mrpc': ['sentence1', 'sentence2'], 'sst2':  ['sentence'], 'qnli' : ['question', 'sentence']}\n",
    "superglue_classification = {'wic': ['sentence1', 'sentence2']}\n",
    "all_classification = {'glue': glue_classification, 'superglue': superglue_classification}\n",
    "\n",
    "model_name = 'bert-base-uncased'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, max_length=1024)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "model.eval()\n",
    "\n",
    "# Define the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device == 'cpu':\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "    \n",
    "model = BertWrapper(model, WindowBert2Attention).to(device)  \n",
    "DEBUG_FLAG = False\n",
    "CUT_SIZE = None if not DEBUG_FLAG else 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed818697",
   "metadata": {},
   "source": [
    "## Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16edb6a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset glue (/home/shapkin/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "762d378aab554f609318a2e492a17400",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset glue (/home/shapkin/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2c1da8a2db44a99a3de86861e497490",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset glue (/home/shapkin/.cache/huggingface/datasets/glue/qnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07943eddd06f4773a9d39c535c29561e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset super_glue (/home/shapkin/.cache/huggingface/datasets/super_glue/wic/1.0.3/bb9675f958ebfee0d5d6dc5476fafe38c79123727a7258d515c450873dbdbbed)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56bb1fa3343f41f2a6a6f27189aa122d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "glue_datasets = load_datasets('glue', list(glue_classification), CUT_SIZE)\n",
    "superglue_datasets = load_datasets('super_glue', list(superglue_classification), CUT_SIZE)\n",
    "\n",
    "all_datasets = {'glue': glue_datasets, 'superglue': superglue_datasets}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b5e3ccc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['mrpc', 'sst2', 'qnli'], ['wic'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(glue_classification), list(superglue_classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4c23c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm_pbar = lambda x, y: tqdm(x, leave=True, position=0, total=len(x), desc=f'{y}')\n",
    "def get_cls_embeddings_for_dataset(dataset_name, dataset, \n",
    "                                   feature_names, model, tokenizer, \n",
    "                                   pbar_func=tqdm_pbar, device=device, CUT_SIZE=CUT_SIZE):\n",
    "    collected_embeddings = defaultdict(list)\n",
    "    \n",
    "    for split, data in dataset.items():\n",
    "        \n",
    "        pbar = pbar_func(data, f\"{split} {dataset_name}\") if pbar_func is not None else data\n",
    "        for example in pbar:\n",
    "            # Encode the input sentences\n",
    "            encoded_inputs = tokenizer.encode(*list(map(lambda x: example[x] , feature_names)), \n",
    "                                              truncation=True, \n",
    "                                              return_tensors='pt')\n",
    "\n",
    "            encoded_inputs = encoded_inputs.to(device)\n",
    "\n",
    "            # Forward pass through the model\n",
    "            with torch.no_grad():\n",
    "                outputs = model(encoded_inputs)\n",
    "\n",
    "            # Get the embedding of the [CLS] token\n",
    "            cls_embedding = outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "            # Append the [CLS] embedding to the list\n",
    "            collected_embeddings[split].append(cls_embedding)\n",
    "         \n",
    "    return collected_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63e9cce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_linear(X_train, y_train):\n",
    "    classifier = LogisticRegression(solver='lbfgs', max_iter=3000)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    return classifier\n",
    "\n",
    "def evaluate_classifier(classifier, X, y=None):\n",
    "    predictions = classifier.predict(X)\n",
    "    return predictions\n",
    "\n",
    "def get_metrics_report(y_true, y_pred):\n",
    "    #accuracy = accuracy_score(y_true, y_pred)\n",
    "    #f1 = f1_score(y_true, y_pred,  average='weighted')\n",
    "    #precision = precision_score(y_true, y_pred,  average='weighted')\n",
    "    #recall = recall_score(y_true, y_pred,  average='weighted', zero_division='warn')\n",
    "    #roc_auc = roc_auc_score(y_true, y_pred)\n",
    "    print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32f161b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GLUE / mrpc\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd4e3f1f767342c3bc38a0af135ce26d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train mrpc:   0%|          | 0/3668 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f3295f316064304bf125e35ae99e0e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation mrpc:   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19adfdca64b948ddbeb5cb0025e0071f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test mrpc:   0%|          | 0/1725 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation evaluation:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.41      0.32      0.36       129\n",
      "           1       0.72      0.79      0.75       279\n",
      "\n",
      "    accuracy                           0.64       408\n",
      "   macro avg       0.56      0.55      0.56       408\n",
      "weighted avg       0.62      0.64      0.63       408\n",
      "\n",
      "GLUE / sst2\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6ab8e13eb72406db961f4c9dc2c78ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train sst2:   0%|          | 0/67349 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3869b1d4f5a8431fb1896e0a87b8e978",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation sst2:   0%|          | 0/872 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ed6af60bd7449318f42964e1f618286",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test sst2:   0%|          | 0/1821 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation evaluation:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.72      0.73       428\n",
      "           1       0.74      0.77      0.75       444\n",
      "\n",
      "    accuracy                           0.74       872\n",
      "   macro avg       0.74      0.74      0.74       872\n",
      "weighted avg       0.74      0.74      0.74       872\n",
      "\n",
      "GLUE / qnli\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e10e44a60f94f8a947ef5ac3f896e7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train qnli:   0%|          | 0/104743 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for dn, datasets in all_datasets.items():\n",
    "    for dataset_name, dataset in datasets.items():\n",
    "        print(f\"{dn.upper()} / {dataset_name}\\n\")\n",
    "        dataset_embeddings = get_cls_embeddings_for_dataset(\n",
    "            dataset_name,\n",
    "            dataset, \n",
    "            all_classification[dn][dataset_name], \n",
    "            model, \n",
    "            tokenizer)\n",
    "        \n",
    "        train_dataset_embeddings = torch.cat(dataset_embeddings['train'], dim=0).cpu()\n",
    "        valid_dataset_embeddings = torch.cat(dataset_embeddings['validation'], dim=0).cpu()\n",
    "        test_dataset_embeddings = torch.cat(dataset_embeddings['test'], dim=0).cpu()\n",
    "        \n",
    "        classif = train_linear(train_dataset_embeddings, [el['label'] for el in dataset['train']])\n",
    "        valid_preds = evaluate_classifier(classif, valid_dataset_embeddings)\n",
    "        print('Validation evaluation:\\n')\n",
    "        get_metrics_report([el['label'] for el in dataset['validation']], valid_preds)\n",
    "        # print(train_dataset_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7e41f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d891cf4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
