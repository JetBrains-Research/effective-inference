{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/shapkin/Projects/effective-inference\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.attention_patterns.gpt2_modules import GPT2Wrapper, MeanGPT2Attention, WindowMEANGPT2Attention\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pretrained GPT2 model and tokenizer\n",
    "model_name = \"gpt2\"\n",
    "model = GPT2Model.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Input sequence\n",
    "text = \"Hello, how are you?\"\n",
    "\n",
    "# Tokenize the input sequence\n",
    "input_ids = tokenizer.encode(text, return_tensors=\"pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass through the original GPT2 model\n",
    "with torch.no_grad():\n",
    "    original_outputs = model(input_ids, output_attentions=True) #, attention_mask=attention_mask, )\n",
    "\n",
    "# Print the original attention values for the first layer and first head\n",
    "original_attention = original_outputs.attentions[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5847, 0.4153, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.4934, 0.2707, 0.2359, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3603, 0.2561, 0.1748, 0.2089, 0.0000, 0.0000],\n",
       "        [0.3512, 0.1681, 0.1816, 0.1726, 0.1264, 0.0000],\n",
       "        [0.2652, 0.1326, 0.1420, 0.0939, 0.0755, 0.2908]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_attention[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 3., 4.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(torch.tensor([[[1, 2, 3], [3, 4, 5]]]).float(), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check full mean attention model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the GPT2 model with MeanGPT2Attention\n",
    "modified_model = GPT2Wrapper(model, MeanGPT2Attention)\n",
    "\n",
    "# Forward pass through the modified GPT2 model\n",
    "with torch.no_grad():\n",
    "    modified_outputs = modified_model(input_ids, output_attentions=True) #attention_mask=attention_maskattention_mask=attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPastAndCrossAttentions(last_hidden_state=tensor([[[-3.8862e-02, -1.6771e-02, -1.9057e-01,  ..., -7.0633e-02,\n",
       "          -6.5166e-02, -6.8840e-02],\n",
       "         [ 1.4571e-01, -1.6931e-02, -2.7997e-01,  ..., -5.6498e-02,\n",
       "           2.0403e-01,  7.4639e-02],\n",
       "         [ 1.2030e-01,  1.4468e-01, -8.5607e-01,  ..., -1.3272e-01,\n",
       "          -3.1428e-02,  4.4152e-02],\n",
       "         [ 6.7250e-02,  5.1031e-02, -5.9873e-01,  ..., -5.5376e-05,\n",
       "           5.8601e-02,  1.9829e-01],\n",
       "         [ 1.6319e-02,  1.0318e-01, -3.7880e-01,  ..., -7.0631e-02,\n",
       "          -5.5554e-02,  3.4876e-01],\n",
       "         [-5.8700e-02, -4.7532e-02, -5.0778e-01,  ...,  1.2607e-01,\n",
       "           2.5306e-02,  1.6212e-01]]]), past_key_values=(None, None, None, None, None, None, None, None, None, None, None, None), hidden_states=None, attentions=(None, None, None, None, None, None, None, None, None, None, None, None), cross_attentions=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modified_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check window mean model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the GPT2 model with MeanGPT2Attention\n",
    "modified_model = GPT2Wrapper(model, WindowMEANGPT2Attention)\n",
    "\n",
    "# Forward pass through the modified GPT2 model\n",
    "with torch.no_grad():\n",
    "    modified_outputs = modified_model(input_ids, output_attentions=True) #attention_mask=attention_maskattention_mask=attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPastAndCrossAttentions(last_hidden_state=tensor([[[-0.0021, -0.0990, -0.1563,  ..., -0.1623, -0.2919, -0.2121],\n",
       "         [ 0.0395, -0.0610, -0.1954,  ..., -0.1705, -0.3608, -0.1235],\n",
       "         [-0.0270, -0.0686, -0.2441,  ..., -0.2972, -0.3942, -0.1822],\n",
       "         [ 0.0438, -0.1145, -0.3154,  ..., -0.2262, -0.3822, -0.1357],\n",
       "         [ 0.0246, -0.0928, -0.2125,  ..., -0.2400, -0.4121, -0.1072],\n",
       "         [ 0.0346, -0.0640, -0.1508,  ..., -0.1758, -0.3959, -0.1177]]]), past_key_values=(None, None, None, None, None, None, None, None, None, None, None, None), hidden_states=None, attentions=(None, None, None, None, None, None, None, None, None, None, None, None), cross_attentions=None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modified_outputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
