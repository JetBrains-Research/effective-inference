{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a850946e",
   "metadata": {},
   "source": [
    "## Import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0341042b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Anton.Shapkin/Documents/Projects/effective-inference\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84306765",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModel, GPT2LMHeadModel\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n",
    "from sklearn.metrics import classification_report\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "import torch\n",
    "#from progressbar import progressbar\n",
    "from tqdm.auto import tqdm\n",
    "from utils.prepare_dataset import load_datasets, cut_datasets\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157f45f0",
   "metadata": {},
   "source": [
    "## Define hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49bf79c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Define datasets\n",
    "#['mrpc', 'sst2', 'cola', 'rte', 'qnli']\n",
    "glue_classification = {'mrpc': ['sentence1', 'sentence2'], 'sst2':  ['sentence'], 'qnli' : ['question', 'sentence']}\n",
    "superglue_classification = {'wic': ['sentence1', 'sentence2']}\n",
    "all_classification = {'glue': glue_classification, 'superglue': superglue_classification}\n",
    "\n",
    "model_name = 'bert-base-uncased'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, max_length=1024)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "model.eval()\n",
    "\n",
    "# Define the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device == 'cpu':\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "DEBUG_FLAG = True\n",
    "CUT_SIZE = None if not DEBUG_FLAG else 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed818697",
   "metadata": {},
   "source": [
    "## Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16edb6a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset glue (/Users/Anton.Shapkin/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "911fe944c4f74c398f81a9eea5dc24e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset glue (/Users/Anton.Shapkin/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5129ce8d5314ceaad16d84f900742d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset glue (/Users/Anton.Shapkin/.cache/huggingface/datasets/glue/qnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ddcd912e0414e9ab126dd985ff4b0e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset super_glue (/Users/Anton.Shapkin/.cache/huggingface/datasets/super_glue/wic/1.0.3/bb9675f958ebfee0d5d6dc5476fafe38c79123727a7258d515c450873dbdbbed)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "031ea7fcd0494a079789e57ead6335f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "glue_datasets = load_datasets('glue', list(glue_classification), CUT_SIZE)\n",
    "superglue_datasets = load_datasets('super_glue', list(superglue_classification), CUT_SIZE)\n",
    "\n",
    "all_datasets = {'glue': glue_datasets, 'superglue': superglue_datasets}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b5e3ccc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['mrpc', 'sst2', 'qnli'], ['wic'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(glue_classification), list(superglue_classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4c23c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm_pbar = lambda x, y: tqdm(x, leave=True, position=0, total=len(x), desc=f'{y}')\n",
    "def get_cls_embeddings_for_dataset(dataset_name, dataset, \n",
    "                                   feature_names, model, tokenizer, \n",
    "                                   pbar_func=tqdm_pbar, device=device, CUT_SIZE=CUT_SIZE):\n",
    "    collected_embeddings = defaultdict(list)\n",
    "    \n",
    "    for split, data in dataset.items():\n",
    "        \n",
    "        pbar = pbar_func(data, f\"{split} {dataset_name}\") if pbar_func is not None else data\n",
    "        for example in pbar:\n",
    "            # Encode the input sentences\n",
    "            encoded_inputs = tokenizer.encode(*list(map(lambda x: example[x] , feature_names)), \n",
    "                                              truncation=True, \n",
    "                                              return_tensors='pt')\n",
    "\n",
    "            encoded_inputs = encoded_inputs.to(device)\n",
    "\n",
    "            # Forward pass through the model\n",
    "            with torch.no_grad():\n",
    "                outputs = model(encoded_inputs)\n",
    "\n",
    "            # Get the embedding of the [CLS] token\n",
    "            cls_embedding = outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "            # Append the [CLS] embedding to the list\n",
    "            collected_embeddings[split].append(cls_embedding)\n",
    "         \n",
    "    return collected_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63e9cce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_linear(X_train, y_train):\n",
    "    classifier = LogisticRegression(solver='lbfgs', max_iter=3000)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    return classifier\n",
    "\n",
    "def evaluate_classifier(classifier, X, y=None):\n",
    "    predictions = classifier.predict(X)\n",
    "    return predictions\n",
    "\n",
    "def get_metrics_report(y_true, y_pred):\n",
    "    #accuracy = accuracy_score(y_true, y_pred)\n",
    "    #f1 = f1_score(y_true, y_pred,  average='weighted')\n",
    "    #precision = precision_score(y_true, y_pred,  average='weighted')\n",
    "    #recall = recall_score(y_true, y_pred,  average='weighted', zero_division='warn')\n",
    "    #roc_auc = roc_auc_score(y_true, y_pred)\n",
    "    print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32f161b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GLUE / mrpc\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52bfa0c19acd4ed8818f7205dbf6b1f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train mrpc:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a08416e58c7f46a6b35fd68920212a63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation mrpc:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10b5209704da4b2e9b7127dccbcf34eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test mrpc:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation evaluation:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.35      0.42        31\n",
      "           1       0.74      0.84      0.79        69\n",
      "\n",
      "    accuracy                           0.69       100\n",
      "   macro avg       0.62      0.60      0.60       100\n",
      "weighted avg       0.67      0.69      0.67       100\n",
      "\n",
      "GLUE / sst2\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afa5669a141c44f28ffc0cc1e64a4a11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train sst2:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7072e032c256417a8d1aa1b9e115bd43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation sst2:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "558d72fd94224f05a340873430eae4b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test sst2:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation evaluation:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.73      0.73        48\n",
      "           1       0.75      0.75      0.75        52\n",
      "\n",
      "    accuracy                           0.74       100\n",
      "   macro avg       0.74      0.74      0.74       100\n",
      "weighted avg       0.74      0.74      0.74       100\n",
      "\n",
      "GLUE / qnli\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59c3a08aa4884a4e83a1263c19d6b524",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train qnli:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c85355bc8b7a4789abc8a671eaf8b678",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation qnli:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b950d6d4a6841edbedd7b4cd05e0d2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test qnli:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation evaluation:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.38      0.44        45\n",
      "           1       0.58      0.71      0.64        55\n",
      "\n",
      "    accuracy                           0.56       100\n",
      "   macro avg       0.55      0.54      0.54       100\n",
      "weighted avg       0.55      0.56      0.55       100\n",
      "\n",
      "SUPERGLUE / wic\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31b07244d5134289b14e3ea6a599d124",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train wic:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac292338b7d84643ae2744f4124618f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation wic:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cc39e99fa784ff8985233ccaa81877a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test wic:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation evaluation:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.57      0.61        53\n",
      "           1       0.57      0.66      0.61        47\n",
      "\n",
      "    accuracy                           0.61       100\n",
      "   macro avg       0.61      0.61      0.61       100\n",
      "weighted avg       0.62      0.61      0.61       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for dn, datasets in all_datasets.items():\n",
    "    for dataset_name, dataset in datasets.items():\n",
    "        print(f\"{dn.upper()} / {dataset_name}\\n\")\n",
    "        dataset_embeddings = get_cls_embeddings_for_dataset(\n",
    "            dataset_name,\n",
    "            dataset, \n",
    "            all_classification[dn][dataset_name], \n",
    "            model, \n",
    "            tokenizer)\n",
    "        \n",
    "        train_dataset_embeddings = torch.cat(dataset_embeddings['train'], dim=0).cpu()\n",
    "        valid_dataset_embeddings = torch.cat(dataset_embeddings['validation'], dim=0).cpu()\n",
    "        test_dataset_embeddings = torch.cat(dataset_embeddings['test'], dim=0).cpu()\n",
    "        \n",
    "        classif = train_linear(train_dataset_embeddings, [el['label'] for el in dataset['train']])\n",
    "        valid_preds = evaluate_classifier(classif, valid_dataset_embeddings)\n",
    "        print('Validation evaluation:\\n')\n",
    "        get_metrics_report([el['label'] for el in dataset['validation']], valid_preds)\n",
    "        # print(train_dataset_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7e41f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
