{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a850946e",
   "metadata": {},
   "source": [
    "## Import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "105fa189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sasha/effective-inference\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84306765",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModel, GPT2LMHeadModel\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n",
    "from sklearn.metrics import classification_report\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "import torch\n",
    "#from progressbar import progressbar\n",
    "from tqdm.auto import tqdm\n",
    "from utils.prepare_dataset import load_datasets, cut_datasets\n",
    "from collections import defaultdict\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157f45f0",
   "metadata": {},
   "source": [
    "## Define hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49bf79c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define datasets\n",
    "#['mrpc', 'sst2', 'cola', 'rte', 'qnli']\n",
    "model_name = 'gpt2'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, max_length=1024)\n",
    "tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "model.eval()\n",
    "\n",
    "translation_datasets_artifacts = {\n",
    "    \"fr-en\": ('Translate from English to French: ', 'fr', 'French'),\n",
    "    \"ru-en\": ('Translate from English to Russian: ', 'ru', 'Russian')\n",
    "}\n",
    "\n",
    "# Define the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device == 'cpu':\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "DEBUG_FLAG = True\n",
    "CUT_SIZE = None if not DEBUG_FLAG else 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d411c49",
   "metadata": {},
   "source": [
    "## Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ed0158f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f50105c55a1e473a806c7441be38bef5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/2.97k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4772b94739b4703b624e51d716c67be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/15.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7404c985b7c54702a65962ea0190cee1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/9.37k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be44c095eda647cc99a0fcf06cfadb08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/41.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52ee56a674e64ceaaaa5bc9a468bae39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe85a5ebc6fd465387c17ea3861b18e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/658M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b914dc04d8204fc4b0ad102156d6680d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/919M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5fcef498acf4f4ca48aaa9b9b19e68b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/2.37G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f10a666968a945729945f2e1bd1e0051",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/80.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dfa6902a34d4d14b2b19986c95c5287",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/2.60G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "851968616dd44b26a05ba0e2321bd044",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/38.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85189eea09494f0f80d3beb35e9dc688",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "778fd1ad0bf5408bb2841efb57f45be6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aae28708a90e4099adfa954c5953d55f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/40836715 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f24d70d172a94cfbb94e260e2a649d86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "421c0d823d5545c7b60a7e3dff14cb11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/3003 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4abf8f35b3dd424a8701c9c5b18d5925",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56cf0e0271d040fb9608119b05bcd9c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/9.49M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e25cd6c05dec47ad8df5d42d42973bc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ce55a2e0bb443cdbde7926be3cd18e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77f1ff95fcf34bf1904448c5f581804e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1486965 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e71f55ad8164724828499bd6a45ba12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15ae6550c95e478db981de866afd5dae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/3003 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "translation_datasets = load_datasets('wmt14', [\"fr-en\", \"ru-en\"], CUT_SIZE)\n",
    "\n",
    "all_datasets = {'translation': translation_datasets}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "703f0725",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'translation': {'en': 'Resumption of the session',\n",
       "   'fr': 'Reprise de la session'}},\n",
       " {'translation': {'en': 'I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.',\n",
       "   'fr': 'Je déclare reprise la session du Parlement européen qui avait été interrompue le vendredi 17 décembre dernier et je vous renouvelle tous mes vux en espérant que vous avez passé de bonnes vacances.'}})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translation_datasets['fr-en']['train'][0], translation_datasets['fr-en']['train'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "da8f08f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm_pbar = lambda x, y: tqdm(x, leave=True, position=0, total=len(x), desc=f'{y}')\n",
    "def get_translations_for_dataset(\n",
    "    dataset_name, dataset, \n",
    "    artifacts, model, tokenizer, \n",
    "    pbar_func=tqdm_pbar, device=device, CUT_SIZE=CUT_SIZE\n",
    "):\n",
    "    collected_translations = defaultdict(list)\n",
    "    \n",
    "    for split, data in dataset.items():\n",
    "        if split != 'validation':\n",
    "            continue\n",
    "        \n",
    "        pbar = pbar_func(data, f\"{split} {dataset_name}\") if pbar_func is not None else data\n",
    "        for example in pbar:\n",
    "            # Encode the input sentences\n",
    "            ex1, g1 = dataset['train'][0][\"translation\"][\"en\"], dataset['train'][0][\"translation\"][artifacts[1]] \n",
    "            ex2, g2 = dataset['train'][1][\"translation\"][\"en\"], dataset['train'][1][\"translation\"][artifacts[1]]\n",
    "            ex3, g3 = dataset['train'][2][\"translation\"][\"en\"], dataset['train'][2][\"translation\"][artifacts[1]]\n",
    "            ex4, g4 = dataset['train'][3][\"translation\"][\"en\"], dataset['train'][3][\"translation\"][artifacts[1]]\n",
    "            ex5, g5 = dataset['train'][4][\"translation\"][\"en\"], dataset['train'][4][\"translation\"][artifacts[1]]\n",
    "            target_name = artifacts[2]\n",
    "            sample_src = example[\"translation\"][\"en\"]\n",
    "            prompt = f\"English: {ex1}\\n{target_name}: {g1}\" + \\\n",
    "                     f\"\\n\\nEnglish: {ex2}\\n{target_name}: {g2}\" + \\\n",
    "                     f\"\\n\\nEnglish: {ex3}\\n{target_name}: {g3}\" + \\\n",
    "                     f\"\\n\\nEnglish: {ex4}\\n{target_name}: {g4}\" + \\\n",
    "                     f\"\\n\\nEnglish: {ex5}\\n{target_name}: {g5}\" + \\\n",
    "                     f\"\\n\\nEnglish: {sample_src}\\n{target_name}: \"\n",
    "            \n",
    "            input_ids = tokenizer.encode(prompt, \n",
    "                                         truncation=True,  return_tensors=\"pt\").to(device)\n",
    "            attention_mask = torch.ones_like(input_ids).to(device)  # Create attention mask with ones\n",
    "\n",
    "            # Perform the generation\n",
    "            with torch.no_grad():\n",
    "                translation = model.generate(inputs=input_ids.to(device), attention_mask=attention_mask.to(device),\n",
    "                                             max_new_tokens=256, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "            # Decode the generated output\n",
    "            translation = tokenizer.decode(translation[0], skip_special_tokens=True)\n",
    "            #print(translation)\n",
    "            if f'English: {sample_src}' in translation:\n",
    "                translation = translation.split(f'English: {sample_src}')[1].strip()\n",
    "            #print('----->', '\\n', translation)\n",
    "            if 'English: ' in translation:\n",
    "                translation = translation.split('English: ')[0].strip()\n",
    "        \n",
    "            if f'{target_name}: ' in translation:\n",
    "                translation = translation.split(f'{target_name}: ')[1].strip()\n",
    "            #print(translation)\n",
    "\n",
    "            collected_translations[split].append(translation)\n",
    "         \n",
    "    return collected_translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "655a1266",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(src, prediction, gold, give_example=True):\n",
    "    prediction = np.array(prediction)\n",
    "    gold = np.array(gold)\n",
    "    # Calculate BLEU score\n",
    "    smoothie = SmoothingFunction().method3\n",
    "    bleu_score = corpus_bleu(gold, prediction, smoothing_function=smoothie)\n",
    "    print(\"BLEU score:\", bleu_score)\n",
    "\n",
    "    # Calculate exact match (EM) score\n",
    "    exact_match = np.mean(prediction == gold)\n",
    "    print(\"Exact Match (EM) score:\", exact_match)\n",
    "    \n",
    "    print(f'Example:\\nSrc:  {src[0]}\\nTgt:  {gold[0]}\\nPred: {prediction[0]}\\n\\n')\n",
    "    return bleu_score, exact_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0a15eb27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRANSLATION / fr-en\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba905f626b9848b1920e57cd8d575939",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation fr-en:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score: 0.0004335440242797244\n",
      "Exact Match (EM) score: 0.0\n",
      "Example:\n",
      "Src:  A Republican strategy to counter the re-election of Obama\n",
      "Tgt:  Une stratégie républicaine pour contrer la réélection d'Obama\n",
      "Pred: était en ce n'est pas, comme un certain nombre de collègues me l'ont demandé, que nous observions une minute de silence pour toutes les victimes, des tempêtes notamment, dans les différents pays de l'Union européenne qui ont été touchés.\n",
      "\n",
      "\n",
      "TRANSLATION / ru-en\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a831c49dd45490bac146d3f20010546",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation ru-en:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score: 0.0013973130863582958\n",
      "Exact Match (EM) score: 0.0\n",
      "Example:\n",
      "Src:  A Republican strategy to counter the re-election of Obama\n",
      "Tgt:  Республиканская стратегия сопротивления повторному избранию Обамы\n",
      "Pred: это готовая к использованию паста, которая наносится шпателем или пальцами в виде закругленного перехода в углы сталелитейного кокиль от горячего абразивного стального литья.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for dn, datasets in all_datasets.items():\n",
    "    for dataset_name, dataset in datasets.items():\n",
    "        print(f\"{dn.upper()} / {dataset_name}\\n\")\n",
    "        dataset_translations = get_translations_for_dataset(\n",
    "            dataset_name,\n",
    "            dataset, \n",
    "            translation_datasets_artifacts[dataset_name], \n",
    "            model, \n",
    "            tokenizer)\n",
    "        \n",
    "        to_language = translation_datasets_artifacts[dataset_name][1]\n",
    "        \n",
    "        val_gold_translation = [el['translation'][to_language].strip() for el in dataset['validation']]\n",
    "        src_val = [el['translation']['en'].strip() for el in dataset['validation']]\n",
    "        pred = [el.strip() for  el in dataset_translations['validation']]\n",
    "        \n",
    "        get_scores(src_val, pred, val_gold_translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c8bf32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
