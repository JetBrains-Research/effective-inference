{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "132681fa-7b1f-4ff5-a55e-9c90353188a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/shapkin/effective-inference\n"
     ]
    }
   ],
   "source": [
    "%cd ../.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85b67a0-fe46-4104-a66e-bf8bdabb756c",
   "metadata": {},
   "source": [
    "## Import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "570ebef6-a677-45ff-8ee9-69f8ee0747fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import yaml\n",
    "import h5py\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import json\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils.dataset_cache import cache_embeddings, get_dataset_for_regression, build_dataset_from_cached, load_cached_dataset\n",
    "from utils.dataset_cache import build_dict_dataset_from_cached\n",
    "from utils.prepare_dataset import load_datasets, cut_datasets\n",
    "from utils.config import ConfigWrapper\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from typing import Tuple, List, Dict, Optional, Union\n",
    "from numpy.random import shuffle\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00e31b89-adfc-4ecf-90b0-b0fa10375fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.attentions.bert.linear import BertWrapperLin, LinearClassifierBertAttention, LinearAttention\n",
    "from utils.dataset_utils import get_dict_batch, prepare_batches\n",
    "from utils.train_linear_utils import train_epoch, eval_epoch, plot_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1d904b-77f3-4850-85e9-2a32cc667fc4",
   "metadata": {},
   "source": [
    "## Project configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e04b0aa1-8b62-467c-b551-661a0dcb150e",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = 'config.yaml'\n",
    "\n",
    "with open(config_path, \"r\") as f:\n",
    "    config = ConfigWrapper(yaml.load(f, Loader=yaml.FullLoader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fb0245e8-7e08-418f-82be-3640ef85eee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_linear_model(X_train, X_test, y_train, y_test, config, save_pattern='', \n",
    "                       use_plots=False, save_final_results=False, \n",
    "                       verbose=False, use_pbars=False, save_model=False):\n",
    "    \n",
    "    add_ = 0 if len(X_train) % config.attention_config.train_batch_size == 0 else 1\n",
    "    total_len = (len(X_train) // config.attention_config.train_batch_size) + add_\n",
    "    \n",
    "    model = LinearAttention(config.attention_config).to(config.general.device)\n",
    "\n",
    "    for param_name, param in model.named_parameters():\n",
    "        print(param_name, param)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.01, weight_decay=1)\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler = None #torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.001, steps_per_epoch=total_len, epochs=config.general.num_epochs)\n",
    "    # scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "    train_log = []\n",
    "    val_log = []\n",
    "    for epoch in range(config.general.num_epochs):\n",
    "        if use_plots:\n",
    "            clear_output()\n",
    "            \n",
    "        train_loss, _ = train_epoch(model, optimizer, criterion, X_train, y_train, config, scheduler=scheduler, use_pbar=use_pbars)\n",
    "        val_loss, val_preds = eval_epoch(model, criterion, X_test, y_test, config, use_pbar=use_pbars)\n",
    "        train_log.extend(train_loss)\n",
    "        steps = len(train_loss)\n",
    "        val_log.append((steps * (epoch + 1), np.mean(val_loss)))\n",
    "        \n",
    "        if use_plots:\n",
    "            print(f'{epoch} -- VAL R2 score:', r2_score(y_test, val_preds))\n",
    "            plot_history(train_log, val_log)\n",
    "        elif verbose:\n",
    "            print(f'{epoch} -- Mean train loss:', np.mean(train_loss))\n",
    "            print(f'{epoch} -- Mean val loss:', np.mean(val_loss))\n",
    "            print(f'{epoch} -- VAL R2 score:', r2_score(y_test, val_preds))\n",
    "            print()\n",
    "\n",
    "        if epoch + 1 == config.general.num_epochs and save_final_results and save_pattern != '':\n",
    "            if not os.path.exists(f'{config.data.data_path}/linear_models'):\n",
    "                os.makedirs(f'{config.data.data_path}/linear_models')\n",
    "            if not os.path.exists(f'{config.data.data_path}/linear_models/{save_pattern}'):\n",
    "                os.makedirs(f'{config.data.data_path}/linear_models/{save_pattern}')\n",
    "            with open(f'{config.data.data_path}/linear_models/{save_pattern}/preds.json', 'wb') as f:\n",
    "                np.save(f, val_preds) # json.dump(val_preds, f)\n",
    "            with open(f'{config.data.data_path}/linear_models/{save_pattern}/true.json', 'wb') as f:\n",
    "                np.save(f, y_test) # json.dump(y_test, f)\n",
    "\n",
    "    if save_model:\n",
    "        if not os.path.exists(f'{config.data.data_path}/linear_models'):\n",
    "            os.makedirs(f'{config.data.data_path}/linear_models')\n",
    "        if not os.path.exists(f'{config.data.data_path}/linear_models/{save_pattern}'):\n",
    "            os.makedirs(f'{config.data.data_path}/linear_models/{save_pattern}')\n",
    "        model.to('cpu')\n",
    "        torch.save(model.state_dict(), f'{config.data.data_path}/linear_models/{save_pattern}/model.pth')\n",
    "\n",
    "    if epoch + 1 == config.general.num_epochs and not verbose and not use_plots:\n",
    "        print(f'Final val loss:', np.mean(val_loss))\n",
    "        print(f'Final val R2 score:', r2_score(y_test, val_preds))\n",
    "    return model\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b42217fc-f58a-4a04-9280-52e5cfffdb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(config.model.model_name, max_length=config.general.max_len)\n",
    "initial_model = AutoModel.from_pretrained(config.model.model_name).to(config.general.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e968d196-e199-435d-9054-a13c94d11210",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'imdb': DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['text', 'label'],\n",
       "         num_rows: 25000\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['text', 'label'],\n",
       "         num_rows: 25000\n",
       "     })\n",
       "     unsupervised: Dataset({\n",
       "         features: ['text', 'label'],\n",
       "         num_rows: 50000\n",
       "     })\n",
       " })}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_datasets = load_datasets(config.data.train_datasets, config.data.cut_size)\n",
    "train_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f751ad-9be8-4158-9b00-968a3432917f",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5ae194ba-c00f-46e6-bebb-a7fc3f8d99f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a6e1691ddd4459c9ec02728fa71e391",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m pbar \u001b[38;5;241m=\u001b[39m tqdm(total\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m, position\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer_N \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m12\u001b[39m):\n\u001b[0;32m---> 19\u001b[0m     X_train, y_train, X_test, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_dict_dataset_from_cached\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_datasets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_N\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m                                                                  \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m                                                                  \u001b[49m\u001b[43msplit_hidden\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain size:\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mlen\u001b[39m(X_train))\n\u001b[1;32m     23\u001b[0m     train_linear_model(X_train, X_test, y_train, y_test, config, save_pattern\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mmodel_save_pattern\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_N\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m     24\u001b[0m                        use_plots\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, save_final_results\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \n\u001b[1;32m     25\u001b[0m                        verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, use_pbars\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, save_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/effective-inference/utils/dataset_cache.py:267\u001b[0m, in \u001b[0;36mbuild_dict_dataset_from_cached\u001b[0;34m(config, train_datasets, layer, heads, features, split_hidden)\u001b[0m\n\u001b[1;32m    265\u001b[0m             full_data_to_linear[fn] \u001b[38;5;241m=\u001b[39m f[fn][()][\u001b[38;5;28mslice\u001b[39m[\u001b[38;5;241m0\u001b[39m]:\u001b[38;5;28mslice\u001b[39m[\u001b[38;5;241m1\u001b[39m]]\n\u001b[1;32m    266\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 267\u001b[0m             full_data_to_linear[fn] \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m]\u001b[49m[()]\n\u001b[1;32m    268\u001b[0m     tgt \u001b[38;5;241m=\u001b[39m f[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m][()] \n\u001b[1;32m    270\u001b[0m X_train\u001b[38;5;241m.\u001b[39mappend(deepcopy(full_data_to_linear))\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/effective-inference-puaXtOsB-py3.11/lib/python3.11/site-packages/h5py/_hl/group.py:357\u001b[0m, in \u001b[0;36mGroup.__getitem__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid HDF5 object reference\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(name, (\u001b[38;5;28mbytes\u001b[39m, \u001b[38;5;28mstr\u001b[39m)):\n\u001b[0;32m--> 357\u001b[0m     oid \u001b[38;5;241m=\u001b[39m \u001b[43mh5o\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_e\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlapl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lapl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccessing a group is done with bytes or str, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    360\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mtype\u001b[39m(name)))\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5o.pyx:190\u001b[0m, in \u001b[0;36mh5py.h5o.open\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5i.pyx:43\u001b[0m, in \u001b[0;36mh5py.h5i.wrap_identifier\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:405\u001b[0m, in \u001b[0;36mparent\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if config.attention_config.split_heads or config.attention_config.model_for_each_head:\n",
    "    pbar = tqdm(total=len(config.attention_config.layers_to_train) * len(config.attention_config.heads_to_train), position=0, leave=True)\n",
    "    for layer_N in config.attention_config.layers_to_train:\n",
    "        for head_N in config.attention_config.heads_to_train:\n",
    "            print(f'Training {layer_N} layer, {head_N} head')\n",
    "            X_train, y_train, X_test, y_test = build_dict_dataset_from_cached(config, train_datasets, layer=layer_N, heads=[head_N], \n",
    "                                                                      features=config.attention_config.features, \n",
    "                                                                      split_hidden=config.attention_config.split_heads_in_data)\n",
    "            print('Train size:', len(X_train))\n",
    "            print(X_train[10]['hidden_to'].shape)\n",
    "            #train_linear_model(X_train, X_test, y_train, y_test, config, save_pattern=f'{config.data.model_save_pattern}_{layer_N}_{head_N}', \n",
    "            #                   use_plots=False, save_final_results=True, \n",
    "            #                   verbose=True, use_pbars=False, save_model=True)\n",
    "            pbar.update(1)\n",
    "\n",
    "else:\n",
    "    pbar = tqdm(total=12, position=0, leave=True)\n",
    "    for layer_N in range(12):\n",
    "        X_train, y_train, X_test, y_test = build_dict_dataset_from_cached(config, train_datasets, layer=layer_N, heads=[0, 1, 2], \n",
    "                                                                      features=config.attention_config.features, \n",
    "                                                                      split_hidden=False)\n",
    "        print('Train size:', len(X_train))\n",
    "        train_linear_model(X_train, X_test, y_train, y_test, config, save_pattern=f'{config.data.model_save_pattern}_{layer_N}', \n",
    "                           use_plots=False, save_final_results=True, \n",
    "                           verbose=True, use_pbars=False, save_model=True)\n",
    "        pbar.update(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
