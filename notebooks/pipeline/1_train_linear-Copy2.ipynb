{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "132681fa-7b1f-4ff5-a55e-9c90353188a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/shapkin/effective-inference\n"
     ]
    }
   ],
   "source": [
    "%cd ../.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85b67a0-fe46-4104-a66e-bf8bdabb756c",
   "metadata": {},
   "source": [
    "## Import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "570ebef6-a677-45ff-8ee9-69f8ee0747fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import yaml\n",
    "import h5py\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import json\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils.dataset_cache import cache_embeddings, get_dataset_for_regression, build_dataset_from_cached, load_cached_dataset\n",
    "from utils.dataset_cache import build_dict_dataset_from_cached\n",
    "from utils.prepare_dataset import load_datasets, cut_datasets\n",
    "from utils.config import ConfigWrapper\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from typing import Tuple, List, Dict, Optional, Union\n",
    "from numpy.random import shuffle\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00e31b89-adfc-4ecf-90b0-b0fa10375fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.attentions.bert.linear import BertWrapperLin, LinearClassifierBertAttention, LinearAttention\n",
    "from utils.dataset_utils import get_dict_batch, prepare_batches\n",
    "from utils.train_linear_utils import train_epoch, eval_epoch, plot_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1d904b-77f3-4850-85e9-2a32cc667fc4",
   "metadata": {},
   "source": [
    "## Project configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e04b0aa1-8b62-467c-b551-661a0dcb150e",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = 'config.yaml'\n",
    "\n",
    "with open(config_path, \"r\") as f:\n",
    "    config = ConfigWrapper(yaml.load(f, Loader=yaml.FullLoader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fb0245e8-7e08-418f-82be-3640ef85eee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_linear_model(X_train, X_test, y_train, y_test, config, save_pattern='', \n",
    "                       use_plots=False, save_final_results=False, \n",
    "                       verbose=False, use_pbars=False, save_model=False):\n",
    "    \n",
    "    add_ = 0 if len(X_train) % config.attention_config.train_batch_size == 0 else 1\n",
    "    total_len = (len(X_train) // config.attention_config.train_batch_size) + add_\n",
    "    \n",
    "    model = LinearAttention(config.attention_config).to(config.general.device)\n",
    "\n",
    "    for param_name, param in model.named_parameters():\n",
    "        print(param_name, param)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.01, weight_decay=1)\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler = None #torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.001, steps_per_epoch=total_len, epochs=config.general.num_epochs)\n",
    "    # scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "    train_log = []\n",
    "    val_log = []\n",
    "    for epoch in range(config.general.num_epochs):\n",
    "        if use_plots:\n",
    "            clear_output()\n",
    "            \n",
    "        train_loss, _ = train_epoch(model, optimizer, criterion, X_train, y_train, config, scheduler=scheduler, use_pbar=use_pbars)\n",
    "        val_loss, val_preds = eval_epoch(model, criterion, X_test, y_test, config, use_pbar=use_pbars)\n",
    "        train_log.extend(train_loss)\n",
    "        steps = len(train_loss)\n",
    "        val_log.append((steps * (epoch + 1), np.mean(val_loss)))\n",
    "        \n",
    "        if use_plots:\n",
    "            print(f'{epoch} -- VAL R2 score:', r2_score(y_test, val_preds))\n",
    "            plot_history(train_log, val_log)\n",
    "        elif verbose:\n",
    "            print(f'{epoch} -- Mean train loss:', np.mean(train_loss))\n",
    "            print(f'{epoch} -- Mean val loss:', np.mean(val_loss))\n",
    "            print(f'{epoch} -- VAL R2 score:', r2_score(y_test, val_preds))\n",
    "            print()\n",
    "\n",
    "        if epoch + 1 == config.general.num_epochs and save_final_results and save_pattern != '':\n",
    "            if not os.path.exists(f'{config.data.data_path}/linear_models'):\n",
    "                os.makedirs(f'{config.data.data_path}/linear_models')\n",
    "            if not os.path.exists(f'{config.data.data_path}/linear_models/{save_pattern}'):\n",
    "                os.makedirs(f'{config.data.data_path}/linear_models/{save_pattern}')\n",
    "            with open(f'{config.data.data_path}/linear_models/{save_pattern}/preds.json', 'wb') as f:\n",
    "                np.save(f, val_preds) # json.dump(val_preds, f)\n",
    "            with open(f'{config.data.data_path}/linear_models/{save_pattern}/true.json', 'wb') as f:\n",
    "                np.save(f, y_test) # json.dump(y_test, f)\n",
    "\n",
    "    if save_model:\n",
    "        if not os.path.exists(f'{config.data.data_path}/linear_models'):\n",
    "            os.makedirs(f'{config.data.data_path}/linear_models')\n",
    "        if not os.path.exists(f'{config.data.data_path}/linear_models/{save_pattern}'):\n",
    "            os.makedirs(f'{config.data.data_path}/linear_models/{save_pattern}')\n",
    "        model.to('cpu')\n",
    "        torch.save(model.state_dict(), f'{config.data.data_path}/linear_models/{save_pattern}/model.pth')\n",
    "\n",
    "    if epoch + 1 == config.general.num_epochs and not verbose and not use_plots:\n",
    "        print(f'Final val loss:', np.mean(val_loss))\n",
    "        print(f'Final val R2 score:', r2_score(y_test, val_preds))\n",
    "    return model\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b42217fc-f58a-4a04-9280-52e5cfffdb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(config.model.model_name, max_length=config.general.max_len)\n",
    "initial_model = AutoModel.from_pretrained(config.model.model_name).to(config.general.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e968d196-e199-435d-9054-a13c94d11210",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'imdb': DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['text', 'label'],\n",
       "         num_rows: 25000\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['text', 'label'],\n",
       "         num_rows: 25000\n",
       "     })\n",
       "     unsupervised: Dataset({\n",
       "         features: ['text', 'label'],\n",
       "         num_rows: 50000\n",
       "     })\n",
       " })}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_datasets = load_datasets(config.data.train_datasets, config.data.cut_size)\n",
    "train_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f751ad-9be8-4158-9b00-968a3432917f",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5ae194ba-c00f-46e6-bebb-a7fc3f8d99f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a6e1691ddd4459c9ec02728fa71e391",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m pbar \u001b[38;5;241m=\u001b[39m tqdm(total\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m, position\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer_N \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m12\u001b[39m):\n\u001b[0;32m---> 19\u001b[0m     X_train, y_train, X_test, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_dict_dataset_from_cached\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_datasets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_N\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m                                                                  \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m                                                                  \u001b[49m\u001b[43msplit_hidden\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain size:\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mlen\u001b[39m(X_train))\n\u001b[1;32m     23\u001b[0m     train_linear_model(X_train, X_test, y_train, y_test, config, save_pattern\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mmodel_save_pattern\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_N\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m     24\u001b[0m                        use_plots\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, save_final_results\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \n\u001b[1;32m     25\u001b[0m                        verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, use_pbars\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, save_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/effective-inference/utils/dataset_cache.py:267\u001b[0m, in \u001b[0;36mbuild_dict_dataset_from_cached\u001b[0;34m(config, train_datasets, layer, heads, features, split_hidden)\u001b[0m\n\u001b[1;32m    265\u001b[0m             full_data_to_linear[fn] \u001b[38;5;241m=\u001b[39m f[fn][()][\u001b[38;5;28mslice\u001b[39m[\u001b[38;5;241m0\u001b[39m]:\u001b[38;5;28mslice\u001b[39m[\u001b[38;5;241m1\u001b[39m]]\n\u001b[1;32m    266\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 267\u001b[0m             full_data_to_linear[fn] \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m]\u001b[49m[()]\n\u001b[1;32m    268\u001b[0m     tgt \u001b[38;5;241m=\u001b[39m f[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m][()] \n\u001b[1;32m    270\u001b[0m X_train\u001b[38;5;241m.\u001b[39mappend(deepcopy(full_data_to_linear))\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/effective-inference-puaXtOsB-py3.11/lib/python3.11/site-packages/h5py/_hl/group.py:357\u001b[0m, in \u001b[0;36mGroup.__getitem__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid HDF5 object reference\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(name, (\u001b[38;5;28mbytes\u001b[39m, \u001b[38;5;28mstr\u001b[39m)):\n\u001b[0;32m--> 357\u001b[0m     oid \u001b[38;5;241m=\u001b[39m \u001b[43mh5o\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_e\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlapl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lapl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccessing a group is done with bytes or str, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    360\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mtype\u001b[39m(name)))\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5o.pyx:190\u001b[0m, in \u001b[0;36mh5py.h5o.open\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5i.pyx:43\u001b[0m, in \u001b[0;36mh5py.h5i.wrap_identifier\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:405\u001b[0m, in \u001b[0;36mparent\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if config.attention_config.split_heads or config.attention_config.model_for_each_head:\n",
    "    pbar = tqdm(total=len(config.attention_config.layers_to_train) * len(config.attention_config.heads_to_train), position=0, leave=True)\n",
    "    for layer_N in config.attention_config.layers_to_train:\n",
    "        for head_N in config.attention_config.heads_to_train:\n",
    "            print(f'Training {layer_N} layer, {head_N} head')\n",
    "            X_train, y_train, X_test, y_test = build_dict_dataset_from_cached(config, train_datasets, layer=layer_N, heads=[head_N], \n",
    "                                                                      features=config.attention_config.features, \n",
    "                                                                      split_hidden=config.attention_config.split_heads_in_data)\n",
    "            print('Train size:', len(X_train))\n",
    "            print(X_train[10]['hidden_to'].shape)\n",
    "            #train_linear_model(X_train, X_test, y_train, y_test, config, save_pattern=f'{config.data.model_save_pattern}_{layer_N}_{head_N}', \n",
    "            #                   use_plots=False, save_final_results=True, \n",
    "            #                   verbose=True, use_pbars=False, save_model=True)\n",
    "            pbar.update(1)\n",
    "\n",
    "else:\n",
    "    pbar = tqdm(total=12, position=0, leave=True)\n",
    "    for layer_N in range(12):\n",
    "        X_train, y_train, X_test, y_test = build_dict_dataset_from_cached(config, train_datasets, layer=layer_N, heads=[0, 1, 2], \n",
    "                                                                      features=config.attention_config.features, \n",
    "                                                                      split_hidden=False)\n",
    "        print('Train size:', len(X_train))\n",
    "        train_linear_model(X_train, X_test, y_train, y_test, config, save_pattern=f'{config.data.model_save_pattern}_{layer_N}', \n",
    "                           use_plots=False, save_final_results=True, \n",
    "                           verbose=True, use_pbars=False, save_model=True)\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "1e2af375-5660-4865-8c2c-74a9fa32f86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = build_dict_dataset_from_cached(config, train_datasets, layer=0, heads=[0], \n",
    "                                                                      features=config.attention_config.features, \n",
    "                                                                      split_hidden=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "8a6511ae-5a73-4d10-a61f-4aceb54cdf4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_vectors(X_train, y_train):\n",
    "    dataset, target = [], []\n",
    "    for i in range(len(X_train)):\n",
    "        fv = []\n",
    "        for k, v in X_train[i].items():\n",
    "            try:\n",
    "                fv += list(v)\n",
    "            except:\n",
    "                fv += [v]\n",
    "        dataset.append(fv)\n",
    "        target.append(y_train[i])\n",
    "    return np.array(dataset), np.array(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "1cd5d846-7ab7-4046-8961-1f24634f8137",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1, y_train1 = to_vectors(X_train, y_train)\n",
    "X_test1, y_test1 = to_vectors(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "005470db-d2d8-4182-a996-128381df0cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.linear_model import SGDRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "9f9066e7-de08-42bf-8d74-2a28f3b740a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Ridge(solver='svd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "4379b124-0bbb-4032-bcfa-d22a11eda43e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-7 {color: black;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Ridge(solver=&#x27;svd&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" checked><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Ridge</label><div class=\"sk-toggleable__content\"><pre>Ridge(solver=&#x27;svd&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "Ridge(solver='svd')"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.fit(X_train1, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "0e36869b-316b-4ee7-bae0-df0253ceebf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.14876008e-02,  9.58622339e-03, -4.41515484e-03, ...,\n",
       "       -1.20941502e+00,  1.10310109e-01, -1.10338978e-14])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "ba6a0182-c72a-4570-a297-3f942bc808d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.27447040713421234"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = m.predict(X_train1)\n",
    "r2_score(y_train1, preds)\n",
    "preds = m.predict(X_test1)\n",
    "r2_score(y_test1, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ccd19d-76cf-4de5-8e80-2a5deeb87f16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2e9af2-32b7-4aef-90d5-1292afdd1093",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "5aa3a034-c271-46eb-90ce-832c0a5ab796",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.dot(np.dot(np.linalg.inv(np.dot(X_train1.T, X_train1) + np.ones_like(np.dot(X_train1.T, X_train1)) * 0.01), X_train1.T), y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "cae76e05-0032-4a38-ab1a-e33449019edc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.32631248e+06,  1.38765318e+06,  1.43136212e+06, ...,\n",
       "        3.07740085e+04,  1.27252901e+02, -8.87666222e+05])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c032120a-b78c-464d-8459-acc1a120d14d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb5561f-260c-4d96-8555-9bf5321f31ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "63c2a037-d5bb-4de7-865c-11b20f4ffc21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-5.64006054, -6.48562395, -6.24644707, ..., -6.5668683 ,\n",
       "       -6.4783497 , -6.37182276])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "1db50c40-6bcb-4b4a-a7df-ee2e0c965c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "\n",
    "scaler = sklearn.preprocessing.StandardScaler()\n",
    "X_train2 = scaler.fit_transform(X_train1)\n",
    "X_test2 = scaler.transform(X_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "4eb1b474-0fc2-4d21-bb43-0068d244917a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2, y_train2 = torch.tensor(X_train2, dtype=torch.float32), torch.tensor(y_train1, dtype=torch.float32) \n",
    "X_test2, y_test2 = torch.tensor(X_test2, dtype=torch.float32), torch.tensor(y_test1, dtype=torch.float32) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "90932b92-01a6-4479-a655-917be8bfd18e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2152, dtype=torch.float64)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion(y_train2, torch.tensor(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "875d8259-0ce5-4867-b940-8da4221f3ea9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1547,)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train1[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "2a853c54-9cfe-4922-8e74-592a7259cdbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2810423/110061140.py:3: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  model.weight.data = torch.tensor([w], dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "# Linear model\n",
    "model = nn.Linear(1547, 1)\n",
    "model.weight.data = torch.tensor([w], dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "70d5e181-cb1b-4ab7-b798-1b6edfc6de28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.3263e+06,  1.3877e+06,  1.4314e+06,  ...,  3.0774e+04,\n",
       "          1.2725e+02, -8.8767e+05]])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "22a7094e-3661-4cf1-8fb2-059f010d92ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.3263e+06,  1.3877e+06,  1.4314e+06,  ...,  3.0774e+04,\n",
       "          1.2725e+02, -8.8767e+05]])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47904267-fb13-4a66-abf0-5cbce28b04a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "24ea5f68-fd9f-4abd-852b-1f4f3f998246",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(X_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "aa3cef21-a794-458c-9e61-932b07efeeae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-6.836387443022793e+17"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_test1, outputs.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cdc38b-cb44-4632-8efc-bec1acfe66a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "f809c759-c0f4-47be-8063-e36339f511d7",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'float' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[113], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     17\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 18\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.0001\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (epoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m], Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/effective-inference-puaXtOsB-py3.11/lib/python3.11/site-packages/torch/optim/optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m                                \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 280\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    283\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/effective-inference-puaXtOsB-py3.11/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/effective-inference-puaXtOsB-py3.11/lib/python3.11/site-packages/torch/optim/lbfgs.py:312\u001b[0m, in \u001b[0;36mLBFGS.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    309\u001b[0m state\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_iter\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    311\u001b[0m \u001b[38;5;66;03m# evaluate initial f(x) and df/dx\u001b[39;00m\n\u001b[0;32m--> 312\u001b[0m orig_loss \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(orig_loss)\n\u001b[1;32m    314\u001b[0m current_evals \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/effective-inference-puaXtOsB-py3.11/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'float' object is not callable"
     ]
    }
   ],
   "source": [
    "# Linear model\n",
    "model = nn.Linear(1547, 1)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.LBFGS(model.parameters(), lr=1)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    outputs = model(X_train2)\n",
    "    loss = criterion(outputs, y_train2)\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step(0.0001)\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "589910ea-4ccc-4d17-8012-5bd0f2c43533",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(X_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "44b76afe-ead2-4ff5-b171-21b7a03e321f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-69.78649707264105"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_test1, outputs.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52863490-7f48-4b8e-ba07-60747458105e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3f83004c-d9fc-40e5-a962-d93509fd6532",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1196,  0.2523, -0.0407,  ...,  0.2852,  0.0034,  0.0000],\n",
       "        [ 0.0077,  0.4746,  0.6170,  ...,  0.5000,  0.0020,  0.0000],\n",
       "        [-0.7503,  0.0204,  0.5124,  ...,  0.2666,  0.0037,  0.0000],\n",
       "        ...,\n",
       "        [-0.2463,  0.9201,  0.0722,  ...,  0.3633,  0.0027,  0.0000],\n",
       "        [ 0.7329,  0.7134,  0.1441,  ...,  0.3682,  0.0027,  0.0000],\n",
       "        [-0.7599, -0.7485, -0.3053,  ...,  0.5000,  0.0020,  0.0000]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d1763b2d-93d7-4165-b67d-24dee1c05e78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.weight.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30777c3-0053-4baf-a0f4-c22d67b448f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "22be6c1f-dfe6-4b50-907c-4377f4ee1f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = m.predict(X_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "37905a7d-c005-441c-b93b-f3df147a1abf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.27447040713421134"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_test1, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f70d2e-fe44-4787-9ec2-5c245612cdc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(LinearAttention, self).__init__()\n",
    "        self.config = config\n",
    "        self.features = config['features']\n",
    "        self.device = config['device']\n",
    "        self.batch_size = config['batch_size']\n",
    "\n",
    "        self.dim_size = config['d_model']\n",
    "        if config.split_heads:\n",
    "            self.dim_size = config['d_model'] // config['num_heads']\n",
    "                        \n",
    "        for k in self.features:                \n",
    "            if 'hidden' in k:\n",
    "                learnable_parameters = f'torch.nn.Linear(in_features={self.dim_size}, out_features=1, bias=False)'\n",
    "                exec(f\"self.{k} = {learnable_parameters}\")\n",
    "            else:\n",
    "                learnable_parameters = f'nn.Parameter(torch.randn(1), requires_grad=True)'\n",
    "                exec(f\"self.{k} = {learnable_parameters}\")\n",
    "                    \n",
    "            \n",
    "    def forward(self, seq_len_arg=None, **kwargs):\n",
    "        if seq_len_arg is not None:\n",
    "            result = torch.zeros((self.batch_size, seq_len_arg, seq_len_arg), device=self.device)\n",
    "            \n",
    "            for arg_name, arg_value in kwargs.items():\n",
    "                namespace = {'cur_result': None, 'self': self, 'arg_name': arg_name, 'arg_value': arg_value}\n",
    "                if 'hidden' in arg_name:\n",
    "                    exec(f\"cur_result = self.{arg_name}(arg_value)\", namespace)\n",
    "                else:\n",
    "                    exec(f\"cur_result = self.{arg_name} * arg_value\", namespace)\n",
    "                if 'from' in arg_name:\n",
    "                    result += namespace['cur_result'].T\n",
    "                else:\n",
    "                    result += namespace['cur_result']\n",
    "        else:\n",
    "            for arg_name, arg_value in kwargs.items():\n",
    "                bs = len(arg_value)\n",
    "                break\n",
    "            result = torch.zeros((bs, 1), device=self.device, )\n",
    "            \n",
    "            for arg_name, arg_value in kwargs.items():\n",
    "                namespace = {'cur_result': None, 'self': self, 'arg_name': arg_name, 'arg_value': arg_value}\n",
    "                if 'hidden' in arg_name:\n",
    "                    exec(f\"cur_result = self.{arg_name}(arg_value)\", namespace)\n",
    "                else:\n",
    "                    exec(f\"cur_result = self.{arg_name} * arg_value\", namespace)\n",
    "\n",
    "                result += namespace['cur_result'].view((bs, 1))\n",
    "            \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8507c655-2818-471b-8939-31d77ad10348",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_to Parameter containing:\n",
      "tensor([-0.3278], device='cuda:1', requires_grad=True)\n",
      "pos_from Parameter containing:\n",
      "tensor([0.1027], device='cuda:1', requires_grad=True)\n",
      "relev_pos_from Parameter containing:\n",
      "tensor([0.8275], device='cuda:1', requires_grad=True)\n",
      "relev_pos_to Parameter containing:\n",
      "tensor([-0.4740], device='cuda:1', requires_grad=True)\n",
      "inv_pos_from Parameter containing:\n",
      "tensor([-0.5632], device='cuda:1', requires_grad=True)\n",
      "inv_pos_to Parameter containing:\n",
      "tensor([0.0237], device='cuda:1', requires_grad=True)\n",
      "inv_relev_pos_from Parameter containing:\n",
      "tensor([-0.9353], device='cuda:1', requires_grad=True)\n",
      "inv_relev_pos_to Parameter containing:\n",
      "tensor([0.1786], device='cuda:1', requires_grad=True)\n",
      "seq_len Parameter containing:\n",
      "tensor([-0.2316], device='cuda:1', requires_grad=True)\n",
      "inv_seq_len Parameter containing:\n",
      "tensor([0.6134], device='cuda:1', requires_grad=True)\n",
      "head_num Parameter containing:\n",
      "tensor([-2.3959], device='cuda:1', requires_grad=True)\n",
      "hidden_to.weight Parameter containing:\n",
      "tensor([[-3.0319e-02,  2.5694e-02,  2.7288e-03, -2.6970e-02,  2.7621e-02,\n",
      "         -2.2351e-02, -3.2589e-02, -1.8543e-02,  2.7459e-02,  1.2043e-02,\n",
      "          1.9557e-02, -9.0550e-03,  2.2064e-02,  1.9743e-02,  1.9211e-02,\n",
      "          2.9173e-02, -8.6971e-03,  6.8713e-03, -2.1930e-02, -2.7672e-03,\n",
      "          1.2084e-03,  3.4002e-02,  3.2685e-02, -1.4791e-02, -1.1069e-02,\n",
      "          1.9607e-02, -1.1578e-02, -3.0757e-02, -2.3357e-02, -1.1322e-03,\n",
      "         -2.3785e-02,  1.3715e-02,  6.9367e-03,  3.4294e-03, -1.7241e-02,\n",
      "          2.5390e-02,  2.6937e-02, -2.0353e-02,  3.0473e-02,  1.2301e-02,\n",
      "         -9.8599e-03,  1.2493e-02,  2.2958e-03,  5.5479e-03,  2.7392e-02,\n",
      "         -1.9956e-03, -5.9088e-03,  9.8639e-03, -5.8759e-03, -1.7397e-02,\n",
      "          2.9810e-02,  3.6016e-02,  8.6855e-03, -1.6442e-02, -3.2697e-02,\n",
      "          1.6110e-02,  1.7784e-02, -9.3664e-03,  2.6298e-02,  2.8189e-02,\n",
      "          2.8109e-02,  2.0950e-02, -1.4300e-02, -1.0072e-02,  1.3334e-02,\n",
      "         -3.2200e-02, -4.4241e-03, -3.2197e-02, -3.0086e-02,  1.2431e-02,\n",
      "         -1.9387e-02, -1.6031e-02, -1.3785e-02,  2.4498e-02,  2.2255e-02,\n",
      "          1.0330e-02, -2.8409e-02,  1.0920e-02,  3.0473e-02, -2.1101e-02,\n",
      "         -1.2947e-02, -2.0578e-02,  1.5539e-02,  1.5806e-04,  1.9971e-02,\n",
      "          9.4766e-03, -2.2637e-02,  1.1013e-02,  2.8807e-02, -2.3574e-02,\n",
      "          3.0074e-02,  2.5717e-02,  2.9408e-02,  2.3077e-02,  3.4210e-02,\n",
      "          2.4225e-03,  2.9203e-02,  1.9231e-02, -1.4446e-02, -6.4826e-03,\n",
      "          2.8865e-02, -8.8695e-03,  1.1316e-02,  2.7549e-02,  1.6183e-02,\n",
      "         -7.5008e-04,  1.3728e-02, -2.6417e-02,  2.9261e-02, -2.6489e-02,\n",
      "          2.8799e-02,  2.9553e-02, -1.2654e-02,  3.5488e-02, -2.3944e-02,\n",
      "         -3.5956e-04, -5.4462e-03,  1.6800e-02,  1.9931e-02, -2.6402e-02,\n",
      "         -1.3228e-02,  3.3195e-02,  1.1769e-02,  6.8042e-03,  2.8339e-02,\n",
      "          2.3133e-02, -1.3666e-02, -1.3338e-02, -2.5971e-02, -9.6890e-03,\n",
      "          1.1678e-03, -1.1197e-02,  2.7433e-02,  3.9333e-03,  3.9763e-03,\n",
      "         -1.0503e-02,  8.5394e-03,  1.9340e-02, -3.4361e-02,  2.9291e-02,\n",
      "         -2.5482e-02, -1.8303e-02,  3.0879e-02, -2.1449e-02,  6.8291e-03,\n",
      "          2.5483e-02,  1.9207e-03,  3.4242e-02,  1.3277e-02, -2.7259e-02,\n",
      "          1.3294e-02, -1.2258e-02,  2.4871e-03,  3.2838e-03,  1.0453e-02,\n",
      "         -2.8422e-02,  1.2963e-02,  1.3681e-02, -5.2316e-04,  3.4641e-02,\n",
      "          3.1745e-02,  4.4801e-03, -2.1856e-02,  2.0112e-02,  1.8225e-02,\n",
      "         -2.3397e-02,  1.6379e-03,  1.4642e-02,  2.5638e-02, -3.7032e-04,\n",
      "         -3.2519e-02, -9.1649e-03, -1.8376e-02, -3.1666e-02, -9.6162e-03,\n",
      "         -1.8440e-02,  3.4306e-02,  3.1566e-02, -2.0612e-02,  2.7542e-02,\n",
      "          2.7126e-02, -2.6700e-02, -2.9376e-02, -2.2255e-02,  3.8977e-03,\n",
      "         -5.4782e-03, -1.1875e-02,  5.0202e-03,  2.1429e-03,  3.0137e-02,\n",
      "          1.9816e-02,  1.5133e-02, -1.9881e-02,  2.0054e-02,  2.5252e-02,\n",
      "          2.6660e-02,  1.6472e-02,  6.7942e-03,  1.5535e-02, -2.1277e-02,\n",
      "         -2.8504e-02,  5.9500e-04, -4.7256e-03,  1.7091e-02, -1.2485e-02,\n",
      "          9.2276e-03,  1.3285e-02,  2.8468e-02,  3.4375e-02,  1.6200e-02,\n",
      "         -5.4112e-03,  2.8717e-02,  1.8469e-03, -4.5719e-03,  3.3967e-02,\n",
      "         -3.1112e-02, -2.1565e-02, -3.7548e-03, -2.8434e-02,  2.1998e-02,\n",
      "         -3.2925e-02, -2.4323e-02, -6.7016e-03,  3.2885e-02, -1.4844e-02,\n",
      "          1.8010e-02, -3.0813e-02, -3.0462e-02,  3.4495e-02, -1.7420e-02,\n",
      "          5.3169e-03,  2.2649e-02, -3.0508e-02,  2.1163e-02,  3.6221e-03,\n",
      "         -3.3419e-02,  1.3405e-02, -3.3038e-02, -8.8157e-03, -2.8865e-02,\n",
      "         -2.5070e-02, -1.9299e-02,  3.4503e-03, -2.0885e-02, -1.0893e-02,\n",
      "         -6.8870e-03,  2.2522e-03, -3.6076e-02,  3.6972e-03,  7.0507e-05,\n",
      "         -1.6998e-02,  1.6933e-02, -2.1047e-02, -3.3804e-02, -1.6330e-02,\n",
      "         -2.8373e-03, -6.9641e-03, -3.1754e-02, -2.5032e-04,  3.2729e-02,\n",
      "          2.5443e-02, -1.7717e-02,  1.4821e-02,  5.6091e-03,  1.4475e-02,\n",
      "          3.0207e-02, -1.0145e-02,  1.2266e-02, -1.5011e-02,  1.3845e-02,\n",
      "         -3.1325e-02,  8.4439e-03,  1.6019e-02, -3.1576e-02, -1.2863e-02,\n",
      "         -1.7595e-03,  1.5454e-02,  1.0166e-02, -1.6663e-02,  3.2130e-02,\n",
      "         -2.4329e-02,  2.5956e-02,  1.1288e-02,  2.4952e-02,  1.3895e-02,\n",
      "         -1.3185e-02, -3.0755e-02,  1.5300e-02, -2.3922e-02,  2.6519e-03,\n",
      "          3.5808e-02,  2.5761e-02,  2.6028e-02,  3.1495e-02, -1.5591e-02,\n",
      "          9.9690e-03,  2.7011e-02, -2.6793e-02,  2.1287e-02,  2.8790e-02,\n",
      "         -4.7597e-03, -1.1867e-02, -2.8175e-02, -2.2231e-02,  8.6890e-03,\n",
      "          3.4003e-03, -1.5601e-02,  3.3113e-03, -2.0593e-02,  7.9149e-03,\n",
      "         -2.3042e-02,  6.9583e-03,  2.0927e-02, -2.0847e-02,  9.1696e-03,\n",
      "         -1.7006e-02, -1.9114e-03, -2.0238e-02,  4.2798e-03,  2.0361e-02,\n",
      "         -3.3149e-02, -9.9081e-03,  2.2512e-02, -3.1667e-02, -3.1814e-02,\n",
      "          2.2364e-02, -2.0043e-02, -3.5223e-02,  2.3546e-02, -3.4673e-02,\n",
      "         -2.1063e-03, -2.5847e-02,  2.7034e-03, -4.7420e-03, -1.6217e-02,\n",
      "          8.4741e-03, -3.5752e-02, -4.9578e-03,  8.1327e-03, -3.5790e-02,\n",
      "         -3.2249e-02, -3.5640e-02, -1.0901e-02,  1.4301e-02,  1.9475e-02,\n",
      "          7.8220e-03, -2.0552e-02,  6.3111e-03,  7.5768e-03,  2.1207e-02,\n",
      "         -1.6337e-03, -2.4812e-02,  2.6969e-02,  6.7605e-03, -1.9274e-02,\n",
      "          4.2398e-04, -2.8495e-02, -1.1937e-02,  1.3422e-02, -2.9955e-02,\n",
      "          2.0140e-03,  1.6995e-02,  1.4033e-04, -1.8778e-02,  1.5834e-02,\n",
      "         -2.2211e-02, -7.0585e-03,  1.3109e-02,  2.1818e-02, -2.4005e-02,\n",
      "          3.2925e-02,  1.4599e-02, -3.0847e-02,  5.7446e-03, -2.2705e-02,\n",
      "          6.4130e-04,  1.6388e-02,  7.2332e-03,  1.8359e-02, -2.3799e-02,\n",
      "          3.5655e-02, -3.1558e-03, -4.9919e-03, -1.0768e-02, -8.0751e-03,\n",
      "         -4.7564e-03, -7.3458e-03,  7.6212e-04, -1.1586e-02,  2.3841e-02,\n",
      "          1.7643e-02,  2.9231e-02,  3.0012e-02,  2.1934e-02,  1.5374e-02,\n",
      "         -3.4591e-02, -2.5797e-02,  1.3772e-02,  6.6116e-03,  1.0692e-02,\n",
      "         -1.5401e-02,  1.0470e-02,  2.3642e-02,  2.0960e-03, -1.6831e-02,\n",
      "          2.8356e-02,  2.4574e-02, -3.3171e-02, -3.0864e-02, -2.4402e-02,\n",
      "          2.8443e-02, -3.5907e-02,  1.8237e-02,  1.2473e-02,  2.4158e-02,\n",
      "          1.6367e-02, -2.7178e-02, -2.6554e-02, -8.5457e-03,  2.4867e-02,\n",
      "         -5.8029e-03,  3.3426e-02,  1.8382e-02, -2.9084e-02,  2.0454e-02,\n",
      "          4.8733e-03, -3.2899e-02, -1.6213e-02, -1.5699e-02,  2.5892e-02,\n",
      "         -3.2608e-02,  3.0663e-02, -2.8082e-02, -2.6568e-02, -2.6735e-02,\n",
      "          2.0147e-02, -1.3699e-02,  3.3904e-02,  1.2294e-02,  2.6557e-02,\n",
      "          4.7053e-03, -1.6264e-02, -7.6135e-03, -3.3919e-02, -2.0328e-02,\n",
      "          1.8959e-03,  3.5357e-02, -7.1236e-03, -5.2906e-03,  2.6941e-04,\n",
      "         -1.6823e-02,  6.7968e-03, -2.9415e-02,  2.9380e-02, -6.0507e-03,\n",
      "         -3.5504e-02, -2.5860e-02, -1.7327e-02, -8.7367e-03,  7.3386e-03,\n",
      "          3.5862e-02,  3.0551e-02,  2.1576e-02, -1.8199e-02, -1.9733e-02,\n",
      "         -9.0336e-03, -8.7892e-03, -1.0931e-03, -3.0985e-02,  3.5249e-02,\n",
      "          1.1317e-02, -4.7092e-03, -1.8847e-02,  9.8009e-03,  7.1863e-03,\n",
      "          1.6000e-03, -1.3446e-02, -3.1867e-03,  1.7616e-02,  1.7446e-02,\n",
      "         -2.4847e-02, -1.0228e-02, -3.5008e-02,  2.8816e-02,  2.0083e-02,\n",
      "         -2.8901e-02,  2.0792e-02, -2.8393e-02, -2.8788e-02, -2.5599e-02,\n",
      "          1.9682e-02, -2.5858e-02, -2.9850e-02,  7.3894e-03,  2.6983e-02,\n",
      "          7.7554e-03,  9.5146e-03, -1.3029e-02,  3.2468e-02, -3.0287e-02,\n",
      "         -1.6051e-02,  3.7926e-03,  9.6495e-03, -2.8224e-03, -3.9928e-03,\n",
      "          3.5751e-02, -3.4375e-02,  3.4003e-02, -3.3860e-02,  1.0430e-02,\n",
      "          9.5995e-03,  1.7211e-02,  1.1182e-02, -1.4640e-03, -3.4979e-02,\n",
      "          3.1125e-02, -3.1651e-02,  2.9705e-02, -1.9019e-02,  2.9181e-02,\n",
      "          3.8395e-03,  2.6438e-02, -3.4025e-02, -1.4215e-02, -4.4984e-03,\n",
      "         -9.5237e-03, -3.1063e-02, -2.4692e-03,  2.0320e-02, -3.2749e-02,\n",
      "         -2.9630e-02, -4.8218e-03, -1.9705e-02, -3.1859e-02,  2.9080e-02,\n",
      "          2.3069e-02, -2.3495e-02, -4.0239e-03, -3.4020e-02, -1.1403e-02,\n",
      "          2.3826e-02, -3.0769e-02, -9.6403e-05, -5.0242e-03, -3.3210e-02,\n",
      "          3.5257e-02,  3.1457e-02,  1.5033e-02, -1.6654e-02,  3.4512e-02,\n",
      "         -3.1597e-02,  3.3265e-02, -3.1180e-02, -4.0132e-03, -2.7881e-02,\n",
      "         -1.1597e-02, -2.5019e-03, -1.7936e-02, -2.4737e-02,  2.9503e-02,\n",
      "          5.1566e-03,  7.3447e-03,  6.2870e-03, -9.9401e-03, -2.8052e-02,\n",
      "         -1.7093e-02,  1.5862e-02, -2.0262e-03, -9.7969e-03,  2.4383e-03,\n",
      "         -6.6247e-03, -2.7310e-02,  3.0071e-02, -3.5856e-02, -2.8216e-02,\n",
      "         -2.8302e-02,  1.8579e-02,  1.4933e-02, -1.5343e-03, -1.8332e-02,\n",
      "          1.2672e-02,  3.5600e-02,  3.1423e-03, -3.0491e-02, -1.4268e-03,\n",
      "          2.3083e-02,  1.4610e-02, -7.9118e-03, -6.6150e-03,  1.7094e-02,\n",
      "         -2.2280e-02,  1.2364e-02,  3.1125e-02,  3.4268e-02,  3.2384e-02,\n",
      "          2.8843e-02,  2.4415e-02, -2.3760e-02,  6.4057e-03, -1.8915e-02,\n",
      "          6.8638e-03,  2.4667e-02,  3.5128e-02, -3.0180e-03, -7.1924e-03,\n",
      "          3.3918e-02, -2.8304e-02,  1.9786e-02, -8.3801e-03,  2.1121e-02,\n",
      "         -3.2007e-02,  7.6558e-03,  3.3660e-03, -1.4256e-02,  2.5786e-02,\n",
      "          1.6032e-02, -3.2041e-02, -1.1569e-02, -1.4004e-02,  2.3515e-02,\n",
      "          1.8040e-02, -1.8546e-02,  1.3224e-02,  2.0237e-02,  2.4974e-02,\n",
      "         -2.0196e-02, -2.2733e-02,  2.3425e-03, -3.2138e-02, -3.2276e-02,\n",
      "          2.5977e-02, -5.2443e-03,  1.2285e-02, -2.5126e-02,  2.4858e-02,\n",
      "         -1.3247e-03,  1.9202e-02,  6.9007e-03,  1.8808e-02,  2.6000e-03,\n",
      "          3.3045e-02, -3.4272e-02,  1.3827e-02,  3.2654e-02,  1.1917e-02,\n",
      "         -2.1732e-02,  2.0973e-02, -2.7583e-02,  2.5726e-02, -1.6044e-02,\n",
      "          3.5728e-02, -1.7206e-02,  5.2866e-03, -1.2376e-04,  2.1118e-02,\n",
      "         -2.9362e-03, -3.2825e-02,  3.0053e-02,  2.2791e-02, -2.8352e-02,\n",
      "         -1.8805e-02, -5.9345e-05,  2.0867e-02, -3.5822e-03, -2.2856e-02,\n",
      "          1.4803e-02, -2.2729e-02, -2.7862e-02,  6.1522e-03,  1.0455e-02,\n",
      "          3.4750e-02,  2.3021e-03, -2.1673e-02,  1.2785e-02,  1.7240e-02,\n",
      "          3.2876e-02, -2.5220e-02, -2.6424e-02, -4.3931e-03, -3.5296e-02,\n",
      "          5.9579e-03,  2.9740e-02,  1.0414e-02, -1.7465e-02,  5.8806e-03,\n",
      "          3.0501e-02, -7.4082e-05, -2.2974e-02, -2.3478e-02, -2.2121e-02,\n",
      "         -2.7066e-02,  1.6117e-02, -3.3596e-02, -6.5904e-03, -3.1408e-02,\n",
      "          3.1694e-02, -1.0576e-02, -2.1707e-02, -8.4688e-03, -2.1867e-02,\n",
      "         -1.6887e-02,  8.1557e-03, -1.2834e-02,  1.9988e-02, -5.1110e-03,\n",
      "         -2.2731e-02,  2.4181e-02,  5.9899e-03, -1.2782e-02, -9.7906e-03,\n",
      "         -2.8887e-02,  5.8635e-03, -2.9917e-02,  2.1639e-02, -1.1009e-02,\n",
      "          5.5891e-04, -2.6902e-02,  2.0404e-03, -2.4875e-02,  2.0705e-02,\n",
      "          6.0682e-03, -6.6342e-03, -3.3017e-02, -2.3871e-02, -8.7208e-03,\n",
      "          3.9766e-03,  3.3733e-02,  1.0219e-02,  3.4904e-02,  1.9537e-02,\n",
      "          1.3973e-02, -2.6801e-02, -2.9874e-02,  1.9064e-02, -1.2529e-02,\n",
      "         -9.4993e-04, -2.4820e-02, -1.6700e-02,  2.7086e-02, -6.2244e-03,\n",
      "         -2.8569e-02, -3.2343e-03,  7.0025e-04,  2.8286e-02,  2.6734e-02,\n",
      "          4.7241e-03,  2.2156e-02,  2.3491e-02, -1.6342e-02,  1.4875e-02,\n",
      "         -4.4126e-04, -2.7528e-02,  3.4159e-02, -1.3510e-02, -1.3645e-02,\n",
      "          2.8979e-02,  1.1092e-02, -1.3207e-03, -2.7108e-02, -3.3430e-02,\n",
      "          1.6750e-02, -1.6838e-03, -2.9401e-02, -2.8861e-02,  2.6961e-02,\n",
      "          1.8246e-02, -1.5820e-03,  2.6521e-02]], device='cuda:1',\n",
      "       requires_grad=True)\n",
      "hidden_from.weight Parameter containing:\n",
      "tensor([[ 2.0887e-02, -2.8967e-02, -1.2380e-05, -2.5859e-02,  1.1141e-02,\n",
      "         -3.5143e-02,  1.5465e-02,  2.4139e-02, -3.5180e-02,  1.5019e-02,\n",
      "          1.0424e-02,  2.6776e-02, -1.4152e-05, -2.3283e-02,  1.6491e-02,\n",
      "          2.8062e-02,  2.1646e-02, -1.9644e-02, -2.5715e-03, -3.0533e-02,\n",
      "         -8.4131e-03,  6.6700e-03,  3.5719e-02,  2.5612e-02, -1.7941e-03,\n",
      "          5.4571e-03,  2.2201e-02, -3.1071e-02, -1.7498e-02,  1.6505e-02,\n",
      "          6.5118e-03, -3.5862e-02, -7.4829e-03,  2.2768e-02,  2.4318e-02,\n",
      "         -2.1040e-02,  9.8822e-03, -8.2495e-03, -2.3474e-02,  8.0400e-03,\n",
      "          2.0841e-02,  2.0907e-02, -3.4742e-02,  1.4788e-02, -3.5149e-02,\n",
      "          9.0545e-03,  1.7739e-02, -2.4750e-02,  5.2590e-04,  2.5708e-02,\n",
      "         -7.9297e-03,  3.8983e-03,  2.7472e-02,  2.5511e-02, -1.5204e-02,\n",
      "         -1.1804e-02, -9.9506e-03, -1.9199e-02,  3.0750e-02,  1.7285e-02,\n",
      "          1.1170e-02,  1.1545e-02, -1.9174e-02, -2.7424e-02, -3.2659e-02,\n",
      "         -3.5650e-02, -1.8892e-02,  1.4144e-03, -1.9885e-02,  4.2165e-03,\n",
      "          3.1340e-02, -2.4097e-02,  2.2536e-02, -3.1477e-02,  3.0065e-02,\n",
      "         -1.3305e-02, -3.4397e-02, -1.9223e-02, -3.2014e-02, -2.2606e-02,\n",
      "         -2.6343e-02,  2.3249e-02, -6.6830e-04, -1.0445e-02, -3.4593e-02,\n",
      "          2.2822e-02,  3.0395e-02,  3.5314e-02, -3.1218e-02,  1.9821e-02,\n",
      "          9.3987e-03,  1.1040e-02,  2.2838e-02, -1.2332e-02, -3.2069e-02,\n",
      "         -1.0048e-02, -1.2704e-02, -1.4210e-02, -2.2059e-02,  3.4691e-02,\n",
      "          6.8399e-03, -2.9992e-02,  1.7958e-02, -2.5441e-03,  8.5216e-03,\n",
      "         -3.4413e-02, -1.9451e-02, -2.7056e-02,  3.2629e-02,  2.8455e-02,\n",
      "         -5.8521e-03,  3.3399e-02, -3.5741e-02, -2.3564e-02, -1.3472e-02,\n",
      "          3.2055e-02, -1.6577e-02,  1.2723e-02,  2.9036e-02, -2.0981e-02,\n",
      "         -1.0836e-02,  2.1946e-02, -3.1711e-02, -3.5308e-02,  2.7381e-02,\n",
      "          2.6077e-02,  3.5233e-02, -5.3058e-03, -1.4700e-03, -5.2940e-03,\n",
      "         -3.0293e-02, -5.1137e-03,  1.5607e-02, -2.7246e-02, -1.0402e-02,\n",
      "         -1.3918e-02,  4.5593e-03,  2.0681e-02, -6.2157e-03, -1.6060e-03,\n",
      "         -2.3573e-02,  3.2012e-02, -2.8734e-02, -3.2240e-03,  1.1984e-02,\n",
      "         -2.9450e-02,  2.6224e-02,  3.2100e-02, -2.7634e-02,  7.0421e-03,\n",
      "          3.4280e-02, -8.4102e-03,  2.3673e-02,  7.1687e-04,  8.8787e-03,\n",
      "         -1.6990e-02,  2.9578e-02,  1.4232e-02,  2.1385e-02,  1.4017e-02,\n",
      "          1.3099e-02, -8.4205e-03,  1.7044e-02, -1.1791e-02,  3.5866e-02,\n",
      "         -7.7895e-04,  2.1210e-02,  2.3100e-02,  1.7995e-03, -1.5197e-02,\n",
      "         -2.7251e-02,  1.3593e-02,  2.8304e-02, -2.2059e-02, -1.2916e-02,\n",
      "          2.5505e-03,  1.4199e-02,  2.7371e-02,  3.0105e-03, -9.6409e-03,\n",
      "         -2.0148e-02, -9.1728e-03,  8.4783e-03,  3.0795e-02, -1.8413e-02,\n",
      "          4.0289e-03, -1.7076e-02, -4.0869e-03,  3.3749e-03, -1.0118e-02,\n",
      "         -2.4742e-02, -1.9054e-02, -3.1682e-02,  2.7498e-02,  1.2671e-02,\n",
      "          1.0975e-02, -2.1930e-02, -1.9144e-02,  3.3236e-02,  2.8184e-02,\n",
      "          2.0567e-02, -3.3049e-02,  2.5760e-03,  1.8347e-03,  4.3978e-03,\n",
      "          3.3279e-02, -2.0567e-02,  1.6019e-02, -2.5792e-02, -1.6879e-02,\n",
      "          6.3480e-03,  2.0312e-02,  1.2791e-02,  2.3801e-02,  1.6088e-06,\n",
      "         -1.5273e-02,  3.1727e-03, -2.6682e-02,  2.2507e-02, -1.3795e-02,\n",
      "         -2.7616e-02,  2.3586e-02, -9.5918e-03,  1.4649e-02, -6.9710e-03,\n",
      "          4.6923e-03,  2.6318e-02,  2.4880e-02, -3.5620e-02,  2.7956e-02,\n",
      "          3.0911e-02,  1.0620e-02, -1.5567e-03,  3.4126e-02, -3.3838e-02,\n",
      "          3.0233e-02,  2.5605e-02,  2.9289e-03, -2.1282e-02, -3.1271e-02,\n",
      "         -1.5502e-02, -1.5163e-02,  1.7419e-02,  2.4603e-02, -3.1638e-03,\n",
      "          2.4052e-02, -7.2012e-03, -2.9811e-02,  7.5618e-03,  2.0745e-02,\n",
      "         -1.1981e-02,  3.5763e-02,  8.0655e-03, -7.6943e-03, -8.6478e-03,\n",
      "          2.0414e-02, -3.1610e-02,  1.7990e-02, -1.6173e-02, -8.4578e-03,\n",
      "         -3.2964e-02, -3.2957e-02, -2.7606e-02, -1.3500e-02,  2.7222e-02,\n",
      "          3.1085e-03, -1.1122e-02, -5.5617e-03, -1.9151e-02,  1.9671e-03,\n",
      "          1.3498e-02, -1.6353e-02, -2.3876e-02, -3.5244e-02, -2.3167e-02,\n",
      "          1.2980e-02, -1.4085e-02,  1.1257e-02, -6.8188e-04,  6.0500e-03,\n",
      "          3.3365e-02, -2.9422e-02,  3.4158e-02,  3.1437e-02, -1.5047e-02,\n",
      "         -1.3310e-02, -1.7446e-02, -6.7425e-03,  1.4892e-02,  1.1911e-02,\n",
      "         -2.0425e-02, -3.4837e-02,  5.0510e-03,  3.4736e-02,  3.5816e-02,\n",
      "          3.3103e-02, -3.9446e-04, -5.1878e-03,  1.2397e-02, -2.1897e-02,\n",
      "          2.6144e-02,  3.9490e-03,  2.9539e-02,  4.2869e-03,  4.6694e-03,\n",
      "          3.3147e-02, -2.6293e-02,  2.5071e-02, -2.4910e-02,  2.1378e-02,\n",
      "         -1.1413e-02, -2.9756e-03,  2.4380e-03,  3.1042e-02, -3.1242e-02,\n",
      "          2.6927e-02,  2.0114e-02, -2.0847e-02,  1.9336e-02, -9.6131e-03,\n",
      "          7.1680e-03, -3.3030e-02,  3.5125e-02,  2.0319e-03, -1.8420e-02,\n",
      "          8.0115e-03, -1.7596e-02, -3.5669e-02, -2.7910e-02, -4.0928e-03,\n",
      "          4.5194e-03,  3.0154e-02, -4.8224e-03,  6.4580e-03,  7.4628e-03,\n",
      "         -3.5223e-02, -2.0199e-02,  3.1104e-02, -2.4340e-02,  3.4862e-02,\n",
      "         -3.4842e-02, -2.4598e-03, -9.6565e-03, -7.9761e-03,  4.2485e-03,\n",
      "          2.3394e-03, -1.3728e-02, -1.7822e-02,  2.3683e-02,  3.4989e-02,\n",
      "          1.0305e-02,  2.8452e-02, -7.4042e-03, -3.0796e-02, -2.4213e-02,\n",
      "         -2.7229e-02, -2.4555e-02, -1.7453e-02,  1.0080e-02, -1.0785e-02,\n",
      "         -1.2215e-02,  2.4412e-02, -1.9335e-02,  1.9290e-02, -3.4386e-02,\n",
      "          3.7468e-03,  3.3894e-02, -1.2278e-02,  2.0723e-02,  8.8263e-03,\n",
      "         -1.3829e-02, -1.8989e-02, -3.3619e-03, -1.9156e-02, -3.4627e-02,\n",
      "          2.4277e-02, -1.1441e-02,  8.0116e-03,  1.9141e-02,  2.7764e-02,\n",
      "          4.0051e-04, -1.4769e-02, -2.9992e-02,  2.0297e-02, -1.1091e-02,\n",
      "          1.3749e-02,  2.6889e-02, -2.0501e-02,  5.2713e-03, -1.0098e-03,\n",
      "          1.1091e-02, -3.4823e-02,  1.6010e-02, -1.7398e-03, -4.1763e-03,\n",
      "          3.1118e-02,  3.1485e-02,  2.1095e-02, -1.4254e-02, -2.2117e-02,\n",
      "          1.7542e-02, -1.5693e-03,  2.3084e-02,  1.9712e-02,  2.0853e-02,\n",
      "         -1.1269e-02,  1.8184e-02,  1.1392e-02,  2.9259e-02,  1.3431e-02,\n",
      "          1.6898e-02,  2.0875e-02,  2.3306e-02,  1.2167e-02, -1.7915e-02,\n",
      "          3.3805e-02,  1.9384e-02,  2.6152e-02,  2.0536e-02, -2.1125e-03,\n",
      "          2.6383e-02, -1.0843e-03, -1.9374e-03,  2.1204e-02,  3.2390e-02,\n",
      "         -2.5182e-02, -5.6354e-03, -1.1303e-02, -2.8958e-02,  6.1455e-03,\n",
      "         -3.5833e-02,  2.2061e-02, -2.1469e-02, -1.2214e-02, -2.1740e-02,\n",
      "          3.4069e-03, -3.5341e-02, -2.4497e-02, -2.4363e-03, -2.2262e-02,\n",
      "          1.2453e-02, -1.3988e-02,  3.2219e-03,  1.0417e-02,  7.6809e-03,\n",
      "          1.6706e-02,  2.0325e-02,  2.9853e-03,  3.3405e-02, -1.1752e-03,\n",
      "          1.9663e-02, -9.1382e-03,  3.4500e-02, -8.2216e-03, -1.8009e-03,\n",
      "          3.3935e-03, -9.4934e-03,  2.9816e-03, -2.2670e-02,  2.0173e-02,\n",
      "         -3.0887e-02, -1.1121e-03,  1.1884e-02, -2.5920e-02, -3.4086e-02,\n",
      "         -2.5575e-02,  1.7092e-02, -1.2243e-02,  4.0802e-03,  2.4012e-02,\n",
      "          1.6647e-02,  2.0307e-02, -4.5097e-04,  1.2048e-02,  1.4521e-02,\n",
      "          2.8736e-02, -9.7497e-03, -1.2458e-02,  1.3057e-02, -2.3056e-02,\n",
      "         -2.6480e-02, -3.4893e-02, -2.2471e-02, -2.3886e-02,  3.0999e-02,\n",
      "          1.7075e-02, -6.6908e-03,  2.0088e-02,  3.2143e-02, -1.7174e-02,\n",
      "          2.9991e-02, -2.4565e-02, -1.5495e-02, -1.0164e-02,  1.7636e-02,\n",
      "          1.7279e-02,  1.1565e-02,  1.0208e-02,  2.2882e-02,  3.4905e-02,\n",
      "          2.9714e-02, -3.1549e-02,  2.0323e-02, -2.8039e-02,  2.4708e-02,\n",
      "         -3.3324e-02, -4.9597e-03, -1.0885e-02,  1.5680e-02, -1.3760e-02,\n",
      "         -6.8324e-03,  3.2415e-02,  1.6264e-02,  2.4739e-02, -1.6918e-02,\n",
      "          3.5125e-02, -2.8382e-03,  2.0658e-02,  2.8898e-02,  1.4999e-02,\n",
      "         -1.7317e-02,  5.3668e-03, -1.9175e-02, -2.7363e-03, -1.9377e-02,\n",
      "          2.3195e-02,  7.4312e-03,  7.8610e-04,  7.5180e-03, -2.2277e-02,\n",
      "         -3.5084e-02, -2.9980e-02,  7.8501e-04,  6.7601e-03, -1.1348e-02,\n",
      "          3.0776e-02, -1.0324e-02,  1.9210e-02, -3.2402e-02,  2.2045e-02,\n",
      "          3.5029e-03,  1.3533e-02, -1.2405e-02, -2.6579e-02, -1.8609e-02,\n",
      "          1.0520e-02,  9.0532e-03,  1.4002e-02, -3.4944e-02,  1.3744e-02,\n",
      "         -3.4118e-02,  1.0725e-02, -1.6764e-02, -1.9552e-02, -1.6537e-02,\n",
      "          2.7159e-02, -3.2439e-02,  3.1679e-02,  2.5946e-02,  2.4771e-02,\n",
      "          1.1970e-02, -2.4508e-02,  6.7548e-03, -1.1784e-02, -1.9167e-02,\n",
      "          1.3223e-02, -1.4106e-02, -4.6881e-03, -1.3222e-02, -3.8697e-03,\n",
      "          1.7411e-02,  2.0335e-02, -9.2927e-03, -2.1498e-02,  2.1779e-02,\n",
      "          3.2329e-02,  2.2977e-02,  1.9516e-02,  1.5072e-03, -2.9104e-02,\n",
      "         -2.7742e-02,  6.0000e-03,  4.0325e-03,  2.6041e-02,  3.0691e-02,\n",
      "          1.9920e-02, -1.0704e-02,  3.4869e-02,  2.9895e-02, -9.7080e-03,\n",
      "         -1.6446e-02, -2.3013e-02, -3.8584e-04, -1.7776e-02,  2.8384e-02,\n",
      "          3.5755e-02,  1.4339e-02,  1.6110e-02,  1.6404e-02,  3.0072e-02,\n",
      "         -1.6152e-02,  8.4140e-03, -7.6673e-03, -3.4244e-02, -2.5142e-02,\n",
      "          3.3280e-02,  3.2354e-02,  2.6663e-02,  6.3468e-03,  8.8938e-03,\n",
      "         -1.4618e-02,  2.3472e-02, -1.9320e-02,  3.5484e-03,  1.6277e-02,\n",
      "          1.0092e-02, -8.3305e-03, -2.8223e-02, -5.8369e-03, -3.3240e-02,\n",
      "          2.6735e-02, -1.6336e-03,  2.9098e-02,  2.0548e-03, -3.0090e-02,\n",
      "         -1.7464e-02, -1.1216e-02,  2.6931e-02,  2.9407e-02,  1.3356e-02,\n",
      "         -2.7512e-02, -7.9804e-03, -1.5129e-02,  1.7549e-02, -3.2915e-02,\n",
      "          2.3603e-02,  3.3218e-02, -2.1788e-02, -2.3995e-02,  1.1257e-02,\n",
      "          7.5590e-03,  2.8197e-04,  2.8127e-02, -5.7947e-03,  3.1727e-02,\n",
      "         -1.6202e-02,  2.4142e-02,  2.7596e-02,  2.4803e-02,  1.9185e-02,\n",
      "         -2.0323e-02, -3.4177e-02, -1.8825e-03, -1.6511e-02, -1.7321e-02,\n",
      "          1.5951e-02, -8.9817e-03,  4.3919e-03, -2.0713e-02, -2.5237e-02,\n",
      "          1.3758e-02, -5.1985e-05,  3.5259e-02, -4.1090e-03,  7.7880e-03,\n",
      "         -2.9178e-03,  5.0807e-03, -3.3428e-02,  2.1018e-02,  1.3510e-02,\n",
      "          1.1361e-02,  2.4164e-02, -1.6896e-02,  2.2256e-02,  3.2793e-02,\n",
      "          3.6756e-03, -3.2269e-02,  3.0511e-02,  5.7546e-03, -2.3312e-02,\n",
      "         -1.3509e-02, -1.9637e-02, -1.0713e-02, -3.4672e-03,  2.4629e-02,\n",
      "         -3.7388e-03, -6.5362e-03, -1.7109e-02,  1.5219e-02, -2.0885e-02,\n",
      "         -2.2897e-02, -1.1478e-02,  3.4338e-03,  2.1133e-02, -1.0393e-03,\n",
      "          1.6169e-02,  2.2430e-03, -3.2774e-02, -3.1041e-02,  2.0742e-02,\n",
      "          3.3517e-02,  5.7255e-03,  2.9972e-02, -2.4673e-02, -6.9228e-03,\n",
      "          5.2517e-03,  2.8743e-03, -2.8092e-03,  2.7854e-02, -7.0343e-03,\n",
      "          5.1784e-03,  2.2339e-02,  2.9140e-02,  2.4065e-02, -1.6143e-04,\n",
      "         -2.8739e-02,  2.0403e-02, -3.2219e-03, -2.5171e-02, -3.1654e-02,\n",
      "         -5.7559e-03,  1.2272e-03, -1.4701e-02, -1.1564e-02,  5.4030e-03,\n",
      "         -3.8111e-03, -1.1493e-03,  2.0322e-03,  3.3618e-02,  2.2713e-02,\n",
      "          1.8286e-03, -1.8952e-02,  1.7342e-03,  3.5224e-02, -2.7482e-02,\n",
      "          2.8998e-02, -1.3350e-02, -1.3244e-02,  2.7409e-02,  3.3814e-02,\n",
      "         -1.3862e-03,  6.2816e-03,  2.1151e-02,  2.2169e-03, -3.2702e-02,\n",
      "          2.3011e-02,  3.3180e-02, -2.6414e-02, -2.4739e-02, -1.2468e-03,\n",
      "         -1.6895e-02, -3.4531e-02, -3.1634e-02,  3.0271e-02, -4.2037e-03,\n",
      "         -2.3868e-02, -2.3155e-02, -2.0475e-02, -1.4623e-02,  1.7588e-02,\n",
      "          8.4618e-03,  7.9824e-03,  2.1601e-02, -2.0668e-02,  2.1204e-02,\n",
      "         -2.7586e-02, -1.6493e-02, -7.2637e-03]], device='cuda:1',\n",
      "       requires_grad=True)\n",
      "0 -- Mean train loss: 3.127280030647914\n",
      "0 -- Mean val loss: 0.6390851587057114\n",
      "0 -- VAL R2 score: -1.3250213899234753\n",
      "\n",
      "1 -- Mean train loss: 0.6623412157098453\n",
      "1 -- Mean val loss: 0.530128113925457\n",
      "1 -- VAL R2 score: -0.8392397081728202\n",
      "\n",
      "2 -- Mean train loss: 0.5840214004119237\n",
      "2 -- Mean val loss: 0.6339280903339386\n",
      "2 -- VAL R2 score: -1.1044036951237977\n",
      "\n",
      "3 -- Mean train loss: 0.5618394250671069\n",
      "3 -- Mean val loss: 0.6550541743636131\n",
      "3 -- VAL R2 score: -0.8093364274449344\n",
      "\n",
      "4 -- Mean train loss: 0.5519901484251022\n",
      "4 -- Mean val loss: 0.5541855692863464\n",
      "4 -- VAL R2 score: -0.7780171327614065\n",
      "\n",
      "5 -- Mean train loss: 0.5478204707304637\n",
      "5 -- Mean val loss: 0.6422869265079498\n",
      "5 -- VAL R2 score: -0.8432048167033268\n",
      "\n",
      "6 -- Mean train loss: 0.5226267144083977\n",
      "6 -- Mean val loss: 0.576610192656517\n",
      "6 -- VAL R2 score: -0.7289633435971619\n",
      "\n",
      "7 -- Mean train loss: 0.5228593314687411\n",
      "7 -- Mean val loss: 0.5253352448344231\n",
      "7 -- VAL R2 score: -0.7979119045016927\n",
      "\n",
      "8 -- Mean train loss: 0.542408007880052\n",
      "8 -- Mean val loss: 0.5028913617134094\n",
      "8 -- VAL R2 score: -0.7923465127609794\n",
      "\n",
      "9 -- Mean train loss: 0.5345822736620903\n",
      "9 -- Mean val loss: 0.514609768986702\n",
      "9 -- VAL R2 score: -0.668440312577463\n",
      "\n",
      "10 -- Mean train loss: 0.5347234179576238\n",
      "10 -- Mean val loss: 0.5979808121919632\n",
      "10 -- VAL R2 score: -0.8165184470698421\n",
      "\n",
      "11 -- Mean train loss: 0.5229269435008367\n",
      "11 -- Mean val loss: 0.5785616785287857\n",
      "11 -- VAL R2 score: -0.783740075537426\n",
      "\n",
      "12 -- Mean train loss: 0.5407559941212337\n",
      "12 -- Mean val loss: 0.5274883657693863\n",
      "12 -- VAL R2 score: -0.842590280009833\n",
      "\n",
      "13 -- Mean train loss: 0.5386322359244029\n",
      "13 -- Mean val loss: 0.5778236016631126\n",
      "13 -- VAL R2 score: -0.73289180141462\n",
      "\n",
      "14 -- Mean train loss: 0.5186413312951724\n",
      "14 -- Mean val loss: 0.5600228160619736\n",
      "14 -- VAL R2 score: -0.8988543801119826\n",
      "\n",
      "15 -- Mean train loss: 0.5284658839305242\n",
      "15 -- Mean val loss: 0.6104620471596718\n",
      "15 -- VAL R2 score: -0.9204101211682554\n",
      "\n",
      "16 -- Mean train loss: 0.5259803652763366\n",
      "16 -- Mean val loss: 0.5929827317595482\n",
      "16 -- VAL R2 score: -0.9352202087726598\n",
      "\n",
      "17 -- Mean train loss: 0.5181124890844028\n",
      "17 -- Mean val loss: 0.6010742783546448\n",
      "17 -- VAL R2 score: -1.1565949182104398\n",
      "\n",
      "18 -- Mean train loss: 0.5239917114377022\n",
      "18 -- Mean val loss: 0.6107002347707748\n",
      "18 -- VAL R2 score: -1.2534805202755468\n",
      "\n",
      "19 -- Mean train loss: 0.5574339201052984\n",
      "19 -- Mean val loss: 0.686177521944046\n",
      "19 -- VAL R2 score: -1.2039175236922337\n",
      "\n",
      "20 -- Mean train loss: 0.5453063567479451\n",
      "20 -- Mean val loss: 0.5538589432835579\n",
      "20 -- VAL R2 score: -0.9422614981034645\n",
      "\n",
      "21 -- Mean train loss: 0.5222847188512484\n",
      "21 -- Mean val loss: 0.5330087542533875\n",
      "21 -- VAL R2 score: -1.2340346520582148\n",
      "\n",
      "22 -- Mean train loss: 0.5170401856303215\n",
      "22 -- Mean val loss: 0.5762645974755287\n",
      "22 -- VAL R2 score: -0.8163479600896117\n",
      "\n",
      "23 -- Mean train loss: 0.5497192482153574\n",
      "23 -- Mean val loss: 0.5571040138602257\n",
      "23 -- VAL R2 score: -0.7300005858491352\n",
      "\n",
      "24 -- Mean train loss: 0.5397018412748973\n",
      "24 -- Mean val loss: 0.6313135921955109\n",
      "24 -- VAL R2 score: -0.9733874336913286\n",
      "\n",
      "25 -- Mean train loss: 0.5522048523028692\n",
      "25 -- Mean val loss: 0.5984103232622147\n",
      "25 -- VAL R2 score: -1.2285817539921506\n",
      "\n",
      "26 -- Mean train loss: 0.5338850150505702\n",
      "26 -- Mean val loss: 0.5877029597759247\n",
      "26 -- VAL R2 score: -0.6603984502880076\n",
      "\n",
      "27 -- Mean train loss: 0.5273078968127568\n",
      "27 -- Mean val loss: 0.5786862745881081\n",
      "27 -- VAL R2 score: -0.9286780842843285\n",
      "\n",
      "28 -- Mean train loss: 0.5393064106504123\n",
      "28 -- Mean val loss: 0.5937108621001244\n",
      "28 -- VAL R2 score: -1.227750773968035\n",
      "\n",
      "29 -- Mean train loss: 0.578621602555116\n",
      "29 -- Mean val loss: 0.7031064629554749\n",
      "29 -- VAL R2 score: -1.235259591905451\n",
      "\n",
      "30 -- Mean train loss: 0.5269958709677061\n",
      "30 -- Mean val loss: 0.5134296342730522\n",
      "30 -- VAL R2 score: -0.7224493105524035\n",
      "\n",
      "31 -- Mean train loss: 0.5188265124956767\n",
      "31 -- Mean val loss: 0.5068695470690727\n",
      "31 -- VAL R2 score: -0.8705186838652079\n",
      "\n",
      "32 -- Mean train loss: 0.539795491596063\n",
      "32 -- Mean val loss: 0.5143742114305496\n",
      "32 -- VAL R2 score: -0.8518064009576474\n",
      "\n",
      "33 -- Mean train loss: 0.5028829827904702\n",
      "33 -- Mean val loss: 0.5922583043575287\n",
      "33 -- VAL R2 score: -0.8444699994284361\n",
      "\n",
      "34 -- Mean train loss: 0.5577149684230487\n",
      "34 -- Mean val loss: 0.6343028992414474\n",
      "34 -- VAL R2 score: -0.8363093322914084\n",
      "\n",
      "35 -- Mean train loss: 0.5382597297430038\n",
      "35 -- Mean val loss: 0.5718743279576302\n",
      "35 -- VAL R2 score: -0.8560349403912806\n",
      "\n",
      "36 -- Mean train loss: 0.523161101837953\n",
      "36 -- Mean val loss: 0.5466994121670723\n",
      "36 -- VAL R2 score: -0.8150010864143484\n",
      "\n",
      "37 -- Mean train loss: 0.5152351791659991\n",
      "37 -- Mean val loss: 0.5319798663258553\n",
      "37 -- VAL R2 score: -0.70580962581388\n",
      "\n",
      "38 -- Mean train loss: 0.5357934142152468\n",
      "38 -- Mean val loss: 0.5549446493387222\n",
      "38 -- VAL R2 score: -0.8820661771226515\n",
      "\n",
      "39 -- Mean train loss: 0.5572690680623055\n",
      "39 -- Mean val loss: 0.7755602300167084\n",
      "39 -- VAL R2 score: -0.9481317416405974\n",
      "\n",
      "40 -- Mean train loss: 0.513037949303786\n",
      "40 -- Mean val loss: 0.6045591831207275\n",
      "40 -- VAL R2 score: -0.881291979445392\n",
      "\n",
      "41 -- Mean train loss: 0.5018548712134361\n",
      "41 -- Mean val loss: 0.48066944628953934\n",
      "41 -- VAL R2 score: -0.8647916065814536\n",
      "\n",
      "42 -- Mean train loss: 0.518544748922189\n",
      "42 -- Mean val loss: 0.65148064494133\n",
      "42 -- VAL R2 score: -1.1250932186966245\n",
      "\n",
      "43 -- Mean train loss: 0.5519693245490392\n",
      "43 -- Mean val loss: 0.5204677805304527\n",
      "43 -- VAL R2 score: -0.9512644970450732\n",
      "\n",
      "44 -- Mean train loss: 0.5512347867091497\n",
      "44 -- Mean val loss: 0.6798000782728195\n",
      "44 -- VAL R2 score: -1.0424485910007353\n",
      "\n",
      "45 -- Mean train loss: 0.54395811855793\n",
      "45 -- Mean val loss: 0.6783771142363548\n",
      "45 -- VAL R2 score: -0.9358754889636043\n",
      "\n",
      "46 -- Mean train loss: 0.5593027169505755\n",
      "46 -- Mean val loss: 0.6451676934957504\n",
      "46 -- VAL R2 score: -1.3086647509764173\n",
      "\n",
      "47 -- Mean train loss: 0.5795603652795156\n",
      "47 -- Mean val loss: 0.5671385079622269\n",
      "47 -- VAL R2 score: -0.8591531047363608\n",
      "\n",
      "48 -- Mean train loss: 0.532153635720412\n",
      "48 -- Mean val loss: 0.5548050776124\n",
      "48 -- VAL R2 score: -0.9769943558722052\n",
      "\n",
      "49 -- Mean train loss: 0.5294497778018316\n",
      "49 -- Mean val loss: 0.6193757951259613\n",
      "49 -- VAL R2 score: -1.1356496459264815\n",
      "\n",
      "50 -- Mean train loss: 0.5538362527887026\n",
      "50 -- Mean val loss: 0.7120649367570877\n",
      "50 -- VAL R2 score: -1.165238634080835\n",
      "\n",
      "51 -- Mean train loss: 0.5346425399184227\n",
      "51 -- Mean val loss: 0.6477667689323425\n",
      "51 -- VAL R2 score: -1.0679212577585377\n",
      "\n",
      "52 -- Mean train loss: 0.5488041495283444\n",
      "52 -- Mean val loss: 0.4707055017352104\n",
      "52 -- VAL R2 score: -0.8333744065484616\n",
      "\n",
      "53 -- Mean train loss: 0.5129312485456466\n",
      "53 -- Mean val loss: 0.601185642182827\n",
      "53 -- VAL R2 score: -0.8761531040671631\n",
      "\n",
      "54 -- Mean train loss: 0.5076206962267558\n",
      "54 -- Mean val loss: 0.5431564971804619\n",
      "54 -- VAL R2 score: -0.8682315981482491\n",
      "\n",
      "55 -- Mean train loss: 0.5244570518533389\n",
      "55 -- Mean val loss: 0.6550953984260559\n",
      "55 -- VAL R2 score: -0.6786051594199063\n",
      "\n",
      "56 -- Mean train loss: 0.5405962730447451\n",
      "56 -- Mean val loss: 0.5771442204713821\n",
      "56 -- VAL R2 score: -0.9786856137024591\n",
      "\n",
      "57 -- Mean train loss: 0.5463771834969521\n",
      "57 -- Mean val loss: 0.5168349668383598\n",
      "57 -- VAL R2 score: -0.8413760141356286\n",
      "\n",
      "58 -- Mean train loss: 0.5229001169403394\n",
      "58 -- Mean val loss: 0.5958982035517693\n",
      "58 -- VAL R2 score: -0.9678558190897997\n",
      "\n",
      "59 -- Mean train loss: 0.5300973479946455\n",
      "59 -- Mean val loss: 0.531704731285572\n",
      "59 -- VAL R2 score: -1.1190109267012773\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = train_linear_model(X_train, X_test, y_train, y_test, config, save_pattern=f'{config.data.model_save_pattern}_{0}', \n",
    "                           use_plots=False, save_final_results=True, \n",
    "                           verbose=True, use_pbars=False, save_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c948ff-4326-4c5c-8182-0fee76bb72dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1d4e7d33-1773-4546-b1fd-2f760e305453",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearAttention(\n",
       "  (hidden_to): Linear(in_features=768, out_features=1, bias=False)\n",
       "  (hidden_from): Linear(in_features=768, out_features=1, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(config.general.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0c6cb163-9871-4ae0-8494-b5bf3d976c40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4db8ae221964b74b9207a9f241bcc9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_loss, val_preds = eval_epoch(model, nn.MSELoss(), X_test, y_test, config, use_pbar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bea2794e-42c6-4e01-b4b2-d19604802031",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-7.247595 ],\n",
       "       [-5.6525435],\n",
       "       [-5.801957 ],\n",
       "       [-6.010397 ],\n",
       "       [-6.3579507],\n",
       "       [-6.4027143],\n",
       "       [-6.201969 ],\n",
       "       [-5.9606333],\n",
       "       [-6.28158  ],\n",
       "       [-6.4117446],\n",
       "       [-6.074593 ],\n",
       "       [-5.958689 ],\n",
       "       [-3.7018425],\n",
       "       [-6.09683  ],\n",
       "       [-4.9902015],\n",
       "       [-5.9078703],\n",
       "       [-5.102816 ],\n",
       "       [-6.173346 ],\n",
       "       [-6.507085 ],\n",
       "       [-6.8629436],\n",
       "       [-7.1392684],\n",
       "       [-6.2972198],\n",
       "       [-5.7428403],\n",
       "       [-6.851408 ],\n",
       "       [-5.840838 ],\n",
       "       [-6.2860622],\n",
       "       [-6.629168 ],\n",
       "       [-5.484012 ],\n",
       "       [-5.9730744],\n",
       "       [-5.4034266],\n",
       "       [-6.5269356],\n",
       "       [-6.764206 ],\n",
       "       [-6.027009 ],\n",
       "       [-7.2928915],\n",
       "       [-4.3291903],\n",
       "       [-6.3096848],\n",
       "       [-6.346401 ],\n",
       "       [-6.515431 ],\n",
       "       [-4.9723926],\n",
       "       [-5.4534526],\n",
       "       [-6.667586 ],\n",
       "       [-6.984903 ],\n",
       "       [-5.9428144],\n",
       "       [-6.340772 ],\n",
       "       [-6.461209 ],\n",
       "       [-5.121102 ],\n",
       "       [-5.3070703],\n",
       "       [-5.3553753],\n",
       "       [-6.3443723],\n",
       "       [-7.0033636],\n",
       "       [-6.0889544],\n",
       "       [-6.1538815],\n",
       "       [-6.441015 ],\n",
       "       [-5.9627366],\n",
       "       [-5.4292026],\n",
       "       [-6.194271 ],\n",
       "       [-5.5998263],\n",
       "       [-7.1969757],\n",
       "       [-6.401623 ],\n",
       "       [-5.9094067],\n",
       "       [-5.854056 ],\n",
       "       [-6.2517796],\n",
       "       [-6.6021833],\n",
       "       [-5.941377 ],\n",
       "       [-6.3732085],\n",
       "       [-7.468663 ],\n",
       "       [-6.381769 ],\n",
       "       [-5.888922 ],\n",
       "       [-6.4177113],\n",
       "       [-6.557854 ],\n",
       "       [-6.2780824],\n",
       "       [-5.682767 ],\n",
       "       [-6.4699073],\n",
       "       [-5.9288116],\n",
       "       [-5.7600975],\n",
       "       [-6.654286 ],\n",
       "       [-6.908759 ],\n",
       "       [-6.3601704],\n",
       "       [-5.9709764],\n",
       "       [-5.504643 ],\n",
       "       [-7.618527 ],\n",
       "       [-6.634057 ],\n",
       "       [-5.172142 ],\n",
       "       [-7.205681 ],\n",
       "       [-6.611843 ],\n",
       "       [-5.0374966],\n",
       "       [-5.3082056],\n",
       "       [-6.973053 ],\n",
       "       [-5.9324317],\n",
       "       [-6.8309917],\n",
       "       [-6.0296183],\n",
       "       [-8.015153 ],\n",
       "       [-5.455618 ],\n",
       "       [-6.421318 ],\n",
       "       [-5.8944526],\n",
       "       [-6.18897  ],\n",
       "       [-6.578402 ],\n",
       "       [-5.5382347],\n",
       "       [-6.033996 ],\n",
       "       [-5.788627 ],\n",
       "       [-6.6417146],\n",
       "       [-6.581856 ],\n",
       "       [-5.2564583],\n",
       "       [-6.389243 ],\n",
       "       [-5.947073 ],\n",
       "       [-5.6157594],\n",
       "       [-6.3686824],\n",
       "       [-6.3668723],\n",
       "       [-6.3924985],\n",
       "       [-5.6796002],\n",
       "       [-5.941156 ],\n",
       "       [-5.8248277],\n",
       "       [-5.740116 ],\n",
       "       [-3.946641 ],\n",
       "       [-5.219052 ],\n",
       "       [-6.0080395],\n",
       "       [-6.7893558],\n",
       "       [-5.744751 ],\n",
       "       [-6.191607 ],\n",
       "       [-6.4294834],\n",
       "       [-7.050775 ],\n",
       "       [-5.0279   ],\n",
       "       [-6.7557135],\n",
       "       [-7.2395105],\n",
       "       [-5.680828 ],\n",
       "       [-6.2349257],\n",
       "       [-5.665073 ],\n",
       "       [-5.549674 ],\n",
       "       [-5.3469324],\n",
       "       [-7.0402274],\n",
       "       [-5.6966233],\n",
       "       [-5.680044 ],\n",
       "       [-6.757373 ],\n",
       "       [-6.235203 ],\n",
       "       [-7.1415477],\n",
       "       [-6.580537 ],\n",
       "       [-5.7388277],\n",
       "       [-6.82877  ],\n",
       "       [-5.8533783],\n",
       "       [-6.6260333],\n",
       "       [-5.4610887],\n",
       "       [-5.66753  ],\n",
       "       [-5.857393 ],\n",
       "       [-5.9359636],\n",
       "       [-6.305954 ],\n",
       "       [-5.741223 ],\n",
       "       [-5.6098695],\n",
       "       [-6.2502627],\n",
       "       [-6.927321 ],\n",
       "       [-5.85624  ],\n",
       "       [-6.164502 ],\n",
       "       [-5.6590295],\n",
       "       [-6.2739224],\n",
       "       [-6.0411987],\n",
       "       [-5.205336 ],\n",
       "       [-6.5559916],\n",
       "       [-6.4805174],\n",
       "       [-5.6254244],\n",
       "       [-6.177579 ],\n",
       "       [-6.041918 ],\n",
       "       [-4.097748 ],\n",
       "       [-5.634298 ],\n",
       "       [-4.8491836],\n",
       "       [-6.8704896],\n",
       "       [-6.110001 ],\n",
       "       [-5.1542916],\n",
       "       [-6.449021 ],\n",
       "       [-4.8219833],\n",
       "       [-5.9592834],\n",
       "       [-6.767369 ],\n",
       "       [-5.774237 ],\n",
       "       [-6.968994 ],\n",
       "       [-5.762323 ],\n",
       "       [-5.8269744],\n",
       "       [-6.5596614],\n",
       "       [-6.2149186],\n",
       "       [-6.019484 ],\n",
       "       [-6.4834065],\n",
       "       [-7.198407 ],\n",
       "       [-6.342128 ],\n",
       "       [-5.657022 ],\n",
       "       [-6.535998 ],\n",
       "       [-5.652233 ],\n",
       "       [-6.403468 ],\n",
       "       [-5.8291125],\n",
       "       [-6.828626 ],\n",
       "       [-6.258901 ],\n",
       "       [-5.6277695],\n",
       "       [-6.057559 ],\n",
       "       [-4.6308045],\n",
       "       [-6.0377097],\n",
       "       [-5.6860013],\n",
       "       [-7.0066104],\n",
       "       [-6.6673975],\n",
       "       [-5.2765174],\n",
       "       [-6.284325 ],\n",
       "       [-5.9545817],\n",
       "       [-6.670281 ],\n",
       "       [-7.2520146],\n",
       "       [-5.560743 ],\n",
       "       [-6.76168  ],\n",
       "       [-5.502435 ],\n",
       "       [-7.2539034],\n",
       "       [-5.9639597],\n",
       "       [-6.3561482],\n",
       "       [-5.7563124],\n",
       "       [-6.4265056],\n",
       "       [-5.8494205],\n",
       "       [-4.67897  ],\n",
       "       [-4.80997  ],\n",
       "       [-6.349258 ],\n",
       "       [-5.243696 ],\n",
       "       [-6.104964 ],\n",
       "       [-5.3047304],\n",
       "       [-6.031637 ],\n",
       "       [-5.896472 ],\n",
       "       [-5.536828 ],\n",
       "       [-5.874675 ],\n",
       "       [-6.774267 ],\n",
       "       [-6.885566 ],\n",
       "       [-6.0369577],\n",
       "       [-7.09277  ],\n",
       "       [-6.3150363],\n",
       "       [-4.747201 ],\n",
       "       [-5.852372 ],\n",
       "       [-6.104987 ],\n",
       "       [-6.6745496],\n",
       "       [-5.2519646],\n",
       "       [-5.9395685],\n",
       "       [-6.026071 ],\n",
       "       [-5.287466 ],\n",
       "       [-4.2697625],\n",
       "       [-6.8617435],\n",
       "       [-7.016675 ],\n",
       "       [-7.527912 ],\n",
       "       [-6.764712 ],\n",
       "       [-6.529795 ],\n",
       "       [-7.391788 ],\n",
       "       [-5.5783424],\n",
       "       [-5.8042006],\n",
       "       [-5.815602 ],\n",
       "       [-5.453403 ],\n",
       "       [-6.3652287],\n",
       "       [-5.4761467],\n",
       "       [-5.2647767],\n",
       "       [-6.4223948],\n",
       "       [-6.630084 ],\n",
       "       [-5.550927 ],\n",
       "       [-7.261811 ],\n",
       "       [-6.0974355],\n",
       "       [-6.598464 ],\n",
       "       [-6.4036236],\n",
       "       [-4.8356633],\n",
       "       [-6.258689 ],\n",
       "       [-6.0461664],\n",
       "       [-5.6555495],\n",
       "       [-6.76948  ],\n",
       "       [-6.6079865],\n",
       "       [-7.086761 ],\n",
       "       [-6.394225 ],\n",
       "       [-5.4077077],\n",
       "       [-5.146603 ],\n",
       "       [-6.811654 ],\n",
       "       [-5.7536383],\n",
       "       [-6.7294507],\n",
       "       [-5.649827 ],\n",
       "       [-6.306651 ],\n",
       "       [-5.6723266],\n",
       "       [-6.238355 ],\n",
       "       [-6.6858277],\n",
       "       [-7.02753  ],\n",
       "       [-5.59896  ],\n",
       "       [-6.688501 ],\n",
       "       [-7.214868 ],\n",
       "       [-6.0758367],\n",
       "       [-7.2100673],\n",
       "       [-6.137352 ],\n",
       "       [-4.972377 ],\n",
       "       [-6.2130904],\n",
       "       [-6.091746 ],\n",
       "       [-6.2467976],\n",
       "       [-6.388222 ],\n",
       "       [-5.5292387],\n",
       "       [-6.610661 ],\n",
       "       [-6.618069 ],\n",
       "       [-7.052325 ],\n",
       "       [-5.025551 ],\n",
       "       [-6.3023453],\n",
       "       [-7.2473345],\n",
       "       [-5.57447  ],\n",
       "       [-5.8230863],\n",
       "       [-7.05039  ],\n",
       "       [-6.7612348],\n",
       "       [-6.525615 ],\n",
       "       [-5.9155774],\n",
       "       [-6.120426 ],\n",
       "       [-5.432018 ],\n",
       "       [-5.862184 ],\n",
       "       [-5.949027 ],\n",
       "       [-6.489058 ],\n",
       "       [-5.679387 ],\n",
       "       [-6.2248282],\n",
       "       [-4.9843855],\n",
       "       [-6.5379653],\n",
       "       [-6.691632 ],\n",
       "       [-6.26729  ],\n",
       "       [-4.808613 ],\n",
       "       [-5.795266 ],\n",
       "       [-6.19161  ],\n",
       "       [-4.9384437],\n",
       "       [-6.241075 ],\n",
       "       [-4.8541064],\n",
       "       [-7.922274 ],\n",
       "       [-6.687229 ],\n",
       "       [-5.628172 ],\n",
       "       [-5.4650564],\n",
       "       [-6.529808 ],\n",
       "       [-6.248017 ],\n",
       "       [-7.1815705],\n",
       "       [-6.2967925],\n",
       "       [-6.237058 ],\n",
       "       [-7.343072 ],\n",
       "       [-6.6981306],\n",
       "       [-6.701703 ],\n",
       "       [-5.23891  ],\n",
       "       [-6.1418414],\n",
       "       [-6.531668 ],\n",
       "       [-5.714537 ],\n",
       "       [-6.60442  ],\n",
       "       [-6.9868145],\n",
       "       [-6.626912 ],\n",
       "       [-5.859816 ],\n",
       "       [-6.0618696],\n",
       "       [-5.722531 ],\n",
       "       [-6.6381216],\n",
       "       [-5.813135 ],\n",
       "       [-6.345123 ],\n",
       "       [-5.6440306],\n",
       "       [-8.129148 ],\n",
       "       [-6.0996475],\n",
       "       [-6.3178983],\n",
       "       [-6.2323413],\n",
       "       [-5.96181  ],\n",
       "       [-5.5615954],\n",
       "       [-5.9442267],\n",
       "       [-5.4157214],\n",
       "       [-5.483071 ],\n",
       "       [-7.1139708],\n",
       "       [-4.878368 ],\n",
       "       [-6.437278 ],\n",
       "       [-6.5288563],\n",
       "       [-7.2404785],\n",
       "       [-6.643312 ],\n",
       "       [-7.2017426],\n",
       "       [-5.3012896],\n",
       "       [-6.9286056],\n",
       "       [-6.171535 ],\n",
       "       [-6.2236476],\n",
       "       [-6.033856 ],\n",
       "       [-7.3875327],\n",
       "       [-6.6907454],\n",
       "       [-6.3630934],\n",
       "       [-5.37334  ],\n",
       "       [-5.736406 ],\n",
       "       [-5.812301 ],\n",
       "       [-5.78084  ],\n",
       "       [-5.633629 ],\n",
       "       [-6.0144877],\n",
       "       [-6.007785 ],\n",
       "       [-5.8260245],\n",
       "       [-6.6360974],\n",
       "       [-6.083828 ],\n",
       "       [-6.4708486],\n",
       "       [-5.8898015],\n",
       "       [-5.1207943],\n",
       "       [-5.3672113],\n",
       "       [-6.049622 ],\n",
       "       [-5.5982714],\n",
       "       [-6.5300274],\n",
       "       [-6.2210336],\n",
       "       [-6.008059 ],\n",
       "       [-5.8835187],\n",
       "       [-5.1570516],\n",
       "       [-5.103718 ],\n",
       "       [-6.7318254],\n",
       "       [-6.4468713],\n",
       "       [-5.6011443],\n",
       "       [-6.959523 ],\n",
       "       [-6.9867015],\n",
       "       [-5.8491874],\n",
       "       [-6.1135283],\n",
       "       [-6.459561 ],\n",
       "       [-5.911953 ],\n",
       "       [-6.8771334],\n",
       "       [-6.3764987],\n",
       "       [-6.8524294],\n",
       "       [-6.3636985],\n",
       "       [-5.6548862],\n",
       "       [-5.452878 ],\n",
       "       [-6.825528 ],\n",
       "       [-6.5807915],\n",
       "       [-6.4343944],\n",
       "       [-5.4323077],\n",
       "       [-6.7443314]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "47105161-f3ce-496c-b6db-043df07f8acd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.8787578157832978"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_test1, val_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f3cb70f9-a8b9-4e7d-8321-fe72cd3dc82c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.8787578157832978"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_test1, val_preds.squeeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "36b242a0-1cf4-42f3-8b61-76449dfad489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_to Parameter containing:\n",
      "tensor([-1.4713], device='cuda:1', requires_grad=True)\n",
      "pos_from Parameter containing:\n",
      "tensor([0.2917], device='cuda:1', requires_grad=True)\n",
      "relev_pos_from Parameter containing:\n",
      "tensor([-0.4342], device='cuda:1', requires_grad=True)\n",
      "relev_pos_to Parameter containing:\n",
      "tensor([-0.3354], device='cuda:1', requires_grad=True)\n",
      "inv_pos_from Parameter containing:\n",
      "tensor([-0.1581], device='cuda:1', requires_grad=True)\n",
      "inv_pos_to Parameter containing:\n",
      "tensor([1.2451], device='cuda:1', requires_grad=True)\n",
      "inv_relev_pos_from Parameter containing:\n",
      "tensor([0.4964], device='cuda:1', requires_grad=True)\n",
      "inv_relev_pos_to Parameter containing:\n",
      "tensor([-0.5488], device='cuda:1', requires_grad=True)\n",
      "seq_len Parameter containing:\n",
      "tensor([-0.6138], device='cuda:1', requires_grad=True)\n",
      "inv_seq_len Parameter containing:\n",
      "tensor([-0.1583], device='cuda:1', requires_grad=True)\n",
      "head_num Parameter containing:\n",
      "tensor([-0.2370], device='cuda:1', requires_grad=True)\n",
      "hidden_to.weight Parameter containing:\n",
      "tensor([[-1.6579e-02,  8.2888e-03,  2.2594e-02,  5.2212e-02,  1.5133e-02,\n",
      "         -2.7904e-02, -5.7362e-02,  3.2257e-02, -2.2660e-02,  9.3527e-04,\n",
      "          5.5305e-04,  1.1288e-02,  2.4298e-02, -7.2989e-02, -4.2963e-02,\n",
      "         -2.1385e-02,  1.4090e-02,  1.2774e-01, -7.7978e-02, -7.4751e-02,\n",
      "         -7.5116e-03,  4.6118e-02, -2.8558e-02, -5.7025e-02,  5.8951e-02,\n",
      "         -2.8399e-02, -3.3832e-02,  4.8973e-02,  1.5305e-02, -3.6483e-02,\n",
      "          3.0286e-02, -5.4245e-02, -2.5938e-03,  2.5417e-02, -8.8341e-02,\n",
      "         -2.6330e-02,  2.1450e-02,  2.3789e-03, -8.1016e-02, -1.0330e-03,\n",
      "          6.3597e-03,  5.2607e-02,  4.2673e-02, -3.3679e-02, -3.5857e-02,\n",
      "         -5.5840e-02,  1.5080e-02, -1.7583e-02,  4.5135e-02, -3.1729e-02,\n",
      "         -3.1336e-02, -1.3692e-02, -2.2973e-02,  1.0429e-02, -4.0790e-02,\n",
      "         -6.2211e-03,  6.4374e-02,  4.7051e-02, -1.7607e-02,  3.2484e-02,\n",
      "          3.7343e-02, -2.6498e-02, -6.2159e-02,  7.8985e-02,  3.7707e-02,\n",
      "         -6.6495e-02,  2.6299e-02,  8.5306e-02,  4.2448e-02,  1.7188e-02,\n",
      "         -3.5099e-02, -2.6398e-02,  2.4278e-02,  2.5835e-02, -7.2293e-02,\n",
      "         -5.9773e-02,  1.7289e-02, -6.2568e-02, -8.5667e-02,  2.6562e-02,\n",
      "         -4.0959e-02,  2.4924e-02, -4.7879e-02, -1.7547e-03, -8.7819e-03,\n",
      "         -4.0742e-02,  1.8019e-02, -1.2548e-01, -1.9413e-02, -3.2654e-02,\n",
      "         -2.0948e-02, -5.1912e-02,  9.4686e-03, -1.8661e-02, -2.2025e-02,\n",
      "          1.4065e-02, -1.4086e-01,  3.1546e-02, -1.2031e-02,  4.3417e-02,\n",
      "          8.6379e-03, -6.7850e-02,  1.4223e-02,  5.4131e-02, -4.7946e-04,\n",
      "          5.5531e-02,  7.9760e-02,  1.0292e-02, -3.4301e-02,  3.5064e-02,\n",
      "         -2.8567e-02, -2.0701e-03,  1.0602e-01,  4.2315e-02,  6.4138e-03,\n",
      "         -5.4689e-02,  3.2610e-03,  6.0377e-02, -2.7720e-02, -3.2980e-02,\n",
      "          2.6060e-03, -6.4915e-01,  1.8294e-03, -2.9647e-02,  8.1373e-02,\n",
      "         -4.6603e-02, -3.5107e-02, -1.8493e-02, -3.6043e-02,  1.5004e-02,\n",
      "          8.3203e-03, -1.2559e-02, -4.8293e-02, -3.8803e-02,  2.0848e-02,\n",
      "          1.4976e-02,  1.2380e-02, -2.4555e-02,  4.5684e-02,  3.7945e-02,\n",
      "         -2.0374e-02, -4.5862e-02,  8.7303e-03, -6.7778e-02, -4.2683e-02,\n",
      "          5.9012e-02, -2.8726e-03,  4.5587e-02,  8.1552e-03, -1.6807e-02,\n",
      "          2.5201e-02, -3.8726e-02,  1.2606e-01, -4.5251e-02, -2.4151e-02,\n",
      "         -1.8869e-02, -8.4003e-02,  3.2916e-03, -3.1568e-02, -3.6233e-01,\n",
      "         -1.2771e-02, -2.8068e-02, -3.5299e-02, -2.8127e-02, -1.4121e-02,\n",
      "         -3.6113e-02, -3.6264e-02,  2.4554e-02,  1.3792e-01, -6.5159e-02,\n",
      "         -4.7566e-02, -3.5022e-02,  5.9993e-03, -1.0225e-01, -3.9803e-02,\n",
      "          2.8094e-01, -6.0418e-02, -7.5498e-02, -3.4467e-02, -3.4197e-03,\n",
      "          1.3041e-02,  5.3464e-02, -3.4228e-02, -1.0618e-02, -5.3390e-02,\n",
      "         -1.3011e-02, -8.2223e-02, -7.2467e-02, -1.2952e-02, -7.0114e-02,\n",
      "          1.5369e-02,  4.3032e-02, -7.5018e-02, -1.6271e-02, -3.7581e-02,\n",
      "         -6.8671e-02,  1.6164e-02, -6.5819e-02,  4.1011e-02, -6.5867e-03,\n",
      "         -3.9595e-02, -4.7401e-02, -1.3211e-03, -3.4077e-02,  1.5764e-02,\n",
      "         -8.2654e-02, -1.0241e-01, -7.7354e-02,  4.6151e-03,  8.5017e-03,\n",
      "         -1.3379e-03, -3.5349e-02, -7.1084e-02,  6.5924e-04, -5.1189e-02,\n",
      "         -3.8750e-02,  1.9257e-01,  2.8725e-02, -1.1318e-01, -3.1035e-02,\n",
      "         -1.0863e-02, -3.6793e-02, -4.5414e-02,  4.2634e-02, -2.8526e-02,\n",
      "         -6.7742e-02,  1.5821e-03, -2.8581e-02, -6.1872e-02,  3.6640e-03,\n",
      "         -6.9819e-02, -4.3171e-02, -1.6689e-03, -3.6779e-02, -7.3635e-02,\n",
      "          1.0664e-02, -2.6948e-02, -9.8026e-02,  1.5347e-02,  1.8516e-03,\n",
      "          1.4840e-02, -7.9695e-04,  5.8464e-03, -4.5869e-02, -1.6659e-02,\n",
      "         -3.0383e-02,  8.7888e-02, -5.1777e-02,  5.3380e-02,  4.5369e-02,\n",
      "         -4.3303e-02,  8.4939e-03, -3.8113e-02,  6.0348e-02, -2.3852e-02,\n",
      "         -7.2790e-02,  2.4124e-02,  5.9437e-02, -6.7589e-02, -2.0692e-02,\n",
      "         -3.0347e-02,  4.0265e-02, -1.1043e-01, -5.2615e-02, -2.5184e-02,\n",
      "         -1.7433e-02,  1.9414e-02, -1.0345e-01, -6.0744e-02, -5.3861e-02,\n",
      "         -9.8505e-03, -6.9951e-04, -3.3057e-02,  8.7127e-03,  4.1357e-02,\n",
      "         -1.6817e-02,  3.9607e-02, -7.5999e-03, -5.3250e-02, -1.4398e-02,\n",
      "         -2.3636e-02,  3.7041e-02, -2.5589e-02, -7.2299e-02,  1.0331e-02,\n",
      "          6.1261e-02, -2.2838e-02, -7.8937e-03, -3.6156e-02, -8.9223e-03,\n",
      "         -3.0272e-02, -8.4230e-02, -7.6945e-03,  4.2982e-03,  1.0709e-02,\n",
      "          7.7536e-02,  1.2860e-02, -1.5792e-02,  5.7417e-02, -5.1646e-03,\n",
      "          5.6703e-03,  1.8632e-02,  2.6455e-03, -4.6598e-02, -9.0207e-02,\n",
      "         -8.9981e-03,  2.0528e-02,  1.5423e-02,  1.4633e-01, -1.0101e-02,\n",
      "          2.4959e-02, -1.2366e-02,  2.0824e-02,  6.0279e-02, -5.2659e-02,\n",
      "          5.0549e-02, -3.0451e-02, -5.3946e-02, -6.4503e-02, -5.1417e-03,\n",
      "          6.4123e-03,  5.0127e-05,  3.0542e-02,  3.5679e-02,  8.5763e-03,\n",
      "         -9.0617e-02, -9.8950e-02,  3.2785e-02, -8.0703e-03, -2.3113e-02,\n",
      "          4.0662e-02,  1.1958e-01, -3.9870e-02, -3.1490e-02, -2.4779e-03,\n",
      "         -1.8344e-03,  1.5911e-02, -4.3348e-02,  1.2375e-02, -1.4060e-01,\n",
      "         -4.1583e-03, -1.1326e-02,  3.8878e-02, -4.9221e-02,  3.3272e-02,\n",
      "         -1.6833e-01,  1.5053e-02, -3.0052e-02, -4.4228e-02,  5.8474e-02,\n",
      "         -5.8273e-02, -5.3639e-02,  6.2374e-02,  1.3263e-02,  7.6325e-02,\n",
      "          4.8868e-02, -1.0937e-02, -5.7536e-02,  3.9656e-02, -1.0291e-02,\n",
      "         -3.9571e-02, -4.3613e-02, -2.4438e-02, -3.8526e-02, -4.8468e-02,\n",
      "         -6.3625e-03, -2.9480e-02, -5.6081e-02, -7.7905e-02, -4.6980e-02,\n",
      "          5.7647e-02, -7.9616e-03, -5.8863e-03, -1.5192e-02, -1.2355e-02,\n",
      "          1.4926e-02, -3.4295e-03,  4.5584e-02,  6.6394e-03, -7.6921e-02,\n",
      "          2.7220e-02,  8.1350e-02, -1.0467e-02,  1.5855e-03, -6.8122e-03,\n",
      "          1.3829e-02,  3.1302e-02, -1.4757e-02,  7.4916e-03,  4.9508e-02,\n",
      "          6.7008e-03,  3.5400e-02, -4.4202e-03,  1.8933e-02, -5.9401e-02,\n",
      "          1.6068e-02,  7.8392e-03,  2.8063e-02, -3.3099e-02, -2.7331e-02,\n",
      "          8.2296e-03,  1.6850e-02, -2.9868e-02,  4.8584e-03, -8.5738e-02,\n",
      "          7.9436e-02, -5.5986e-02,  8.3154e-02, -4.1995e-02, -2.7434e-02,\n",
      "          2.3085e-02, -8.9739e-03,  1.8691e-02, -2.5910e-02,  3.1727e-02,\n",
      "         -3.6928e-02, -4.6305e-02, -7.2027e-03, -2.2952e-02, -2.6139e-02,\n",
      "         -1.6067e-03, -2.1431e-02, -6.2286e-02, -3.5486e-02,  4.7291e-03,\n",
      "         -4.9954e-02, -5.0402e-02,  2.2518e-02,  3.9373e-02,  6.6693e-02,\n",
      "         -7.3270e-02, -6.2776e-03, -3.8402e-02, -9.0743e-03, -4.2734e-02,\n",
      "         -1.8874e-02, -5.0149e-02,  1.2008e-02,  3.1718e-02, -1.5391e-02,\n",
      "          1.6950e-02, -6.6872e-02, -3.3540e-02,  1.1946e-01, -1.4611e-02,\n",
      "          2.5337e-02,  5.0798e-03, -2.6573e-02,  1.6263e-02,  2.0508e-03,\n",
      "          8.2622e-03, -4.4195e-02, -1.0915e-01, -2.0881e-02,  1.8966e-02,\n",
      "         -1.6637e-02, -3.7395e-02, -5.3265e-02, -6.6411e-03,  4.5327e-02,\n",
      "         -3.1795e-02,  3.4207e-02, -8.0636e-02, -2.1027e-02, -5.1481e-02,\n",
      "          4.4644e-02,  1.1447e-02, -3.5154e-02, -5.7673e-02, -9.2282e-03,\n",
      "          3.0775e-02, -2.7174e-02,  2.7115e-02, -2.9235e-02,  1.0447e-02,\n",
      "         -2.2386e-02,  1.5204e-02, -1.6403e-02,  3.1894e-03, -4.3514e-02,\n",
      "          4.1575e-02, -6.9223e-02, -9.3826e-03, -2.6895e-02,  2.4738e-02,\n",
      "          1.0662e-02, -3.0183e-03, -2.6897e-02,  2.9502e-02, -2.3309e-02,\n",
      "         -7.2684e-03,  1.6137e-02,  3.1968e-03,  4.4556e-02,  6.8089e-02,\n",
      "         -5.2924e-04,  9.0765e-02, -1.2969e-02, -4.6574e-02,  3.7206e-04,\n",
      "          1.3824e-02, -1.7506e-02, -3.6591e-02, -3.6303e-03, -7.8443e-02,\n",
      "          3.6088e-02, -3.9948e-02,  3.8854e-02,  5.8345e-02, -3.6808e-02,\n",
      "          8.4673e-02, -1.3749e-02, -2.8988e-02,  1.8966e-02,  4.1740e-02,\n",
      "         -3.9219e-02, -8.6022e-03,  1.1676e-01,  3.2891e-02, -1.3174e-02,\n",
      "         -8.2615e-02, -2.6395e-02, -5.8999e-02,  1.3380e-02,  2.5718e-02,\n",
      "          2.5947e-03,  3.9860e-02, -3.7063e-04, -9.6654e-02,  2.7717e-01,\n",
      "         -4.2548e-02, -5.7832e-02, -4.8220e-03, -2.4721e-02,  6.5432e-02,\n",
      "          1.7331e-02, -1.5784e-03, -6.2192e-02, -1.6103e-02, -1.8719e-02,\n",
      "         -2.4146e-02,  3.2750e-01, -9.4466e-03, -1.1509e-02, -2.8798e-02,\n",
      "          8.2370e-02,  2.9957e-02,  6.4511e-02, -4.4992e-02, -3.3695e-02,\n",
      "         -2.1996e-03, -3.8666e-02, -2.2650e-02, -4.6270e-02,  1.0928e-03,\n",
      "         -5.7857e-02, -3.8553e-02, -4.1311e-02, -4.8906e-02, -3.8494e-02,\n",
      "          1.2257e-03,  7.0721e-02,  5.2720e-02, -4.8164e-02, -3.6720e-02,\n",
      "         -3.7063e-02, -6.6065e-02,  1.5310e-02, -4.2084e-02,  2.2031e-03,\n",
      "         -8.8667e-02,  3.4613e-03,  6.2502e-03, -1.9924e-02, -4.8210e-02,\n",
      "          1.4403e-02, -3.5521e-02,  1.2293e-01, -2.8445e-02, -5.7013e-02,\n",
      "          2.6029e-02, -1.1368e-02, -4.3023e-02, -8.4669e-03, -2.0380e-02,\n",
      "         -3.6614e-02, -7.4782e-02, -1.3302e-02,  3.7426e-04, -3.7692e-02,\n",
      "          3.8866e-03,  4.1003e-02, -3.5185e-02, -6.8784e-02, -2.4937e-02,\n",
      "         -9.5324e-03, -1.8366e-02,  9.5538e-03, -2.9022e-02, -1.4896e-02,\n",
      "         -2.1920e-03, -6.0115e-02,  1.5650e-03,  1.8262e-02, -9.0718e-04,\n",
      "         -1.6123e-02,  4.1092e-02, -4.5320e-02, -2.6314e-03, -6.6734e-02,\n",
      "         -3.0004e-02,  1.3221e-02, -3.9917e-02, -4.1393e-02, -1.6501e-01,\n",
      "          2.8066e-02, -8.3165e-02,  2.5560e-02,  2.1579e-02, -1.1008e-02,\n",
      "         -1.4164e-02, -6.3799e-03,  2.5005e-02, -2.3751e-02, -2.3031e-02,\n",
      "         -3.5693e-02, -5.7341e-02,  3.3279e-02,  3.5177e-02, -2.8286e-02,\n",
      "          2.5920e-02, -7.0850e-03, -4.6282e-03, -1.9275e-02, -1.6863e-02,\n",
      "          2.3130e-02, -7.3236e-02, -4.0108e-02, -2.5343e-02,  3.0277e-02,\n",
      "          3.0139e-02, -7.8188e-02,  2.0237e-03, -2.1862e-02, -2.2076e-02,\n",
      "         -5.8293e-03, -2.9574e-02,  1.3683e-02,  1.1890e-03, -4.2967e-02,\n",
      "          8.7447e-03, -2.2470e-03, -4.1366e-03, -1.7148e-02, -4.1604e-01,\n",
      "          1.7295e-02,  2.4880e-02,  8.6014e-05, -5.3724e-02, -9.3074e-02,\n",
      "         -3.6196e-02, -1.0307e-02,  5.6600e-04, -6.4170e-02, -3.1125e-02,\n",
      "          8.8459e-02, -4.5395e-02,  3.3734e-02,  4.3210e-02,  9.3389e-03,\n",
      "          5.1142e-02, -3.9159e-02, -8.7154e-02,  6.1227e-02, -9.7368e-02,\n",
      "         -4.1811e-02, -8.3558e-02, -5.7378e-02, -9.4141e-03,  2.1205e-02,\n",
      "         -2.7071e-02,  1.3042e-02, -9.5935e-02,  6.8222e-02,  2.8851e-02,\n",
      "          5.1765e-02,  4.9895e-03,  2.0631e-02,  4.3780e-02, -1.3529e-02,\n",
      "         -3.8647e-02,  3.8504e-02, -3.2610e-02,  4.9570e-02,  3.8231e-02,\n",
      "         -4.2519e-02, -6.6948e-03, -9.0488e-03, -4.4729e-02, -1.3313e-01,\n",
      "         -3.4474e-02, -2.2513e-02,  5.9912e-02, -2.6605e-02,  2.3039e-02,\n",
      "         -8.9276e-03, -1.1365e-01, -2.4999e-02, -2.1302e-02, -4.2675e-02,\n",
      "         -1.9091e-02,  2.0144e-02, -1.8997e-02, -4.4214e-02, -8.6814e-03,\n",
      "         -1.1780e-02,  2.8413e-02, -2.2781e-02, -4.0763e-02, -1.8524e-02,\n",
      "          4.5119e-02,  6.6365e-02, -3.4323e-02, -7.2258e-03, -3.6642e-02,\n",
      "         -2.7853e-02, -3.0942e-02, -5.6152e-03, -2.0275e-02, -1.1114e-01,\n",
      "         -8.0691e-03, -5.1210e-02,  4.7385e-03, -6.7102e-02,  1.8422e-02,\n",
      "         -1.6739e-02, -7.3929e-02,  3.4164e-02, -1.4850e-02, -3.3439e-03,\n",
      "          4.0210e-02,  7.5910e-03, -6.3322e-02,  5.6936e-04, -3.2057e-02,\n",
      "          2.6002e-02, -5.3539e-03,  2.2387e-02, -4.2653e-02,  2.7120e-02,\n",
      "         -5.3708e-03,  5.8819e-02, -6.4474e-02, -4.9945e-02, -3.5918e-02,\n",
      "         -5.0913e-02,  1.0691e-01, -1.5406e-02, -2.2913e-02,  2.7520e-02,\n",
      "         -1.8905e-02, -7.2966e-02, -5.3334e-02,  1.0434e-01, -9.8736e-02,\n",
      "          3.5530e-02, -2.0179e-02, -4.2295e-02]], device='cuda:1',\n",
      "       requires_grad=True)\n",
      "hidden_from.weight Parameter containing:\n",
      "tensor([[ 3.1500e-02, -1.3921e-01,  1.1066e-01,  3.9160e-02, -5.8688e-02,\n",
      "          1.6322e-02, -2.7557e-02,  3.6578e-02,  2.9392e-02, -9.5382e-02,\n",
      "          7.5081e-02, -5.3732e-02, -7.6604e-02, -1.0865e-01, -8.8991e-02,\n",
      "         -1.2384e-02,  7.2988e-02,  8.2638e-02,  1.2071e-01, -3.7999e-02,\n",
      "         -1.8493e-01,  1.2380e-01, -1.1243e-02, -6.5707e-02,  5.9245e-02,\n",
      "         -8.1716e-02, -1.9372e-01, -5.4591e-02,  1.0986e-01, -5.4824e-02,\n",
      "          4.5174e-02, -9.2311e-02, -1.1635e-01,  7.2868e-02, -2.4784e-02,\n",
      "          1.2047e-02, -1.1793e-01, -7.7725e-03,  6.0415e-02, -1.7343e-02,\n",
      "          2.1815e-02,  1.6442e-02, -4.8475e-02, -7.0942e-02,  7.2295e-02,\n",
      "          8.0549e-02,  1.2067e-01, -9.5735e-02, -1.1570e-01, -1.6236e-02,\n",
      "          4.1237e-02, -2.9412e-02,  3.3070e-02, -1.1870e-01, -1.4639e-01,\n",
      "          1.4548e-01,  8.2339e-03,  2.0056e-03,  6.3316e-02,  2.3661e-02,\n",
      "          1.0494e-01, -4.4388e-02,  1.3237e-02,  1.4426e-01, -5.6962e-02,\n",
      "          1.9117e-02,  4.2023e-02,  8.7032e-03,  2.7564e-02, -9.2726e-02,\n",
      "          6.5057e-02,  5.7897e-02,  1.4578e-01,  1.2936e-01, -1.1528e-01,\n",
      "         -4.0587e-02, -1.6612e-02, -9.9333e-02,  9.2356e-02,  7.3686e-02,\n",
      "         -2.4292e-02,  2.1160e-02,  9.2770e-02,  9.5034e-02, -9.9569e-03,\n",
      "          5.7723e-02,  7.9928e-03, -9.7614e-02,  8.4901e-02,  6.7845e-02,\n",
      "          6.1643e-03, -6.0401e-02, -6.0267e-02, -1.9763e-02, -4.2813e-02,\n",
      "          1.7312e-01, -1.3857e-01,  1.4529e-01, -3.1670e-02,  9.1050e-02,\n",
      "         -1.4620e-02, -3.8607e-02, -2.3297e-02, -3.4316e-02, -1.8504e-01,\n",
      "         -1.0257e-01, -6.1744e-02,  1.7774e-03,  2.4097e-02,  1.6957e-01,\n",
      "         -6.9364e-03, -3.2877e-02,  7.8094e-02,  1.0710e-02,  1.1351e-01,\n",
      "          4.1546e-02, -6.0483e-02,  4.6199e-02,  2.2075e-03, -9.7119e-02,\n",
      "          6.3021e-02, -7.8332e-01,  4.7858e-02, -4.7661e-02, -2.3870e-02,\n",
      "         -3.3047e-02,  7.6530e-02,  4.7205e-02, -2.3647e-02,  3.5333e-02,\n",
      "         -3.4938e-03, -4.4405e-02, -5.2922e-03,  3.1761e-02,  2.6862e-02,\n",
      "          1.1236e-01,  2.2601e-02, -1.7918e-01,  3.5733e-02,  8.2087e-02,\n",
      "          1.6934e-01,  3.1853e-02,  1.4352e-01,  7.5014e-02,  1.8698e-02,\n",
      "          1.2023e-01, -4.7345e-02,  4.9431e-03, -5.9042e-02, -8.8998e-02,\n",
      "         -6.7742e-02,  3.8586e-03,  2.2441e-01, -9.9885e-02, -1.5937e-01,\n",
      "          1.3973e-01,  5.5712e-02,  4.3121e-02,  6.9594e-03, -3.8882e-01,\n",
      "         -1.1011e-01, -5.0977e-02, -1.1406e-01, -1.8857e-01, -4.9601e-02,\n",
      "         -3.5610e-03, -2.6603e-02, -1.3189e-01,  2.0290e-01, -5.4611e-02,\n",
      "         -3.0149e-02, -2.2045e-01,  4.0911e-03, -7.8637e-02, -1.8988e-01,\n",
      "          2.1893e-01,  6.1765e-02, -1.3564e-01,  9.3296e-02,  4.7690e-02,\n",
      "          9.3062e-02,  8.6217e-02,  3.3506e-03, -9.5225e-02, -1.8383e-02,\n",
      "         -6.0456e-02, -9.4059e-03, -4.9113e-02, -9.1902e-02,  1.6503e-01,\n",
      "          5.5366e-02, -4.6168e-02, -1.9183e-02,  1.1662e-01, -1.3547e-01,\n",
      "         -2.4080e-02, -1.9305e-02, -7.1843e-02, -1.6080e-02,  1.1049e-02,\n",
      "          8.1870e-02, -5.8694e-02, -3.9137e-01, -2.2156e-01, -7.3363e-02,\n",
      "         -1.1874e-01, -9.6684e-02, -3.9213e-02, -1.1562e-01,  5.6877e-02,\n",
      "          6.5757e-02,  5.6839e-03, -4.0725e-02, -4.8011e-02, -1.5574e-01,\n",
      "         -2.9895e-01,  2.8314e-01, -1.2421e-02,  3.1768e-02,  1.3436e-02,\n",
      "          7.1858e-02,  4.1733e-02,  7.8735e-04,  6.9624e-03,  5.1918e-02,\n",
      "         -1.5079e-01, -3.6448e-02, -7.9199e-02,  1.0066e-01, -5.3290e-02,\n",
      "         -4.6840e-02, -8.7757e-02, -1.6437e-01,  1.5604e-01, -9.9052e-02,\n",
      "          2.7553e-02, -2.2463e-02, -7.5908e-02, -1.2541e-02,  6.6798e-02,\n",
      "          2.9798e-02,  1.1281e-01,  1.7211e-02, -1.0765e-01, -2.1253e-02,\n",
      "         -1.2287e-01,  5.3454e-02,  1.0200e-01,  1.4377e-01,  1.7372e-01,\n",
      "          1.0450e-01, -4.9600e-02, -6.4744e-02,  5.6195e-02, -1.9140e-02,\n",
      "          4.4204e-02,  1.4087e-01,  6.7762e-02, -2.5115e-02,  8.1564e-02,\n",
      "         -1.1024e-01, -2.0121e-03, -1.3658e-01,  6.9713e-03, -1.1273e-01,\n",
      "         -4.2324e-02, -1.3848e-02,  4.5946e-02, -4.8126e-02, -4.4004e-03,\n",
      "         -4.3409e-02, -3.5741e-02,  6.1747e-02, -4.6773e-02,  2.5006e-02,\n",
      "          4.3643e-02, -2.9147e-02, -6.3707e-02,  8.3533e-02,  5.8904e-02,\n",
      "          5.3944e-02,  7.3874e-02, -6.0554e-02, -8.3472e-02, -3.9729e-02,\n",
      "          1.1236e-01, -2.8471e-02, -1.6200e-01, -3.1725e-02,  1.8564e-01,\n",
      "         -3.8689e-02, -1.7494e-01,  1.7231e-01,  5.6004e-03, -6.1988e-02,\n",
      "          5.2590e-02,  2.8135e-01, -8.4015e-03,  1.0476e-02, -1.2142e-01,\n",
      "          5.3966e-03,  1.6236e-02,  1.6202e-03,  1.8126e-02, -1.4141e-01,\n",
      "          4.4886e-02,  4.0130e-02, -1.4547e-01,  2.0956e-01, -1.6572e-01,\n",
      "          6.6579e-02,  1.0108e-02, -4.7908e-02,  5.5073e-02, -2.3251e-01,\n",
      "         -6.0771e-02, -3.5522e-02,  2.4184e-02, -1.6433e-01,  1.4611e-02,\n",
      "         -3.3999e-02,  4.5120e-02,  4.8519e-02,  2.1246e-02, -1.1331e-01,\n",
      "          5.9953e-02, -1.0815e-01, -1.0198e-05,  1.6331e-01, -6.1476e-02,\n",
      "          1.2300e-01,  4.4990e-02,  7.2217e-02,  3.4422e-02, -9.7394e-03,\n",
      "          2.4862e-02, -5.0165e-02,  8.0534e-02,  3.5110e-02, -2.2911e-02,\n",
      "          1.3118e-01, -1.2803e-01,  8.8702e-03, -3.9133e-02,  2.2128e-02,\n",
      "         -2.7252e-01, -6.7302e-02,  8.1050e-02,  4.6087e-02, -5.2877e-02,\n",
      "         -9.1792e-02, -1.4843e-01,  9.2640e-03, -1.7274e-01, -5.2769e-02,\n",
      "          1.7180e-02,  5.6482e-02, -5.5729e-02,  4.4786e-02, -4.8135e-02,\n",
      "         -3.7398e-02, -7.6414e-02, -1.6569e-01, -8.5023e-03,  6.1334e-02,\n",
      "         -1.7456e-01, -8.3101e-02,  2.5082e-02,  4.6303e-02,  1.7090e-02,\n",
      "          2.5703e-02, -1.0453e-01, -5.1529e-02, -8.5895e-02, -7.0550e-02,\n",
      "         -4.8388e-02,  1.4518e-01,  4.2174e-02, -6.7094e-02, -2.4225e-02,\n",
      "          3.0986e-02,  2.9213e-01, -6.5365e-02,  1.6293e-01, -6.3580e-02,\n",
      "         -1.3504e-01, -1.2446e-01, -3.2843e-02,  6.5577e-02, -1.7225e-03,\n",
      "         -9.2158e-02,  1.0960e-01,  4.2314e-02,  3.6202e-02, -8.6526e-02,\n",
      "          4.1008e-02, -3.4521e-02,  4.0810e-02,  1.9981e-02, -1.1237e-02,\n",
      "          4.2188e-02,  1.1655e-01,  1.7927e-02, -8.1169e-02,  2.1798e-03,\n",
      "          1.0463e-01,  4.2427e-02,  8.9547e-02, -1.1616e-01,  1.1215e-02,\n",
      "         -1.3548e-02,  1.6839e-01,  7.5894e-03,  1.5363e-02, -5.3223e-02,\n",
      "         -1.3320e-01,  1.9630e-01, -1.4713e-02,  7.1857e-02, -1.8510e-01,\n",
      "          4.2503e-02,  1.1775e-01, -1.2054e-01,  4.1864e-02, -4.7640e-02,\n",
      "          5.2451e-02, -1.0513e-01, -1.0500e-03, -3.2784e-02, -1.2247e-02,\n",
      "         -1.8425e-03,  8.1308e-02,  1.9629e-03, -6.5412e-03,  7.9993e-04,\n",
      "         -1.9115e-01, -4.3968e-02,  4.2141e-02, -3.5366e-02,  1.2174e-01,\n",
      "         -1.2945e-01,  2.4732e-02,  2.4950e-01,  1.0266e-01, -4.5998e-02,\n",
      "         -3.3470e-02,  6.9758e-02, -5.5535e-02,  7.5283e-02, -2.3281e-02,\n",
      "          3.8073e-02,  8.3365e-02, -1.5035e-01,  6.3256e-02,  4.2415e-02,\n",
      "         -3.5990e-02, -7.6785e-02, -1.2216e-02, -1.4926e-01, -4.7939e-02,\n",
      "          1.3968e-01,  2.7801e-01,  7.8088e-03, -5.1474e-02, -6.0943e-02,\n",
      "          1.1529e-01,  7.4316e-02, -6.0071e-02, -5.4622e-02, -2.1707e-02,\n",
      "         -5.0729e-02,  2.8817e-02, -6.0611e-03,  1.9689e-01, -7.5809e-02,\n",
      "          1.9510e-01, -4.5099e-02,  7.7764e-02, -3.3630e-02, -1.4369e-02,\n",
      "         -9.0916e-02, -4.4041e-02, -1.8649e-01, -5.6906e-02,  8.8580e-02,\n",
      "          3.9942e-02, -3.6183e-03, -3.3999e-02,  6.0238e-02, -1.5206e-01,\n",
      "          2.5886e-02,  2.0452e-02, -6.6542e-02, -7.1504e-02, -1.8433e-02,\n",
      "          3.4205e-02,  1.3848e-01,  1.1852e-02,  9.4663e-02,  1.0267e-01,\n",
      "          1.1337e-01, -1.1252e-01,  5.3624e-02, -6.2079e-02, -1.8349e-02,\n",
      "         -4.8077e-02, -1.5835e-02, -1.0021e-01, -6.3182e-02, -5.5547e-02,\n",
      "         -2.3551e-01, -6.2436e-02, -5.5101e-02, -4.7725e-02, -8.5739e-02,\n",
      "         -3.8319e-02,  8.9304e-02, -9.0831e-02,  1.4915e-01,  2.4215e-01,\n",
      "         -1.2169e-01,  1.0614e-01, -6.2746e-02, -4.4509e-04, -2.8068e-02,\n",
      "          1.7214e-02,  1.3442e-02, -8.1027e-03,  1.6001e-01,  3.2501e-01,\n",
      "          5.3412e-02,  7.0678e-02, -4.4914e-02, -9.8260e-02, -2.6367e-02,\n",
      "         -5.4062e-02,  2.1728e-01, -4.6554e-03, -7.6175e-02,  2.3575e-01,\n",
      "         -2.0638e-03,  3.7606e-01,  4.4382e-02, -2.6156e-02, -8.2283e-02,\n",
      "          8.8427e-02,  8.9170e-02,  3.7586e-02,  6.6749e-02,  1.6638e-02,\n",
      "          9.3540e-02, -2.8879e-02, -2.2534e-01,  5.0588e-02,  1.8839e-02,\n",
      "         -1.2390e-02, -8.2843e-02, -6.0494e-02, -9.5109e-02,  2.5039e-02,\n",
      "         -8.0814e-03,  2.0372e-02,  4.0546e-02,  1.6159e-01,  7.8839e-02,\n",
      "         -1.0686e-01, -7.3054e-02,  9.3275e-03, -1.1772e-01,  1.1333e-01,\n",
      "          1.9010e-02, -1.3903e-02,  4.5235e-02, -1.8435e-02, -1.7640e-02,\n",
      "          5.5652e-02, -1.3418e-01,  3.4282e-01, -3.2419e-02,  1.7752e-02,\n",
      "         -5.8971e-02,  1.0776e-01,  2.4595e-02,  7.1439e-02,  3.0998e-02,\n",
      "         -3.3406e-02, -7.5092e-02,  1.4278e-01, -4.9012e-02, -1.8287e-02,\n",
      "         -7.9222e-02,  4.4485e-02,  2.6789e-03,  6.1661e-02,  6.9171e-02,\n",
      "         -1.9816e-02,  1.7687e-01,  2.1059e-02, -6.1541e-02,  5.0450e-02,\n",
      "         -6.8413e-02, -2.8357e-03,  3.1597e-02, -9.1701e-05, -1.6625e-01,\n",
      "         -1.3462e-01, -3.8600e-02, -5.0950e-02, -2.8638e-02,  1.2184e-01,\n",
      "         -1.2622e-02, -6.3830e-02, -1.1920e-01, -6.2387e-02, -2.0302e-01,\n",
      "          2.2694e-02, -2.8211e-03,  7.0806e-02,  3.2599e-02, -2.1909e-01,\n",
      "          9.0974e-02, -2.2803e-02, -5.1863e-02,  1.6244e-01, -4.0207e-03,\n",
      "         -1.3918e-01,  1.5293e-02,  1.8000e-02,  6.1718e-02, -6.6829e-02,\n",
      "         -4.4577e-02,  8.4771e-04, -1.4124e-02, -2.9192e-02,  3.1385e-02,\n",
      "         -3.0115e-02,  3.7392e-02, -4.7334e-02,  6.0131e-02,  1.4145e-02,\n",
      "          6.6013e-02, -3.8079e-02, -6.1602e-02, -1.1886e-02, -1.7889e-02,\n",
      "         -6.2982e-02, -2.2873e-02,  1.2460e-02, -1.1696e-01,  3.3758e-02,\n",
      "          2.0606e-01, -1.2964e-01, -4.0428e-03,  7.4269e-02, -5.3386e-01,\n",
      "          6.4328e-02,  6.3518e-02,  1.4789e-02, -4.9674e-02, -7.0432e-02,\n",
      "         -2.0020e-01, -1.9785e-01,  2.5122e-03, -1.7540e-02, -4.2286e-02,\n",
      "         -2.4594e-02, -2.5283e-02, -2.0179e-02,  7.5165e-03, -4.1034e-02,\n",
      "          5.1604e-03, -2.6235e-02, -2.3048e-01,  1.4802e-02,  6.7781e-04,\n",
      "          1.0080e-01, -1.1386e-01, -2.3040e-03, -1.5903e-01,  1.2933e-01,\n",
      "         -2.6894e-02,  5.9358e-02, -8.9296e-02,  1.0567e-01,  8.1470e-02,\n",
      "          9.6365e-02, -1.0676e-03,  2.0331e-01,  3.9182e-03, -8.8243e-02,\n",
      "         -1.2259e-01,  3.9705e-02,  3.3805e-02,  3.0964e-02,  6.2763e-02,\n",
      "         -2.9507e-02, -1.0994e-02, -3.6017e-02,  2.6029e-02, -1.6127e-01,\n",
      "          1.0165e-01,  1.7653e-02,  9.1078e-02, -3.7716e-02,  7.6694e-02,\n",
      "         -3.1059e-02, -5.9977e-02, -4.9874e-02,  4.2342e-03, -1.0322e-01,\n",
      "          4.3429e-02, -5.7126e-02, -9.9104e-02, -1.3406e-01,  1.5829e-01,\n",
      "          2.3144e-02, -6.7308e-02,  5.5285e-02,  1.1414e-02, -5.6583e-02,\n",
      "         -5.5520e-02,  3.9885e-02, -1.4619e-01, -1.2316e-01, -4.2091e-02,\n",
      "         -1.1439e-02, -1.3029e-02,  8.3134e-02,  1.0497e-01, -2.0790e-02,\n",
      "         -5.6710e-02, -9.7006e-02,  1.7075e-01,  2.4675e-01, -6.1914e-02,\n",
      "         -9.4697e-02,  6.3277e-02, -1.6435e-02,  1.2627e-01,  1.9682e-02,\n",
      "          7.0850e-02, -3.9969e-02, -1.5944e-01, -8.4345e-02, -1.6550e-01,\n",
      "         -2.3285e-02,  8.3142e-03,  1.3758e-01,  5.8290e-03,  6.8663e-02,\n",
      "         -6.8494e-02,  1.4302e-01, -4.0904e-02,  5.5191e-02,  8.1138e-02,\n",
      "         -2.1986e-01,  1.1765e-01,  1.2968e-01, -4.7770e-03, -2.1841e-02,\n",
      "          2.3930e-02,  8.4790e-02, -6.9719e-02, -1.0534e-01, -1.5532e-01,\n",
      "          8.7650e-03, -8.1522e-02, -1.4788e-02]], device='cuda:1',\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for param_name, param in model.named_parameters():\n",
    "    print(param_name, param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e38df42d-ace4-4270-9c16-465235867120",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0.9457], device='cuda:1', requires_grad=True)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f407b6-14f2-4606-8df4-099be5cbc3c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "701e8bb3-35be-4284-8885-ac6b5d15880e",
   "metadata": {},
   "source": [
    "## Approach 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1d559231-d30e-452b-8e3b-9250d17b7d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_N, head_N = 0, 0\n",
    "X_train, y_train, X_test, y_test = build_dict_dataset_from_cached(config, train_datasets, layer=layer_N, heads=[head_N], \n",
    "                                                                  features=config.attention_config.features, \n",
    "                                                                  split_hidden=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5a2b6775-8b9d-4385-b8d8-8d8450af37ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = BertWrapperLin(initial_model, LinearClassifierBertAttention, config, layer_nums=[6, 7, 8, 9, 10, 11])\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "51be2a68-ff13-4798-800b-2235e1f3932e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param_name, param in linear_model.named_parameters():\n",
    "    if 'LinearAttention' in param_name:\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "998e4840-20f4-4d42-932f-e1576a064dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.fc = nn.Linear(768, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "397bdc04-27c1-47fd-bd46-4b83d0e250ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd4b67147e754fba93c27425da7b5a58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2]) torch.Size([1])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'criterion' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[103], line 31\u001b[0m\n\u001b[1;32m     27\u001b[0m gold_label \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([ex[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]])\u001b[38;5;241m.\u001b[39mto(config\u001b[38;5;241m.\u001b[39mgeneral\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(pred_label\u001b[38;5;241m.\u001b[39mshape, gold_label\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 31\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m(pred_label, gold_label)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'criterion' is not defined"
     ]
    }
   ],
   "source": [
    "linear_head = SimpleNet().to(config.general.device)\n",
    "\n",
    "dataset_names = list(train_datasets)\n",
    "dataset_name = 'imdb'\n",
    "dataset_idx = 0\n",
    "\n",
    "for ex_idx, ex in tqdm(enumerate(train_datasets[dataset_name]['train'])):\n",
    "    if ex_idx > config.data.cache_cut_size:\n",
    "        continue\n",
    "    field1, field2 = config.data.train_datasets_fields[dataset_idx]\n",
    "    if field2 != '':\n",
    "        encoded_inputs = tokenizer.encode(\n",
    "                        ex[field1],\n",
    "                        ex[field2],\n",
    "                        truncation=True,\n",
    "                        return_tensors='pt'\n",
    "                    ).to(config.general.device)\n",
    "    else:\n",
    "        encoded_inputs = tokenizer.encode(\n",
    "                        ex[field1],\n",
    "                        truncation=True,\n",
    "                        return_tensors='pt'\n",
    "                    ).to(config.general.device)\n",
    "\n",
    "    emb = linear_model(encoded_inputs).last_hidden_state[:, 0, :]\n",
    "    pred_label = linear_head(emb)\n",
    "    gold_label = torch.tensor([ex['label']]).to(config.general.device)\n",
    "\n",
    "    \n",
    "    print(pred_label.shape, gold_label.shape)\n",
    "    loss = criterion(pred_label, gold_label)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "597c2155-a73f-465e-8c93-15ea5f776063",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.',\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "319ce3e4-21dc-4b5c-affb-8776ef628307",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_model.to(config.general.device)\n",
    "linear_model(encoded_inputs).last_hidden_state[:, 0, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6bafc675-cfb9-4ced-8afa-df849b509ac0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 25000\n",
       "})"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_datasets['imdb']['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772d50fd-fb47-4e94-aa29-7136ec59e61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cls_embeddings_for_dataset(dataset_name, dataset, \n",
    "                                   feature_names, model, tokenizer, \n",
    "                                   pbar_func=tqdm_pbar, device=device, CUT_SIZE=CUT_SIZE):\n",
    "    collected_embeddings = defaultdict(list)\n",
    "\n",
    "    for split, data in dataset.items():\n",
    "        \n",
    "        pbar = pbar_func(data, f\"{split} {dataset_name}\") if pbar_func is not None else data\n",
    "        for example in pbar:\n",
    "            # Encode the input sentences\n",
    "            if len(feature_names) == 2:\n",
    "                encoded_inputs = tokenizer.encode(\n",
    "                    example[feature_names[0]], \n",
    "                    example[feature_names[1]], \n",
    "                    truncation=True, \n",
    "                    return_tensors='pt'\n",
    "                )\n",
    "            else:\n",
    "                encoded_inputs = tokenizer.encode(*list(map(lambda x: example[x] , feature_names)), \n",
    "                                              truncation=True, \n",
    "                                              return_tensors='pt')\n",
    "            \n",
    "            special_token_positions = (encoded_inputs[0] < 103).nonzero().squeeze()\n",
    "            encoded_inputs = encoded_inputs.to(device)\n",
    "\n",
    "            # print(f\"Encoded inputs: {encoded_inputs}\")\n",
    "            # print(f\"Special token positions: {special_token_positions}\")\n",
    "            \n",
    "            # Forward pass through the model\n",
    "            with torch.no_grad():\n",
    "                outputs = model(encoded_inputs)\n",
    "            # print(f'Outputs: {outputs}')\n",
    "\n",
    "            # Get the embedding of the [CLS] token\n",
    "            cls_embedding = outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "            # Append the [CLS] embedding to the list\n",
    "            collected_embeddings[split].append(cls_embedding)\n",
    "         \n",
    "    return collected_embeddings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
