{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "132681fa-7b1f-4ff5-a55e-9c90353188a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/shapkin/effective-inference\n"
     ]
    }
   ],
   "source": [
    "%cd ../.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85b67a0-fe46-4104-a66e-bf8bdabb756c",
   "metadata": {},
   "source": [
    "## Import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "570ebef6-a677-45ff-8ee9-69f8ee0747fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\"\n",
    "\n",
    "import yaml\n",
    "import h5py\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from utils.dataset_cache import cache_embeddings, get_dataset_for_regression, build_dataset_from_cached, load_cached_dataset\n",
    "from utils.dataset_cache import build_dict_dataset_from_cached\n",
    "from utils.prepare_dataset import load_datasets, cut_datasets\n",
    "from utils.config import ConfigWrapper\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from typing import Tuple, List, Dict, Optional, Union\n",
    "from numpy.random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00e31b89-adfc-4ecf-90b0-b0fa10375fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from copy import deepcopy\n",
    "\n",
    "def hidden_to_heads(x, config):\n",
    "    num_attention_heads = config.attention_config.num_heads\n",
    "    attention_head_size =  config.attention_config.d_model // config.attention_config.num_heads\n",
    "    new_x_shape = x.size()[:-1] + (num_attention_heads, attention_head_size)\n",
    "    x = x.view(new_x_shape)\n",
    "    return x\n",
    "\n",
    "class LinearAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(LinearAttention, self).__init__()\n",
    "        self.config = config\n",
    "        self.features = config['features']\n",
    "        self.device = config['device']\n",
    "        self.batch_size = config['batch_size']\n",
    "\n",
    "        self.dim_size = config['d_model']\n",
    "        if config.split_heads:\n",
    "            self.dim_size = config['d_model'] // config['num_heads']\n",
    "                        \n",
    "        for k in self.features:                \n",
    "            if 'hidden' in k:\n",
    "                learnable_parameters = f'torch.nn.Linear(in_features={self.dim_size}, out_features=1)'\n",
    "                exec(f\"self.{k} = {learnable_parameters}\")\n",
    "            else:\n",
    "                learnable_parameters = f'nn.Parameter(torch.randn(1), requires_grad=True)'\n",
    "                exec(f\"self.{k} = {learnable_parameters}\")\n",
    "                    \n",
    "            \n",
    "    def forward(self, seq_len=None, **kwargs):\n",
    "        if seq_len is not None:\n",
    "            result = torch.zeros((self.batch_size, seq_len, seq_len), device=self.device)\n",
    "            \n",
    "            for arg_name, arg_value in kwargs.items():\n",
    "                namespace = {'cur_result': None, 'self': self, 'arg_name': arg_name, 'arg_value': arg_value}\n",
    "                if 'hidden' in arg_name:\n",
    "                    exec(f\"cur_result = self.{arg_name}(arg_value)\", namespace)\n",
    "                else:\n",
    "                    exec(f\"cur_result = self.{arg_name} * arg_value\", namespace)\n",
    "                if 'from' in arg_name:\n",
    "                    result += namespace['cur_result'].T\n",
    "                else:\n",
    "                    result += namespace['cur_result']\n",
    "        else:\n",
    "            result = torch.zeros((self.batch_size, 1), device=self.device)\n",
    "            \n",
    "            for arg_name, arg_value in kwargs.items():\n",
    "                namespace = {'cur_result': None, 'self': self, 'arg_name': arg_name, 'arg_value': arg_value}\n",
    "                if 'hidden' in arg_name:\n",
    "                    exec(f\"cur_result = self.{arg_name}(arg_value)\", namespace)\n",
    "                else:\n",
    "                    exec(f\"cur_result = self.{arg_name} * arg_value\", namespace)\n",
    "                if 'from' in arg_name:\n",
    "                    result += namespace['cur_result'].T\n",
    "                else:\n",
    "                    result += namespace['cur_result']\n",
    "            \n",
    "        return result\n",
    "\n",
    "\n",
    "from transformers.models.bert.modeling_bert import BertSelfAttention, BertModel, \\\n",
    "    BaseModelOutputWithPastAndCrossAttentions\n",
    "\n",
    "class LinearClassifierBertAttention(BertSelfAttention):\n",
    "    \"\"\"\n",
    "    Idea: attention weights are predicted by Linear Classifier\n",
    "    \"\"\"\n",
    "    def __init__(self, bert_config, config):\n",
    "        super(LinearClassifierBertAttention, self).__init__(bert_config)\n",
    "        self.config = config\n",
    "        self.linear_config = config.attention_config\n",
    "\n",
    "        if self.linear_config.split_heads:\n",
    "            for head_num in range(self.linear_config['num_heads']):\n",
    "                learnable_parameters = f'LinearAttention(self.config.attention_config)'\n",
    "                namespace = {'head_num': head_num, 'self': self, 'LinearAttention': LinearAttention}\n",
    "                exec(f\"self.linear_model_{head_num} = {learnable_parameters}\", namespace)\n",
    "        else:\n",
    "            self.linear_model = LinearAttention(config.attention_config)\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            hidden_states: torch.Tensor,\n",
    "            attention_mask: Optional[torch.FloatTensor] = None,\n",
    "            head_mask: Optional[torch.FloatTensor] = None,\n",
    "            encoder_hidden_states: Optional[torch.FloatTensor] = None,\n",
    "            encoder_attention_mask: Optional[torch.FloatTensor] = None,\n",
    "            past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n",
    "            output_attentions: Optional[bool] = False,\n",
    "            # special_tokens_idxs: Optional[List[int]] = [0]\n",
    "    ) -> Tuple[torch.Tensor]:\n",
    "        mixed_query_layer = self.query(hidden_states)\n",
    "\n",
    "        # If this is instantiated as a cross-attention module, the keys\n",
    "        # and values come from an encoder; the attention mask needs to be\n",
    "        # such that the encoder's padding tokens are not attended to.\n",
    "        is_cross_attention = encoder_hidden_states is not None\n",
    "\n",
    "        if is_cross_attention and past_key_value is not None:\n",
    "            # reuse k,v, cross_attentions\n",
    "            key_layer = past_key_value[0]\n",
    "            value_layer = past_key_value[1]\n",
    "            attention_mask = encoder_attention_mask\n",
    "            # special_tokens_idxs = (encoder_hidden_states[0] < 103).nonzero().squeeze()\n",
    "        elif is_cross_attention:\n",
    "            key_layer = self.transpose_for_scores(self.key(encoder_hidden_states))\n",
    "            value_layer = self.transpose_for_scores(self.value(encoder_hidden_states))\n",
    "            attention_mask = encoder_attention_mask\n",
    "            # special_tokens_idxs = (encoder_hidden_states[0] < 103).nonzero().squeeze()\n",
    "        elif past_key_value is not None:\n",
    "            key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
    "            value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
    "            key_layer = torch.cat([past_key_value[0], key_layer], dim=2)\n",
    "            value_layer = torch.cat([past_key_value[1], value_layer], dim=2)\n",
    "        else:\n",
    "            key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
    "            value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
    "\n",
    "        if self.config.attention_config.split_heads:\n",
    "            hidden_states = hidden_to_heads(hidden_states, self.config)\n",
    "\n",
    "            attentions = []\n",
    "            for head_num in range(self.linear_config['num_heads']):\n",
    "                seq_len = hidden_states.shape[1]\n",
    "                positions = torch.arange(seq_len).view(-1, 1)\n",
    "                \n",
    "                full_data_to_linear = {\n",
    "                    'hidden_from': hidden_states[:, :, head_num, :], \n",
    "                    'hidden_to': hidden_states[:, :, head_num, :], \n",
    "                    'pos_from': positions,\n",
    "                    'pos_to': positions,\n",
    "                    'relev_pos_from': seq_len - positions,\n",
    "                    'relev_pos_to': seq_len - positions,\n",
    "                    'inv_pos_from': (positions / seq_len),\n",
    "                    'inv_pos_to': (positions / seq_len),\n",
    "                    'inv_relev_pos_from': ((seq_len - positions) / seq_len),\n",
    "                    'inv_relev_pos_to': ((seq_len - positions) / seq_len),\n",
    "                    'seq_len': seq_len, \n",
    "                    'inv_seq_len': (1 / seq_len)\n",
    "                }\n",
    "                \n",
    "                data_to_linear = {k:full_data_to_linear[k].to(self.linear_config['device']) for k in self.linear_config['features'] if k != 'num_heads'}\n",
    "\n",
    "                namespace = {'predicted_attention': None, 'self': self, 'data_to_linear': data_to_linear, 'seq_len': seq_len}\n",
    "                exec(f\"predicted_attention = self.linear_model_{head_num}(seq_len, **data_to_linear)\", namespace)\n",
    "                # print(namespace['predicted_attention'])\n",
    "                attention_probs = nn.Sigmoid()(torch.exp(namespace['predicted_attention']))  # torch.nn.functional.softmax(predicted_attention, dim=-1)\n",
    "                attentions.append(attention_probs)\n",
    "\n",
    "            attention_probs = torch.stack(attentions, dim=1)\n",
    "            context_layer = torch.matmul(attention_probs, value_layer)\n",
    "    \n",
    "            context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "            new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
    "            context_layer = context_layer.view(new_context_layer_shape)\n",
    "    \n",
    "            outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n",
    "    \n",
    "            if self.is_decoder:\n",
    "                outputs = outputs + (past_key_value,)\n",
    "            return outputs\n",
    "\n",
    "        else:\n",
    "            seq_len = hidden_states.shape[1]\n",
    "            positions = torch.arange(seq_len).view(-1, 1)\n",
    "            full_data_to_linear = {\n",
    "                'hidden_from': hidden_states, \n",
    "                'hidden_to': hidden_states, \n",
    "                'pos_from': positions,\n",
    "                'pos_to': positions,\n",
    "                'relev_pos_from': seq_len - positions,\n",
    "                'relev_pos_to': seq_len - positions,\n",
    "                'inv_pos_from': (positions / seq_len),\n",
    "                'inv_pos_to': (positions / seq_len),\n",
    "                'inv_relev_pos_from': ((seq_len - positions) / seq_len),\n",
    "                'inv_relev_pos_to': ((seq_len - positions) / seq_len),\n",
    "                'seq_len': seq_len, \n",
    "                'inv_seq_len': (1 / seq_len),\n",
    "            }\n",
    "            if 'head_num' in self.linear_config['features']:\n",
    "                attentions = []\n",
    "                for head_num in range(self.linear_config['num_heads']):\n",
    "                    full_data_to_linear['head_num'] = torch.tensor([head_num])\n",
    "\n",
    "                    data_to_linear = {k:full_data_to_linear[k].to(self.linear_config['device']) for k in self.linear_config['features']}\n",
    "                    predicted_attention= self.linear_model(seq_len, **data_to_linear)\n",
    "                \n",
    "                    attention_probs = nn.Sigmoid()(torch.exp(predicted_attention))  # torch.nn.functional.softmax(predicted_attention, dim=-1)\n",
    "                    attentions.append(attention_probs)\n",
    "\n",
    "                    attention_probs = torch.stack(attentions, dim=1)\n",
    "                    context_layer = torch.matmul(attention_probs, value_layer.squeeze(1))\n",
    "                    context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "                    new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
    "                    context_layer = context_layer.view(new_context_layer_shape)\n",
    "            \n",
    "                    outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n",
    "            \n",
    "                    if self.is_decoder:\n",
    "                        outputs = outputs + (past_key_value,)\n",
    "                    return outputs\n",
    "                \n",
    "            else:\n",
    "                data_to_linear = {k:full_data_to_linear[k].to(self.linear_config['device']) for k in self.linear_config['features']}\n",
    "                predicted_attention= self.linear_model(seq_len, **data_to_linear)\n",
    "            \n",
    "                attention_probs = nn.Sigmoid()(torch.exp(predicted_attention))  # torch.nn.functional.softmax(predicted_attention, dim=-1)\n",
    "                context_layer = torch.matmul(attention_probs, value_layer.squeeze(1))\n",
    "        \n",
    "                context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "                new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
    "                context_layer = context_layer.view(new_context_layer_shape)\n",
    "        \n",
    "                outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n",
    "        \n",
    "                if self.is_decoder:\n",
    "                    outputs = outputs + (past_key_value,)\n",
    "        return outputs\n",
    "\n",
    "class BertWrapperLin(nn.Module):\n",
    "    def __init__(self, model, new_attention_class, linear_config, layer_nums=None, window_size=2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.bert_model = deepcopy(model)\n",
    "        self.layer_nums = layer_nums\n",
    "\n",
    "        # Create a list of modules to modify\n",
    "        modules_to_modify = []\n",
    "        for i in range(len(self.bert_model.encoder.layer)):\n",
    "            if (layer_nums is not None and i in layer_nums) or (layer_nums is None):\n",
    "                mean_attention = new_attention_class(self.bert_model.config, linear_config) # self.bert_model.config, \n",
    "                #mean_attention.set_window_size(window_size)\n",
    "                mean_attention.load_state_dict(self.bert_model.encoder.layer[i].attention.self.state_dict(), strict=False)\n",
    "\n",
    "                self.bert_model.encoder.layer[i].attention.self = mean_attention\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        return self.bert_model(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1d904b-77f3-4850-85e9-2a32cc667fc4",
   "metadata": {},
   "source": [
    "## Project configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e04b0aa1-8b62-467c-b551-661a0dcb150e",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = 'config.yaml'\n",
    "\n",
    "with open(config_path, \"r\") as f:\n",
    "    config = ConfigWrapper(yaml.load(f, Loader=yaml.FullLoader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b42217fc-f58a-4a04-9280-52e5cfffdb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(config.model.model_name, max_length=config.general.max_len)\n",
    "initial_model = AutoModel.from_pretrained(config.model.model_name).to(config.general.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f751ad-9be8-4158-9b00-968a3432917f",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5d09564-dff2-46c5-8a75-b554de6d78e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, y_train, X_test, y_test = load_cached_dataset(config, layer=0)\n",
    "#X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8fff4dd3-bb18-4050-b1c5-b5fc8d7fd4b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'imdb': DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['text', 'label'],\n",
       "         num_rows: 25000\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['text', 'label'],\n",
       "         num_rows: 25000\n",
       "     })\n",
       "     unsupervised: Dataset({\n",
       "         features: ['text', 'label'],\n",
       "         num_rows: 50000\n",
       "     })\n",
       " })}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_datasets = load_datasets(config.data.train_datasets, config.data.cut_size)\n",
    "train_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35c16802-4311-4483-addb-83224ef4bbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = build_dict_dataset_from_cached(config, train_datasets, layer=0, heads=[0], \n",
    "                                                                  features=config.attention_config.features, \n",
    "                                                                  split_hidden=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45e6437b-323c-4d91-8046-e1c64606f2e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.15553807, 0.21780878)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]['hidden_to'][0], X_train[1000]['hidden_to'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18640995-f73f-42eb-b3dc-1e9333db0d27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]['hidden_to'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c72eb1f1-d2c2-4c36-bb2a-deb03ce77336",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1606, 85)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train), len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53249470-2473-4bf5-8c6b-ec84e821ef6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hidden_to': array([ 0.15553807,  0.14143425,  0.13302758,  0.31399602, -0.65938884,\n",
       "        -1.0736477 ,  0.6093533 ,  0.13589564,  1.3214531 ,  0.667912  ,\n",
       "         0.17945454,  0.4693082 ,  0.30648527, -0.56361777, -0.20076695,\n",
       "         0.9098844 ,  0.59665406,  0.5579244 , -1.4222709 ,  0.18325962,\n",
       "        -1.105002  ,  1.2219864 , -0.40134352, -0.9862202 , -0.27676457,\n",
       "        -0.7115183 , -0.48043513, -0.41555184,  0.5822979 , -0.8090197 ,\n",
       "         0.809246  , -0.16602594, -0.07341174,  0.117907  , -0.9402012 ,\n",
       "         0.02974713,  0.22651967, -0.3060124 ,  0.05891408,  0.43850982,\n",
       "         0.10242728,  0.91733706,  0.64147025, -0.3555956 , -1.7645459 ,\n",
       "        -0.2814301 , -0.00459639,  0.25998768, -1.391335  ,  0.47268033,\n",
       "         0.24658613, -0.22695369, -0.24742895, -0.01804959,  0.80036235,\n",
       "        -1.1185328 ,  0.7725253 ,  0.06924633,  0.02469535, -0.3253744 ,\n",
       "         0.24401177, -0.05173851, -0.20923023,  0.3738086 ], dtype=float32),\n",
       " 'hidden_from': array([-0.5322135 ,  0.8565724 ,  0.28411248,  0.3839768 , -0.38932878,\n",
       "         0.06061743,  0.532775  ,  0.24783337,  0.7117512 ,  0.64445454,\n",
       "         0.2871134 ,  0.1809729 ,  0.46275878,  0.14287014,  0.1733651 ,\n",
       "         0.52665913, -0.27349785,  0.35000074,  0.02611221,  0.5630992 ,\n",
       "         0.7152667 ,  0.3269152 ,  0.49727026,  0.24026959, -0.03571624,\n",
       "         0.93480307, -0.13262245,  0.00744497, -0.82679754, -0.00889084,\n",
       "         0.2401894 , -0.68283576, -0.23470257, -0.59558636,  0.18417245,\n",
       "         0.35697263, -0.17601633,  0.37470755,  0.6255137 , -0.11884202,\n",
       "         0.2315966 ,  0.45983014,  0.8661316 ,  0.18557653, -0.41406643,\n",
       "        -0.5566996 , -0.619365  , -1.0416942 , -0.6579131 ,  0.05833613,\n",
       "         0.49939355, -0.31823692,  0.23418532, -0.3759385 , -0.7754654 ,\n",
       "         0.2667748 ,  0.03216245, -0.3828705 , -0.91286016,  0.00417323,\n",
       "        -0.23364432,  0.14542806, -0.35714105, -0.4782217 ], dtype=float32),\n",
       " 'pos_to': 0.0966796875,\n",
       " 'pos_from': 0.232421875,\n",
       " 'head_num': 0}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ab727266-e7a2-4b57-8a46-05f5b1d4dfa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dict_batch(samples_arr, device):\n",
    "    sample = samples_arr[0]\n",
    "    final = {}\n",
    "    for k, _ in sample.items():\n",
    "        batched_feature = []\n",
    "        for el in samples_arr:\n",
    "            if 'hidden' in k:\n",
    "                batched_feature.append(torch.tensor(el[k]))\n",
    "            else:\n",
    "                batched_feature.append(el[k])\n",
    "\n",
    "        if 'hidden' in k:\n",
    "            batched_feature = torch.stack(batched_feature)\n",
    "        else:\n",
    "            batched_feature = torch.tensor(batched_feature)\n",
    "        final[k] = batched_feature.to(device)\n",
    "    return final\n",
    "    \n",
    "def prepare_batches(dataset, n, device):\n",
    "    shuffle_idx = np.arange(len(dataset))\n",
    "    np.random.shuffle(shuffle_idx)\n",
    "    new_dataset = dataset[shuffle_idx]\n",
    "    print(new_dataset)\n",
    "    # looping till length l\n",
    "    for i in range(0, len(new_dataset), n): \n",
    "        yield get_dict_batch[i:i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "746ce020-0d3e-420e-b606-e1792d2faf30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'hidden_to': array([-0.19079687,  0.21273507,  0.66632915, -0.44872472, -0.48511204,\n",
      "        -0.17139994,  0.33636194, -0.01300793, -0.33155817, -0.35787386,\n",
      "         0.1725571 , -0.14330178, -0.61597705,  0.12558405,  0.9203594 ,\n",
      "        -0.25555786, -0.7375698 , -1.3028327 ,  0.59556675,  1.0017891 ,\n",
      "         0.04428435,  0.22370818, -0.09685135,  0.6178254 , -0.425674  ,\n",
      "        -0.5404954 ,  0.50118387,  0.07698976, -0.42920288,  0.7901867 ,\n",
      "         0.52212757,  0.09048381,  0.75964904, -0.60198724,  0.26504874,\n",
      "         0.6656127 , -1.7834715 , -0.78274894,  0.12213472,  0.866579  ,\n",
      "         0.36710104, -0.6980736 ,  0.33897507,  0.06735259,  0.11314931,\n",
      "         0.7412936 ,  0.08072751,  0.34569353, -0.91472894, -0.19712013,\n",
      "         1.1396126 ,  1.4449006 , -0.19016334,  0.26347688, -0.48608357,\n",
      "        -0.40321687, -0.03245781,  0.11642367, -1.5451432 , -0.10034049,\n",
      "         0.8672733 , -0.55858016,  0.5449989 ,  0.30212072], dtype=float32), 'hidden_from': array([ 0.05000687,  0.466558  , -0.02488631, -0.44414666, -0.311005  ,\n",
      "        -0.12576465, -0.38957244,  0.30188477, -0.96244067, -0.14339618,\n",
      "         0.27979338,  0.5360039 ,  0.29512748, -0.40407288,  0.59662664,\n",
      "         0.36375406,  0.63238204, -0.5139759 , -0.5870329 ,  0.5000346 ,\n",
      "         0.5426012 , -0.08431395,  0.30961835,  0.3519586 ,  0.25254658,\n",
      "         0.86560214, -0.25716534,  0.4080539 , -0.0010904 ,  0.24006985,\n",
      "         0.5649901 ,  0.7889419 , -0.66506076,  0.05263292,  0.15027906,\n",
      "         0.9709529 ,  0.34240517, -0.10104315,  0.5488711 , -0.95045686,\n",
      "         0.4905697 , -0.14927135,  0.753386  , -0.0318217 , -0.04187654,\n",
      "        -0.55329937,  0.41928753, -0.23513731, -0.22833912, -0.2824372 ,\n",
      "        -0.5078898 , -0.34775364,  0.00664535, -0.5954281 ,  0.35636863,\n",
      "        -0.14320001,  0.01199923,  0.11114354,  0.31990907, -0.52359015,\n",
      "        -0.34470868,  0.31169134, -0.29154348, -0.01932583], dtype=float32), 'pos_to': 0.3515625, 'pos_from': 0.158203125, 'head_num': 0}\n",
      " {'hidden_to': array([-1.056965  , -0.71556515, -0.11759496,  0.7982177 ,  0.6012182 ,\n",
      "        -0.7628411 ,  0.06920178,  0.47998637,  0.30601668,  0.3234555 ,\n",
      "         0.3052509 , -0.4344576 , -0.668523  ,  0.79088414,  0.23299488,\n",
      "        -0.7533491 , -0.6849802 , -0.8904295 ,  2.0496554 , -0.40218624,\n",
      "        -0.2570612 ,  0.25608918,  0.35376734, -1.0736545 , -0.92873704,\n",
      "         0.07093777,  0.7642192 ,  0.30079493,  0.47002664, -0.7598542 ,\n",
      "         1.2085422 ,  0.08627658, -0.11961322,  0.682744  ,  1.3598411 ,\n",
      "         0.8267941 ,  0.26224333, -0.99132454, -0.2097639 ,  1.076603  ,\n",
      "         0.4923095 ,  0.18909088, -0.91003835,  0.3708344 , -0.39852268,\n",
      "         0.9543657 , -0.12329473,  0.16980308,  0.3084355 ,  0.8324258 ,\n",
      "         0.2000454 ,  0.5526455 ,  1.0541283 ,  0.49517947, -1.4758368 ,\n",
      "         0.44784433,  0.79463434, -0.08447438,  1.2634971 ,  1.2039163 ,\n",
      "        -0.8009801 ,  0.25081918,  0.12872306,  0.39014593], dtype=float32), 'hidden_from': array([ 0.19381371,  0.6654802 ,  0.8130585 , -1.5312039 ,  0.25460985,\n",
      "        -1.14767   ,  0.73862034, -0.12056735, -0.7238065 ,  0.67296726,\n",
      "        -0.10819168,  0.6390895 ,  0.07746933,  0.39305565, -0.16721173,\n",
      "        -0.09541729, -0.30291304,  0.2715665 ,  0.63938856,  0.14368466,\n",
      "         0.6002556 , -0.23027264,  0.81666785,  0.47997883,  0.37445113,\n",
      "         0.6810208 , -0.4345504 ,  0.73094964,  0.7609114 ,  0.35798463,\n",
      "         0.06510746,  1.0390377 , -0.810122  , -0.28007343,  0.73997253,\n",
      "         0.8193202 ,  0.43584254, -0.02138241,  0.32786354, -0.52220154,\n",
      "        -0.10926797, -0.41592583, -0.11798636,  0.43041873,  0.21854189,\n",
      "         0.26747045, -0.5779895 ,  0.5227738 , -0.4575682 , -0.3400692 ,\n",
      "        -0.8987608 ,  0.8582148 , -0.15868609, -0.06913534,  0.22324762,\n",
      "        -0.10783825,  0.99290514, -1.0778519 ,  0.35571703,  0.7186035 ,\n",
      "        -0.7426754 , -0.3744491 , -0.04484463,  0.30478606], dtype=float32), 'pos_to': 0.4453125, 'pos_from': 0.49609375, 'head_num': 0}\n",
      " {'hidden_to': array([ 1.2001535 ,  1.3954307 , -0.50500333,  0.677049  ,  1.1814715 ,\n",
      "         0.15347545,  0.30334154,  0.8009693 , -0.13980967,  0.7603393 ,\n",
      "        -0.6805985 ,  0.41047075, -0.62534   ,  1.14318   , -0.14726326,\n",
      "         0.16395295,  0.10083044,  0.24397843, -0.07945847,  0.48257726,\n",
      "        -0.22922316,  0.77243376, -1.0838506 ,  0.20963307, -0.46245208,\n",
      "         0.7564323 , -0.69844323,  0.31294325,  0.3320424 , -0.2975428 ,\n",
      "         0.7758205 , -1.0939767 , -1.1614327 ,  0.41503924,  0.29500654,\n",
      "        -0.5414553 , -1.7103965 ,  0.3123986 , -0.05376302,  0.04883566,\n",
      "         0.6682237 ,  0.323748  ,  0.2788215 , -0.05878614,  0.18170634,\n",
      "         0.78199816,  0.5296252 ,  0.3294736 , -0.23883247,  0.34121168,\n",
      "        -0.22233608, -0.33745316,  0.17764229, -0.27355358,  1.0176587 ,\n",
      "        -1.2125574 , -0.3594871 , -0.03561619, -0.20523447, -0.11691857,\n",
      "         0.29221156, -0.4670584 , -0.948477  ,  0.07611936], dtype=float32), 'hidden_from': array([-0.89609516,  0.48650792,  0.41888884,  0.44275615, -0.4236102 ,\n",
      "         0.1225149 ,  0.7042111 , -0.08919352,  0.4351954 ,  0.6216182 ,\n",
      "        -0.01629549,  0.19282041,  0.9137797 , -0.2317752 , -0.20359468,\n",
      "         0.29100993,  0.38082317,  0.01543657, -0.05813825,  0.37412202,\n",
      "         0.7630463 ,  0.47022444,  0.140041  ,  0.22555572,  0.53083354,\n",
      "         0.36377698,  0.05533246,  0.61038876, -1.0783017 , -0.6081386 ,\n",
      "         0.16681674, -0.56510556, -0.23432942, -0.3548375 ,  0.14872606,\n",
      "         0.46533442,  0.35226098, -0.3178431 , -0.06989575, -0.15422757,\n",
      "         0.25423986,  0.3846752 ,  0.733893  ,  0.2922925 , -0.02088888,\n",
      "        -0.51889575, -0.5093731 , -0.5681525 , -0.4496778 ,  0.06213526,\n",
      "         0.37417856, -0.28138408, -0.10434206, -0.18907766, -0.5509321 ,\n",
      "         0.24385926, -0.35565796, -0.5234784 , -0.74474376,  0.5330924 ,\n",
      "        -0.19929844, -0.07959928,  0.07745946, -0.21158877], dtype=float32), 'pos_to': 0.126953125, 'pos_from': 0.0654296875, 'head_num': 0}\n",
      " ...\n",
      " {'hidden_to': array([ 0.20649634, -0.02547991,  0.39906213,  0.881625  ,  0.94530797,\n",
      "         0.24440047, -0.01538364, -0.566143  ,  0.4252586 ,  0.47536728,\n",
      "         0.8404201 ,  0.7863758 ,  0.31566697, -0.5625304 , -0.3025793 ,\n",
      "        -0.8531553 ,  0.25386477, -0.3228571 , -0.88805026, -0.00283296,\n",
      "         0.9421345 ,  1.0698494 ,  1.1704005 ,  0.33664063,  0.05431643,\n",
      "        -0.5643565 , -0.09347213, -0.8130215 , -0.35268906,  0.9019099 ,\n",
      "        -1.0490372 , -0.09756437, -0.69730437, -0.54564065, -0.13112181,\n",
      "         0.33314407,  0.60384214, -0.7891498 , -0.8549901 ,  0.08344328,\n",
      "        -0.13836215,  0.04254622, -0.87826675, -0.7075136 ,  0.81521404,\n",
      "         0.17203288,  0.07518027,  0.24734211, -0.37115395,  0.18509664,\n",
      "         0.8848052 , -0.44736648,  0.830663  , -1.0692947 , -0.06985716,\n",
      "         0.17600863, -0.17079239, -0.7864058 , -0.61254984, -0.5172318 ,\n",
      "         0.5933496 , -0.25308394, -1.054499  , -0.26786724], dtype=float32), 'hidden_from': array([ 0.14152816,  0.35120875,  0.12202939, -0.08823261, -0.21752164,\n",
      "         0.08608852, -0.26937133, -0.13464245, -0.4031201 ,  0.5347568 ,\n",
      "        -0.01428042,  0.19377178,  0.05816133, -0.5712048 ,  0.56305283,\n",
      "         0.52137417,  0.3454735 , -0.17055625, -0.6026084 ,  0.27919868,\n",
      "         0.37215525, -0.06567872,  0.2014219 ,  0.2619549 ,  0.4182366 ,\n",
      "         0.38666806, -0.5670954 ,  0.3066396 ,  0.17634512,  0.05862726,\n",
      "         0.4206552 ,  0.6739597 , -0.32601643,  0.3141437 ,  0.0512748 ,\n",
      "         0.68144476,  0.3709035 , -0.11938582,  0.320987  , -0.5863744 ,\n",
      "         0.51351964,  0.13845529,  0.74352735,  0.19814314, -0.2686177 ,\n",
      "        -0.60216135,  0.29452705,  0.05525273, -0.20803846, -0.5620487 ,\n",
      "        -0.62380385, -0.53852993,  0.12759411,  0.3519657 ,  0.35516462,\n",
      "         0.3509101 ,  0.2495904 ,  0.01821321,  0.32450265, -0.651931  ,\n",
      "         0.04122728,  0.33882353, -0.59889853,  0.73744446], dtype=float32), 'pos_to': 0.03515625, 'pos_from': 0.0673828125, 'head_num': 0}\n",
      " {'hidden_to': array([-0.34146824,  0.43082613,  0.09343675,  0.07328703, -0.6851362 ,\n",
      "        -0.0193657 ,  0.4315895 , -0.5869683 , -0.73487854,  0.3668635 ,\n",
      "         0.5770648 ,  0.41069138, -0.26077634, -0.22429106, -0.24409153,\n",
      "         0.08532086, -0.0344651 ,  0.37208644, -0.03405664,  0.41437966,\n",
      "        -0.23645969,  0.40108964,  0.4231096 , -0.13546398,  0.30923304,\n",
      "         0.26686114,  0.1821141 ,  0.56227034, -0.21713851, -0.51338464,\n",
      "        -0.44389635,  0.05399376,  0.24671565,  0.12798826,  0.23408237,\n",
      "         0.6385399 ,  0.55766225,  0.01146413,  0.39623877, -0.47536755,\n",
      "         0.11014114,  0.35163182,  0.44955617,  0.1538578 , -0.26300454,\n",
      "        -0.7273104 ,  0.53592557,  0.2969221 ,  0.03606357,  0.5213194 ,\n",
      "        -0.51905257, -0.39869225,  0.4273338 , -0.43020645,  0.09186403,\n",
      "         0.11437482,  0.16874468, -0.6179777 ,  0.33972988, -0.40585104,\n",
      "        -0.47787637, -0.09165957, -0.10299643,  0.4825093 ], dtype=float32), 'hidden_from': array([-1.32270694e+00,  6.11865938e-01, -5.82589209e-01,  1.01570666e+00,\n",
      "         1.30365360e+00,  4.54852164e-01,  1.78471744e+00, -8.06032002e-01,\n",
      "         1.78957462e+00,  1.01832420e-01,  2.90411144e-01, -1.94129720e-01,\n",
      "        -3.74831676e-01, -1.73147574e-01,  1.09627545e+00, -3.80375236e-02,\n",
      "        -1.27883530e+00,  1.15809873e-01, -4.32016879e-01, -2.24990770e-01,\n",
      "        -5.95170081e-01, -1.27112135e-01, -4.09031600e-01,  3.88512641e-01,\n",
      "         5.91590285e-01,  3.68340462e-01, -2.11763337e-01, -6.42616212e-01,\n",
      "        -6.02269351e-01, -1.55423388e-01,  6.24231063e-02, -9.54849422e-01,\n",
      "         9.15617406e-01, -1.11924686e-01,  5.35526097e-01, -1.06368828e+00,\n",
      "         1.63113877e-01, -1.38801694e+00,  1.39364405e-04,  1.06345367e+00,\n",
      "         7.40298748e-01, -1.10581076e+00, -2.72613764e-01, -7.80828059e-01,\n",
      "         2.83942431e-01, -1.01979375e+00,  6.15291595e-01, -1.09769237e+00,\n",
      "        -5.03929332e-02,  3.78939301e-01,  5.69380343e-01, -1.59338236e-01,\n",
      "         4.36815321e-01,  1.13887966e-01, -5.61603129e-01, -3.81520867e-01,\n",
      "         3.13292652e-01,  1.27509668e-01, -2.08431661e-01,  1.18966281e-01,\n",
      "        -8.78876388e-01,  4.11355682e-02,  4.74148579e-02, -9.76349771e-01],\n",
      "       dtype=float32), 'pos_to': 0.02734375, 'pos_from': 0.138671875, 'head_num': 0}\n",
      " {'hidden_to': array([ 1.13176119e+00,  1.11556637e+00,  2.06845393e-03,  3.24286073e-01,\n",
      "        -3.31515610e-01,  6.93988428e-02, -2.71913737e-01,  1.58452702e+00,\n",
      "        -1.45463303e-01,  2.37657741e-01,  1.84927970e-01,  5.99988461e-01,\n",
      "         1.89862624e-01, -9.97949019e-02,  4.59922045e-01,  5.23134209e-02,\n",
      "         1.19146180e+00, -7.76914589e-04,  9.07983072e-03,  1.22026742e+00,\n",
      "         5.69271564e-01,  3.38533103e-01,  1.17229831e+00,  1.62465379e-01,\n",
      "         3.50964695e-01, -4.66730654e-01, -3.24793190e-01, -7.03022122e-01,\n",
      "         6.83689892e-01,  1.04877794e+00,  5.14779747e-01,  7.61256099e-01,\n",
      "        -5.75202823e-01, -7.74275243e-01,  1.04061639e+00,  4.08804387e-01,\n",
      "         1.80929333e-01, -1.02646244e+00, -2.12047845e-01,  8.87931585e-01,\n",
      "         5.22399664e-01, -3.36512715e-01,  5.11756539e-01, -1.31688556e-02,\n",
      "         4.07172233e-01,  7.43538499e-01, -6.48099303e-01, -7.39300668e-01,\n",
      "         5.47783673e-02,  1.28684616e+00,  1.03825845e-01,  3.10500175e-01,\n",
      "         8.46149325e-02, -5.13130426e-01,  5.09411871e-01,  9.28762779e-02,\n",
      "        -6.54172003e-01, -2.93666065e-01,  3.94389391e-01, -4.91608202e-01,\n",
      "         1.91744715e-01, -1.05418587e+00,  2.38741864e-03,  2.39547774e-01],\n",
      "       dtype=float32), 'hidden_from': array([ 0.30467322,  0.12666667, -0.01804085,  0.8033308 , -1.3301088 ,\n",
      "         0.10864317, -1.2488923 , -0.8266216 ,  0.26416442,  1.3892676 ,\n",
      "        -0.38012153,  0.36763325,  0.8600688 , -0.13674757, -0.02147089,\n",
      "        -0.21012333, -0.378351  , -1.3873435 ,  0.04983543,  0.24290381,\n",
      "        -0.1760284 , -0.12457177,  1.3138529 ,  0.3975196 , -1.3029094 ,\n",
      "         0.44300365, -0.6345751 , -0.52427953,  0.9945899 ,  0.11222703,\n",
      "         0.5089188 , -0.09505855,  0.18143529, -0.5071747 ,  0.340041  ,\n",
      "        -0.8195124 , -0.4163663 ,  0.7099951 , -0.11550198,  1.0098661 ,\n",
      "         1.3355708 , -0.05623038,  0.77072227, -0.23705502, -0.5421427 ,\n",
      "        -0.53874904, -1.3350685 , -0.16056655,  0.10905368,  0.5408069 ,\n",
      "        -0.35825047,  0.2580283 , -0.2116674 ,  0.17805283, -0.0911695 ,\n",
      "        -0.21457702,  0.2127763 ,  0.5132331 , -0.76297796, -0.11441457,\n",
      "        -0.21565883,  0.06453809,  0.13283274, -1.4901736 ], dtype=float32), 'pos_to': 0.01953125, 'pos_from': 0.439453125, 'head_num': 0}]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'function' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m a \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprepare_batches\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[29], line 26\u001b[0m, in \u001b[0;36mprepare_batches\u001b[0;34m(dataset, n, device)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# looping till length l\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(new_dataset), n): \n\u001b[0;32m---> 26\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[43mget_dict_batch\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'function' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "a = next(iter(prepare_batches(X_train, 5, 'cuda')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "791af312-3e2b-4aa0-8d74-f0600cb2bb75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hidden_to': tensor([[ 0.1555,  0.1414,  0.1330,  0.3140, -0.6594, -1.0736,  0.6094,  0.1359,\n",
       "           1.3215,  0.6679,  0.1795,  0.4693,  0.3065, -0.5636, -0.2008,  0.9099,\n",
       "           0.5967,  0.5579, -1.4223,  0.1833, -1.1050,  1.2220, -0.4013, -0.9862,\n",
       "          -0.2768, -0.7115, -0.4804, -0.4156,  0.5823, -0.8090,  0.8092, -0.1660,\n",
       "          -0.0734,  0.1179, -0.9402,  0.0297,  0.2265, -0.3060,  0.0589,  0.4385,\n",
       "           0.1024,  0.9173,  0.6415, -0.3556, -1.7645, -0.2814, -0.0046,  0.2600,\n",
       "          -1.3913,  0.4727,  0.2466, -0.2270, -0.2474, -0.0180,  0.8004, -1.1185,\n",
       "           0.7725,  0.0692,  0.0247, -0.3254,  0.2440, -0.0517, -0.2092,  0.3738]],\n",
       "        device='cuda:0'),\n",
       " 'hidden_from': tensor([[-0.5322,  0.8566,  0.2841,  0.3840, -0.3893,  0.0606,  0.5328,  0.2478,\n",
       "           0.7118,  0.6445,  0.2871,  0.1810,  0.4628,  0.1429,  0.1734,  0.5267,\n",
       "          -0.2735,  0.3500,  0.0261,  0.5631,  0.7153,  0.3269,  0.4973,  0.2403,\n",
       "          -0.0357,  0.9348, -0.1326,  0.0074, -0.8268, -0.0089,  0.2402, -0.6828,\n",
       "          -0.2347, -0.5956,  0.1842,  0.3570, -0.1760,  0.3747,  0.6255, -0.1188,\n",
       "           0.2316,  0.4598,  0.8661,  0.1856, -0.4141, -0.5567, -0.6194, -1.0417,\n",
       "          -0.6579,  0.0583,  0.4994, -0.3182,  0.2342, -0.3759, -0.7755,  0.2668,\n",
       "           0.0322, -0.3829, -0.9129,  0.0042, -0.2336,  0.1454, -0.3571, -0.4782]],\n",
       "        device='cuda:0'),\n",
       " 'pos_to': tensor([0.0967], device='cuda:0', dtype=torch.float64),\n",
       " 'pos_from': tensor([0.2324], device='cuda:0', dtype=torch.float64),\n",
       " 'head_num': tensor([0], device='cuda:0')}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_dict_batch(X_train[:1], 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a0c7f115-3339-458b-af0d-211d996adb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_model = LinearAttention(config.attention_config).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e9d8b52f-615e-4dd2-bc39-daff3d705498",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2485]], device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_model(**get_dict_batch(X_train[:1], 'cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64855f31-0f67-40e6-a9c3-4b7216de7a41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6d5b96-11e0-4414-986d-cec39a668306",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.stack([torch.tensor([1, 2])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff91b37-ff8f-45b7-b933-ac05cffea51c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebe9283-a627-4f69-97ed-9cd0e67b1ac7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a02ef8b7-a410-4c43-957c-09ca9c3f8578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['geeks', 'for', 'geeks', 'like', 'geeky'], ['nerdy', 'geek', 'love', 'questions', 'words'], ['life']]\n"
     ]
    }
   ],
   "source": [
    "my_list = ['geeks', 'for', 'geeks', 'like',\n",
    "           'geeky','nerdy', 'geek', 'love',\n",
    "               'questions','words', 'life']\n",
    "  \n",
    "# Yield successive n-sized\n",
    "# chunks from l.\n",
    "def divide_chunks(l, n):\n",
    "      \n",
    "    # looping till length l\n",
    "    for i in range(0, len(l), n): \n",
    "        yield l[i:i + n]\n",
    "  \n",
    "# How many elements each\n",
    "# list should have\n",
    "n = 5\n",
    "  \n",
    "x = list(divide_chunks(my_list, n))\n",
    "print (x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822cbdbf-7cde-46e3-a528-45e8e4aa4f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = BertWrapperLin(initial_model, LinearClassifierBertAttention, config, layer_nums=[6, 7, 8, 9, 10, 11])\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425c4e78-b716-4a03-90c4-23882b3b02dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c32b50e-b4ae-4bc9-939f-3d5360cd5eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = BertWrapperLin(initial_model, LinearClassifierBertAttention, config, layer_nums=[6, 7, 8, 9, 10, 11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5786fc47-42be-4ec6-a9fe-c10280c5a7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddce93d-a19a-4dc1-9522-66d22a433496",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_inputs = tokenizer.encode(\n",
    "                                'Hello! My name is... Hi, my name is... Slim Shady',\n",
    "                                truncation=True,\n",
    "                                return_tensors='pt'\n",
    "                            ).to(config.general.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934e9d46-faea-4092-9edf-b2708807cd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = initial_model(encoded_inputs, output_hidden_states=True, output_attentions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a31f1c-3998-45a2-a593-f46e49627c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(aa.attentions), aa.attentions[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d91aa20-6c65-457a-972c-9223e5fef1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model.to(config.general.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae505b7-3df5-40b7-a563-0e5431c07055",
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = linear_model(encoded_inputs.to(config.general.device), output_hidden_states=True, output_attentions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03f1115-9d45-48e5-9a9e-710cbe8916a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(aa.attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49e1d9d-783f-4ccd-821c-e10c667932c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "aa.attentions[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499ad032-69da-43e3-b4d5-0e412cc43763",
   "metadata": {},
   "outputs": [],
   "source": [
    "aa.attentions[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd4e168-4025-421a-a2fa-6fd72924cfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "aa.attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884dee2c-8ac0-4a55-a49c-b63068bc8b02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
